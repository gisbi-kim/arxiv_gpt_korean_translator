URL: https://arxiv.org/abs/2409.03737
Title: Reprogrammable sequencing for physically intelligent under-actuated robots

Original Abstract:
Programming physical intelligence into mechanisms holds great promise for machines that can accomplish tasks such as navigation of unstructured environments while utilizing a minimal amount of computational resources and electronic components. In this study, we introduce a novel design approach for physically intelligent under-actuated mechanisms capable of autonomously adjusting their motion in response to environmental interactions. Specifically, multistability is harnessed to sequence the motion of different degrees of freedom in a programmed order. A key aspect of this approach is that these sequences can be passively reprogrammed through mechanical stimuli that arise from interactions with the environment. To showcase our approach, we construct a four degree of freedom robot capable of autonomously navigating mazes and moving away from obstacles. Remarkably, this robot operates without relying on traditional computational architectures and utilizes only a single linear actuator.

Translated Abstract:
물리적 지능을 메커니즘에 프로그래밍하는 것은 최소한의 계산 자원과 전자 부품을 사용하면서 비정형 환경을 탐색하는 기계에 대해 큰 가능성을 지니고 있어. 

이번 연구에서는 환경과의 상호작용에 따라 자율적으로 움직임을 조절할 수 있는 물리적으로 지능적인 저개입 메커니즘을 위한 새로운 설계 접근법을 소개해. 특히, 다중 안정성을 이용해 다양한 자유도의 움직임을 프로그램된 순서로 조정할 수 있어. 이 접근법의 중요한 점은 이러한 순서가 환경과의 상호작용에서 발생하는 기계적 자극을 통해 수동적으로 재프로그래밍될 수 있다는 거야.

우리의 접근법을 보여주기 위해, 우리는 미로를 자율적으로 탐색하고 장애물에서 벗어날 수 있는 4자유도 로봇을 만들었어. 놀랍게도, 이 로봇은 전통적인 계산 구조에 의존하지 않고 단 하나의 선형 액추에이터만으로 작동해.

================================================================================

URL: https://arxiv.org/abs/2409.03685
Title: View-Invariant Policy Learning via Zero-Shot Novel View Synthesis

Original Abstract:
Large-scale visuomotor policy learning is a promising approach toward developing generalizable manipulation systems. Yet, policies that can be deployed on diverse embodiments, environments, and observational modalities remain elusive. In this work, we investigate how knowledge from large-scale visual data of the world may be used to address one axis of variation for generalizable manipulation: observational viewpoint. Specifically, we study single-image novel view synthesis models, which learn 3D-aware scene-level priors by rendering images of the same scene from alternate camera viewpoints given a single input image. For practical application to diverse robotic data, these models must operate zero-shot, performing view synthesis on unseen tasks and environments. We empirically analyze view synthesis models within a simple data-augmentation scheme that we call View Synthesis Augmentation (VISTA) to understand their capabilities for learning viewpoint-invariant policies from single-viewpoint demonstration data. Upon evaluating the robustness of policies trained with our method to out-of-distribution camera viewpoints, we find that they outperform baselines in both simulated and real-world manipulation tasks. Videos and additional visualizations are available at this https URL.

Translated Abstract:
대규모 비주얼 모터 정책 학습은 일반화 가능한 조작 시스템을 개발하는 데 유망한 접근 방식이야. 하지만 다양한 형태, 환경, 그리고 관찰 모드에서 사용할 수 있는 정책은 아직 잘 찾아지지 않아. 

이번 연구에서는 세계의 대규모 비주얼 데이터를 활용해 일반화 가능한 조작의 한 축인 관찰 관점을 해결하는 방법을 조사했어. 특히, 우리는 단일 이미지로 새로운 시점을 합성하는 모델을 연구했는데, 이 모델은 하나의 입력 이미지로부터 다른 카메라 시점에서 같은 장면의 이미지를 렌더링하며 3D를 인식하는 장면 수준의 사전 지식을 학습해. 

다양한 로봇 데이터에 실제로 적용하려면, 이 모델들이 제로샷 방식으로 작동해야 해. 즉, 보지 못한 작업과 환경에서 뷰 합성을 수행할 수 있어야 해. 우리는 뷰 합성 증강(View Synthesis Augmentation, VISTA)이라는 간단한 데이터 증강 방식 내에서 뷰 합성 모델을 분석해서, 단일 시점 시연 데이터로부터 관점 불변 정책을 학습하는 능력을 이해했어. 

우리 방법으로 훈련된 정책을 외부 카메라 시점에 대해 평가해본 결과, 시뮬레이션과 실제 조작 작업 모두에서 기준선보다 더 나은 성능을 보였어. 비디오와 추가 시각화는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.03614
Title: 1 Modular Parallel Manipulator for Long-Term Soft Robotic Data Collection

Original Abstract:
Performing long-term experimentation or large-scale data collection for machine learning in the field of soft robotics is challenging, due to the hardware robustness and experimental flexibility required. In this work, we propose a modular parallel robotic manipulation platform suitable for such large-scale data collection and compatible with various soft-robotic fabrication methods. Considering the computational and theoretical difficulty of replicating the high-fidelity, faster-than-real-time simulations that enable large-scale data collection in rigid robotic systems, a robust soft-robotic hardware platform becomes a high priority development task for the field.
The platform's modules consist of a pair of off-the-shelf electrical motors which actuate a customizable finger consisting of a compliant parallel structure. The parallel mechanism of the finger can be as simple as a single 3D-printed urethane or molded silicone bulk structure, due to the motors being able to fully actuate a passive structure. This design flexibility allows experimentation with soft mechanism varied geometries, bulk properties and surface properties. Additionally, while the parallel mechanism does not require separate electronics or additional parts, these can be included, and it can be constructed using multi-functional soft materials to study compatible soft sensors and actuators in the learning process. In this work, we validate the platform's ability to be used for policy gradient reinforcement learning directly on hardware in a benchmark 2D manipulation task. We additionally demonstrate compatibility with multiple fingers and characterize the design constraints for compatible extensions.

Translated Abstract:
소프트 로봇 분야에서 기계 학습을 위한 장기 실험이나 대규모 데이터 수집은 하드웨어의 견고성과 실험의 유연성이 필요해서 어려워. 이 연구에서는 대규모 데이터 수집에 적합하고 다양한 소프트 로봇 제작 방법과 호환되는 모듈 형식의 병렬 로봇 조작 플랫폼을 제안해.

이 플랫폼의 모듈은 맞춤형 손가락을 움직이는 시중에 판매되는 전기 모터 두 개로 구성되어 있어. 이 손가락의 병렬 구조는 단일 3D 프린트된 우레탄이나 주형 실리콘 같은 간단한 형태도 가능해. 모터가 수동 구조를 완전히 작동시킬 수 있기 때문이지. 이런 디자인 유연성 덕분에 다양한 형상, 질량 특성, 표면 특성을 가진 부드러운 메커니즘을 실험할 수 있어.

게다가, 이 병렬 메커니즘은 별도의 전자 장치나 추가 부품이 필요하지 않지만, 원하는 경우 포함할 수 있어. 여러 기능을 가진 소프트 재료를 사용해서 학습 과정에서 호환 가능한 소프트 센서와 액추에이터를 연구할 수도 있어. 이 연구에서는 벤치마크 2D 조작 작업에서 하드웨어에서 직접 정책 그래디언트 강화 학습을 수행할 수 있는 플랫폼의 능력을 검증했어. 또한 여러 손가락과의 호환성을 보여주고 호환 가능한 확장에 대한 설계 제약 조건도 정리했어.

================================================================================

URL: https://arxiv.org/abs/2409.03556
Title: MaskVal: Simple but Effective Uncertainty Quantification for 6D Pose Estimation

Original Abstract:
For the use of 6D pose estimation in robotic applications, reliable poses are of utmost importance to ensure a safe, reliable and predictable operational performance. Despite these requirements, state-of-the-art 6D pose estimators often do not provide any uncertainty quantification for their pose estimates at all, or if they do, it has been shown that the uncertainty provided is only weakly correlated with the actual true error. To address this issue, we investigate a simple but effective uncertainty quantification, that we call MaskVal, which compares the pose estimates with their corresponding instance segmentations by rendering and does not require any modification of the pose estimator itself. Despite its simplicity, MaskVal significantly outperforms a state-of-the-art ensemble method on both a dataset and a robotic setup. We show that by using MaskVal, the performance of a state-of-the-art 6D pose estimator is significantly improved towards a safe and reliable operation. In addition, we propose a new and specific approach to compare and evaluate uncertainty quantification methods for 6D pose estimation in the context of robotic manipulation.

Translated Abstract:
로봇 응용 프로그램에서 6D 포즈 추정의 사용에 있어, 신뢰할 수 있는 포즈는 안전하고 신뢰성 있으며 예측 가능한 작동 성능을 보장하기 위해 정말 중요해. 하지만 최신 6D 포즈 추정기들은 포즈 추정에 대한 불확실성 수치를 아예 제공하지 않거나, 제공하더라도 그 불확실성이 실제 오류와는 약한 상관관계만 있는 경우가 많아. 

이 문제를 해결하기 위해, 우리는 MaskVal이라는 간단하지만 효과적인 불확실성 수치화 방법을 조사했어. 이 방법은 포즈 추정을 시각적으로 렌더링하여 해당 인스턴스 세그멘테이션과 비교하는 방식으로, 포즈 추정기를 수정할 필요가 없어. 간단함에도 불구하고, MaskVal은 최신 앙상블 방법보다 데이터셋과 로봇 설정 모두에서 훨씬 뛰어난 성능을 보여줬어. 

MaskVal을 사용하면 최신 6D 포즈 추정기의 성능이 안전하고 신뢰할 수 있는 작동을 향해 크게 개선되는 걸 보여줬어. 또한, 우리는 로봇 조작 context에서 6D 포즈 추정을 위한 불확실성 수치화 방법을 비교하고 평가하는 새로운 구체적인 접근 방식을 제안했어.

================================================================================

URL: https://arxiv.org/abs/2409.03535
Title: Interactive Surgical Liver Phantom for Cholecystectomy Training

Original Abstract:
Training and prototype development in robot-assisted surgery requires appropriate and safe environments for the execution of surgical procedures. Current dry lab laparoscopy phantoms often lack the ability to mimic complex, interactive surgical tasks. This work presents an interactive surgical phantom for the cholecystectomy. The phantom enables the removal of the gallbladder during cholecystectomy by allowing manipulations and cutting interactions with the synthetic tissue. The force-displacement behavior of the gallbladder is modelled based on retraction demonstrations. The force model is compared to the force model of ex-vivo porcine gallbladders and evaluated on its ability to estimate retraction forces.

Translated Abstract:
로봇 보조 수술 훈련과 프로토타입 개발을 위해서는 수술을 안전하게 수행할 수 있는 환경이 필요해. 현재 사용되는 드라이랩 복강경 팬텀은 복잡하고 상호작용이 필요한 수술 작업을 잘 흉내 내지 못해. 

이 연구에서는 담낭 절제술을 위한 인터랙티브 수술 팬텀을 소개해. 이 팬텀은 합성 조직과의 조작 및 절단 상호작용을 통해 담낭을 제거할 수 있게 해줘. 담낭의 힘-변위 행동은 철수 시연을 바탕으로 모델링됐어. 그리고 이 힘 모델은 생체 외의 돼지 담낭 힘 모델과 비교되면서 철수 힘을 추정하는 능력을 평가받았어.

================================================================================

URL: https://arxiv.org/abs/2409.03457
Title: FLAF: Focal Line and Feature-constrained Active View Planning for Visual Teach and Repeat

Original Abstract:
This paper presents FLAF, a focal line and feature-constrained active view planning method for tracking failure avoidance in feature-based visual navigation of mobile robots. Our FLAF-based visual navigation is built upon a feature-based visual teach and repeat (VT\&R) framework, which supports many robotic applications by teaching a robot to navigate on various paths that cover a significant portion of daily autonomous navigation requirements. However, tracking failure in feature-based visual simultaneous localization and mapping (VSLAM) caused by textureless regions in human-made environments is still limiting VT\&R to be adopted in the real world. To address this problem, the proposed view planner is integrated into a feature-based visual SLAM system to build up an active VT\&R system that avoids tracking failure. In our system, a pan-tilt unit (PTU)-based active camera is mounted on the mobile robot. Using FLAF, the active camera-based VSLAM operates during the teaching phase to construct a complete path map and in the repeat phase to maintain stable localization. FLAF orients the robot toward more map points to avoid mapping failures during path learning and toward more feature-identifiable map points beneficial for localization while following the learned trajectory. Experiments in real scenarios demonstrate that FLAF outperforms the methods that do not consider feature-identifiability, and our active VT\&R system performs well in complex environments by effectively dealing with low-texture regions.

Translated Abstract:
이 논문은 FLAF라는 방법을 소개해. 이건 모바일 로봇의 특징 기반 시각 내비게이션에서 추적 실패를 피하기 위한 초점 선과 특징 제약을 이용한 능동적 시점 계획 방법이야. FLAF 기반 시각 내비게이션은 특징 기반 시각 가르치기 및 반복(VT&R) 프레임워크를 바탕으로 만들어져. 이 프레임워크는 로봇이 다양한 경로를 따라 이동하도록 가르쳐주면서, 일상적인 자율 내비게이션에 필요한 많은 응용 프로그램을 지원해.

하지만 인간이 만든 환경에서 텍스처가 없는 지역 때문에 특징 기반 시각 동시 위치 추정 및 지도 작성(VSLAM)에서 추적 실패가 발생하는 문제가 있어. 이 때문에 VT&R이 실제 세계에서 널리 사용되기 어려워. 이 문제를 해결하기 위해, 제안된 시점 계획기가 특징 기반 시각 SLAM 시스템에 통합돼서, 추적 실패를 피하는 능동적인 VT&R 시스템을 만들어냈어.

우리 시스템에서는 팬-틸트 장치(PTU) 기반의 능동 카메라가 모바일 로봇에 장착돼 있어. FLAF를 사용하면, 능동 카메라 기반 VSLAM이 가르치는 단계에서 완전한 경로 지도를 만들고, 반복 단계에서는 안정적인 위치 추정을 유지하도록 작동해. FLAF는 로봇이 경로 학습 중에 매핑 실패를 피하기 위해 더 많은 지도 점을 향하게 하고, 학습한 경로를 따라가면서 위치 추정에 유리한 더 많은 특징을 인식할 수 있는 지도 점을 향하게 해.

실제 상황에서의 실험 결과, FLAF가 특징 인식 가능성을 고려하지 않는 방법들보다 성능이 뛰어난 걸 보여줬어. 우리 능동 VT&R 시스템은 복잡한 환경에서도 저텍스처 지역을 잘 처리하면서 잘 작동해.

================================================================================

URL: https://arxiv.org/abs/2409.03445
Title: Neural HD Map Generation from Multiple Vectorized Tiles Locally Produced by Autonomous Vehicles

Original Abstract:
High-definition (HD) map is a fundamental component of autonomous driving systems, as it can provide precise environmental information about driving scenes. Recent work on vectorized map generation could produce merely 65% local map elements around the ego-vehicle at runtime by one tour with onboard sensors, leaving a puzzle of how to construct a global HD map projected in the world coordinate system under high-quality standards. To address the issue, we present GNMap as an end-to-end generative neural network to automatically construct HD maps with multiple vectorized tiles which are locally produced by autonomous vehicles through several tours. It leverages a multi-layer and attention-based autoencoder as the shared network, of which parameters are learned from two different tasks (i.e., pretraining and finetuning, respectively) to ensure both the completeness of generated maps and the correctness of element categories. Abundant qualitative evaluations are conducted on a real-world dataset and experimental results show that GNMap can surpass the SOTA method by more than 5% F1 score, reaching the level of industrial usage with a small amount of manual modification. We have already deployed it at Navinfo Co., Ltd., serving as an indispensable software to automatically build HD maps for autonomous driving systems.

Translated Abstract:
고해상도(HD) 맵은 자율주행 시스템의 기본 요소야. 이 맵은 주행 장면에 대한 정확한 환경 정보를 제공할 수 있어. 최근 벡터화된 맵 생성에 대한 연구에서는 자율주행 차량이 onboard 센서를 이용해 한 번의 투어로 에고 차량 주변의 지역 맵 요소를 겨우 65%만 생성할 수 있었어. 그래서 고품질 기준에 맞춰 세계 좌표계에 투영된 글로벌 HD 맵을 어떻게 만들지에 대한 문제가 남아있어.

이 문제를 해결하기 위해 우리는 GNMap이라는 엔드 투 엔드 생성 신경망을 제안해. 이 네트워크는 여러 투어를 통해 자율주행 차량이 지역적으로 생성한 벡터화된 타일들을 자동으로 HD 맵으로 구성해줘. GNMap은 다층 및 주의 기반 오토인코더를 공유 네트워크로 활용하고, 두 가지 다른 작업(즉, 사전 훈련과 미세 조정)에서 파라미터를 학습해. 이렇게 하면 생성된 맵의 완전성과 요소 카테고리의 정확성을 동시에 보장할 수 있어.

우리는 실제 데이터셋에 대해 풍부한 정성적 평가를 진행했어. 실험 결과 GNMap이 SOTA 방법보다 5% 이상 F1 점수를 초과할 수 있고, 소량의 수동 수정만으로도 산업 수준의 사용이 가능하다는 걸 보여줬어. 우리는 이미 Navinfo Co., Ltd.에서 이를 배포했으며, 자율주행 시스템을 위한 HD 맵을 자동으로 구축하는 필수 소프트웨어로 사용되고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.03439
Title: KiloBot: A Programming Language for Deploying Perception-Guided Industrial Manipulators at Scale

Original Abstract:
We would like industrial robots to handle unstructured environments with cameras and perception pipelines. In contrast to traditional industrial robots that replay offline-crafted trajectories, online behavior planning is required for these perception-guided industrial applications. Aside from perception and planning algorithms, deploying perception-guided manipulators also requires substantial effort in integration. One approach is writing scripts in a traditional language (such as Python) to construct the planning problem and perform integration with other algorithmic modules & external devices. While scripting in Python is feasible for a handful of robots and applications, deploying perception-guided manipulation at scale (e.g., more than 10000 robot workstations in over 2000 customer sites) becomes intractable. To resolve this challenge, we propose a Domain-Specific Language (DSL) for perception-guided manipulation applications. To scale up the deployment,our DSL provides: 1) an easily accessible interface to construct & solve a sub-class of Task and Motion Planning (TAMP) problems that are important in practical applications; and 2) a mechanism to implement flexible control flow to perform integration and address customized requirements of distinct industrial application. Combined with an intuitive graphical programming frontend, our DSL is mainly used by machine operators without coding experience in traditional programming languages. Within hours of training, operators are capable of orchestrating interesting sophisticated manipulation behaviors with our DSL. Extensive practical deployments demonstrate the efficacy of our method.

Translated Abstract:
우리는 산업 로봇이 카메라와 인식 파이프라인을 이용해 비정형 환경을 처리할 수 있기를 원해. 기존의 산업 로봇은 미리 만들어진 경로를 따라 움직이지만, 인식 기반의 산업 응용 프로그램에는 온라인 행동 계획이 필요해.

인식과 계획 알고리즘 외에도 인식 기반 조작기를 배치하는 데는 상당한 통합 작업이 필요해. 한 가지 방법은 전통적인 언어(예: Python)로 스크립트를 작성해 계획 문제를 구성하고 다른 알고리즘 모듈 및 외부 장치와 통합하는 거야. Python으로 스크립트를 작성하는 건 몇 개의 로봇과 응용 프로그램에는 가능하지만, 10,000개 이상의 로봇 작업 공간을 2,000개 이상의 고객 사이트에 배치하는 건 쉽지 않아.

이 문제를 해결하기 위해 우리는 인식 기반 조작 응용 프로그램을 위한 도메인 특화 언어(DSL)를 제안해. 배치를 더 쉽게 하도록 우리의 DSL은 1) 실용적인 응용 프로그램에서 중요한 하위 클래스의 작업 및 동작 계획(TAMP) 문제를 구성하고 해결할 수 있는 간편한 인터페이스를 제공하고, 2) 통합을 수행하고 각 산업 응용 프로그램의 맞춤 요구 사항을 해결할 수 있는 유연한 제어 흐름을 구현하는 메커니즘을 제공해.

직관적인 그래픽 프로그래밍 프론트엔드와 결합된 우리의 DSL은 전통적인 프로그래밍 언어에 대한 코딩 경험이 없는 기계 운영자들이 주로 사용해. 몇 시간의 교육만으로도 운영자들은 우리의 DSL을 통해 흥미로운 복잡한 조작 행동을 조정할 수 있게 돼. 광범위한 실용적 배치 결과는 우리의 방법의 효과를 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.03429
Title: Reinforcement Learning Approach to Optimizing Profilometric Sensor Trajectories for Surface Inspection

Original Abstract:
High-precision surface defect detection in manufacturing is essential for ensuring quality control. Laser triangulation profilometric sensors are key to this process, providing detailed and accurate surface measurements over a line. To achieve a complete and precise surface scan, accurate relative motion between the sensor and the workpiece is required. It is crucial to control the sensor pose to maintain optimal distance and relative orientation to the surface. It is also important to ensure uniform profile distribution throughout the scanning process. This paper presents a novel Reinforcement Learning (RL) based approach to optimize robot inspection trajectories for profilometric sensors. Building upon the Boustrophedon scanning method, our technique dynamically adjusts the sensor position and tilt to maintain optimal orientation and distance from the surface, while also ensuring a consistent profile distance for uniform and high-quality scanning. Utilizing a simulated environment based on the CAD model of the part, we replicate real-world scanning conditions, including sensor noise and surface irregularities. This simulation-based approach enables offline trajectory planning based on CAD models. Key contributions include the modeling of the state space, action space, and reward function, specifically designed for inspection applications using profilometric sensors. We use Proximal Policy Optimization (PPO) algorithm to efficiently train the RL agent, demonstrating its capability to optimize inspection trajectories with profilometric sensors. To validate our approach, we conducted several experiments where a model trained on a specific training piece was tested on various parts in simulation. Also, we conducted a real-world experiment by executing the optimized trajectory, generated offline from a CAD model, to inspect a part using a UR3e robotic arm model.

Translated Abstract:
제조업에서 고정밀 표면 결함 검출은 품질 관리를 위해 정말 중요해. 레이저 삼각측량 프로파일 센서는 이 과정에서 핵심적인 역할을 하며, 선을 따라서 상세하고 정확한 표면 측정을 제공해. 전체 표면 스캔을 완벽하고 정확하게 수행하려면 센서와 작업물 간의 상대적인 움직임이 정확해야 해. 센서의 위치를 잘 조절해서 표면과 최적의 거리와 방향을 유지하는 게 중요해. 스캔하는 동안 균일한 프로파일 분포를 보장하는 것도 중요하지.

이 논문에서는 프로파일 센서를 위한 로봇 검사 경로를 최적화하기 위해 새로운 강화 학습(RL) 기반 접근법을 제안해. Boustrophedon 스캔 방법을 바탕으로, 우리 기술은 센서의 위치와 기울기를 동적으로 조정해서 표면과의 최적 방향과 거리를 유지하게 해. 그리고 균일하고 고품질 스캔을 위해 일관된 프로파일 거리를 보장해. CAD 모델을 기반으로 한 시뮬레이션 환경을 활용해서, 센서 잡음과 표면 불규칙성을 포함한 실제 스캔 조건을 재현해. 이 시뮬레이션 기반 접근법 덕분에 CAD 모델을 바탕으로 오프라인 경로 계획이 가능해.

주요 기여는 프로파일 센서를 사용한 검사 애플리케이션을 위해 특별히 설계된 상태 공간, 행동 공간, 보상 함수 모델링이야. 우리는 Proximal Policy Optimization(PPO) 알고리즘을 사용해서 RL 에이전트를 효율적으로 훈련시키고, 프로파일 센서를 이용한 검사 경로를 최적화할 수 있다는 걸 보여줬어. 우리의 접근법을 검증하기 위해, 특정 훈련 조각에서 훈련된 모델을 다양한 부품에 테스트하는 여러 실험을 진행했어. 또한 CAD 모델에서 오프라인으로 생성된 최적화된 경로를 사용해 UR3e 로봇 팔 모델로 부품을 검사하는 실제 실험도 했어.

================================================================================

URL: https://arxiv.org/abs/2409.03421
Title: F3T: A soft tactile unit with 3D force and temperature mathematical decoupling ability for robots

Original Abstract:
The human skin exhibits remarkable capability to perceive contact forces and environmental temperatures, providing intricate information essential for nuanced manipulation. Despite recent advancements in soft tactile sensors, a significant challenge remains in accurately decoupling signals - specifically, separating force from directional orientation and temperature - resulting in fail to meet the advanced application requirements of robots. This research proposes a multi-layered soft sensor unit (F3T) designed to achieve isolated measurements and mathematical decoupling of normal pressure, omnidirectional tangential forces, and temperature. We developed a circular coaxial magnetic film featuring a floating-mountain multi-layer capacitor, facilitating the physical decoupling of normal and tangential forces in all directions. Additionally, we incorporated an ion gel-based temperature sensing film atop the tactile sensor. This sensor is resilient to external pressure and deformation, enabling it to measure temperature and, crucially, eliminate capacitor errors induced by environmental temperature changes. This innovative design allows for the decoupled measurement of multiple signals, paving the way for advancements in higher-level robot motion control, autonomous decision-making, and task planning.

Translated Abstract:
인간의 피부는 접촉 힘과 환경 온도를 감지하는 놀라운 능력을 가지고 있어, 섬세한 조작에 필요한 복잡한 정보를 제공합니다. 최근 부드러운 촉각 센서 기술이 발전했지만, 신호를 정확하게 분리하는 데 여전히 큰 문제가 남아 있습니다. 특히 힘을 방향성과 온도에서 분리하는 것이 어려워서 로봇의 고급 응용 요구를 충족시키지 못하고 있습니다.

이 연구에서는 정상 압력, 전 방향의 접촉 힘, 그리고 온도의 분리 측정을 달성하기 위해 다층 소프트 센서 유닛(F3T)을 제안합니다. 우리는 원형 동축 자성 필름을 개발했는데, 이는 떠 있는 산 모양의 다층 커패시터를 특징으로 하여 모든 방향에서 정상 및 접촉 힘을 물리적으로 분리할 수 있도록 합니다. 게다가, 촉각 센서 위에 이온 젤 기반의 온도 감지 필름을 추가했습니다.

이 센서는 외부 압력과 변형에 강해 온도를 측정할 수 있으며, 환경 온도 변화로 인한 커패시터 오류를 없애는 데 중요합니다. 이 혁신적인 설계 덕분에 여러 신호를 분리 측정할 수 있게 되어, 고급 로봇 동작 제어, 자율적인 의사결정, 그리고 작업 계획의 발전에 기여할 수 있는 길이 열립니다.

================================================================================

URL: https://arxiv.org/abs/2409.03403
Title: RoVi-Aug: Robot and Viewpoint Augmentation for Cross-Embodiment Robot Learning

Original Abstract:
Scaling up robot learning requires large and diverse datasets, and how to efficiently reuse collected data and transfer policies to new embodiments remains an open question. Emerging research such as the Open-X Embodiment (OXE) project has shown promise in leveraging skills by combining datasets including different robots. However, imbalances in the distribution of robot types and camera angles in many datasets make policies prone to overfit. To mitigate this issue, we propose RoVi-Aug, which leverages state-of-the-art image-to-image generative models to augment robot data by synthesizing demonstrations with different robots and camera views. Through extensive physical experiments, we show that, by training on robot- and viewpoint-augmented data, RoVi-Aug can zero-shot deploy on an unseen robot with significantly different camera angles. Compared to test-time adaptation algorithms such as Mirage, RoVi-Aug requires no extra processing at test time, does not assume known camera angles, and allows policy fine-tuning. Moreover, by co-training on both the original and augmented robot datasets, RoVi-Aug can learn multi-robot and multi-task policies, enabling more efficient transfer between robots and skills and improving success rates by up to 30%.

Translated Abstract:
로봇 학습을 확대하려면 크고 다양한 데이터셋이 필요해. 수집한 데이터를 효율적으로 재사용하고 새로운 로봇에 정책을 적용하는 방법은 아직 해결되지 않은 문제야. 최근에 나온 연구인 Open-X Embodiment (OXE) 프로젝트는 다른 로봇을 포함한 데이터셋을 결합해 기술을 활용하는 데 가능성을 보여줬어. 하지만 많은 데이터셋에서 로봇 유형과 카메라 각도의 불균형 때문에 정책이 과적합되는 문제가 있어.

이 문제를 해결하기 위해 우리는 RoVi-Aug를 제안해. RoVi-Aug는 최신 이미지-투-이미지 생성 모델을 활용해 다양한 로봇과 카메라 뷰로 시연을 합성해서 로봇 데이터를 증강해. 여러 가지 물리적 실험을 통해, 로봇과 뷰포인트가 증강된 데이터로 학습하면 RoVi-Aug가 전혀 보지 못한 로봇에서 카메라 각도가 크게 다르더라도 제로샷 배포가 가능하다는 걸 보여줬어.

Mirage 같은 테스트 시간 적응 알고리즘과 비교했을 때, RoVi-Aug는 테스트 시간에 추가 처리가 필요 없고, 알려진 카메라 각도를 가정하지 않으며, 정책 세부 조정이 가능해. 게다가 원래의 로봇 데이터셋과 증강된 데이터셋을 함께 학습함으로써 RoVi-Aug는 다중 로봇과 다중 작업 정책을 배우게 되고, 로봇과 기술 간의 효율적인 전이를 가능하게 해. 성공률도 최대 30%까지 향상시킬 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.03369
Title: Fast Payload Calibration for Sensorless Contact Estimation Using Model Pre-training

Original Abstract:
Force and torque sensing is crucial in robotic manipulation across both collaborative and industrial settings. Traditional methods for dynamics identification enable the detection and control of external forces and torques without the need for costly sensors. However, these approaches show limitations in scenarios where robot dynamics, particularly the end-effector payload, are subject to changes. Moreover, existing calibration techniques face trade-offs between efficiency and accuracy due to concerns over joint space coverage. In this paper, we introduce a calibration scheme that leverages pre-trained Neural Network models to learn calibrated dynamics across a wide range of joint space in advance. This offline learning strategy significantly reduces the need for online data collection, whether for selection of the optimal model or identification of payload features, necessitating merely a 4-second trajectory for online calibration. This method is particularly effective in tasks that require frequent dynamics recalibration for precise contact estimation. We further demonstrate the efficacy of this approach through applications in sensorless joint and task compliance, accounting for payload variability.

Translated Abstract:
힘과 토크 센싱은 로봇 조작에서 협력적이거나 산업적인 환경 모두에 매우 중요해. 전통적인 동역학 식별 방법은 비싼 센서를 사용하지 않고도 외부 힘과 토크를 감지하고 제어할 수 있게 해주지만, 로봇 다이나믹스, 특히 끝단 효과기의 페이로드가 변할 때는 한계가 있어. 게다가 기존의 보정 기술은 관절 공간 커버리지 문제 때문에 효율성과 정확성 사이에서 타협을 해야 해.

이 논문에서는 사전 훈련된 신경망 모델을 활용해 다양한 관절 공간에서 보정된 다이나믹스를 미리 학습하는 보정 방안을 소개해. 이 오프라인 학습 전략은 온라인 데이터 수집의 필요성을 크게 줄여줘. 최적의 모델 선택이나 페이로드 특성 식별을 위해서는 단 4초의 경로만 필요해. 이 방법은 정확한 접촉 추정을 위해 자주 동역학을 재보정해야 하는 작업에서 특히 효과적이야. 우리는 또한 이 접근 방식의 효율성을 센서 없는 관절과 작업 적합성에서 보여줬어. 페이로드 변동성을 고려하면서 말이야.

================================================================================

URL: https://arxiv.org/abs/2409.03332
Title: Masked Sensory-Temporal Attention for Sensor Generalization in Quadruped Locomotion

Original Abstract:
With the rising focus on quadrupeds, a generalized policy capable of handling different robot models and sensory inputs will be highly beneficial. Although several methods have been proposed to address different morphologies, it remains a challenge for learning-based policies to manage various combinations of proprioceptive information. This paper presents Masked Sensory-Temporal Attention (MSTA), a novel transformer-based model with masking for quadruped locomotion. It employs direct sensor-level attention to enhance sensory-temporal understanding and handle different combinations of sensor data, serving as a foundation for incorporating unseen information. This model can effectively understand its states even with a large portion of missing information, and is flexible enough to be deployed on a physical system despite the long input sequence.

Translated Abstract:
사족 동물에 대한 관심이 높아짐에 따라, 다양한 로봇 모델과 센서 입력을 처리할 수 있는 일반화된 정책이 매우 유용할 거야. 여러 형태에 맞춘 방법들이 제안되었지만, 학습 기반 정책이 다양한 종류의 체감 정보를 관리하는 건 여전히 어려운 문제야.

이 논문에서는 사족 로봇의 이동을 위한 새로운 모델인 'Masked Sensory-Temporal Attention (MSTA)'를 소개해. 이 모델은 마스킹을 사용한 트랜스포머 기반 모델이야. 직접 센서 수준의 주의를 활용해서 감각-시간 이해를 높이고, 다양한 센서 데이터 조합을 처리할 수 있어. 이는 보지 못한 정보를 포함하는 데 기초가 되지.

이 모델은 많은 정보가 빠져 있어도 자신의 상태를 효과적으로 이해할 수 있고, 긴 입력 시퀀스에도 불구하고 실제 시스템에 배치할 수 있을 만큼 유연해.

================================================================================

URL: https://arxiv.org/abs/2409.03299
Title: Bringing the RT-1-X Foundation Model to a SCARA robot

Original Abstract:
Traditional robotic systems require specific training data for each task, environment, and robot form. While recent advancements in machine learning have enabled models to generalize across new tasks and environments, the challenge of adapting these models to entirely new settings remains largely unexplored. This study addresses this by investigating the generalization capabilities of the RT-1-X robotic foundation model to a type of robot unseen during its training: a SCARA robot from UMI-RTX.
Initial experiments reveal that RT-1-X does not generalize zero-shot to the unseen type of robot. However, fine-tuning of the RT-1-X model by demonstration allows the robot to learn a pickup task which was part of the foundation model (but learned for another type of robot). When the robot is presented with an object that is included in the foundation model but not in the fine-tuning dataset, it demonstrates that only the skill, but not the object-specific knowledge, has been transferred.

Translated Abstract:
전통적인 로봇 시스템은 각각의 작업, 환경, 로봇 형태에 대해 특정한 훈련 데이터가 필요해. 최근 머신러닝의 발전 덕분에 모델들이 새로운 작업과 환경에 일반화할 수 있게 되었지만, 완전히 새로운 환경에 이 모델들을 적응시키는 건 아직 많이 연구되지 않았어. 

이 연구는 RT-1-X 로봇 기초 모델의 일반화 능력을 조사하면서, 훈련 중에 보지 못한 SCARA 로봇을 대상으로 하고 있어. 처음 실험해본 결과, RT-1-X는 보지 못한 로봇에 대해서는 제로샷으로 일반화되지 않는다는 걸 알게 되었어. 하지만, 데모를 통해 RT-1-X 모델을 미세 조정하면 로봇이 기본 모델에서 배운 픽업 작업을 수행할 수 있게 돼. 

로봇이 기본 모델에 포함됐지만 미세 조정 데이터셋에는 없는 물체를 보여주면, 로봇은 기술만 전이된다는 걸 보여줘. 즉, 물체에 대한 특정 지식은 전이되지 않았다는 거야.

================================================================================

URL: https://arxiv.org/abs/2409.03230
Title: Improving agent performance in fluid environments by perceptual pretraining

Original Abstract:
In this paper, we construct a pretraining framework for fluid environment perception, which includes an information compression model and the corresponding pretraining method. We test this framework in a two-cylinder problem through numerical simulation. The results show that after unsupervised pretraining with this framework, the intelligent agent can acquire key features of surrounding fluid environment, thereby adapting more quickly and effectively to subsequent multi-scenario tasks. In our research, these tasks include perceiving the position of the upstream obstacle and actively avoiding shedding vortices in the flow field to achieve drag reduction. Better performance of the pretrained agent is discussed in the sensitivity analysis.

Translated Abstract:
이 논문에서는 유체 환경 인식을 위한 사전 훈련 프레임워크를 만들었어. 이 프레임워크는 정보 압축 모델과 해당하는 사전 훈련 방법을 포함하고 있어. 우리는 이 프레임워크를 두 개의 실린더 문제에 대해 수치 시뮬레이션을 통해 테스트했어.

결과를 보니까, 이 프레임워크로 비지도 사전 훈련을 한 후, 지능형 에이전트가 주변 유체 환경의 핵심 특징을 잘 파악할 수 있게 되었어. 그래서 이후의 다양한 시나리오 작업에 더 빠르고 효과적으로 적응할 수 있었지. 우리 연구에서 다룬 작업들은 상류 장애물의 위치를 인식하고, 흐름 필드에서 shedding vortex를 피해서 항력 감소를 이루는 거야.

사전 훈련된 에이전트의 성능 향상에 대해서는 민감도 분석을 통해 논의했어.

================================================================================

URL: https://arxiv.org/abs/2409.03193
Title: Upper-Limb Rehabilitation with a Dual-Mode Individualized Exoskeleton Robot: A Generative-Model-Based Solution

Original Abstract:
Several upper-limb exoskeleton robots have been developed for stroke rehabilitation, but their rather low level of individualized assistance typically limits their effectiveness and practicability. Individualized assistance involves an upper-limb exoskeleton robot continuously assessing feedback from a stroke patient and then meticulously adjusting interaction forces to suit specific conditions and online changes. This paper describes the development of a new upper-limb exoskeleton robot with a novel online generative capability that allows it to provide individualized assistance to support the rehabilitation training of stroke patients. Specifically, the upper-limb exoskeleton robot exploits generative models to customize the fine and fit trajectory for the patient, as medical conditions, responses, and comfort feedback during training generally differ between patients. This generative capability is integrated into the two working modes of the upper-limb exoskeleton robot: an active mirroring mode for patients who retain motor abilities on one side of the body and a passive following mode for patients who lack motor ability on both sides of the body. The performance of the upper-limb exoskeleton robot was illustrated in experiments involving healthy subjects and stroke patients.

Translated Abstract:
여러 가지 상지 외골격 로봇이 뇌졸중 재활을 위해 개발되었지만, 개인 맞춤형 지원이 부족해서 효과와 실용성이 제한되는 경우가 많아. 개인 맞춤형 지원은 상지 외골격 로봇이 뇌졸중 환자의 피드백을 계속해서 평가하고, 특정 조건과 변화에 맞게 상호작용 힘을 조정하는 거야.

이 논문에서는 뇌졸중 환자의 재활 훈련을 지원하기 위해 개인 맞춤형 지원을 제공할 수 있는 새로운 상지 외골격 로봇의 개발에 대해 설명해. 특히 이 로봇은 생성 모델을 활용해서 환자에게 맞는 경로를 조정하는데, 각 환자의 의료 상태, 반응, 훈련 중 편안함의 피드백은 다르기 때문에 필요한 과정이야.

이 생성 기능은 상지 외골격 로봇의 두 가지 작동 모드에 통합되어 있어. 하나는 한쪽 몸에 운동 능력이 있는 환자를 위한 능동 미러링 모드고, 다른 하나는 양쪽 몸에 운동 능력이 없는 환자를 위한 수동 추종 모드야. 이 로봇의 성능은 건강한 피실험자와 뇌졸중 환자를 대상으로 한 실험을 통해 보여졌어.

================================================================================

URL: https://arxiv.org/abs/2409.03170
Title: Solving Stochastic Orienteering Problems with Chance Constraints Using Monte Carlo Tree Search

Original Abstract:
We present a new Monte Carlo Tree Search (MCTS) algorithm to solve the stochastic orienteering problem with chance constraints, i.e., a version of the problem where travel costs are random, and one is assigned a bound on the tolerable probability of exceeding the budget. The algorithm we present is online and anytime, i.e., it alternates planning and execution, and the quality of the solution it produces increases as the allowed computational time increases. Differently from most former MCTS algorithms, for each action available in a state the algorithm maintains estimates of both its value and the probability that its execution will eventually result in a violation of the chance constraint. Then, at action selection time, our proposed solution prunes away trajectories that are estimated to violate the failure probability. Extensive simulation results show that this approach can quickly produce high-quality solutions and is competitive with the optimal but time-consuming solution.

Translated Abstract:
우리는 확률적 오리엔티어링 문제를 해결하기 위한 새로운 몬테 카를로 트리 탐색(MCTS) 알고리즘을 소개해. 이 문제는 여행 비용이 랜덤하고 예산을 초과할 확률에 대한 제한이 있는 문제야.

우리가 제안하는 알고리즘은 온라인 방식이면서 언제든지 실행할 수 있어. 즉, 계획과 실행을 번갈아 하면서, 주어진 계산 시간을 늘리면 해결책의 품질도 증가해. 기존의 대부분 MCTS 알고리즘과 다른 점은, 각 상태에서 가능한 행동에 대해 그 가치와 해당 행동이 실패 확률에 위반될 가능성을 모두 추정한다는 거야.

행동을 선택할 때, 우리는 실패 확률을 위반할 것으로 추정되는 경로를 잘라내. 많은 시뮬레이션 결과에 따르면, 이 접근 방식은 빠르게 고품질의 해결책을 생성할 수 있고, 최적이긴 하지만 시간이 많이 걸리는 해결책과 경쟁할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.03166
Title: Continual Skill and Task Learning via Dialogue

Original Abstract:
Continual and interactive robot learning is a challenging problem as the robot is present with human users who expect the robot to learn novel skills to solve novel tasks perpetually with sample efficiency. In this work we present a framework for robots to query and learn visuo-motor robot skills and task relevant information via natural language dialog interactions with human users. Previous approaches either focus on improving the performance of instruction following agents, or passively learn novel skills or concepts. Instead, we used dialog combined with a language-skill grounding embedding to query or confirm skills and/or tasks requested by a user. To achieve this goal, we developed and integrated three different components for our agent. Firstly, we propose a novel visual-motor control policy ACT with Low Rank Adaptation (ACT-LoRA), which enables the existing SoTA ACT model to perform few-shot continual learning. Secondly, we develop an alignment model that projects demonstrations across skill embodiments into a shared embedding allowing us to know when to ask questions and/or demonstrations from users. Finally, we integrated an existing LLM to interact with a human user to perform grounded interactive continual skill learning to solve a task. Our ACT-LoRA model learns novel fine-tuned skills with a 100% accuracy when trained with only five demonstrations for a novel skill while still maintaining a 74.75% accuracy on pre-trained skills in the RLBench dataset where other models fall significantly short. We also performed a human-subjects study with 8 subjects to demonstrate the continual learning capabilities of our combined framework. We achieve a success rate of 75% in the task of sandwich making with the real robot learning from participant data demonstrating that robots can learn novel skills or task knowledge from dialogue with non-expert users using our approach.

Translated Abstract:
로봇의 지속적이고 상호작용적인 학습은 도전적인 문제야. 로봇이 인간 사용자와 함께 있어서, 사용자들은 로봇이 새로운 기술을 배우고 새로운 작업을 계속해서 해결하기를 기대해. 이 연구에서는 로봇이 자연어 대화를 통해 비주얼-모터 기술과 작업 관련 정보를 질문하고 학습할 수 있는 프레임워크를 제안해.

이전 연구들은 주로 지시를 따르는 에이전트의 성능을 향상시키거나, 수동적으로 새로운 기술이나 개념을 배우는 데 집중했어. 대신, 우리는 대화를 사용하고 언어-기술 기반 임베딩을 결합해서 사용자가 요청한 기술이나 작업을 질문하거나 확인할 수 있도록 했어. 

이를 위해 우리는 에이전트를 위한 세 가지 다른 구성 요소를 개발하고 통합했어. 첫째, 우리는 기존의 SoTA ACT 모델이 몇 번의 샷으로 지속적인 학습을 할 수 있게 해주는 새로운 비주얼-모터 제어 정책인 ACT-LoRA를 제안해. 둘째, 우리는 기술의 구현을 공유 임베딩으로 투영하는 정렬 모델을 개발해서, 사용자에게 질문이나 시연을 요청할 시점을 알 수 있도록 했어. 마지막으로, 우리는 기존의 대형 언어 모델(LLM)을 통합해서 인간 사용자와 상호작용하며 grounded interactive continual skill learning을 수행해 작업을 해결할 수 있게 했어. 

우리의 ACT-LoRA 모델은 새로운 기술에 대해 다섯 번의 시연만으로 100% 정확도로 새로운 세밀한 기술을 배우고, RLBench 데이터셋에서 다른 모델들이 크게 부족한 상황에서도 기존 기술에 대해 74.75%의 정확도를 유지해. 우리는 8명의 참가자를 대상으로 한 인간 대상 연구를 통해 우리의 통합 프레임워크의 지속적 학습 능력을 보여주었어. 우리는 실제 로봇이 참가자 데이터를 학습해 샌드위치 만들기 작업에서 75%의 성공률을 달성했어. 이 결과는 로봇이 비전문가 사용자와의 대화를 통해 새로운 기술이나 작업 지식을 배울 수 있다는 것을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.03160
Title: Autonomous Drifting Based on Maximal Safety Probability Learning

Original Abstract:
This paper proposes a novel learning-based framework for autonomous driving based on the concept of maximal safety probability. Efficient learning requires rewards that are informative of desirable/undesirable states, but such rewards are challenging to design manually due to the difficulty of differentiating better states among many safe states. On the other hand, learning policies that maximize safety probability does not require laborious reward shaping but is numerically challenging because the algorithms must optimize policies based on binary rewards sparse in time. Here, we show that physics-informed reinforcement learning can efficiently learn this form of maximally safe policy. Unlike existing drift control methods, our approach does not require a specific reference trajectory or complex reward shaping, and can learn safe behaviors only from sparse binary rewards. This is enabled by the use of the physics loss that plays an analogous role to reward shaping. The effectiveness of the proposed approach is demonstrated through lane keeping in a normal cornering scenario and safe drifting in a high-speed racing scenario.

Translated Abstract:
이 논문은 최대 안전 확률 개념에 기반한 자율 주행을 위한 새로운 학습 기반 프레임워크를 제안해. 효율적인 학습을 위해서는 바람직한 상태와 바람직하지 않은 상태에 대한 정보를 제공하는 보상이 필요한데, 안전한 상태가 많아서 어떤 상태가 더 좋은지 구분하기가 어려워서 보상을 수동으로 설계하는 건 힘들어. 

반면에, 안전 확률을 최대화하는 정책을 배우는 건 힘든 보상 설계를 필요로 하지 않지만, 알고리즘이 시간적으로 드문 이진 보상을 기반으로 정책을 최적화해야 해서 수치적으로 도전적이야. 여기서 우리는 물리 기반 강화 학습이 이러한 형태의 최대 안전 정책을 효율적으로 배울 수 있음을 보여줘. 기존의 드리프트 제어 방법과는 달리, 우리의 접근법은 특정 기준 경로나 복잡한 보상 설계를 필요로 하지 않고, 드문 이진 보상만으로 안전한 행동을 배울 수 있어. 

이건 보상 설계와 비슷한 역할을 하는 물리 손실을 사용함으로써 가능해. 제안한 접근법의 효과는 일반적인 코너링 상황에서 차선을 유지하고, 고속 레이싱 상황에서 안전하게 드리프트하는 것을 통해 입증됐어.

================================================================================

URL: https://arxiv.org/abs/2409.03120
Title: Approximate Environment Decompositions for Robot Coverage Planning using Submodular Set Cover

Original Abstract:
In this paper, we investigate the problem of decomposing 2D environments for robot coverage planning. Coverage path planning (CPP) involves computing a cost-minimizing path for a robot equipped with a coverage or sensing tool so that the tool visits all points in the environment. CPP is an NP-Hard problem, so existing approaches simplify the problem by decomposing the environment into the minimum number of sectors. Sectors are sub-regions of the environment that can each be covered using a lawnmower path (i.e., along parallel straight-line paths) oriented at an angle. However, traditional methods either limit the coverage orientations to be axis-parallel (horizontal/vertical) or provide no guarantees on the number of sectors in the decomposition. We introduce an approach to decompose the environment into possibly overlapping rectangular sectors. We provide an approximation guarantee on the number of sectors computed using our approach for a given environment. We do this by leveraging the submodular property of the sector coverage function, which enables us to formulate the decomposition problem as a submodular set cover (SSC) problem with well-known approximation guarantees for the greedy algorithm. Our approach improves upon existing coverage planning methods, as demonstrated through an evaluation using maps of complex real-world environments.

Translated Abstract:
이 논문에서는 로봇의 커버리지 계획을 위해 2D 환경을 분해하는 문제를 다루고 있어. 커버리지 경로 계획(CPP)은 로봇이 모든 지점을 방문할 수 있도록 비용을 최소화하는 경로를 계산하는 거야. CPP는 NP-하드 문제라서, 기존 방법들은 환경을 최소한의 섹터로 나누어서 간단하게 해결하려고 해. 

섹터는 환경의 하위 영역으로, 각각은 잔디 깎는 기계처럼 평행한 직선 경로를 따라 커버할 수 있어. 하지만 전통적인 방법은 커버 방향을 축에 평행한(horizontal/vertical) 방향으로 제한하거나, 분해된 섹터의 수에 대한 보장이 없어. 

우리는 환경을 겹칠 수도 있는 직사각형 섹터로 분해하는 방법을 제안해. 이 방법을 사용하면 주어진 환경에 대해 계산된 섹터 수에 대한 근사 보장을 제공해. 이를 위해 섹터 커버리지 함수의 서브모듈러 성질을 활용했어. 덕분에 분해 문제를 서브모듈러 집합 커버(SSC) 문제로 정식화할 수 있고, 잘 알려진 탐욕 알고리즘의 근사 보장을 이용할 수 있어. 

우리의 접근 방식은 복잡한 실제 환경의 지도 평가를 통해 기존 커버리지 계획 방법보다 나은 성과를 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.03114
Title: Developing, Analyzing, and Evaluating Self-Drive Algorithms Using Drive-by-Wire Electric Vehicles

Original Abstract:
Reliable lane-following algorithms are essential for safe and effective autonomous driving. This project was primarily focused on developing and evaluating different lane-following programs to find the most reliable algorithm for a Vehicle to Everything (V2X) project. The algorithms were first tested on a simulator and then with real vehicles equipped with a drive-by-wire system using ROS (Robot Operating System). Their performance was assessed through reliability, comfort, speed, and adaptability metrics. The results show that the two most reliable approaches detect both lane lines and use unsupervised learning to separate them. These approaches proved to be robust in various driving scenarios, making them suitable candidates for integration into the V2X project.

Translated Abstract:
신뢰할 수 있는 차선 추적 알고리즘은 안전하고 효과적인 자율주행에 꼭 필요해. 이 프로젝트는 다양한 차선 추적 프로그램을 개발하고 평가해서 V2X 프로젝트에 가장 신뢰할 수 있는 알고리즘을 찾는 데 집중했어.

먼저 이 알고리즘들은 시뮬레이터에서 테스트 되었고, 그 다음에는 ROS(로봇 운영 체제)를 사용한 드라이브-바이-와이어 시스템을 갖춘 실제 차량에서도 테스트했어. 성능은 신뢰성, 편안함, 속도, 적응력 같은 기준으로 평가했어.

결과적으로, 가장 신뢰할 수 있는 두 가지 접근 방식은 차선과 차선의 경계를 감지하고 비지도 학습을 사용해서 이들을 분리했어. 이 방법들은 다양한 주행 시나리오에서도 강력한 성능을 보여서 V2X 프로젝트에 통합하기에 적합한 후보가 되었어.

================================================================================

URL: https://arxiv.org/abs/2409.03107
Title: RoboKoop: Efficient Control Conditioned Representations from Visual Input in Robotics using Koopman Operator

Original Abstract:
Developing agents that can perform complex control tasks from high-dimensional observations is a core ability of autonomous agents that requires underlying robust task control policies and adapting the underlying visual representations to the task. Most existing policies need a lot of training samples and treat this problem from the lens of two-stage learning with a controller learned on top of pre-trained vision models. We approach this problem from the lens of Koopman theory and learn visual representations from robotic agents conditioned on specific downstream tasks in the context of learning stabilizing control for the agent. We introduce a Contrastive Spectral Koopman Embedding network that allows us to learn efficient linearized visual representations from the agent's visual data in a high dimensional latent space and utilizes reinforcement learning to perform off-policy control on top of the extracted representations with a linear controller. Our method enhances stability and control in gradient dynamics over time, significantly outperforming existing approaches by improving efficiency and accuracy in learning task policies over extended horizons.

Translated Abstract:
복잡한 제어 작업을 수행할 수 있는 에이전트를 개발하는 건 자율 에이전트의 핵심 능력인데, 이는 강력한 작업 제어 정책과 작업에 맞게 시각적 표현을 조정하는 걸 필요로 해. 대부분의 기존 정책은 많은 훈련 샘플을 필요로 하고, 사전 훈련된 비전 모델 위에 컨트롤러를 학습하는 두 단계 학습 관점에서 이 문제를 다뤄.

우리는 이 문제를 쿠프만 이론의 관점에서 접근하고, 특정 하위 작업에 맞춰 로봇 에이전트에서 시각적 표현을 학습해. 에이전트를 위한 안정화 제어를 배우는 맥락에서 말이지. 우리는 Contrastive Spectral Koopman Embedding 네트워크를 소개하는데, 이건 에이전트의 시각적 데이터에서 효율적인 선형화된 시각적 표현을 고차원 잠재 공간에서 학습할 수 있게 해줘. 그리고 이 추출된 표현 위에서 선형 컨트롤러와 함께 오프 정책 제어를 수행하기 위해 강화 학습을 활용해.

우리의 방법은 시간이 지남에 따라 기울기 역학에서의 안정성과 제어를 향상시키고, 긴 시간에 걸쳐 작업 정책 학습의 효율성과 정확성을 개선함으로써 기존 접근 방식보다 훨씬 더 나은 성과를 내.

================================================================================

URL: https://arxiv.org/abs/2409.03005
Title: PIETRA: Physics-Informed Evidential Learning for Traversing Out-of-Distribution Terrain

Original Abstract:
Self-supervised learning is a powerful approach for developing traversability models for off-road navigation, but these models often struggle with inputs unseen during training. Existing methods utilize techniques like evidential deep learning to quantify model uncertainty, helping to identify and avoid out-of-distribution terrain. However, always avoiding out-of-distribution terrain can be overly conservative, e.g., when novel terrain can be effectively analyzed using a physics-based model. To overcome this challenge, we introduce Physics-Informed Evidential Traversability (PIETRA), a self-supervised learning framework that integrates physics priors directly into the mathematical formulation of evidential neural networks and introduces physics knowledge implicitly through an uncertainty-aware, physics-informed training loss. Our evidential network seamlessly transitions between learned and physics-based predictions for out-of-distribution inputs. Additionally, the physics-informed loss regularizes the learned model, ensuring better alignment with the physics model. Extensive simulations and hardware experiments demonstrate that PIETRA improves both learning accuracy and navigation performance in environments with significant distribution shifts.

Translated Abstract:
자기 지도 학습은 오프로드 내비게이션을 위한 이동 가능성 모델을 개발하는 데 강력한 접근 방식이야. 하지만 이런 모델들은 훈련 중에 보지 못한 입력에 대해 잘 작동하지 않아. 기존 방법들은 증거 심층 학습 같은 기법을 이용해서 모델의 불확실성을 정량화하고, 이를 통해 분포에서 벗어난 지형을 식별하고 피하는 데 도움을 줘. 하지만 항상 분포에서 벗어난 지형을 피하는 건 너무 조심스러울 수 있어. 예를 들어, 새로운 지형이 물리 기반 모델을 사용해서 효과적으로 분석될 수 있을 때 말이야.

이런 문제를 해결하기 위해 우리는 물리 정보를 반영한 증거 이동 가능성(PIETRA)이라는 자기 지도 학습 프레임워크를 소개해. 이 프레임워크는 증거 신경망의 수학적 공식에 직접 물리적 선행 지식을 통합하고, 불확실성을 인식하는 물리 정보 기반 훈련 손실을 통해 물리 지식을 암묵적으로 도입해. 우리의 증거 네트워크는 분포에서 벗어난 입력에 대해 학습된 예측과 물리 기반 예측 간에 매끄럽게 전환할 수 있어. 게다가, 물리 정보 기반 손실은 학습된 모델을 규칙화해서 물리 모델과 더 잘 맞게 해줘.

광범위한 시뮬레이션과 하드웨어 실험을 통해 PIETRA가 큰 분포 변화가 있는 환경에서 학습 정확도와 내비게이션 성능을 모두 개선한다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.03757
Title: Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding

Original Abstract:
Complex 3D scene understanding has gained increasing attention, with scene encoding strategies playing a crucial role in this success. However, the optimal scene encoding strategies for various scenarios remain unclear, particularly compared to their image-based counterparts. To address this issue, we present a comprehensive study that probes various visual encoding models for 3D scene understanding, identifying the strengths and limitations of each model across different scenarios. Our evaluation spans seven vision foundation encoders, including image-based, video-based, and 3D foundation models. We evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration, each focusing on different aspects of scene understanding. Our evaluations yield key findings: DINOv2 demonstrates superior performance, video models excel in object-level tasks, diffusion models benefit geometric tasks, and language-pretrained models show unexpected limitations in language-related tasks. These insights challenge some conventional understandings, provide novel perspectives on leveraging visual foundation models, and highlight the need for more flexible encoder selection in future vision-language and scene-understanding tasks.

Translated Abstract:
복잡한 3D 장면 이해는 점점 더 많은 관심을 받고 있어. 이 과정에서 장면 인코딩 전략이 중요한 역할을 해. 하지만 다양한 상황에 맞는 최적의 장면 인코딩 전략은 아직 명확하지 않아, 특히 이미지 기반 방법과 비교할 때 더 그렇지.

이 문제를 해결하기 위해, 우리는 3D 장면 이해를 위한 다양한 시각 인코딩 모델을 조사하는 포괄적인 연구를 했어. 각 모델의 강점과 한계를 다양한 상황에서 확인했지. 우리의 평가는 이미지 기반, 비디오 기반, 그리고 3D 기반 모델을 포함한 총 일곱 가지 비전 기초 인코더를 대상으로 진행됐어.

우리는 네 가지 작업에서 이 모델들을 평가했어: 비전-언어 장면 추론, 시각적 기초, 분할, 그리고 등록. 각각은 장면 이해의 다른 측면에 초점을 맞추고 있어. 평가 결과 몇 가지 중요한 발견이 있었어: DINOv2가 뛰어난 성능을 보였고, 비디오 모델은 객체 수준 작업에서 잘했어. 확산 모델은 기하학적 작업에서 이점을 보였고, 언어로 사전 훈련된 모델은 언어 관련 작업에서 예상치 못한 한계를 보였어.

이런 통찰은 몇 가지 기존의 이해를 도전하게 하고, 시각 기초 모델을 활용하는 새로운 관점을 제공해. 또한, 앞으로 비전-언어 및 장면 이해 작업에서 더 유연한 인코더 선택이 필요하다는 점도 강조하고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.03402
Title: Game On: Towards Language Models as RL Experimenters

Original Abstract:
We propose an agent architecture that automates parts of the common reinforcement learning experiment workflow, to enable automated mastery of control domains for embodied agents. To do so, it leverages a VLM to perform some of the capabilities normally required of a human experimenter, including the monitoring and analysis of experiment progress, the proposition of new tasks based on past successes and failures of the agent, decomposing tasks into a sequence of subtasks (skills), and retrieval of the skill to execute - enabling our system to build automated curricula for learning. We believe this is one of the first proposals for a system that leverages a VLM throughout the full experiment cycle of reinforcement learning. We provide a first prototype of this system, and examine the feasibility of current models and techniques for the desired level of automation. For this, we use a standard Gemini model, without additional fine-tuning, to provide a curriculum of skills to a language-conditioned Actor-Critic algorithm, in order to steer data collection so as to aid learning new skills. Data collected in this way is shown to be useful for learning and iteratively improving control policies in a robotics domain. Additional examination of the ability of the system to build a growing library of skills, and to judge the progress of the training of those skills, also shows promising results, suggesting that the proposed architecture provides a potential recipe for fully automated mastery of tasks and domains for embodied agents.

Translated Abstract:
우리는 강화 학습 실험의 일반적인 작업 흐름의 일부를 자동화하는 에이전트 아키텍처를 제안해. 이걸 통해 구체화된 에이전트가 제어 분야에서 자동으로 숙련될 수 있도록 하려는 거야.

이 시스템은 VLM(비주얼 언어 모델)을 활용해서 보통 사람 실험자가 해야 할 몇 가지 작업을 수행해. 여기에는 실험 진행 상황 모니터링, 과거의 성공과 실패를 바탕으로 새로운 작업 제안하기, 작업을 여러 개의 하위 작업(기술)으로 분해하기, 그리고 그 기술을 실행하기 위해 불러오는 것이 포함돼. 이렇게 해서 우리의 시스템이 학습을 위한 자동화된 커리큘럼을 만들 수 있게 돼.

우리는 VLM을 강화 학습의 전체 실험 주기에서 활용하는 시스템을 제안한 첫 번째 사례 중 하나라고 생각해. 그리고 이 시스템의 첫 번째 프로토타입을 제공하고, 원하는 수준의 자동화를 위해 현재 모델과 기술의 가능성을 살펴봤어.

이를 위해, 우리는 추가적인 미세 조정 없이 표준 Gemini 모델을 사용해서 언어 조건화된 액터-크리틱 알고리즘에 기술 커리큘럼을 제공했어. 이렇게 데이터 수집을 조정해서 새로운 기술을 배우는 데 도움을 주려고 했지. 이런 방식으로 수집된 데이터는 로봇 분야에서 제어 정책을 학습하고 반복적으로 개선하는 데 유용한 것으로 나타났어.

시스템이 성장하는 기술 라이브러리를 구축하고, 그 기술의 훈련 진행 상황을 평가하는 능력에 대한 추가 조사를 했을 때도 좋은 결과가 나왔어. 이걸 통해 제안된 아키텍처가 구체화된 에이전트의 작업 및 도메인에 대한 완전 자동화된 숙련의 가능성을 제공할 수 있음을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.03358
Title: MouseSIS: A Frames-and-Events Dataset for Space-Time Instance Segmentation of Mice

Original Abstract:
Enabled by large annotated datasets, tracking and segmentation of objects in videos has made remarkable progress in recent years. Despite these advancements, algorithms still struggle under degraded conditions and during fast movements. Event cameras are novel sensors with high temporal resolution and high dynamic range that offer promising advantages to address these challenges. However, annotated data for developing learning-based mask-level tracking algorithms with events is not available. To this end, we introduce: ($i$) a new task termed \emph{space-time instance segmentation}, similar to video instance segmentation, whose goal is to segment instances throughout the entire duration of the sensor input (here, the input are quasi-continuous events and optionally aligned frames); and ($ii$) \emph{\dname}, a dataset for the new task, containing aligned grayscale frames and events. It includes annotated ground-truth labels (pixel-level instance segmentation masks) of a group of up to seven freely moving and interacting mice. We also provide two reference methods, which show that leveraging event data can consistently improve tracking performance, especially when used in combination with conventional cameras. The results highlight the potential of event-aided tracking in difficult scenarios. We hope our dataset opens the field of event-based video instance segmentation and enables the development of robust tracking algorithms for challenging conditions.\url{this https URL}

Translated Abstract:
최근 몇 년 동안, 대규모 주석이 달린 데이터셋 덕분에 비디오에서 객체를 추적하고 분할하는 기술이 크게 발전했어. 하지만 이런 발전에도 불구하고, 알고리즘은 여전히 열악한 조건이나 빠른 움직임에서는 잘 작동하지 않아. 이벤트 카메라는 높은 시간 해상도와 높은 동적 범위를 가진 새로운 센서로, 이런 문제를 해결할 수 있는 가능성을 보여줘. 하지만 이벤트를 사용한 학습 기반 마스크 레벨 추적 알고리즘을 개발할 수 있는 주석 데이터는 없어.

그래서 우리는 두 가지를 소개해. 첫 번째는 \emph{공간-시간 인스턴스 분할}이라는 새로운 작업이야. 이건 비디오 인스턴스 분할과 비슷한데, 센서 입력의 전체 기간 동안 인스턴스를 분할하는 게 목표야. 여기서 입력은 거의 연속적인 이벤트와 선택적으로 정렬된 프레임이야. 두 번째는 \emph{\dname}이라는 새로운 작업을 위한 데이터셋이야. 이 데이터셋은 정렬된 그레이스케일 프레임과 이벤트를 포함하고 있어. 최대 7마리의 자유롭게 움직이고 상호작용하는 쥐의 주석이 달린 진짜 레이블(픽셀 수준 인스턴스 분할 마스크)도 포함되어 있어.

우리는 이벤트 데이터를 활용하면 추적 성능을 일관되게 개선할 수 있다는 두 가지 참조 방법도 제공해. 특히 기존 카메라와 함께 사용할 때 더욱 효과적이야. 결과는 힘든 상황에서 이벤트 지원 추적의 가능성을 보여줘. 우리 데이터셋이 이벤트 기반 비디오 인스턴스 분할 분야를 열고, 어려운 조건에서도 견고한 추적 알고리즘 개발에 기여하길 바래.

================================================================================

URL: https://arxiv.org/abs/2409.03272
Title: OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving

Original Abstract:
The rise of multi-modal large language models(MLLMs) has spurred their applications in autonomous driving. Recent MLLM-based methods perform action by learning a direct mapping from perception to action, neglecting the dynamics of the world and the relations between action and world dynamics. In contrast, human beings possess world model that enables them to simulate the future states based on 3D internal visual representation and plan actions accordingly. To this end, we propose OccLLaMA, an occupancy-language-action generative world model, which uses semantic occupancy as a general visual representation and unifies vision-language-action(VLA) modalities through an autoregressive model. Specifically, we introduce a novel VQVAE-like scene tokenizer to efficiently discretize and reconstruct semantic occupancy scenes, considering its sparsity and classes imbalance. Then, we build a unified multi-modal vocabulary for vision, language and action. Furthermore, we enhance LLM, specifically LLaMA, to perform the next token/scene prediction on the unified vocabulary to complete multiple tasks in autonomous driving. Extensive experiments demonstrate that OccLLaMA achieves competitive performance across multiple tasks, including 4D occupancy forecasting, motion planning, and visual question answering, showcasing its potential as a foundation model in autonomous driving.

Translated Abstract:
다중 모달 대형 언어 모델(MLLM)의 발전이 자율주행 분야에서의 활용을 촉진하고 있어. 최근 MLLM 기반 방법들은 인식에서 행동으로의 직접적인 매핑을 학습해 행동을 수행하는데, 이 과정에서 세계의 동역학이나 행동과 세계 동역학 간의 관계를 무시하고 있어. 반면, 인간은 3D 내부 시각 표현을 바탕으로 미래 상태를 시뮬레이션하고 그에 맞춰 행동을 계획할 수 있는 세계 모델을 가지고 있어.

이런 배경을 바탕으로, 우리는 OccLLaMA라는 점유-언어-행동 생성 세계 모델을 제안해. 이 모델은 의미론적 점유를 일반적인 시각 표현으로 사용하고, 자가 회귀 모델을 통해 비전-언어-행동(VLA) 모달리티를 통합해. 특히, 우리는 의미론적 점유 장면을 효율적으로 이산화하고 재구성하기 위해 새로운 VQVAE 유사 장면 토크나이저를 도입해. 이 과정에서 점유의 희소성과 클래스 불균형을 고려해.

그 다음, 우리는 비전, 언어, 행동을 위한 통합된 다중 모달 어휘를 구축해. 그리고 LLaMA라는 LLM을 개선해서 통합된 어휘에서 다음 토큰이나 장면 예측을 수행하게 해, 이렇게 해서 자율주행에서 여러 작업을 완료할 수 있게 해. 다양한 실험 결과를 통해 OccLLaMA가 4D 점유 예측, 모션 계획, 시각적 질문 응답 등 여러 작업에서 경쟁력 있는 성능을 보여주며, 자율주행의 기초 모델로서의 가능성을 입증했어.

================================================================================

URL: https://arxiv.org/abs/2409.03153
Title: Can we enhance prosocial behavior? Using post-ride feedback to improve micromobility interactions

Original Abstract:
Micromobility devices, such as e-scooters and delivery robots, hold promise for eco-friendly and cost-effective alternatives for future urban transportation. However, their lack of societal acceptance remains a challenge. Therefore, we must consider ways to promote prosocial behavior in micromobility interactions. We investigate how post-ride feedback can encourage the prosocial behavior of e-scooter riders while interacting with sidewalk users, including pedestrians and delivery robots. Using a web-based platform, we measure the prosocial behavior of e-scooter riders. Results found that post-ride feedback can successfully promote prosocial behavior, and objective measures indicated better gap behavior, lower speeds at interaction, and longer stopping time around other sidewalk actors. The findings of this study demonstrate the efficacy of post-ride feedback and provide a step toward designing methodologies to improve the prosocial behavior of mobility users.

Translated Abstract:
마이크로 모빌리티 기기, 예를 들어 전동 스쿠터와 배달 로봇은 미래 도시 교통의 친환경적이고 비용 효율적인 대안으로 기대되고 있어. 하지만, 사람들이 이 기기를 받아들이는 데에는 아직 어려움이 있어. 그래서 우리는 마이크로 모빌리티 상호작용에서 친사회적 행동을 촉진할 방법을 생각해봐야 해.

이번 연구에서는 전동 스쿠터 이용자들이 보행자나 배달 로봇 같은 보도 사용자와 상호작용할 때, 라이딩 후 피드백이 어떻게 친사회적 행동을 장려할 수 있는지 알아봤어. 웹 기반 플랫폼을 사용해서 전동 스쿠터 이용자들의 친사회적 행동을 측정했는데, 결과적으로 라이딩 후 피드백이 친사회적 행동을 성공적으로 촉진한다는 걸 알게 되었어.

객관적인 측정 결과로는, 다른 보도 사용자들과의 상호작용에서 더 나은 간격 행동, 낮은 속도, 그리고 더 긴 정지 시간이 나타났어. 이 연구 결과는 라이딩 후 피드백의 효과를 보여주고, 모빌리티 사용자들의 친사회적 행동을 개선하기 위한 방법론을 설계하는 데 한 걸음 나아가는 데 기여해.

================================================================================

URL: https://arxiv.org/abs/2409.03061
Title: Incorporating dense metric depth into neural 3D representations for view synthesis and relighting

Original Abstract:
Synthesizing accurate geometry and photo-realistic appearance of small scenes is an active area of research with compelling use cases in gaming, virtual reality, robotic-manipulation, autonomous driving, convenient product capture, and consumer-level photography. When applying scene geometry and appearance estimation techniques to robotics, we found that the narrow cone of possible viewpoints due to the limited range of robot motion and scene clutter caused current estimation techniques to produce poor quality estimates or even fail. On the other hand, in robotic applications, dense metric depth can often be measured directly using stereo and illumination can be controlled. Depth can provide a good initial estimate of the object geometry to improve reconstruction, while multi-illumination images can facilitate relighting. In this work we demonstrate a method to incorporate dense metric depth into the training of neural 3D representations and address an artifact observed while jointly refining geometry and appearance by disambiguating between texture and geometry edges. We also discuss a multi-flash stereo camera system developed to capture the necessary data for our pipeline and show results on relighting and view synthesis with a few training views.

Translated Abstract:
작은 장면의 정확한 기하학과 사진처럼 실감나는 외관을 합성하는 것은 게임, 가상 현실, 로봇 조작, 자율주행, 편리한 제품 촬영, 소비자 수준의 사진 촬영 등에서 매우 중요한 연구 분야야. 

로봇에 장면 기하학과 외관 추정 기법을 적용할 때, 로봇의 움직임 범위가 제한되고 장면이 복잡해서 가능한 시점이 너무 좁아져서 현재의 추정 기법들이 질이 낮은 추정을 하거나 아예 실패하는 경우가 많았어. 반면에 로봇 응용에서는 밀도 있는 메트릭 깊이를 스테레오를 사용해 직접 측정할 수 있고 조명도 조절할 수 있어. 깊이는 물체의 기하학에 대한 좋은 초기 추정치를 제공해 재구성을 개선할 수 있고, 여러 조명 이미지는 재조명을 쉽게 해줘.

이 연구에서는 밀도 있는 메트릭 깊이를 신경망 3D 표현의 훈련에 포함시키는 방법을 보여주고, 기하학과 외관을 동시에 다듬을 때 관찰되는 아티팩트를 질감과 기하학 경계를 구분함으로써 해결했어. 또한, 우리 파이프라인에 필요한 데이터를 수집하기 위해 개발한 다중 플래시 스테레오 카메라 시스템에 대해서도 이야기하고, 몇 개의 훈련 뷰로 재조명과 시점 합성 결과를 보여줄 거야.

================================================================================

URL: https://arxiv.org/abs/2409.02920
Title: RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)

Original Abstract:
Effective collaboration of dual-arm robots and their tool use capabilities are increasingly important areas in the advancement of robotics. These skills play a significant role in expanding robots' ability to operate in diverse real-world environments. However, progress is impeded by the scarcity of specialized training data. This paper introduces RoboTwin, a novel benchmark dataset combining real-world teleoperated data with synthetic data from digital twins, designed for dual-arm robotic scenarios. Using the COBOT Magic platform, we have collected diverse data on tool usage and human-robot interaction. We present a innovative approach to creating digital twins using AI-generated content, transforming 2D images into detailed 3D models. Furthermore, we utilize large language models to generate expert-level training data and task-specific pose sequences oriented toward functionality. Our key contributions are: 1) the RoboTwin benchmark dataset, 2) an efficient real-to-simulation pipeline, and 3) the use of language models for automatic expert-level data generation. These advancements are designed to address the shortage of robotic training data, potentially accelerating the development of more capable and versatile robotic systems for a wide range of real-world applications. The project page is available at this https URL

Translated Abstract:
이중팔 로봇의 효과적인 협력과 도구 사용 능력은 로봇 기술 발전에서 점점 더 중요한 분야야. 이런 기술들은 로봇이 다양한 실제 환경에서 작동할 수 있는 능력을 확장하는 데 큰 역할을 해. 하지만, 전문 훈련 데이터가 부족해서 발전이 더디고 있어. 

이 논문에서는 RoboTwin이라는 새로운 벤치마크 데이터셋을 소개해. 이 데이터셋은 실제 원격 조작 데이터와 디지털 트윈에서 생성된 합성 데이터를 결합한 거야. 이중팔 로봇 시나리오를 위해 디자인됐어. COBOT Magic 플랫폼을 사용해서 도구 사용과 인간-로봇 상호작용에 대한 다양한 데이터를 수집했어. 

우리는 AI 생성 콘텐츠를 사용해 디지털 트윈을 만드는 혁신적인 접근 방식을 제안해. 2D 이미지를 상세한 3D 모델로 변환하는 방식이지. 게다가, 우리는 대형 언어 모델을 활용해 전문가 수준의 훈련 데이터와 기능 중심의 특정 작업 포즈 시퀀스를 생성했어. 

우리의 주요 기여는: 1) RoboTwin 벤치마크 데이터셋, 2) 효율적인 현실-시뮬레이션 파이프라인, 3) 자동 전문가 수준 데이터 생성을 위한 언어 모델의 사용이야. 이런 발전은 로봇 훈련 데이터의 부족 문제를 해결하기 위해 고안된 거고, 다양한 실제 응용을 위한 더 능력 있고 다재다능한 로봇 시스템 개발을 가속화할 수 있을 거야. 프로젝트 페이지는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.02871
Title: Hybrid Imitation-Learning Motion Planner for Urban Driving

Original Abstract:
With the release of open source datasets such as nuPlan and Argoverse, the research around learning-based planners has spread a lot in the last years. Existing systems have shown excellent capabilities in imitating the human driver behaviour, but they struggle to guarantee safe closed-loop driving. Conversely, optimization-based planners offer greater security in short-term planning scenarios. To confront this challenge, in this paper we propose a novel hybrid motion planner that integrates both learning-based and optimization-based techniques. Initially, a multilayer perceptron (MLP) generates a human-like trajectory, which is then refined by an optimization-based component. This component not only minimizes tracking errors but also computes a trajectory that is both kinematically feasible and collision-free with obstacles and road boundaries. Our model effectively balances safety and human-likeness, mitigating the trade-off inherent in these objectives. We validate our approach through simulation experiments and further demonstrate its efficacy by deploying it in real-world self-driving vehicles.

Translated Abstract:
최근 nuPlan과 Argoverse 같은 오픈 소스 데이터셋이 공개되면서, 학습 기반 플래너에 대한 연구가 많이 진행되고 있어. 기존 시스템들은 인간 운전자의 행동을 잘 모방하는 능력을 보여주지만, 안전한 폐쇄 루프 주행을 보장하는 데는 어려움을 겪고 있어. 반면에 최적화 기반 플래너는 단기 계획 시나리오에서 더 높은 안전성을 제공해.

이런 문제를 해결하기 위해, 이 논문에서는 학습 기반과 최적화 기반 기술을 통합한 새로운 하이브리드 모션 플래너를 제안해. 처음에 다층 퍼셉트론(MLP)이 인간 같은 궤적을 생성하고, 그 다음에 최적화 기반 구성 요소가 이를 다듬어. 이 구성 요소는 추적 오류를 최소화할 뿐만 아니라, 동역학적으로 가능하고 장애물이나 도로 경계와 충돌하지 않는 궤적도 계산해.

우리 모델은 안전성과 인간 유사성을 효과적으로 균형 잡아서, 이 두 목표 사이의 상충 문제를 완화해. 우리는 시뮬레이션 실험을 통해 우리의 접근 방식을 검증하고, 실제 자율주행 차량에 적용하여 그 효능을 추가로 입증했어.

================================================================================

URL: https://arxiv.org/abs/2409.02863
Title: CONClave -- Secure and Robust Cooperative Perception for CAVs Using Authenticated Consensus and Trust Scoring

Original Abstract:
Connected Autonomous Vehicles have great potential to improve automobile safety and traffic flow, especially in cooperative applications where perception data is shared between vehicles. However, this cooperation must be secured from malicious intent and unintentional errors that could cause accidents. Previous works typically address singular security or reliability issues for cooperative driving in specific scenarios rather than the set of errors together. In this paper, we propose CONClave, a tightly coupled authentication, consensus, and trust scoring mechanism that provides comprehensive security and reliability for cooperative perception in autonomous vehicles. CONClave benefits from the pipelined nature of the steps such that faults can be detected significantly faster and with less compute. Overall, CONClave shows huge promise in preventing security flaws, detecting even relatively minor sensing faults, and increasing the robustness and accuracy of cooperative perception in CAVs while adding minimal overhead.

Translated Abstract:
연결된 자율주행차는 자동차 안전과 교통 흐름을 크게 개선할 수 있는 잠재력이 있어. 특히 차량 간에 인식 데이터를 공유하는 협력적인 상황에서 더욱 그렇지. 하지만 이런 협력은 악의적인 의도나 실수로 인한 사고를 막기 위해 안전하게 지켜져야 해. 

이전 연구들은 일반적으로 특정 상황에서 협동 주행을 위한 보안이나 신뢰성 문제를 따로 다루었어. 그런데 이 논문에서는 CONClave라는 걸 제안해. 이건 인증, 합의, 신뢰 점수 매기기를 긴밀하게 연결한 메커니즘으로, 자율주행차의 협력적 인식에 대한 포괄적인 보안과 신뢰성을 제공해. 

CONClave는 단계가 파이프라인처럼 연결되어 있어서, 고장을 훨씬 더 빠르고 적은 계산으로 감지할 수 있어. 전반적으로 CONClave는 보안 결함을 예방하고, 비교적 작은 센서 고장도 감지하며, 자율주행차의 협력적 인식의 강인성과 정확성을 높이는 데 큰 가능성을 보여줘. 게다가 추가적인 부담도 최소화해.

================================================================================

URL: https://arxiv.org/abs/2409.02738
Title: SOAR: Simultaneous Exploration and Photographing with Heterogeneous UAVs for Fast Autonomous Reconstruction

Original Abstract:
Unmanned Aerial Vehicles (UAVs) have gained significant popularity in scene reconstruction. This paper presents SOAR, a LiDAR-Visual heterogeneous multi-UAV system specifically designed for fast autonomous reconstruction of complex environments. Our system comprises a LiDAR-equipped explorer with a large field-of-view (FoV), alongside photographers equipped with cameras. To ensure rapid acquisition of the scene's surface geometry, we employ a surface frontier-based exploration strategy for the explorer. As the surface is progressively explored, we identify the uncovered areas and generate viewpoints incrementally. These viewpoints are then assigned to photographers through solving a Consistent Multiple Depot Multiple Traveling Salesman Problem (Consistent-MDMTSP), which optimizes scanning efficiency while ensuring task consistency. Finally, photographers utilize the assigned viewpoints to determine optimal coverage paths for acquiring images. We present extensive benchmarks in the realistic simulator, which validates the performance of SOAR compared with classical and state-of-the-art methods. For more details, please see our project page at this https URL}{this http URL.

Translated Abstract:
무인 항공기(UAV)가 장면 재구성에서 큰 인기를 얻고 있어. 이 논문에서는 SOAR라는 시스템을 소개하는데, 이건 복잡한 환경을 빠르게 자율적으로 재구성하기 위해 특별히 설계된 LiDAR-Visual 이종 다중 UAV 시스템이야. 

우리 시스템은 넓은 시야각을 가진 LiDAR 장착 탐사기와 카메라가 장착된 사진작가들로 구성되어 있어. 장면의 표면 기하학을 빠르게 획득하기 위해 탐사기는 표면 경계 기반 탐사 전략을 사용해. 표면이 점진적으로 탐사되면서, 우리는 아직 탐사되지 않은 지역을 찾아내고, 점차적으로 시점을 생성해.

이 시점들은 Consistent Multiple Depot Multiple Traveling Salesman Problem (Consistent-MDMTSP)을 풀어서 사진작가들에게 할당돼. 이 과정은 스캔 효율성을 최적화하고 작업 일관성을 보장해. 마지막으로, 사진작가들은 할당된 시점을 이용해 이미지를 얻기 위한 최적의 경로를 정해.

우리는 현실적인 시뮬레이터에서 SOAR의 성능을 기존 방법들과 최신 방법들과 비교해 광범위한 벤치마크를 제시했어. 더 자세한 내용은 우리 프로젝트 페이지에서 확인해봐.

================================================================================

URL: https://arxiv.org/abs/2409.02724
Title: Surgical Task Automation Using Actor-Critic Frameworks and Self-Supervised Imitation Learning

Original Abstract:
Surgical robot task automation has recently attracted great attention due to its potential to benefit both surgeons and patients. Reinforcement learning (RL) based approaches have demonstrated promising ability to provide solutions to automated surgical manipulations on various tasks. To address the exploration challenge, expert demonstrations can be utilized to enhance the learning efficiency via imitation learning (IL) approaches. However, the successes of such methods normally rely on both states and action labels. Unfortunately action labels can be hard to capture or their manual annotation is prohibitively expensive owing to the requirement for expert knowledge. It therefore remains an appealing and open problem to leverage expert demonstrations composed of pure states in RL. In this work, we present an actor-critic RL framework, termed AC-SSIL, to overcome this challenge of learning with state-only demonstrations collected by following an unknown expert policy. It adopts a self-supervised IL method, dubbed SSIL, to effectively incorporate demonstrated states into RL paradigms by retrieving from demonstrates the nearest neighbours of the query state and utilizing the bootstrapping of actor networks. We showcase through experiments on an open-source surgical simulation platform that our method delivers remarkable improvements over the RL baseline and exhibits comparable performance against action based IL methods, which implies the efficacy and potential of our method for expert demonstration-guided learning scenarios.

Translated Abstract:
최근에 수술 로봇 작업 자동화가 많은 주목을 받고 있어. 이건 수술사와 환자 모두에게 이득을 줄 수 있는 가능성이 크기 때문이야. 강화 학습(RL) 기반 접근 방식은 다양한 작업에서 자동 수술 조작을 위한 솔루션을 제공하는 데 유망한 능력을 보여줬어.

탐색 문제를 해결하기 위해 전문가의 시연을 활용하면 모방 학습(IL) 접근 방식을 통해 학습 효율성을 높일 수 있어. 하지만 이런 방법의 성공은 보통 상태와 행동 레이블 모두에 의존해. 문제는 행동 레이블을 포착하기 어렵거나, 전문가 지식이 필요해서 수동으로 주석을 다는 게 비싸다는 거야. 그래서 RL에서 순수한 상태로 구성된 전문가 시연을 활용하는 건 여전히 매력적이고 열린 문제로 남아 있어.

이번 연구에서는 AC-SSIL이라는 액터-크리틱 RL 프레임워크를 제안해. 이건 알려지지 않은 전문가 정책을 따라 수집한 상태만 있는 시연으로 학습하는 문제를 극복하려고 해. SSIL이라는 자기 지도 IL 방법을 채택해서, 시연된 상태를 RL 패러다임에 효과적으로 통합하고, 쿼리 상태의 가장 가까운 이웃을 찾아 액터 네트워크의 부트스트래핑을 이용해.

우리는 오픈 소스 수술 시뮬레이션 플랫폼에서 실험을 통해, 우리의 방법이 RL 기준선에 비해 뛰어난 개선을 보여주고, 행동 기반 IL 방법과 비슷한 성능을 보인다는 걸 보여줬어. 이건 전문가 시연에 기반한 학습 시나리오에서 우리의 방법이 효과적이고 잠재력이 있다는 걸 의미해.

================================================================================

URL: https://arxiv.org/abs/2409.02680
Title: A Low-Cost Real-Time Spiking System for Obstacle Detection based on Ultrasonic Sensors and Rate Coding

Original Abstract:
Since the advent of mobile robots, obstacle detection has been a topic of great interest. It has also been a subject of study in neuroscience, where flying insects and bats could be considered two of the most interesting cases in terms of vision-based and sound-based mechanisms for obstacle detection, respectively. Currently, many studies focus on vision-based obstacle detection, but not many can be found regarding sound-based obstacle detection. This work focuses on the latter approach, which also makes use of a Spiking Neural Network to exploit the advantages of these architectures and achieve an approach closer to biology. The complete system was tested through a series of experiments that confirm the validity of the spiking architecture for obstacle detection. It is empirically demonstrated that, when the distance between the robot and the obstacle decreases, the output firing rate of the system increases in response as expected, and vice versa. Therefore, there is a direct relation between the two. Furthermore, there is a distance threshold between detectable and undetectable objects which is also empirically measured in this work. An in-depth study on how this system works at low level based on the Inter-Spike Interval concept was performed, which may be useful in the future development of applications based on spiking filters.

Translated Abstract:
모바일 로봇이 등장한 이후 장애물 탐지는 큰 관심을 끌어온 주제야. 특히, 비행 곤충이나 박쥐 같은 생물들이 시각이나 소리를 이용해 장애물을 탐지하는 방식은 신경과학에서도 흥미로운 사례로 여겨져. 현재 많은 연구가 시각 기반의 장애물 탐지에 집중되고 있지만, 소리 기반 탐지에 대한 연구는 많지 않아. 

이번 연구는 소리 기반 탐지에 중점을 두고 있어. 스파이킹 신경망(Spiking Neural Network)을 활용해서 이러한 구조의 장점을 살리고 생물학에 더 가까운 접근을 시도했어. 실험을 통해 장애물 탐지에 대한 스파이킹 구조의 유효성을 확인했어. 로봇과 장애물 사이의 거리가 가까워질수록 시스템의 출력 발화율이 증가하는 걸 실험적으로 보여줬고, 반대로 거리가 멀어지면 발화율이 감소하는 것도 확인했어. 그러니까 두 가지는 직접적인 관계가 있어.

또한, 탐지 가능한 물체와 탐지 불가능한 물체 사이에 거리 임계값도 실험적으로 측정했어. 이 시스템이 어떻게 작동하는지에 대한 심층 연구도 진행했는데, 이는 스파이킹 필터 기반의 응용 프로그램 개발에 유용할 수 있을 거야.

================================================================================

URL: https://arxiv.org/abs/2409.02669
Title: Causality-Aware Transformer Networks for Robotic Navigation

Original Abstract:
Recent advances in machine learning algorithms have garnered growing interest in developing versatile Embodied AI systems. However, current research in this domain reveals opportunities for improvement. First, the direct adoption of RNNs and Transformers often overlooks the specific differences between Embodied AI and traditional sequential data modelling, potentially limiting its performance in Embodied AI tasks. Second, the reliance on task-specific configurations, such as pre-trained modules and dataset-specific logic, compromises the generalizability of these methods. We address these constraints by initially exploring the unique differences between Embodied AI tasks and other sequential data tasks through the lens of Causality, presenting a causal framework to elucidate the inadequacies of conventional sequential methods for Embodied AI. By leveraging this causal perspective, we propose Causality-Aware Transformer (CAT) Networks for Navigation, featuring a Causal Understanding Module to enhance the models's Environmental Understanding capability. Meanwhile, our method is devoid of task-specific inductive biases and can be trained in an End-to-End manner, which enhances the method's generalizability across various contexts. Empirical evaluations demonstrate that our methodology consistently surpasses benchmark performances across a spectrum of settings, tasks and simulation environments. Extensive ablation studies reveal that the performance gains can be attributed to the Causal Understanding Module, which demonstrates effectiveness and efficiency in both Reinforcement Learning and Supervised Learning settings.

Translated Abstract:
최근 머신러닝 알고리즘의 발전 덕분에 다양한 임베디드 AI 시스템을 개발하는 데 관심이 커지고 있어. 하지만 현재 이 분야의 연구를 보면 개선할 점이 있어. 

첫째, RNN이나 Transformer 같은 모델을 그냥 사용하는 건 임베디드 AI와 전통적인 순차 데이터 모델링의 차이를 무시할 수 있어. 이러면 임베디드 AI 작업에서 성능이 떨어질 수 있어. 둘째, 특정 작업에 맞춘 설정, 예를 들어 미리 훈련된 모듈이나 특정 데이터셋에 맞춘 로직에 의존하는 건 이런 방법들의 일반화 능력을 떨어뜨려. 

우리는 먼저 임베디드 AI 작업과 다른 순차 데이터 작업의 고유한 차이를 인과관계(Causality) 관점에서 살펴보면서, 전통적인 순차 방법의 한계를 설명할 수 있는 인과적 프레임워크를 제안해. 이 인과적 관점을 활용해, 내비게이션을 위한 인과성 인식 Transformer(CAT) 네트워크를 제안하고, 여기에는 모델의 환경 이해 능력을 높이기 위한 인과 이해 모듈이 포함돼. 

게다가, 우리의 방법은 특정 작업에 맞춘 편향이 없고 엔드 투 엔드 방식으로 훈련할 수 있어서 다양한 맥락에서 일반화 능력이 높아. 실제 평가 결과, 우리의 방법이 여러 설정, 작업, 시뮬레이션 환경에서 벤치마크 성능을 꾸준히 초과하는 걸 보여줬어. 많은 연구 결과에 따르면, 성능 향상은 인과 이해 모듈 덕분이라는 것을 알 수 있고, 이 모듈은 강화 학습과 감독 학습 환경 모두에서 효과적이고 효율적이야.

================================================================================

URL: https://arxiv.org/abs/2409.02636
Title: Mamba as a motion encoder for robotic imitation learning

Original Abstract:
Recent advancements in imitation learning, particularly with the integration of LLM techniques, are set to significantly improve robots' dexterity and adaptability. In this study, we propose using Mamba, a state-of-the-art architecture with potential applications in LLMs, for robotic imitation learning, highlighting its ability to function as an encoder that effectively captures contextual information. By reducing the dimensionality of the state space, Mamba operates similarly to an autoencoder. It effectively compresses the sequential information into state variables while preserving the essential temporal dynamics necessary for accurate motion prediction. Experimental results in tasks such as cup placing and case loading demonstrate that despite exhibiting higher estimation errors, Mamba achieves superior success rates compared to Transformers in practical task execution. This performance is attributed to Mamba's structure, which encompasses the state space model. Additionally, the study investigates Mamba's capacity to serve as a real-time motion generator with a limited amount of training data.

Translated Abstract:
최근 모방 학습에서의 발전, 특히 LLM 기술과의 통합이 로봇의 손재주와 적응력을 크게 향상시킬 것으로 기대되고 있어. 이 연구에서는 Mamba라는 최신 아키텍처를 사용해 로봇 모방 학습을 제안해. Mamba는 맥락 정보를 효과적으로 포착하는 인코더로 작동할 수 있어.

Mamba는 상태 공간의 차원을 줄이면서 오토인코더처럼 작동해. 순차적인 정보를 상태 변수로 압축하면서도 정확한 동작 예측을 위해 필요한 시간적 동역학을 잘 유지해. 컵 놓기와 케이스 적재 같은 작업에서 실험 결과를 보면, Mamba는 추정 오류가 더 높게 나타났지만, 실제 작업 수행에서 Transformers보다 더 높은 성공률을 기록했어. 이 성과는 Mamba의 구조가 상태 공간 모델을 포함하고 있기 때문이야.

또한, 이 연구에서는 Mamba가 제한된 양의 훈련 데이터로 실시간 동작 생성기로 기능할 수 있는지에 대해서도 살펴봤어.

================================================================================

URL: https://arxiv.org/abs/2409.02635
Title: Modelling, Design Optimization and Prototype development of Knee Exoskeleton

Original Abstract:
This study focuses on enhancing the design of an existing knee exoskeleton by addressing limitations in the range of motion (ROM) during Sit-to-Stand (STS) motions. While current knee exoskeletons emphasize toughness and rehabilitation, their closed-loop mechanisms hinder optimal ROM, which is crucial for effective rehabilitation. This research aims to optimize the exoskeleton design to achieve the necessary ROM, improving its functionality in rehabilitation. This can be achieved by utilizing kinematic modeling and formulation, the existing design was represented in the non-linear and non-convex mathematical functions. Optimization techniques, considering constraints based on human leg measurements, were applied to determine the best dimensions for the exoskeleton. This resulted in a significant increase in ROM compared to existing models. A MATLAB program was developed to compare the ROM of the optimized exoskeleton with the original design.
To validate the practicality of the optimized design, analysis was conducted using a mannequin with average human dimensions, followed by constructing a cardboard dummy model to confirm simulation results. The STS motion of an average human was captured using a camera and TRACKER software, and the motion was compared with that of the dummy model to identify any misalignments between the human and exoskeleton knee joints. Furthermore, a prototype of the knee joint exoskeleton is being developed to further investigate misalignments and improve the design. Future work includes the use of EMG sensors for more detailed analysis and better results.

Translated Abstract:
이 연구는 무릎 외골격의 디자인을 개선하는 데 초점을 맞추고 있어. 기존의 무릎 외골격은 튼튼함과 재활에 중점을 두고 있지만, Sit-to-Stand (STS) 동작 중에 필요한 운동 범위(ROM)를 제한하는 폐쇄 루프 메커니즘이 문제야. 이 연구는 외골격 디자인을 최적화해서 필요한 ROM을 달성하고 재활 기능을 개선하는 게 목표야.

이를 위해 운동학 모델링과 수식화를 사용했어. 기존 디자인을 비선형 및 비볼록 수학 함수로 표현했지. 인간 다리 치수를 고려한 제약 조건을 적용하여 외골격의 최적 치수를 결정하는 최적화 기법을 사용했어. 그 결과 기존 모델에 비해 ROM이 크게 증가했어. MATLAB 프로그램을 만들어서 최적화된 외골격의 ROM과 원래 디자인을 비교했어.

최적화된 디자인의 실용성을 검증하기 위해 평균 인간 치수를 가진 마네킹을 사용해서 분석을 했고, 시뮬레이션 결과를 확인하기 위해 카드보드 더미 모델을 만들었어. 평균 인간의 STS 동작을 카메라와 TRACKER 소프트웨어로 촬영하고, 더미 모델과 비교해서 인간과 외골격 무릎 관절 간의 불일치를 확인했어. 게다가 무릎 관절 외골격의 프로토타입을 개발 중인데, 이걸 통해 불일치를 더 조사하고 디자인을 개선할 계획이야. 앞으로는 EMG 센서를 사용해서 더 세밀한 분석과 더 나은 결과를 얻을 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.02556
Title: Want a Ride? Attitudes Towards Autonomous Driving and Behavior in Autonomous Vehicles

Original Abstract:
Research conducted previously has focused on either attitudes toward or behaviors associated with autonomous driving. In this paper, we bridge these two dimensions by exploring how attitudes towards autonomous driving influence behavior in an autonomous car. We conducted a field experiment with twelve participants engaged in non-driving related tasks. Our findings indicate that attitudes towards autonomous driving do not affect participants' driving interventions in vehicle control and eye glance behavior. Therefore, studies on autonomous driving technology lacking field tests might be unreliable for assessing the potential behaviors, attitudes, and acceptance of autonomous vehicles.

Translated Abstract:
이전 연구들은 자율주행에 대한 태도나 행동 중 하나에만 집중해왔어. 이 논문에서는 자율주행에 대한 태도가 자율차에서의 행동에 어떻게 영향을 미치는지 살펴보려고 해. 우리는 비운전 관련 작업을 하는 12명의 참가자와 함께 현장 실험을 했어.

우리의 결과는 자율주행에 대한 태도가 참가자들의 차량 제어 개입이나 시선 행동에는 영향을 미치지 않는다는 걸 보여줘. 그래서 현장 테스트가 없는 자율주행 기술 연구는 자율차의 행동, 태도, 수용 가능성을 평가하는 데 신뢰할 수 없을 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.02531
Title: Modular pipeline for small bodies gravity field modeling: an efficient representation of variable density spherical harmonics coefficients

Original Abstract:
Proximity operations to small bodies, such as asteroids and comets, demand high levels of autonomy to achieve cost-effective, safe, and reliable Guidance, Navigation and Control (GNC) solutions. Enabling autonomous GNC capabilities in the vicinity of these targets is thus vital for future space applications. However, the highly non-linear and uncertain environment characterizing their vicinity poses unique challenges that need to be assessed to grant robustness against unknown shapes and gravity fields. In this paper, a pipeline designed to generate variable density gravity field models is proposed, allowing the generation of a coherent set of scenarios that can be used for design, validation, and testing of GNC algorithms. The proposed approach consists in processing a polyhedral shape model of the body with a given density distribution to compute the coefficients of the spherical harmonics expansion associated with the gravity field. To validate the approach, several comparison are conducted against analytical solutions, literature results, and higher fidelity models, across a diverse set of targets with varying morphological and physical properties. Simulation results demonstrate the effectiveness of the methodology, showing good performances in terms of modeling accuracy and computational efficiency. This research presents a faster and more robust framework for generating environmental models to be used in simulation and hardware-in-the-loop testing of onboard GNC algorithms.

Translated Abstract:
소행성이나 혜성과 같은 작은 천체에 가까이 접근하는 작업은 비용 효율적이고 안전하며 신뢰할 수 있는 유도, 항법 및 제어(GNC) 솔루션을 위해 높은 자율성이 필요해. 이런 목표 주변에서 자율적인 GNC 기능을 활성화하는 게 미래의 우주 응용에 정말 중요해. 하지만 이 지역은 매우 비선형적이고 불확실한 환경이어서, 알려지지 않은 모양과 중력장에 대해 강건성을 보장하기 위해 특별한 도전 과제가 있어.

이 논문에서는 변동 밀도의 중력장 모델을 생성하는 파이프라인을 제안해. 이걸 통해 디자인, 검증, GNC 알고리즘 테스트에 사용할 수 있는 일관된 시나리오 세트를 만들 수 있어. 제안된 접근 방식은 주어진 밀도 분포를 가진 다각형 모양 모델을 처리해서 중력장과 관련된 구형 조화 함수의 계수를 계산하는 거야.

이 접근 방식을 검증하기 위해 여러 분석 솔루션, 문헌 결과, 그리고 더 높은 정밀도의 모델과 비교해봤어. 다양한 형태학적 및 물리적 특성을 가진 목표에 대해 성능을 평가했지. 시뮬레이션 결과는 이 방법론의 효과를 보여주고, 모델링 정확성과 계산 효율성 면에서 좋은 성과를 냈어. 이 연구는 시뮬레이션과 하드웨어-in-the-loop 테스트에 사용될 환경 모델을 생성하는 더 빠르고 강건한 프레임워크를 제시하고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.02503
Title: eRSS-RAMP: A Rule-Adherence Motion Planner Based on Extended Responsibility-Sensitive Safety for Autonomous Driving

Original Abstract:
Driving safety and responsibility determination are indispensable pieces of the puzzle for autonomous driving. They are also deeply related to the allocation of right-of-way and the determination of accident liability. Therefore, Intel/Mobileye designed the responsibility-sensitive safety (RSS) framework to further enhance the safety regulation of autonomous driving, which mathematically defines rules for autonomous vehicles (AVs) behaviors in various traffic scenarios. However, the RSS framework's rules are relatively rudimentary in certain scenarios characterized by interaction uncertainty, especially those requiring collaborative driving during emergency collision avoidance. Besides, the integration of the RSS framework with motion planning is rarely discussed in current studies. Therefore, we proposed a rule-adherence motion planner (RAMP) based on the extended RSS (eRSS) regulation for non-connected and connected AVs in merging and emergency-avoiding scenarios. The simulation results indicate that the proposed method can achieve faster and safer lane merging performance (53.0% shorter merging length and a 73.5% decrease in merging time), and allows for more stable steering maneuvers in emergency collision avoidance, resulting in smoother paths for ego vehicle and surrounding vehicles.

Translated Abstract:
자율 주행에서 운전 안전과 책임 결정은 매우 중요한 요소야. 이건 우선권 배분과 사고 책임 결정과도 깊은 관련이 있어. 그래서 Intel/Mobileye는 책임 민감 안전(RSS) 프레임워크를 만들었어. 이 프레임워크는 자율주행차가 다양한 교통 상황에서 어떻게 행동해야 하는지를 수학적으로 정의해.

하지만 RSS 프레임워크의 규칙은 상호작용 불확실성이 있는 특정 상황에서는 좀 단순해. 특히 긴급 충돌 회피를 위한 협력 주행이 필요한 경우에는 더 그렇지. 그리고 현재 연구에서는 RSS 프레임워크와 모션 플래닝을 통합하는 것에 대해 잘 다루어지지 않고 있어.

그래서 우리는 비연결 및 연결 자율주행차가 합류하거나 긴급 회피 상황에서 사용할 수 있는 확장된 RSS(eRSS) 규정을 기반으로 한 규칙 준수 모션 플래너(RAMP)를 제안했어. 시뮬레이션 결과에 따르면, 제안한 방법이 더 빠르고 안전한 차선 합류 성능을 보여줬고(합류 길이가 53.0% 짧아지고, 합류 시간은 73.5% 줄어듦), 긴급 충돌 회피 시 더 안정적인 조향을 가능하게 해서 자차와 주변 차량의 경로가 더 부드러워졌어.

================================================================================

URL: https://arxiv.org/abs/2409.02502
Title: Dispelling Four Challenges in Inertial Motion Tracking with One Recurrent Inertial Graph-based Estimator (RING)

Original Abstract:
In this paper, we extend the Recurrent Inertial Graph-based Estimator (RING), a novel neural-network-based solution for Inertial Motion Tracking (IMT), to generalize across a large range of sampling rates, and we demonstrate that it can overcome four real-world challenges: inhomogeneous magnetic fields, sensor-to-segment misalignment, sparse sensor setups, and nonrigid sensor attachment. RING can estimate the rotational state of a three-segment kinematic chain with double hinge joints from inertial data, and achieves an experimental mean-absolute-(tracking)-error of 8.10 +/- 1.19 degrees if all four challenges are present simultaneously. The network is trained on simulated data yet evaluated on experimental data, highlighting its remarkable ability to zero-shot generalize from simulation to experiment. We conduct an ablation study to analyze the impact of each of the four challenges on RING's performance, we showcase its robustness to varying sampling rates, and we demonstrate that RING is capable of real-time operation. This research not only advances IMT technology by making it more accessible and versatile but also enhances its potential for new application domains including non-expert use of sparse IMT with nonrigid sensor attachments in unconstrained environments.

Translated Abstract:
이 논문에서는 Inertial Motion Tracking (IMT)을 위한 새로운 신경망 기반 솔루션인 Recurrent Inertial Graph-based Estimator (RING)를 발전시켜서 다양한 샘플링 속도에 맞춰 일반화하는 방법을 제시해. 그리고 RING이 네 가지 실제 문제를 해결할 수 있음을 보여줘: 불균일한 자기장, 센서와 세그먼트 간의 정렬 문제, 드문 센서 배치, 그리고 비강체 센서 부착 문제.

RING은 관성 데이터를 이용해 세 개의 세그먼트로 이루어진 이중 힌지 조인트의 회전 상태를 추정할 수 있어. 만약 네 가지 문제가 동시에 발생하면, 실험에서 평균 절대 추적 오차는 8.10 +/- 1.19 도가 나와. 이 네트워크는 시뮬레이션 데이터로 훈련되지만 실험 데이터로 평가되어 시뮬레이션에서 실험으로의 제로샷 일반화 능력이 뛰어난 것을 보여줘.

우리는 RING의 성능에 미치는 네 가지 문제의 영향을 분석하기 위해 분석 연구를 진행했고, 다양한 샘플링 속도에 대한 강인성을 보여줬어. RING은 실시간 작동도 가능하다는 걸 입증했어. 이 연구는 IMT 기술을 더 접근 가능하고 다재다능하게 만들어서, 비전문가가 비강체 센서를 이용한 드문 IMT를 자유로운 환경에서 사용할 수 있는 새로운 응용 분야로 발전할 수 있는 가능성을 높여.

================================================================================

URL: https://arxiv.org/abs/2409.02444
Title: USV-AUV Collaboration Framework for Underwater Tasks under Extreme Sea Conditions

Original Abstract:
Autonomous underwater vehicles (AUVs) are valuable for ocean exploration due to their flexibility and ability to carry communication and detection units. Nevertheless, AUVs alone often face challenges in harsh and extreme sea conditions. This study introduces a unmanned surface vehicle (USV)-AUV collaboration framework, which includes high-precision multi-AUV positioning using USV path planning via Fisher information matrix optimization and reinforcement learning for multi-AUV cooperative tasks. Applied to a multi-AUV underwater data collection task scenario, extensive simulations validate the framework's feasibility and superior performance, highlighting exceptional coordination and robustness under extreme sea conditions. The simulation code will be made available as open-source to foster future research in this area.

Translated Abstract:
자율 수중 차량(AUV)은 바다 탐사에 정말 유용해. 유연성도 좋고, 통신 및 탐지 장비를 실을 수 있는 능력이 있거든. 하지만 AUV 혼자서는 힘든 바다의 거친 환경에서 어려움을 겪기도 해.

이 연구에서는 무인 수상 차량(USV)과 AUV의 협력 구조를 소개해. 여기서는 USV 경로 계획을 통해 피셔 정보 행렬 최적화와 강화 학습을 이용해서 AUV의 위치를 정확하게 잡고, 여러 AUV가 협력할 수 있는 작업을 수행할 수 있게 해. 

다중 AUV가 수중 데이터 수집을 하는 시나리오에 적용해봤는데, 많은 시뮬레이션을 통해 이 구조의 실행 가능성과 뛰어난 성능을 검증했어. 특히 극한의 바다 환경에서도 뛰어난 협동과 강인함을 보여줬어. 이 시뮬레이션 코드는 오픈소스로 제공할 예정이라서, 앞으로 이 분야에서 연구가 더 활발해질 거야.

================================================================================

URL: https://arxiv.org/abs/2409.02437
Title: Fuzzy Logic Control for Indoor Navigation of Mobile Robots

Original Abstract:
Autonomous mobile robots have many applications in indoor unstructured environment, wherein optimal movement of the robot is needed. The robot therefore needs to navigate in unknown and dynamic environments. This paper presents an implementation of fuzzy logic controller for navigation of mobile robot in an unknown dynamically cluttered environment. Fuzzy logic controller is used here as it is capable of making inferences even under uncertainties. It helps in rule generation and decision making process in order to reach the goal position under various situations. Sensor readings from the robot and the desired direction of motion are inputs to the fuzz logic controllers and the acceleration of the respective wheels are the output of the controller. Hence, the mobile robot avoids obstacles and reaches the goal position. Keywords: Fuzzy Logic Controller, Membership Functions, Takagi-Sugeno-Kang FIS, Centroid Defuzzification

Translated Abstract:
자율 이동 로봇은 실내의 비구조적인 환경에서 다양한 용도로 사용돼. 그래서 로봇의 최적 움직임이 필요해. 로봇은 낯선 환경에서 움직여야 하고, 그런 환경은 동적으로 변할 수 있어.

이 논문에서는 비정형으로 어수선한 환경에서 이동 로봇을 위한 퍼지 논리 제어기를 구현한 내용을 다루고 있어. 퍼지 논리 제어기는 불확실한 상황에서도 추론을 할 수 있어서 유용해. 이 제어기는 다양한 상황에서 목표 위치에 도달하기 위한 규칙 생성과 의사결정 과정을 도와줘.

로봇의 센서 데이터와 원하는 이동 방향이 퍼지 논리 제어기의 입력으로 사용되고, 각 바퀴의 가속도가 제어기의 출력이 돼. 이렇게 해서 이동 로봇은 장애물을 피하고 목표 위치에 도달할 수 있어. 

주요 용어: 퍼지 논리 제어기, 멤버십 함수, 다카기-스구노-강 퍼지 추론 시스템, 중심 탈퍼지.

================================================================================

URL: https://arxiv.org/abs/2409.02436
Title: Occlusion-Based Cooperative Transport for Concave Objects with a Swarm of Miniature Mobile Robots

Original Abstract:
An occlusion based strategy for collective transport of a concave object using a swarm of mobile robots has been proposed in this paper. We aim to overcome the challenges of transporting concave objects using decentralized approach. The interesting aspect of this task is that the agents have no prior knowledge about the geometry of the object and do not explicitly communicate with each other. The concept is to eliminate the concavity of the object by filling a number of robots in its cavity and then carry out an occlusion based transport strategy on the newly formed convex object or "pseudo object". We divide our work into two parts: concavity filling of various concave objects and occlusion based collective transport of convex objects.

Translated Abstract:
이 논문에서는 모바일 로봇 swarm을 사용해 오목한 물체를 집단으로 운반하는 방법을 제안했어. 우리는 분산 방식으로 오목한 물체를 운반하는 데 필요한 여러 가지 어려움을 극복하는 걸 목표로 해.

이 작업의 흥미로운 점은, 로봇들이 물체의 형태에 대한 사전 지식이 없고 서로 명확하게 소통하지 않는다는 거야. 이 방법의 핵심은 여러 로봇들이 오목한 부분을 채워서 물체의 오목함을 없애고, 그걸 바탕으로 새롭게 형성된 볼록한 물체나 "유사 물체"를 가지고 운반하는 거야.

우리 연구는 두 부분으로 나누어져 있어: 다양한 오목한 물체의 오목함을 채우는 것과, 그로 만들어진 볼록한 물체의 오클루전 기반 집단 운반이야.

================================================================================

URL: https://arxiv.org/abs/2409.02383
Title: Reinforcement Learning for Wheeled Mobility on Vertically Challenging Terrain

Original Abstract:
Off-road navigation on vertically challenging terrain, involving steep slopes and rugged boulders, presents significant challenges for wheeled robots both at the planning level to achieve smooth collision-free trajectories and at the control level to avoid rolling over or getting stuck. Considering the complex model of wheel-terrain interactions, we develop an end-to-end Reinforcement Learning (RL) system for an autonomous vehicle to learn wheeled mobility through simulated trial-and-error experiences. Using a custom-designed simulator built on the Chrono multi-physics engine, our approach leverages Proximal Policy Optimization (PPO) and a terrain difficulty curriculum to refine a policy based on a reward function to encourage progress towards the goal and penalize excessive roll and pitch angles, which circumvents the need of complex and expensive kinodynamic modeling, planning, and control. Additionally, we present experimental results in the simulator and deploy our approach on a physical Verti-4-Wheeler (V4W) platform, demonstrating that RL can equip conventional wheeled robots with previously unrealized potential of navigating vertically challenging terrain.

Translated Abstract:
수직적으로 도전적인 지형에서 오프로드 내비게이션은 휠 로봇에게 큰 도전 과제를 안겨줍니다. 가파른 경사와 울퉁불퉁한 바위가 있는 환경에서 부드럽고 충돌 없는 경로를 계획하는 것뿐만 아니라, 조작 단계에서도 넘어지거나 갇히지 않도록 해야 합니다.

우리는 바퀴와 지형 간의 복잡한 상호작용 모델을 고려하여, 자율주행 차량이 시뮬레이션을 통해 시행착오를 경험하며 바퀴 구동 능력을 배우도록 하는 종단 간 강화 학습(RL) 시스템을 개발했습니다. Chrono 멀티 물리 엔진을 기반으로 한 맞춤형 시뮬레이터를 사용하여, 우리의 접근 방식은 Proximal Policy Optimization (PPO)과 지형 난이도 커리큘럼을 활용합니다. 이를 통해 목표를 향한 진행을 장려하고 지나치게 기울어지거나 뒤집히는 각도를 처벌하는 보상 함수를 기반으로 정책을 다듬습니다. 이렇게 하면 복잡하고 비싼 운동 동역학 모델링, 계획, 제어를 피할 수 있습니다.

또한, 우리는 시뮬레이터에서의 실험 결과를 제시하고, 물리적 Verti-4-Wheeler (V4W) 플랫폼에 우리의 접근 방식을 배포했습니다. 이로써 RL이 기존의 휠 로봇에게 수직적으로 도전적인 지형을 탐색할 수 있는 가능성을 제공할 수 있음을 보여줍니다.

================================================================================

URL: https://arxiv.org/abs/2409.02337
Title: Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback

Original Abstract:
Ultrasound is widely employed for clinical intervention and diagnosis, due to its advantages of offering non-invasive, radiation-free, and real-time imaging. However, the accessibility of this dexterous procedure is limited due to the substantial training and expertise required of operators. The robotic ultrasound (RUS) offers a viable solution to address this limitation; nonetheless, achieving human-level proficiency remains challenging. Learning from demonstrations (LfD) methods have been explored in RUS, which learns the policy prior from a dataset of offline demonstrations to encode the mental model of the expert sonographer. However, active engagement of experts, i.e. Coaching, during the training of RUS has not been explored thus far. Coaching is known for enhancing efficiency and performance in human training. This paper proposes a coaching framework for RUS to amplify its performance. The framework combines DRL (self-supervised practice) with sparse expert's feedback through coaching. The DRL employs an off-policy Soft Actor-Critic (SAC) network, with a reward based on image quality rating. The coaching by experts is modeled as a Partially Observable Markov Decision Process (POMDP), which updates the policy parameters based on the correction by the expert. The validation study on phantoms showed that coaching increases the learning rate by $25\%$ and the number of high-quality image acquisition by $74.5\%$.

Translated Abstract:
초음파는 비침습적이고 방사선이 없으며 실시간 이미지를 제공하는 장점 덕분에 임상 개입과 진단에 널리 사용되고 있어. 하지만 이 복잡한 절차는 조작자에게 필요한 훈련과 전문성이 많아서 접근성이 제한돼. 

로봇 초음파(RUS)는 이 문제를 해결할 수 있는 좋은 방법이긴 한데, 인간 수준의 능력을 갖추는 건 여전히 어려워. 그래서 RUS에서 시연 학습(LfD) 방법이 연구되고 있는데, 이건 전문가의 초음파 시연 데이터를 바탕으로 정책을 학습해 전문가의 정신 모델을 인코딩하는 방식이야. 하지만 RUS 훈련 중에 전문가의 적극적인 참여, 즉 코칭이 아직 연구되지 않았다는 점이 문제야. 코칭은 사람의 훈련에서 효율성과 성과를 높이는 데 도움을 주는 것으로 알려져 있어.

이 논문에서는 RUS의 성과를 높이기 위한 코칭 프레임워크를 제안해. 이 프레임워크는 자기 지도 학습(DRL)과 전문가의 피드백을 결합해. DRL은 이미지 품질 평점에 기반한 보상을 사용하는 오프 정책 소프트 액터-크리틱(SAC) 네트워크를 사용해. 전문가의 코칭은 부분 관찰 가능한 마르코프 결정 과정(POMDP)으로 모델링돼서, 전문가의 수정에 따라 정책 매개변수를 업데이트해. 

가상 모델을 이용한 검증 연구 결과, 코칭을 통해 학습 속도가 25% 증가하고 고품질 이미지 획득 수가 74.5% 증가한 것을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.02334
Title: YoloTag: Vision-based Robust UAV Navigation with Fiducial Markers

Original Abstract:
By harnessing fiducial markers as visual landmarks in the environment, Unmanned Aerial Vehicles (UAVs) can rapidly build precise maps and navigate spaces safely and efficiently, unlocking their potential for fluent collaboration and coexistence with humans. Existing fiducial marker methods rely on handcrafted feature extraction, which sacrifices accuracy. On the other hand, deep learning pipelines for marker detection fail to meet real-time runtime constraints crucial for navigation applications. In this work, we propose YoloTag \textemdash a real-time fiducial marker-based localization system. YoloTag uses a lightweight YOLO v8 object detector to accurately detect fiducial markers in images while meeting the runtime constraints needed for navigation. The detected markers are then used by an efficient perspective-n-point algorithm to estimate UAV states. However, this localization system introduces noise, causing instability in trajectory tracking. To suppress noise, we design a higher-order Butterworth filter that effectively eliminates noise through frequency domain analysis. We evaluate our algorithm through real-robot experiments in an indoor environment, comparing the trajectory tracking performance of our method against other approaches in terms of several distance metrics.

Translated Abstract:
무인 항공기(UAV)가 환경에서 시각적 랜드마크로 피듀셜 마커를 활용하면, 빠르고 정확한 지도를 만들고 안전하게 공간을 탐색할 수 있어 인간과의 협력과 공존 가능성을 높일 수 있어. 기존의 피듀셜 마커 방법들은 수작업으로 특징을 추출하는데, 이건 정확성을 떨어뜨려. 반면에, 마커 탐지를 위한 딥러닝 파이프라인은 내비게이션 애플리케이션에 중요한 실시간 실행 조건을 충족하지 못해.

이 연구에서는 YoloTag라는 실시간 피듀셜 마커 기반 위치 추정 시스템을 제안해. YoloTag는 경량 YOLO v8 객체 탐지기를 사용해서 이미지에서 피듀셜 마커를 정확하게 탐지하고, 내비게이션에 필요한 실행 조건도 충족해. 탐지된 마커는 효율적인 관점-점 알고리즘에 의해 UAV 상태를 추정하는 데 사용돼. 하지만 이 위치 추정 시스템은 노이즈를 초래해서 궤적 추적에 불안정성을 일으켜.

그래서 우리는 노이즈를 억제하기 위해 주파수 영역 분석을 통해 효과적으로 노이즈를 제거하는 고차 버터워스 필터를 설계했어. 우리는 실내 환경에서 실제 로봇 실험을 통해 우리의 알고리즘을 평가하고, 여러 거리 메트릭을 기준으로 다른 접근 방식과 궤적 추적 성능을 비교했어.

================================================================================

URL: https://arxiv.org/abs/2409.02324
Title: Visual Servoing for Robotic On-Orbit Servicing: A Survey

Original Abstract:
On-orbit servicing (OOS) activities will power the next big step for sustainable exploration and commercialization of space. Developing robotic capabilities for autonomous OOS operations is a priority for the space industry. Visual Servoing (VS) enables robots to achieve the precise manoeuvres needed for critical OOS missions by utilizing visual information for motion control. This article presents an overview of existing VS approaches for autonomous OOS operations with space manipulator systems (SMS). We divide the approaches according to their contribution to the typical phases of a robotic OOS mission: a) Recognition, b) Approach, and c) Contact. We also present a discussion on the reviewed VS approaches, identifying current trends. Finally, we highlight the challenges and areas for future research on VS techniques for robotic OOS.

Translated Abstract:
우주에서의 서비스 활동(OOS)은 지속 가능한 탐사와 상업화를 위한 다음 큰 단계를 이끌어낼 거야. 그래서 자율 OOS 작업을 위한 로봇 기술 개발이 우주 산업의 최우선 과제가 되고 있어. 

Visual Servoing (VS)은 로봇이 OOS 임무를 수행할 때 필요한 정밀한 동작을 할 수 있도록 시각 정보를 이용해 모션 제어를 가능하게 해. 이 글에서는 우주 조작기 시스템(SMS)을 이용한 자율 OOS 작업을 위한 기존 VS 접근법에 대한 개요를 제시해.

우리는 이 접근법들을 로봇 OOS 임무의 일반적인 단계에 따라 나눴어: a) 인식, b) 접근, c) 접촉. 그리고 리뷰한 VS 접근법에 대한 논의도 포함시켜서 현재의 경향을 파악했어. 마지막으로, 로봇 OOS를 위한 VS 기술의 도전 과제와 향후 연구 분야도 강조했어.

================================================================================

URL: https://arxiv.org/abs/2409.02312
Title: Investigating Mixed Reality for Communication Between Humans and Mobile Manipulators

Original Abstract:
This article investigates mixed reality (MR) to enhance human-robot collaboration (HRC). The proposed solution adopts MR as a communication layer to convey a mobile manipulator's intentions and upcoming actions to the humans with whom it interacts, thus improving their collaboration. A user study involving 20 participants demonstrated the effectiveness of this MR-focused approach in facilitating collaborative tasks, with a positive effect on overall collaboration performances and human satisfaction.

Translated Abstract:
이 논문은 혼합 현실(MR)을 활용해 인간-로봇 협업(HRC)을 향상시키는 방법을 연구해. 제안한 솔루션은 MR을 소통의 층으로 사용해서 로봇의 의도와 다가오는 행동을 사람들이 이해할 수 있도록 전달해, 그래서 협업이 더 잘 되도록 도와줘.

20명의 참가자가 참여한 사용자 연구 결과, MR 중심의 접근 방식이 협업 과제를 수행하는 데 효과적이라는 게 나타났어. 이 방법은 전체적인 협업 성과와 인간의 만족도에도 긍정적인 영향을 미쳤어.

================================================================================

URL: https://arxiv.org/abs/2409.02305
Title: Kinesthetic Teaching in Robotics: a Mixed Reality Approach

Original Abstract:
As collaborative robots become more common in manufacturing scenarios and adopted in hybrid human-robot teams, we should develop new interaction and communication strategies to ensure smooth collaboration between agents. In this paper, we propose a novel communicative interface that uses Mixed Reality as a medium to perform Kinesthetic Teaching (KT) on any robotic platform. We evaluate our proposed approach in a user study involving multiple subjects and two different robots, comparing traditional physical KT with holographic-based KT through user experience questionnaires and task-related metrics.

Translated Abstract:
협동 로봇이 제조 현장에 점점 더 많이 사용되고, 인간-로봇 하이브리드 팀에서도 채택되고 있어. 그래서 우리는 이러한 에이전트들 간의 원활한 협력을 보장하기 위해 새로운 상호작용 및 커뮤니케이션 전략을 개발해야 해.

이 논문에서는 혼합 현실을 매개체로 사용하여 어떤 로봇 플랫폼에서도 체감 가르치기(Kinesthetic Teaching, KT)를 수행할 수 있는 새로운 커뮤니케이션 인터페이스를 제안해. 우리는 여러 사용자를 포함한 사용자 연구에서 이 접근 방식을 평가했고, 두 가지 다른 로봇을 사용했어. 전통적인 물리적 KT와 홀로그램 기반 KT를 비교하면서 사용자 경험 설문조사와 작업 관련 지표를 통해 결과를 분석했지.

================================================================================

URL: https://arxiv.org/abs/2409.02290
Title: Unsupervised Welding Defect Detection Using Audio And Video

Original Abstract:
In this work we explore the application of AI to robotic welding. Robotic welding is a widely used technology in many industries, but robots currently do not have the capability to detect welding defects which get introduced due to various reasons in the welding process. We describe how deep-learning methods can be applied to detect weld defects in real-time by recording the welding process with microphones and a camera. Our findings are based on a large database with more than 4000 welding samples we collected which covers different weld types, materials and various defect categories. All deep learning models are trained in an unsupervised fashion because the space of possible defects is large and the defects in our data may contain biases. We demonstrate that a reliable real-time detection of most categories of weld defects is feasible both from audio and video, with improvements achieved by combining both modalities. Specifically, the multi-modal approach achieves an average Area-under-ROC-Curve (AUC) of 0.92 over all eleven defect types in our data. We conclude the paper with an analysis of the results by defect type and a discussion of future work.

Translated Abstract:
이번 연구에서는 로봇 용접에 AI를 적용하는 방법을 탐구했어. 로봇 용접은 여러 산업에서 널리 사용되는 기술인데, 지금까지 로봇은 용접 과정에서 생기는 결함을 감지할 수 있는 능력이 없었어. 

우리는 마이크와 카메라로 용접 과정을 기록해서 딥러닝 방법을 적용하여 실시간으로 용접 결함을 감지하는 방법을 설명해. 이 연구는 우리가 수집한 4000개 이상의 다양한 용접 샘플로 구성된 큰 데이터베이스를 바탕으로 하고 있어. 샘플은 다양한 용접 유형과 재료, 결함 종류를 포함하고 있어. 

모든 딥러닝 모델은 비지도 학습 방식으로 훈련했어. 왜냐하면 결함의 종류가 다양하고, 우리의 데이터에 있는 결함이 편향될 수 있기 때문이야. 우리는 오디오와 비디오를 모두 활용해서 대부분의 결함 카테고리를 신뢰성 있게 실시간으로 감지할 수 있다는 것을 보여줬어. 두 가지 모달리티를 결합해서 더 나은 결과를 얻을 수 있었고, 특히 멀티모달 접근 방식은 데이터에 있는 11가지 결함 유형에 대해 평균 0.92의 ROC 곡선 아래 면적(AUC)을 달성했어.

마지막으로, 결함 유형별 결과 분석과 향후 연구 방향에 대한 논의로 논문을 마무리했어.

================================================================================

URL: https://arxiv.org/abs/2409.02273
Title: SlipNet: Slip Cost Map for Autonomous Navigation on Heterogeneous Deformable Terrains

Original Abstract:
Autonomous space rovers face significant challenges when navigating deformable and heterogeneous terrains during space exploration. The variability in terrain types, influenced by different soil properties, often results in severe wheel slip, compromising navigation efficiency and potentially leading to entrapment. This paper proposes SlipNet, an approach for predicting slip in segmented regions of heterogeneous deformable terrain surfaces to enhance navigation algorithms. Unlike previous methods, SlipNet does not depend on prior terrain classification, reducing prediction errors and misclassifications through dynamic terrain segmentation and slip assignment during deployment while maintaining a history of terrain classes. This adaptive reclassification mechanism has improved prediction performance. Extensive simulation results demonstrate that our model (DeepLab v3+ + SlipNet) achieves better slip prediction performance than the TerrainNet, with a lower mean absolute error (MAE) in five terrain sample tests.

Translated Abstract:
자율 우주 로버는 우주 탐사 중에 변형 가능한 다양한 지형을 탐색할 때 큰 어려움에 부딪혀. 지형의 종류가 다르면 토양의 특성이 달라지는데, 이로 인해 바퀴가 심하게 미끄러지는 경우가 많아. 이렇게 되면 탐색 효율성이 떨어지고, 심지어 갇히는 상황이 발생할 수 있어.

이 연구에서는 SlipNet이라는 방법을 제안해. SlipNet은 다양한 변형 가능한 지형 표면의 구분된 영역에서 미끄러짐을 예측해서 탐색 알고리즘을 개선하는 데 도움을 줘. 이전 방법들과는 달리, SlipNet은 미리 지형을 분류할 필요가 없어서 예측 오류와 잘못된 분류를 줄여줘. 이건 동적인 지형 분할과 미끄러짐 할당을 통해 이루어지며, 지형 클래스의 역사도 계속 유지해.

이런 적응형 재분류 메커니즘 덕분에 예측 성능이 향상됐어. 여러 시뮬레이션 결과를 보면, 우리의 모델(DeepLab v3+ + SlipNet)이 TerrainNet보다 더 좋은 미끄러짐 예측 성능을 보여주고, 다섯 가지 지형 샘플 테스트에서 평균 절대 오차(MAE)가 더 낮았어.

================================================================================

URL: https://arxiv.org/abs/2409.02647
Title: Learning-Based Error Detection System for Advanced Vehicle Instrument Cluster Rendering

Original Abstract:
The automotive industry is currently expanding digital display options with every new model that comes onto the market. This entails not just an expansion in dimensions, resolution, and customization choices, but also the capability to employ novel display effects like overlays while assembling the content of the display cluster. Unfortunately, this raises the need for appropriate monitoring systems that can detect rendering errors and apply appropriate countermeasures when required. Classical solutions such as Cyclic Redundancy Checks (CRC) will soon be no longer viable as any sort of alpha blending, warping of scaling of content can cause unwanted CRC violations. Therefore, we propose a novel monitoring approach to verify correctness of displayed content using telltales (e.g. warning signs) as example. It uses a learning-based approach to separate "good" telltales, i.e. those that a human driver will understand correctly, and "corrupted" telltales, i.e. those that will not be visible or perceived correctly. As a result, it possesses inherent resilience against individual pixel errors and implicitly supports changing backgrounds, overlay or scaling effects. This is underlined by our experimental study where all "corrupted" test patterns were correctly classified, while no false alarms were triggered.

Translated Abstract:
자동차 산업은 새로운 모델이 출시될 때마다 디지털 디스플레이 옵션을 계속 확장하고 있어. 이건 단순히 크기, 해상도, 그리고 개인화 옵션의 확대만을 의미하는 게 아니라, 디스플레이 클러스터의 내용을 조합할 때 오버레이 같은 새로운 디스플레이 효과를 사용할 수 있는 능력도 포함돼. 

하지만, 이로 인해 렌더링 오류를 감지하고 필요할 때 적절한 대응을 할 수 있는 모니터링 시스템이 필요해져. 전통적인 솔루션인 순환 여분 검사(CRC)는 더 이상 유효하지 않을 거야. 알파 블렌딩이나 콘텐츠의 왜곡, 스케일링이 발생하면 원치 않는 CRC 위반이 생길 수 있거든. 

그래서 우리는 경고 신호 같은 예시를 사용해서 표시된 콘텐츠의 정확성을 검증하는 새로운 모니터링 접근 방식을 제안해. 이 방법은 학습 기반 접근 방식을 이용해서, 인간 운전자가 올바르게 이해할 수 있는 "좋은" 경고 신호와 제대로 보이지 않거나 잘 인식되지 않는 "손상된" 경고 신호를 구분해. 

결과적으로 이 방법은 개별 픽셀 오류에 대해 내재적인 강인성을 가지고 있고, 배경 변화나 오버레이, 스케일링 효과도 암묵적으로 지원해. 이건 우리의 실험 연구에서도 확인됐어. 모든 "손상된" 테스트 패턴이 정확하게 분류되었고, 잘못된 경고는 전혀 발생하지 않았어.

================================================================================

URL: https://arxiv.org/abs/2409.02598
Title: SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments

Original Abstract:
Vision-based surgical navigation has received increasing attention due to its non-invasive, cost-effective, and flexible advantages. In particular, a critical element of the vision-based navigation system is tracking surgical instruments. Compared with 2D instrument tracking methods, 3D instrument tracking has broader value in clinical practice, but is also more challenging due to weak texture, occlusion, and lack of Computer-Aided Design (CAD) models for 3D registration. To solve these challenges, we propose the SurgTrack, a two-stage 3D instrument tracking method for CAD-free and robust real-world applications. In the first registration stage, we incorporate an Instrument Signed Distance Field (SDF) modeling the 3D representation of instruments, achieving CAD-freed 3D registration. Due to this, we can obtain the location and orientation of instruments in the 3D space by matching the video stream with the registered SDF model. In the second tracking stage, we devise a posture graph optimization module, leveraging the historical tracking results of the posture memory pool to optimize the tracking results and improve the occlusion robustness. Furthermore, we collect the Instrument3D dataset to comprehensively evaluate the 3D tracking of surgical instruments. The extensive experiments validate the superiority and scalability of our SurgTrack, by outperforming the state-of-the-arts with a remarkable improvement. The code and dataset are available at this https URL.

Translated Abstract:
시각 기반 수술 내비게이션이 비침습적이고 비용 효율적이며 유연하다는 장점 덕분에 점점 더 주목받고 있어. 특히, 시각 기반 내비게이션 시스템에서 중요한 요소는 수술 도구 추적이야. 2D 도구 추적 방법에 비해 3D 도구 추적은 임상에서 더 넓은 가치를 가지지만, 텍스처가 약하고, 가려짐 현상, 그리고 3D 등록을 위한 CAD 모델 부족 때문에 더 어려워.

이런 문제를 해결하기 위해 우리는 SurgTrack이라는 두 단계의 3D 도구 추적 방법을 제안해. 첫 번째 등록 단계에서는 도구의 3D 표현을 모델링하는 Instrument Signed Distance Field (SDF)를 사용해서 CAD 없이 3D 등록을 달성해. 이렇게 해서 비디오 스트림을 등록된 SDF 모델과 매칭함으로써 3D 공간에서 도구의 위치와 방향을 얻을 수 있어.

두 번째 추적 단계에서는 자세 그래프 최적화 모듈을 개발했어. 이 모듈은 자세 메모리 풀의 과거 추적 결과를 활용해서 추적 결과를 최적화하고 가려짐에 대한 내구성을 높여. 게다가, 우리는 수술 도구의 3D 추적을 종합적으로 평가하기 위해 Instrument3D 데이터셋을 수집했어.

광범위한 실험을 통해 우리의 SurgTrack이 기존 방법들보다 우수하고 확장 가능하다는 것을 입증했어. 성능이 눈에 띄게 개선되었어. 코드와 데이터셋은 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.02561
Title: Vision-Language Navigation with Continual Learning

Original Abstract:
Vision-language navigation (VLN) is a critical domain within embedded intelligence, requiring agents to navigate 3D environments based on natural language instructions. Traditional VLN research has focused on improving environmental understanding and decision accuracy. However, these approaches often exhibit a significant performance gap when agents are deployed in novel environments, mainly due to the limited diversity of training data. Expanding datasets to cover a broader range of environments is impractical and costly. We propose the Vision-Language Navigation with Continual Learning (VLNCL) paradigm to address this challenge. In this paradigm, agents incrementally learn new environments while retaining previously acquired knowledge. VLNCL enables agents to maintain an environmental memory and extract relevant knowledge, allowing rapid adaptation to new environments while preserving existing information. We introduce a novel dual-loop scenario replay method (Dual-SR) inspired by brain memory replay mechanisms integrated with VLN agents. This method facilitates consolidating past experiences and enhances generalization across new tasks. By utilizing a multi-scenario memory buffer, the agent efficiently organizes and replays task memories, thereby bolstering its ability to adapt quickly to new environments and mitigating catastrophic forgetting. Our work pioneers continual learning in VLN agents, introducing a novel experimental setup and evaluation metrics. We demonstrate the effectiveness of our approach through extensive evaluations and establish a benchmark for the VLNCL paradigm. Comparative experiments with existing continual learning and VLN methods show significant improvements, achieving state-of-the-art performance in continual learning ability and highlighting the potential of our approach in enabling rapid adaptation while preserving prior knowledge.

Translated Abstract:
비전-언어 내비게이션(VLN)은 임베디드 인텔리전스에서 중요한 분야로, 에이전트가 자연어 지침을 바탕으로 3D 환경을 탐색해야 해. 기존의 VLN 연구는 환경 이해와 의사결정 정확성을 높이는 데 집중했어. 하지만 이러한 접근법은 에이전트를 새로운 환경에 배치했을 때 성능 차이가 크게 나타나. 그 이유는 훈련 데이터의 다양성이 부족하기 때문이야. 다양한 환경을 다루는 데이터셋을 확장하는 건 비현실적이고 비용이 많이 들어.

우리는 이 문제를 해결하기 위해 지속 학습을 활용한 비전-언어 내비게이션(VLNCL) 패러다임을 제안해. 이 패러다임에서는 에이전트가 새로운 환경을 점진적으로 배우면서 이전에 습득한 지식을 유지해. VLNCL은 에이전트가 환경에 대한 기억을 유지하고 관련 지식을 추출할 수 있게 해줘. 그래서 새로운 환경에 빠르게 적응하면서도 기존 정보를 보존할 수 있어.

우리는 뇌의 기억 재생 메커니즘에서 영감을 받은 새로운 이중 루프 시나리오 재생 방법(Dual-SR)을 소개해. 이 방법은 과거 경험을 정리하고 새로운 작업에 대한 일반화를 향상시키는 데 도움을 줘. 다중 시나리오 메모리 버퍼를 활용해서 에이전트는 작업 기억을 효율적으로 정리하고 재생할 수 있어. 이렇게 하면 새로운 환경에 빠르게 적응하고 치명적인 망각을 줄일 수 있어.

우리의 연구는 VLN 에이전트에서 지속 학습을 선도하며, 새로운 실험 설정과 평가 지표를 도입해. 우리는 광범위한 평가를 통해 우리의 접근법의 효과를 입증하고 VLNCL 패러다임에 대한 기준을 확립했어. 기존의 지속 학습 및 VLN 방법과의 비교 실험에서 상당한 개선이 나타났고, 지속 학습 능력에서 최첨단 성능을 달성했어. 이는 우리의 접근법이 이전 지식을 보존하면서 빠른 적응을 가능하게 하는 잠재력을 강조해.

================================================================================

URL: https://arxiv.org/abs/2409.02522
Title: Cog-GA: A Large Language Models-based Generative Agent for Vision-Language Navigation in Continuous Environments

Original Abstract:
Vision Language Navigation in Continuous Environments (VLN-CE) represents a frontier in embodied AI, demanding agents to navigate freely in unbounded 3D spaces solely guided by natural language instructions. This task introduces distinct challenges in multimodal comprehension, spatial reasoning, and decision-making. To address these challenges, we introduce Cog-GA, a generative agent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA employs a dual-pronged strategy to emulate human-like cognitive processes. Firstly, it constructs a cognitive map, integrating temporal, spatial, and semantic elements, thereby facilitating the development of spatial memory within LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints, strategically optimizing the exploration trajectory to maximize navigational efficiency. Each waypoint is accompanied by a dual-channel scene description, categorizing environmental cues into 'what' and 'where' streams as the brain. This segregation enhances the agent's attentional focus, enabling it to discern pertinent spatial information for navigation. A reflective mechanism complements these strategies by capturing feedback from prior navigation experiences, facilitating continual learning and adaptive replanning. Extensive evaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art performance and ability to simulate human-like navigation behaviors. This research significantly contributes to the development of strategic and interpretable VLN-CE agents.

Translated Abstract:
비전 언어 내비게이션 연속 환경(VLN-CE)은 구현된 AI의 최전선으로, 에이전트가 자연어 지침만으로 무한한 3D 공간에서 자유롭게 탐색해야 하는 과제를 말해. 이 작업은 다중 모달 이해, 공간 추론, 의사결정에서 여러 가지 도전 과제를 제시해.

이런 문제를 해결하기 위해 우리는 Cog-GA라는 생성 에이전트를 소개해. 이 에이전트는 VLN-CE 작업을 위해 대규모 언어 모델(LLM)을 기반으로 만들어졌어. Cog-GA는 인간처럼 사고하는 과정을 모방하기 위해 두 가지 전략을 사용해. 첫 번째로, Cog-GA는 인지 지도를 구성해. 이 지도는 시간적, 공간적, 의미적 요소를 통합해서 LLM 안에 공간 기억을 발전시킬 수 있게 해.

두 번째로, Cog-GA는 경로의 예측 메커니즘을 사용해. 이걸 통해 탐색 경로를 전략적으로 최적화하여 내비게이션 효율성을 극대화해. 각 경로 지점은 '무엇'과 '어디'라는 두 가지 채널의 장면 설명과 함께 제공되는데, 이게 마치 뇌처럼 환경 신호를 분류하게 해. 이렇게 구분함으로써 에이전트는 내비게이션에 필요한 공간 정보를 더 잘 파악할 수 있어.

또한, 반영 메커니즘이 이러한 전략을 보완해 이전 탐색 경험에서 피드백을 받아 지속적인 학습과 적응적 재계획을 도와줘. VLN-CE 벤치마크에서 실시된 광범위한 평가를 통해 Cog-GA의 최첨단 성능과 인간 같은 내비게이션 행동을 시뮬레이션할 수 있는 능력이 검증되었어. 이 연구는 전략적이고 해석 가능한 VLN-CE 에이전트를 개발하는 데 크게 기여해.

================================================================================

URL: https://arxiv.org/abs/2409.02395
Title: Deep Brain Ultrasound Ablation Thermal Dose Modeling with in Vivo Experimental Validation

Original Abstract:
Intracorporeal needle-based therapeutic ultrasound (NBTU) is a minimally invasive option for intervening in malignant brain tumors, commonly used in thermal ablation procedures. This technique is suitable for both primary and metastatic cancers, utilizing a high-frequency alternating electric field (up to 10 MHz) to excite a piezoelectric transducer. The resulting rapid deformation of the transducer produces an acoustic wave that propagates through tissue, leading to localized high-temperature heating at the target tumor site and inducing rapid cell death. To optimize the design of NBTU transducers for thermal dose delivery during treatment, numerical modeling of the acoustic pressure field generated by the deforming piezoelectric transducer is frequently employed. The bioheat transfer process generated by the input pressure field is used to track the thermal propagation of the applicator over time. Magnetic resonance thermal imaging (MRTI) can be used to experimentally validate these models. Validation results using MRTI demonstrated the feasibility of this model, showing a consistent thermal propagation pattern. However, a thermal damage isodose map is more advantageous for evaluating therapeutic efficacy. To achieve a more accurate simulation based on the actual brain tissue environment, a new finite element method (FEM) simulation with enhanced damage evaluation capabilities was conducted. The results showed that the highest temperature and ablated volume differed between experimental and simulation results by 2.1884°C (3.71%) and 0.0631 cm$^3$ (5.74%), respectively. The lowest Pearson correlation coefficient (PCC) for peak temperature was 0.7117, and the lowest Dice coefficient for the ablated area was 0.7021, indicating a good agreement in accuracy between simulation and experiment.

Translated Abstract:
내부에서 바늘을 이용한 치료 초음파(NBTU)는 악성 뇌종양 치료를 위한 최소 침습 옵션으로, 주로 열 제거 절차에서 사용돼. 이 기술은 원발성 암과 전이성 암 모두에 적합하고, 최대 10MHz의 고주파 교류 전기장을 사용해 압전 변환기를 자극해. 변환기가 빠르게 변형되면 초음파가 생성되고, 이 초음파는 조직을 통해 전파돼서 목표 종양 부위에서 고온으로 가열돼 빠른 세포 사멸을 유도해.

NBTU 변환기의 열량 전달을 최적화하기 위해서, 변형되는 압전 변환기가 만들어내는 음압장에 대한 수치 모델링이 자주 사용돼. 입력 압력장으로 생성된 생체열 전달 과정을 통해서, 시간에 따라 적용기가 어떻게 열을 전파하는지 추적할 수 있어. 자기 공명 열 이미징(MRTI)을 사용해 이 모델을 실험적으로 검증할 수 있어. MRTI를 이용한 검증 결과는 이 모델의 가능성을 보여주며, 일관된 열 전파 패턴을 나타냈어. 하지만, 열 손상 등온선 맵이 치료 효과를 평가하는 데 더 유리해.

실제 뇌 조직 환경을 바탕으로 더 정확한 시뮬레이션을 위해, 손상 평가 능력을 강화한 새로운 유한 요소법(FEM) 시뮬레이션을 진행했어. 결과는 실험과 시뮬레이션 간의 최고 온도와 절삭 부피가 각각 2.1884°C (3.71%)와 0.0631 cm³ (5.74%) 차이가 났다는 걸 보여줬어. 최고 온도에 대한 피어슨 상관 계수(PCC)의 최저 값은 0.7117이었고, 절삭 면적에 대한 다이스 계수의 최저 값은 0.7021로, 시뮬레이션과 실험 간의 정확성에서 좋은 일치를 나타냈어.

================================================================================

URL: https://arxiv.org/abs/2409.02389
Title: Multi-modal Situated Reasoning in 3D Scenes

Original Abstract:
Situation awareness is essential for understanding and reasoning about 3D scenes in embodied AI agents. However, existing datasets and benchmarks for situated understanding are limited in data modality, diversity, scale, and task scope. To address these limitations, we propose Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated question-answering pairs across 9 distinct question categories, covering complex scenarios within 3D scenes. We introduce a novel interleaved multi-modal input setting in our benchmark to provide text, image, and point cloud for situation and question description, resolving ambiguity in previous single-modality convention (e.g., text). Additionally, we devise the Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models' situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN highlight the limitations of existing vision-language models and underscore the importance of handling multi-modal interleaved inputs and situation modeling. Experiments on data scaling and cross-domain transfer further demonstrate the efficacy of leveraging MSQA as a pre-training dataset for developing more powerful situated reasoning models.

Translated Abstract:
상황 인식은 몸체 AI 에이전트가 3D 장면을 이해하고 추론하는 데 중요해. 하지만 기존의 데이터셋과 벤치마크는 데이터 유형, 다양성, 규모, 작업 범위에서 한계가 있어. 이 문제를 해결하기 위해 우리는 다중 모달 상황 질문 응답(MSQA)이라는 대규모 다중 모달 상황 추론 데이터셋을 제안해. 이 데이터셋은 3D 장면 그래프와 비전-언어 모델(VLM)을 활용해서 다양한 실제 3D 장면을 수집했어.

MSQA는 9개의 다른 질문 카테고리에서 251,000개의 질문-답변 쌍을 포함하고 있어. 이 데이터셋은 3D 장면 내의 복잡한 시나리오를 다루고 있어. 우리 벤치마크에서는 텍스트, 이미지, 포인트 클라우드를 상황과 질문 설명에 사용하기 위해 새로운 다중 모달 입력 방식을 도입했어. 이렇게 하면 이전의 단일 모달 방식(예: 텍스트)에서 생겼던 모호성을 해결할 수 있어.

또한, 우리는 모델의 상황 추론을 평가하기 위한 다중 모달 상황 다음 단계 탐색(MSNN) 벤치마크를 만들었어. MSQA와 MSNN에 대한 종합적인 평가는 기존 비전-언어 모델의 한계를 강조하고, 다중 모달 혼합 입력과 상황 모델링 처리의 중요성을 보여줘. 데이터 확장과 크로스 도메인 전이에 대한 실험은 MSQA를 사전 훈련 데이터셋으로 활용하여 더 강력한 상황 추론 모델을 개발하는 데 효과적임을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.02115
Title: Deep Neural Implicit Representation of Accessibility for Multi-Axis Manufacturing

Original Abstract:
One of the main concerns in design and process planning for multi-axis additive and subtractive manufacturing is collision avoidance between moving objects (e.g., tool assemblies) and stationary objects (e.g., a part unified with fixtures). The collision measure for various pairs of relative rigid translations and rotations between the two pointsets can be conceptualized by a compactly supported scalar field over the 6D non-Euclidean configuration space. Explicit representation and computation of this field is costly in both time and space. If we fix $O(m)$ sparsely sampled rotations (e.g., tool orientations), computation of the collision measure field as a convolution of indicator functions of the 3D pointsets over a uniform grid (i.e., voxelized geometry) of resolution $O(n^3)$ via fast Fourier transforms (FFTs) scales as in $O(mn^3 \log n)$ in time and $O(mn^3)$ in space. In this paper, we develop an implicit representation of the collision measure field via deep neural networks (DNNs). We show that our approach is able to accurately interpolate the collision measure from a sparse sampling of rotations, and can represent the collision measure field with a small memory footprint. Moreover, we show that this representation can be efficiently updated through fine-tuning to more efficiently train the network on multi-resolution data, as well as accommodate incremental changes to the geometry (such as might occur in iterative processes such as topology optimization of the part subject to CNC tool accessibility constraints).

Translated Abstract:
다축 적층 및 절삭 제조에서 디자인과 공정 계획에서 가장 큰 문제 중 하나는 이동하는 물체(예: 도구 조립체)와 정지된 물체(예: 고정구와 통합된 부품) 간의 충돌을 피하는 거야.

두 점 집합 간의 상대적인 고체 변환과 회전에 대한 충돌 측정은 6차원 비유클리드 구성 공간에서 compactly supported scalar field로 개념화할 수 있어. 이 필드를 명시적으로 표현하고 계산하는 건 시간과 공간 모두에서 비쌈. 만약 $O(m)$ 개의 희소하게 샘플링된 회전(예: 도구 방향)을 고정하면, 충돌 측정 필드를 3D 점 집합의 지시 함수의 합성곱으로 계산할 수 있어. 이때 균일한 그리드(즉, voxelized geometry)의 해상도는 $O(n^3)$이고, 빠른 푸리에 변환(FFTs)을 사용하면 시간 복잡도는 $O(mn^3 \log n)$이고 공간 복잡도는 $O(mn^3)$이야.

이 논문에서는 심층 신경망(DNNs)을 사용해 충돌 측정 필드의 암묵적 표현을 개발했어. 우리의 접근 방식이 희소한 회전 샘플링에서 충돌 측정을 정확하게 보간할 수 있는 걸 보여주고, 작은 메모리 공간으로 충돌 측정 필드를 표현할 수 있다는 걸 증명했어. 게다가 이 표현은 미세 조정을 통해 효율적으로 업데이트할 수 있어서, 다중 해상도 데이터로 네트워크를 더 효과적으로 훈련시킬 수 있고, 지오메트리의 점진적인 변화(예: CNC 도구 접근성 제약에 따른 부품의 토폴로지 최적화와 같은 반복적인 과정에서 발생할 수 있는 변화)에 잘 대응할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.02084
Title: GraspSplats: Efficient Manipulation with 3D Feature Splatting

Original Abstract:
The ability for robots to perform efficient and zero-shot grasping of object parts is crucial for practical applications and is becoming prevalent with recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap for representations to support such a capability, existing methods rely on neural fields (NeRFs) via differentiable rendering or point-based projection methods. However, we demonstrate that NeRFs are inappropriate for scene changes due to their implicitness and point-based methods are inaccurate for part localization without rendering-based optimization. To amend these issues, we propose GraspSplats. Using depth supervision and a novel reference feature computation method, GraspSplats generates high-quality scene representations in under 60 seconds. We further validate the advantages of Gaussian-based representation by showing that the explicit and optimized geometry in GraspSplats is sufficient to natively support (1) real-time grasp sampling and (2) dynamic and articulated object manipulation with point trackers. With extensive experiments on a Franka robot, we demonstrate that GraspSplats significantly outperforms existing methods under diverse task settings. In particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO, and 2D detection methods.

Translated Abstract:
로봇이 물체의 부품을 효율적으로 잡는 능력은 실제 응용 프로그램에 정말 중요해. 최근 비전-언어 모델(VLMs)의 발전 덕분에 이 기술이 점점 보편화되고 있어. 

2D에서 3D로의 전환을 위해 기존 방법들은 신경 필드(NeRFs)를 사용하지만, 이건 장면 변화에 적합하지 않아. 그리고 포인트 기반 방법은 렌더링 기반 최적화 없이 부품 위치를 정확하게 찾기 어려워. 이런 문제를 해결하기 위해 우리는 GraspSplats를 제안해. 

GraspSplats는 깊이 감독(depth supervision)과 새로운 참조 특성 계산 방법을 사용해서 60초 이내에 고품질 장면 표현을 생성해. 우리가 보여준 바에 따르면, Gaussian 기반 표현은 GraspSplats의 명확하고 최적화된 기하학이 실시간으로 그립 샘플링과 동적이고 관절이 있는 물체 조작을 지원하기에 충분하다는 걸 알 수 있어.

Franka 로봇을 사용한 다양한 실험을 통해 GraspSplats가 기존 방법들보다 훨씬 더 뛰어난 성능을 보인다는 걸 입증했어. 특히 GraspSplats는 NeRF 기반 방법인 F3RM과 LERF-TOGO, 그리고 2D 탐지 방법들보다 성능이 더 좋아.

================================================================================

URL: https://arxiv.org/abs/2409.02035
Title: A Modern Take on Visual Relationship Reasoning for Grasp Planning

Original Abstract:
Interacting with real-world cluttered scenes pose several challenges to robotic agents that need to understand complex spatial dependencies among the observed objects to determine optimal pick sequences or efficient object retrieval strategies. Existing solutions typically manage simplified scenarios and focus on predicting pairwise object relationships following an initial object detection phase, but often overlook the global context or struggle with handling redundant and missing object relations. In this work, we present a modern take on visual relational reasoning for grasp planning. We introduce D3GD, a novel testbed that includes bin picking scenes with up to 35 objects from 97 distinct categories. Additionally, we propose D3G, a new end-to-end transformer-based dependency graph generation model that simultaneously detects objects and produces an adjacency matrix representing their spatial relationships. Recognizing the limitations of standard metrics, we employ the Average Precision of Relationships for the first time to evaluate model performance, conducting an extensive experimental benchmark. The obtained results establish our approach as the new state-of-the-art for this task, laying the foundation for future research in robotic manipulation. We publicly release the code and dataset at this https URL.

Translated Abstract:
복잡한 현실 세계의 장면과 상호작용하는 것은 로봇에게 여러 가지 도전 과제가 있어. 로봇은 관찰한 물체들 간의 복잡한 공간적 의존성을 이해해야 최적의 집기 순서나 효율적인 물체 검색 전략을 결정할 수 있어. 기존의 해결책들은 보통 단순한 상황을 다루고, 초기 물체 탐지 후 쌍별 물체 관계를 예측하는 데 집중해. 하지만 전체적인 맥락을 간과하거나 중복된 관계와 누락된 관계를 처리하는 데 어려움을 겪는 경우가 많아.

이 연구에서는 물체 집기를 위한 시각적 관계 추론에 대한 현대적인 접근 방식을 제시해. D3GD라는 새로운 테스트베드를 도입하는데, 여기에는 97개 서로 다른 카테고리에서 최대 35개의 물체가 포함된 박스 픽킹 장면이 있어. 그리고 D3G라는 새로운 엔드 투 엔드 변환기 기반 의존성 그래프 생성 모델도 제안해. 이 모델은 물체를 동시에 탐지하고 그들의 공간적 관계를 나타내는 인접 행렬을 생성해.

기존의 표준 평가 지표의 한계를 인식하고, 처음으로 관계의 평균 정밀도를 사용해 모델 성능을 평가했어. 광범위한 실험 벤치마크를 수행한 결과, 우리의 접근 방식이 이 작업의 새로운 최첨단 기술로 자리 잡았고, 로봇 조작 관련 미래 연구의 기초가 되고 있어. 우리는 코드를 공개하고 데이터셋도 이 URL에서 제공할 거야.

================================================================================

URL: https://arxiv.org/abs/2409.01953
Title: Learning Resilient Formation Control of Drones with Graph Attention Network

Original Abstract:
The rapid advancement of drone technology has significantly impacted various sectors, including search and rescue, environmental surveillance, and industrial inspection. Multidrone systems offer notable advantages such as enhanced efficiency, scalability, and redundancy over single-drone operations. Despite these benefits, ensuring resilient formation control in dynamic and adversarial environments, such as under communication loss or cyberattacks, remains a significant challenge. Classical approaches to resilient formation control, while effective in certain scenarios, often struggle with complex modeling and the curse of dimensionality, particularly as the number of agents increases. This paper proposes a novel, learning-based formation control for enhancing the adaptability and resilience of multidrone formations using graph attention networks (GATs). By leveraging GAT's dynamic capabilities to extract internode relationships based on the attention mechanism, this GAT-based formation controller significantly improves the robustness of drone formations against various threats, such as Denial of Service (DoS) attacks. Our approach not only improves formation performance in normal conditions but also ensures the resilience of multidrone systems in variable and adversarial environments. Extensive simulation results demonstrate the superior performance of our method over baseline formation controllers. Furthermore, the physical experiments validate the effectiveness of the trained control policy in real-world flights.

Translated Abstract:
드론 기술의 빠른 발전은 수색 및 구조, 환경 감시, 산업 검사 등 다양한 분야에 큰 영향을 미쳤어. 멀티 드론 시스템은 단일 드론 작업보다 효율성, 확장성, 중복성 같은 여러 장점을 제공해. 하지만 통신 장애나 사이버 공격 같은 동적이고 적대적인 환경에서 강인한 형성 제어를 보장하는 건 여전히 큰 도전이야.

전통적인 강인 형성 제어 방법은 특정 상황에서는 효과적이지만, 많은 에이전트가 있을 경우 복잡한 모델링과 차원의 저주 때문에 어려움을 겪어. 이 논문에서는 그래프 주의 네트워크(GAT)를 사용해 멀티 드론 형성의 적응력과 강인성을 높이기 위한 새로운 학습 기반 형성 제어 방법을 제안해.

GAT의 동적 기능을 활용해서 주의 메커니즘에 기반한 노드 간 관계를 추출함으로써, 이 GAT 기반 형성 제어기는 다양한 위협, 특히 서비스 거부(DoS) 공격에 대해 드론 형성의 강인성을 크게 향상시켜. 우리의 접근 방식은 일반적인 조건에서 형성 성능을 개선할 뿐만 아니라, 가변적이고 적대적인 환경에서도 멀티 드론 시스템의 강인성을 보장해. 

광범위한 시뮬레이션 결과는 우리의 방법이 기존 형성 제어기보다 뛰어난 성능을 보인다는 것을 보여줘. 게다가 실제 비행에서 훈련된 제어 정책의 효과를 검증하는 물리적 실험도 진행했어.

================================================================================

URL: https://arxiv.org/abs/2409.01947
Title: Evaluating the precision of the HTC VIVE Ultimate Tracker with robotic and human movements under varied environmental conditions

Original Abstract:
The HTC VIVE Ultimate Tracker, utilizing inside-out tracking with internal stereo cameras providing 6 DoF tracking without external cameras, offers a cost-efficient and straightforward setup for motion tracking. Initially designed for the gaming and VR industry, we explored its application beyond VR, providing source code for data capturing in both C++ and Python without requiring a VR headset. This study is the first to evaluate the tracker's precision across various experimental scenarios. To assess the robustness of the tracking precision, we employed a robotic arm as a precise and repeatable source of motion. Using the OptiTrack system as a reference, we conducted tests under varying experimental conditions: lighting, movement velocity, environmental changes caused by displacing objects in the scene, and human movement in front of the trackers, as well as varying the displacement size relative to the calibration center. On average, the HTC VIVE Ultimate Tracker achieved a precision of 4.98 mm +/- 4 mm across various conditions. The most critical factors affecting accuracy were lighting conditions, movement velocity, and range of motion relative to the calibration center. For practical evaluation, we captured human movements with 5 trackers in realistic motion capture scenarios. Our findings indicate sufficient precision for capturing human movements, validated through two tasks: a low-dynamic pick-and-place task and high-dynamic fencing movements performed by an elite athlete. Even though its precision is lower than that of conventional fixed-camera-based motion capture systems and its performance is influenced by several factors, the HTC VIVE Ultimate Tracker demonstrates adequate accuracy for a variety of motion tracking applications. Its ability to capture human or object movements outside of VR or MOCAP environments makes it particularly versatile.

Translated Abstract:
HTC VIVE Ultimate Tracker는 내부 스테레오 카메라를 이용해 외부 카메라 없이 6 자유도(DoF) 추적을 할 수 있는 인사이드-아웃 추적 기술을 사용해. 이거 덕분에 모션 트래킹을 위한 비용 효율적이고 간편한 설치가 가능해. 원래는 게임과 VR 산업을 위해 설계됐지만, 우리는 이걸 VR 외의 분야에서도 활용할 수 있는 방법을 찾아봤어. 그래서 VR 헤드셋 없이도 데이터 캡처를 할 수 있는 C++와 Python 소스 코드도 제공했어.

이 연구는 다양한 실험 상황에서 트래커의 정확성을 평가한 첫 번째 사례야. 추적 정확성을 검증하기 위해 로봇 팔을 사용했어. 이 로봇 팔은 정밀하고 반복적인 움직임을 제공해. OptiTrack 시스템을 기준으로 삼아서 조명, 움직임 속도, 장면 내 물체 이동에 따른 환경 변화, 트래커 앞의 인간 움직임 등 다양한 실험 조건에서 테스트를 진행했어. 그리고 보정 중심에 대한 이동 크기를 다르게 하면서 실험했지. 평균적으로 HTC VIVE Ultimate Tracker는 다양한 조건에서 4.98mm +/- 4mm의 정확도를 기록했어. 정확도에 가장 큰 영향을 미친 요인은 조명 조건, 움직임 속도, 그리고 보정 중심에 대한 움직임 범위였어.

실용적인 평가를 위해 5개의 트래커를 사용해서 현실적인 모션 캡처 상황에서 인간의 움직임을 캡처했어. 우리의 결과는 인간의 움직임을 캡처하는 데 충분한 정확성을 보여주었고, 두 가지 작업으로 검증했어: 저속으로 물건을 집고 놓는 작업과 엘리트 운동선수가 수행한 고속 펜싱 동작이야. 비록 그 정확도가 전통적인 고정 카메라 기반 모션 캡처 시스템보다 낮고 여러 요인의 영향을 받지만, HTC VIVE Ultimate Tracker는 다양한 모션 트래킹 응용 프로그램에 대한 충분한 정확성을 보여줘. VR이나 MOCAP 환경 외에서 인간이나 물체의 움직임을 캡처할 수 있는 능력 덕분에 특히 활용도가 높아.

================================================================================

URL: https://arxiv.org/abs/2409.01915
Title: Integration of Augmented Reality and Mobile Robot Indoor SLAM for Enhanced Spatial Awareness

Original Abstract:
This research explores the integration of indoor Simultaneous Localization and Mapping (SLAM) with Augmented Reality (AR) to enhance situational awareness, improving safety in hazardous or emergency situations. The main contribution of this work is to enable mobile robots to provide real-time spatial perception to users who are not co-located with the robot. This is a comprehensive approach, including selecting suitable sensors for indoor SLAM, designing and building a platform, developing methods to display maps on AR devices, implementing this into software on an AR device, and improving the robustness of communication and localization between the robot and AR device in real-world testing. By taking this approach and analyzing each component of the integrated system, this paper highlights numerous areas for future research that can further advance the integration of SLAM and AR technologies. These advancements aim to significantly improve safety and efficiency during rescue operations.

Translated Abstract:
이 연구는 실내에서 동시 위치 추정과 지도 작성(SLAM)과 증강 현실(AR)을 결합해 상황 인식을 향상시키고, 위험하거나 비상 상황에서의 안전성을 높이는 방법을 탐구하고 있어. 이 연구의 주요 기여는 모바일 로봇이 로봇과 같은 공간에 있지 않은 사용자에게 실시간으로 공간 정보를 제공할 수 있게 하는 거야.

이 접근 방식은 실내 SLAM에 적합한 센서를 선택하고, 플랫폼을 설계 및 구축하며, AR 기기에서 지도를 표시하는 방법을 개발하고, 이를 AR 기기 소프트웨어에 구현하는 것까지 포괄해. 그리고 실제 테스트에서 로봇과 AR 기기 간의 통신과 위치 추정의 안정성을 개선하는 것도 포함돼.

이런 방법을 사용하고 통합 시스템의 각 구성 요소를 분석함으로써, 이 논문은 SLAM과 AR 기술의 통합을 더 발전시킬 수 있는 여러 연구 분야를 강조하고 있어. 이러한 발전은 구조 작업 중 안전성과 효율성을 크게 향상시키는 것을 목표로 하고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.01900
Title: Securing Federated Learning in Robot Swarms using Blockchain Technology

Original Abstract:
Federated learning is a new approach to distributed machine learning that offers potential advantages such as reducing communication requirements and distributing the costs of training algorithms. Therefore, it could hold great promise in swarm robotics applications. However, federated learning usually requires a centralized server for the aggregation of the models. In this paper, we present a proof-of-concept implementation of federated learning in a robot swarm that does not compromise decentralization. To do so, we use blockchain technology to enable our robot swarm to securely synchronize a shared model that is the aggregation of the individual models without relying on a central server. We then show that introducing a single malfunctioning robot can, however, heavily disrupt the training process. To prevent such situations, we devise protection mechanisms that are implemented through secure and tamper-proof blockchain smart contracts. Our experiments are conducted in ARGoS, a physics-based simulator for swarm robotics, using the Ethereum blockchain protocol which is executed by each simulated robot.

Translated Abstract:
연합 학습은 분산 머신 러닝의 새로운 접근 방식으로, 통신 요구 사항을 줄이고 알고리즘 훈련 비용을 분산시킬 수 있는 장점이 있어. 그래서 군집 로봇 응용에 큰 가능성을 가지고 있어. 하지만 연합 학습은 보통 모델 집계를 위한 중앙 서버가 필요해.

이 논문에서는 중앙 집중화를 해치지 않으면서 로봇 군집에서 연합 학습을 구현한 개념 증명을 소개해. 이를 위해 블록체인 기술을 사용해서 로봇 군집이 중앙 서버에 의존하지 않고, 개별 모델의 집계인 공유 모델을 안전하게 동기화할 수 있도록 했어.

하지만 고장 난 로봇 하나가 훈련 과정을 크게 방해할 수 있다는 점도 보여줘. 이런 상황을 방지하기 위해서, 우리는 안전하고 변조가 불가능한 블록체인 스마트 계약을 통해 보호 메커니즘을 고안했어. 

우리의 실험은 군집 로봇을 위한 물리 기반 시뮬레이터인 ARGoS에서 수행되었고, 각 시뮬레이션 로봇이 실행하는 이더리움 블록체인 프로토콜을 사용했어.

================================================================================

URL: https://arxiv.org/abs/2409.01792
Title: Three-dimensional geometric resolution of the inverse kinematics of a 7 degree of freedom articulated arm

Original Abstract:
This work presents a three-dimensional geometric resolution method to calculate the complete inverse kinematics of a 7-degree-of-freedom articulated arm, including the hand itself. The method is classified as an analytical method with geometric solution, since it obtains a precise solution in a closed number of steps, converting the inverse kinematic problem into a three-dimensional geometric model. To simplify the problem, the kinematic decoupling method is used, so that the position of the wrist is calculated independently on one hand with information on the orientation of the hand, and the angles of the rest of the arm are calculated from the wrist.

Translated Abstract:
이 연구에서는 손을 포함한 7자유도 관절 팔의 완전한 역운동학을 계산하는 3차원 기하학적 해법을 제시해. 이 방법은 기하학적 해법을 사용하는 분석적 방법으로 분류되는데, 이는 역운동학 문제를 3차원 기하학 모델로 변환해서 정해진 단계에서 정확한 해를 얻기 때문이야.

문제를 쉽게 풀기 위해서 운동학 분리 방법을 사용해. 이렇게 하면 손의 방향 정보를 바탕으로 손목의 위치를 독립적으로 계산하고, 나머지 팔의 각도는 손목에서 계산해.

================================================================================

URL: https://arxiv.org/abs/2409.01768
Title: Mapping Safe Zones for Co-located Human-UAV Interaction

Original Abstract:
Recent advances in robotics bring us closer to the reality of living, co-habiting, and sharing personal spaces with robots. However, it is not clear how close a co-located robot can be to a human in a shared environment without making the human uncomfortable or anxious. This research aims to map safe and comfortable zones for co-located aerial robots. The objective is to identify the distances at which a drone causes discomfort to a co-located human and to create a map showing no-fly, moderate-fly, and safe-fly zones. We recruited a total of 18 participants and conducted two indoor laboratory experiments, one with a single drone and the other set with two drones. Our results show that multiple drones cause more discomfort when close to a co-located human than a single drone. We observed that distances below 200 cm caused discomfort, the moderate fly zone was 200 - 300 cm, and the safe-fly zone was any distance greater than 300 cm in single drone experiments. The safe zones were pushed further away by 100 cm for the multiple drone experiments. In this paper, we present the preliminary findings on safe-fly zones for multiple drones. Further work would investigate the impact of a higher number of aerial robots, the speed of approach, direction of travel, and noise level on co-located humans, and autonomously develop 3D models of trust zones and safe zones for co-located aerial swarms.

Translated Abstract:
최근 로봇 기술의 발전 덕분에 로봇과 함께 살고, 같은 공간에서 지내는 것이 점점 현실이 되어가고 있어. 하지만, 같은 공간에 있는 로봇이 인간에게 얼마나 가까이 다가갈 수 있는지는 아직 잘 몰라. 너무 가까이 다가가면 인간이 불편하거나 불안해할 수 있거든. 

이 연구는 공중에 떠 있는 로봇들이 안전하고 편안한 공간을 찾는 데 초점을 맞추고 있어. 드론이 인간에게 불편함을 주는 거리를 파악하고, 비행 금지 구역, 중간 비행 구역, 안전 비행 구역을 표시한 지도를 만드는 게 목표야. 총 18명의 참가자를 모집해서, 하나의 드론이 있는 실험과 두 개의 드론이 있는 실험을 두 번 진행했어. 

결과적으로, 여러 개의 드론이 인간에게 가까이 있을 때 더 많은 불편함을 유발한다는 걸 알게 되었어. 200cm 이하의 거리에서는 불편함이 느껴졌고, 200cm에서 300cm 사이가 중간 비행 구역, 300cm 이상은 안전 비행 구역으로 나타났어. 두 개의 드론 실험에서는 안전 구역이 100cm 더 멀어진 걸 확인했어. 

이 논문에서는 여러 드론에 대한 안전 비행 구역에 대한 초기 발견을 발표해. 앞으로는 더 많은 공중 로봇의 수, 접근 속도, 이동 방향, 소음 수준 등이 인간에게 미치는 영향을 연구할 예정이야. 그리고 함께 있는 공중 로봇 떼를 위한 신뢰 구역과 안전 구역의 3D 모델을 자율적으로 개발할 계획이야.

================================================================================

URL: https://arxiv.org/abs/2409.01652
Title: ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation

Original Abstract:
Representing robotic manipulation tasks as constraints that associate the robot and the environment is a promising way to encode desired robot behaviors. However, it remains unclear how to formulate the constraints such that they are 1) versatile to diverse tasks, 2) free of manual labeling, and 3) optimizable by off-the-shelf solvers to produce robot actions in real-time. In this work, we introduce Relational Keypoint Constraints (ReKep), a visually-grounded representation for constraints in robotic manipulation. Specifically, ReKep is expressed as Python functions mapping a set of 3D keypoints in the environment to a numerical cost. We demonstrate that by representing a manipulation task as a sequence of Relational Keypoint Constraints, we can employ a hierarchical optimization procedure to solve for robot actions (represented by a sequence of end-effector poses in SE(3)) with a perception-action loop at a real-time frequency. Furthermore, in order to circumvent the need for manual specification of ReKep for each new task, we devise an automated procedure that leverages large vision models and vision-language models to produce ReKep from free-form language instructions and RGB-D observations. We present system implementations on a wheeled single-arm platform and a stationary dual-arm platform that can perform a large variety of manipulation tasks, featuring multi-stage, in-the-wild, bimanual, and reactive behaviors, all without task-specific data or environment models. Website at this https URL.

Translated Abstract:
로봇 조작 작업을 로봇과 환경을 연결하는 제약 조건으로 표현하는 것은 원하는 로봇 행동을 코드화하는 유망한 방법이야. 하지만, 이 제약 조건을 어떻게 만들어야 하는지 아직 명확하지 않아. 즉, 1) 다양한 작업에 맞춤화 가능하고, 2) 수동 레이블이 필요 없으며, 3) 일반적인 솔버로 최적화할 수 있어서 실시간으로 로봇 행동을 생성할 수 있어야 해.

이번 연구에서는 Relational Keypoint Constraints (ReKep)라는 시각적으로 기반한 제약 조건 표현 방식을 소개해. ReKep는 환경에서 3D 키포인트 집합을 숫자 비용으로 매핑하는 파이썬 함수로 표현돼. 조작 작업을 Relational Keypoint Constraints의 시퀀스로 표현함으로써, 우리는 계층적 최적화 절차를 사용해 로봇 행동을 해결할 수 있다는 걸 보여줬어. 이 로봇 행동은 SE(3)에서의 엔드 이펙터 자세 시퀀스로 나타낼 수 있고, 인식-행동 루프를 통해 실시간으로 처리할 수 있어.

또한, 매번 새로운 작업에 대해 ReKep를 수동으로 지정할 필요가 없도록, 우리는 대규모 비전 모델과 비전-언어 모델을 활용해 자유로운 형태의 언어 지침과 RGB-D 관측으로부터 ReKep를 생성하는 자동화된 절차를 고안했어. 우리는 바퀴가 달린 단일 팔 플랫폼과 고정된 이중 팔 플랫폼에서 시스템 구현을 보여줬어. 이 시스템은 다양한 조작 작업을 수행할 수 있고, 다단계, 실제 환경에서의 이중 손, 반응 행동 등을 포함해. 모든 작업은 특정 작업 데이터나 환경 모델 없이 가능해.

================================================================================

URL: https://arxiv.org/abs/2409.01646
Title: BEVNav: Robot Autonomous Navigation Via Spatial-Temporal Contrastive Learning in Bird's-Eye View

Original Abstract:
Goal-driven mobile robot navigation in map-less environments requires effective state representations for reliable decision-making. Inspired by the favorable properties of Bird's-Eye View (BEV) in point clouds for visual perception, this paper introduces a novel navigation approach named BEVNav. It employs deep reinforcement learning to learn BEV representations and enhance decision-making reliability. First, we propose a self-supervised spatial-temporal contrastive learning approach to learn BEV representations. Spatially, two randomly augmented views from a point cloud predict each other, enhancing spatial features. Temporally, we combine the current observation with consecutive frames' actions to predict future features, establishing the relationship between observation transitions and actions to capture temporal cues. Then, incorporating this spatial-temporal contrastive learning in the Soft Actor-Critic reinforcement learning framework, our BEVNav offers a superior navigation policy. Extensive experiments demonstrate BEVNav's robustness in environments with dense pedestrians, outperforming state-of-the-art methods across multiple benchmarks. \rev{The code will be made publicly available at this https URL.

Translated Abstract:
목표 기반의 모바일 로봇 내비게이션은 지도 없는 환경에서 신뢰할 수 있는 의사 결정을 위해 효과적인 상태 표현이 필요해. 이 논문에서는 시각 인식을 위한 포인트 클라우드에서 Bird's-Eye View (BEV)의 유리한 특성에 영감을 받아 BEVNav라는 새로운 내비게이션 접근 방식을 소개해. 

BEVNav는 깊은 강화 학습을 사용해서 BEV 표현을 배우고 의사 결정의 신뢰성을 높여. 먼저, 우리는 BEV 표현을 배우기 위한 자기 감독형 시공간 대조 학습 접근 방식을 제안해. 공간적으로는, 포인트 클라우드에서 무작위로 증강된 두 개의 뷰가 서로를 예측해서 공간적 특징을 강화해. 시간적으로는, 현재 관찰한 것과 이전 프레임의 행동을 결합해서 미래의 특징을 예측해. 이렇게 하면 관찰 변화와 행동 간의 관계를 설정해서 시간적 단서를 포착할 수 있어.

그 다음으로, 이 시공간 대조 학습을 Soft Actor-Critic 강화 학습 프레임워크에 통합해서 BEVNav는 뛰어난 내비게이션 정책을 제공해. 다양한 실험 결과, BEVNav는 밀집한 보행자가 있는 환경에서도 강력한 성능을 보여주면서 최신 방법들을 능가해. 

코드는 이 URL에서 공개될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.01630
Title: SafeEmbodAI: a Safety Framework for Mobile Robots in Embodied AI Systems

Original Abstract:
Embodied AI systems, including AI-powered robots that autonomously interact with the physical world, stand to be significantly advanced by Large Language Models (LLMs), which enable robots to better understand complex language commands and perform advanced tasks with enhanced comprehension and adaptability, highlighting their potential to improve embodied AI capabilities. However, this advancement also introduces safety challenges, particularly in robotic navigation tasks. Improper safety management can lead to failures in complex environments and make the system vulnerable to malicious command injections, resulting in unsafe behaviours such as detours or collisions. To address these issues, we propose \textit{SafeEmbodAI}, a safety framework for integrating mobile robots into embodied AI systems. \textit{SafeEmbodAI} incorporates secure prompting, state management, and safety validation mechanisms to secure and assist LLMs in reasoning through multi-modal data and validating responses. We designed a metric to evaluate mission-oriented exploration, and evaluations in simulated environments demonstrate that our framework effectively mitigates threats from malicious commands and improves performance in various environment settings, ensuring the safety of embodied AI systems. Notably, In complex environments with mixed obstacles, our method demonstrates a significant performance increase of 267\% compared to the baseline in attack scenarios, highlighting its robustness in challenging conditions.

Translated Abstract:
체화된 AI 시스템, 즉 물리적인 세상과 자율적으로 상호작용하는 AI 로봇은 대형 언어 모델(LLMs)의 도움으로 크게 발전할 수 있어. 이 모델들은 로봇이 복잡한 언어 명령을 더 잘 이해하고 고급 작업을 수행할 수 있도록 도와줘. 그래서 체화된 AI의 능력을 향상시킬 수 있는 잠재력이 커. 

하지만 이런 발전은 안전 문제도 가져와. 특히 로봇이 내비게이션을 할 때 더욱 그렇지. 안전 관리가 제대로 안 되면 복잡한 환경에서 실패할 수 있고, 악의적인 명령이 주입될 위험이 있어. 이로 인해 우회하거나 충돌 같은 위험한 행동을 할 수 있어.

이런 문제를 해결하기 위해 우리는 \textit{SafeEmbodAI}라는 안전 프레임워크를 제안해. 이 프레임워크는 모바일 로봇을 체화된 AI 시스템에 통합하는 데 도움을 줘. \textit{SafeEmbodAI}는 안전한 프롬프트, 상태 관리, 그리고 안전 검증 메커니즘을 포함하고 있어, LLM이 다중 모달 데이터를 통해 추론하고 응답을 검증하는 데 도움을 줘.

우리는 미션 중심 탐사를 평가하기 위한 지표를 만들었고, 시뮬레이션 환경에서 평가해보니, 이 프레임워크가 악의적인 명령으로부터의 위협을 효과적으로 줄이고 다양한 환경에서도 성능을 향상시킨다는 걸 보여줬어. 특히, 복잡한 장애물이 섞인 환경에서는 우리의 방법이 공격 시나리오에서 기준선보다 267% 더 나은 성과를 보여줘. 이건 어려운 조건에서도 강력한 성능을 발휘한다는 걸 강조해.

================================================================================

URL: https://arxiv.org/abs/2409.01617
Title: High Precision Positioning System

Original Abstract:
SAPPO is a high-precision, low-cost and highly scalable indoor localization system. The system is designed using modified HC-SR04 ultrasound transducers as a base to be used as distance meters between beacons and mobile robots. Additionally, it has a very unusual arrangement of its elements, such that the beacons and the array of transmitters of the mobile robot are located in very close planes, in a horizontal emission arrangement, parallel to the ground, achieving a range per transducer of almost 12 meters. SAPPO represents a significant leap forward in ultrasound localization systems, in terms of reducing the density of beacons while maintaining average precision in the millimeter range.

Translated Abstract:
SAPPO는 정밀도가 높고, 비용이 저렴하며, 확장성이 뛰어난 실내 위치 추적 시스템이야. 이 시스템은 HC-SR04 초음파 변환기를 수정해서 만들었고, 비콘과 이동 로봇 사이의 거리 측정기로 사용돼.

또한, 요소들이 아주 독특하게 배치되어 있어서 비콘과 이동 로봇의 송신기 배열이 매우 가까운 평면에 위치해. 이 배열은 수평으로 바닥과 평행하게 배치되어 있어서, 각 변환기가 거의 12미터까지 거리 측정이 가능해. 

SAPPO는 초음파 위치 추적 시스템에서 비콘의 밀도를 줄이면서도 평균 정밀도를 밀리미터 범위로 유지하는 데 큰 발전을 이뤘어.

================================================================================

URL: https://arxiv.org/abs/2409.01581
Title: GaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing Color Point Clouds via 3D Gaussian Splatting

Original Abstract:
Dense colored point clouds enhance visual perception and are of significant value in various robotic applications. However, existing learning-based point cloud upsampling methods are constrained by computational resources and batch processing strategies, which often require subdividing point clouds into smaller patches, leading to distortions that degrade perceptual quality. To address this challenge, we propose a novel 2D-3D hybrid colored point cloud upsampling framework (GaussianPU) based on 3D Gaussian Splatting (3DGS) for robotic perception. This approach leverages 3DGS to bridge 3D point clouds with their 2D rendered images in robot vision systems. A dual scale rendered image restoration network transforms sparse point cloud renderings into dense representations, which are then input into 3DGS along with precise robot camera poses and interpolated sparse point clouds to reconstruct dense 3D point clouds. We have made a series of enhancements to the vanilla 3DGS, enabling precise control over the number of points and significantly boosting the quality of the upsampled point cloud for robotic scene understanding. Our framework supports processing entire point clouds on a single consumer-grade GPU, such as the NVIDIA GeForce RTX 3090, eliminating the need for segmentation and thus producing high-quality, dense colored point clouds with millions of points for robot navigation and manipulation tasks. Extensive experimental results on generating million-level point cloud data validate the effectiveness of our method, substantially improving the quality of colored point clouds and demonstrating significant potential for applications involving large-scale point clouds in autonomous robotics and human-robot interaction scenarios.

Translated Abstract:
밀집된 색상 점 구름은 시각적 인식을 향상시키고 다양한 로봇 응용 프로그램에서 큰 가치를 지닙니다. 하지만 기존의 학습 기반 점 구름 업샘플링 방법들은 컴퓨터 자원과 배치 처리 전략에 제한을 받아서 점 구름을 더 작은 패치로 나누어야 하고, 이로 인해 왜곡이 생겨서 인식 품질이 떨어지는 문제가 있습니다.

이 문제를 해결하기 위해, 우리는 로봇 인식을 위한 새로운 2D-3D 하이브리드 색상 점 구름 업샘플링 프레임워크인 GaussianPU를 제안합니다. 이 방법은 3D 가우시안 스플래팅(3DGS)을 이용해 로봇 비전 시스템에서 3D 점 구름과 2D 렌더링 이미지를 연결합니다.

이중 스케일 렌더링 이미지 복원 네트워크는 희소한 점 구름 렌더링을 밀집된 표현으로 변환합니다. 이렇게 변환된 이미지는 정확한 로봇 카메라 자세와 보간된 희소 점 구름과 함께 3DGS에 입력되어 밀집된 3D 점 구름을 재구성합니다. 우리는 기본 3DGS에 여러 가지 개선을 추가하여 점의 수를 정확하게 제어하고 로봇 장면 이해를 위한 업샘플링된 점 구름의 품질을 크게 향상시켰습니다.

우리의 프레임워크는 NVIDIA GeForce RTX 3090 같은 일반 소비자급 GPU에서 전체 점 구름을 처리할 수 있도록 지원하며, 이로 인해 세분화할 필요가 없어지고 로봇 내비게이션과 조작 작업을 위한 수백만 개의 점을 가진 고품질 밀집 색상 점 구름을 생성할 수 있습니다. 수백만 개의 점 구름 데이터를 생성하는 실험 결과는 우리의 방법이 효과적임을 입증하며, 색상 점 구름의 품질을 크게 개선하고 자율 로봇 및 인간-로봇 상호작용 시나리오에서 대규모 점 구름을 활용하는 데 큰 가능성을 보여줍니다.

================================================================================

URL: https://arxiv.org/abs/2409.01559
Title: PR2: A Physics- and Photo-realistic Testbed for Embodied AI and Humanoid Robots

Original Abstract:
This paper presents the development of a Physics-realistic and Photo-\underline{r}ealistic humanoid robot testbed, PR2, to facilitate collaborative research between Embodied Artificial Intelligence (Embodied AI) and robotics. PR2 offers high-quality scene rendering and robot dynamic simulation, enabling (i) the creation of diverse scenes using various digital assets, (ii) the integration of advanced perception or foundation models, and (iii) the implementation of planning and control algorithms for dynamic humanoid robot behaviors based on environmental feedback. The beta version of PR2 has been deployed for the simulation track of a nationwide full-size humanoid robot competition for college students, attracting 137 teams and over 400 participants within four months. This competition covered traditional tasks in bipedal walking, as well as novel challenges in loco-manipulation and language-instruction-based object search, marking a first for public college robotics competitions. A retrospective analysis of the competition suggests that future events should emphasize the integration of locomotion with manipulation and perception. By making the PR2 testbed publicly available at this https URL, we aim to further advance education and training in humanoid robotics.

Translated Abstract:
이 논문은 Embodied Artificial Intelligence(구현된 AI)와 로봇 공학 간의 협업 연구를 촉진하기 위해 물리적으로 현실적이고 사진처럼 사실적인 휴머노이드 로봇 테스트베드인 PR2의 개발에 대해 다루고 있어. 

PR2는 고품질의 장면 렌더링과 로봇 동적 시뮬레이션을 제공해서, (i) 다양한 디지털 자산을 이용해 여러 장면을 만들 수 있고, (ii) 고급 인식 또는 기초 모델을 통합할 수 있으며, (iii) 환경 피드백에 기반한 동적 휴머노이드 로봇 행동을 위한 계획 및 제어 알고리즘을 구현할 수 있어.

PR2의 베타 버전은 대학생을 위한 전국 규모의 풀사이즈 휴머노이드 로봇 대회의 시뮬레이션 트랙에 배포되었고, 4개월 만에 137개 팀과 400명 이상의 참가자를 끌어모았어. 이 대회는 이족 보행의 전통적인 과제뿐만 아니라, 새로운 도전인 로코-조작과 언어 지시 기반 물체 탐색도 포함되어 있었어. 이는 공립 대학 로봇 대회에서는 처음 있는 일이야.

대회에 대한 회고 분석에서는 앞으로의 이벤트가 조작과 인식을 결합한 이동 능력의 통합에 더 중점을 두어야 한다고 제안하고 있어. PR2 테스트베드를 이 https URL에서 공개함으로써, 휴머노이드 로봇 교육과 훈련을 더 발전시키고자 해.

================================================================================

URL: https://arxiv.org/abs/2409.01549
Title: DOB-based Wind Estimation of A UAV Using Its Onboard Sensor

Original Abstract:
Unmanned Aerial Vehicles (UAVs) play a crucial role in meteorological research, particularly in environmental wind field measurements. However, several challenges exist in current wind measurement methods using UAVs that need to be addressed. Firstly, the accuracy of measurement is low, and the measurement range is limited. Secondly, the algorithms employed lack robustness and adaptability across different UAV platforms. Thirdly, there are limited approaches available for wind estimation during dynamic flight. Finally, while horizontal plane measurements are feasible, vertical direction estimation is often missing. To tackle these challenges, we present and implement a comprehensive wind estimation algorithm. Our algorithm offers several key features, including the capability to estimate the 3-D wind vector, enabling wind estimation even during dynamic flight of the UAV. Furthermore, our algorithm exhibits adaptability across various UAV platforms. Experimental results in the wind tunnel validate the effectiveness of our algorithm, showcasing improvements such as wind speed accuracy of $0.11$ m/s and wind direction errors of less than $2.8^\circ$. Additionally, our approach extends the measurement range to $10$ m/s.

Translated Abstract:
무인 항공기(UAV)는 기상 연구에서 중요한 역할을 해. 특히 환경 바람 필드 측정에 많이 쓰여. 하지만 현재 UAV를 이용한 바람 측정 방법에는 여러 가지 문제점이 있어. 

첫째로, 측정 정확도가 낮고 측정 범위가 제한적이야. 둘째로, 사용되는 알고리즘이 다양한 UAV 플랫폼에서 강인성과 적응력이 부족해. 셋째로, 동적 비행 중 바람 추정 방법이 거의 없어. 마지막으로, 수평면 측정은 가능하지만 수직 방향 추정은 잘 안 되는 경우가 많아. 

이런 문제를 해결하기 위해 우리는 포괄적인 바람 추정 알고리즘을 제안하고 구현했어. 이 알고리즘은 몇 가지 주요 기능을 제공해. 3차원 바람 벡터를 추정할 수 있어서, UAV가 동적으로 비행하는 동안에도 바람 추정이 가능해. 게다가, 다양한 UAV 플랫폼에 적응할 수 있는 특징도 있어. 

풍동 실험 결과는 우리 알고리즘의 효과를 입증해 줘. 바람 속도 정확도가 $0.11$ m/s이고 바람 방향 오차가 $2.8^\circ$ 미만인 개선 사항을 보여줘. 또, 우리의 접근 방식은 측정 범위를 $10$ m/s로 확장했어.

================================================================================

URL: https://arxiv.org/abs/2409.01504
Title: Situation-aware Autonomous Driving Decision Making with Cooperative Perception on Demand

Original Abstract:
This paper investigates the impact of cooperative perception on autonomous driving decision making on urban roads. The extended perception range contributed by the cooperative perception can be properly leveraged to address the implicit dependencies within the vehicles, thereby the vehicle decision making performance can be improved. Meanwhile, we acknowledge the inherent limitation of wireless communication and propose a Cooperative Perception on Demand (CPoD) strategy, where the cooperative perception will only be activated when the extended perception range is necessary for proper situation-awareness. The situation-aware decision making with CPoD is modeled as a Partially Observable Markov Decision Process (POMDP) and solved in an online manner. The evaluation results demonstrate that the proposed approach can function safely and efficiently for autonomous driving on urban roads.

Translated Abstract:
이 논문은 도시 도로에서 자율 주행의 의사 결정에 협력적 인식이 미치는 영향을 조사해. 협력적 인식이 제공하는 확장된 인식 범위는 차량 간의 암묵적인 의존성을 잘 다룰 수 있게 해줘, 그래서 차량의 의사 결정 성능을 향상시킬 수 있어.

한편, 우리는 무선 통신의 본질적인 한계를 인정하고, 필요할 때만 협력적 인식을 활성화하는 '필요시 협력적 인식(CPoD)' 전략을 제안해. 이 전략은 상황 인식이 필요할 때 확장된 인식 범위를 사용할 수 있게 해줘. CPoD를 활용한 상황 인식 기반 의사 결정은 부분 관찰 마르코프 결정 과정(POMDP)으로 모델링되고, 온라인 방식으로 해결돼.

평가 결과는 제안한 방법이 도시 도로에서 자율 주행을 안전하고 효율적으로 수행할 수 있음을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.01458
Title: Time-Varying Soft-Maximum Barrier Functions for Safety in Unmapped and Dynamic Environments

Original Abstract:
We present a closed-form optimal feedback control method that ensures safety in an a prior unknown and potentially dynamic environment. This article considers the scenario where local perception data (e.g., LiDAR) is obtained periodically, and this data can be used to construct a local control barrier function (CBF) that models a local set that is safe for a period of time into the future. Then, we use a smooth time-varying soft-maximum function to compose the N most recently obtained local CBFs into a single barrier function that models an approximate union of the N most recently obtained local sets. This composite barrier function is used in a constrained quadratic optimization, which is solved in closed form to obtain a safe-and-optimal feedback control. We also apply the time-varying soft-maximum barrier function control to 2 robotic systems (nonholonomic ground robot with nonnegligible inertia, and quadrotor robot), where the objective is to navigate an a priori unknown environment safely and reach a target destination. In these applications, we present a simple approach to generate local CBFs from periodically obtained perception data.

Translated Abstract:
우리는 사전 정보가 없고 변화할 수 있는 환경에서 안전성을 보장하는 최적 피드백 제어 방법을 제시해. 이 논문에서는 지역 인식 데이터(예: LiDAR)를 주기적으로 얻는 상황을 고려해. 이 데이터를 사용해서 일정 시간 동안 안전한 지역을 모델링하는 로컬 제어 장벽 함수(CBF)를 만들 수 있어.

그 다음, 최근에 얻은 N개의 로컬 CBF를 하나의 장벽 함수로 합치기 위해 매끄러운 시간 변동 소프트-맥시멈 함수를 사용해. 이렇게 만든 복합 장벽 함수는 N개의 최근 로컬 집합의 근사 합집합을 모델링해. 이 복합 장벽 함수는 제약 조건이 있는 이차 최적화에 사용되며, 닫힌 형태로 해결해서 안전하고 최적의 피드백 제어를 얻을 수 있어.

우리는 시간 변동 소프트-맥시멈 장벽 함수 제어를 두 개의 로봇 시스템(비홀로노믹 지상 로봇과 쿼드로터 로봇)에 적용해. 이 시스템의 목표는 사전 정보가 없는 환경에서 안전하게 목표 지점에 도달하는 거야. 이러한 응용에서, 주기적으로 얻은 인식 데이터로부터 로컬 CBF를 생성하는 간단한 방법을 제시해.

================================================================================

URL: https://arxiv.org/abs/2409.01326
Title: Grounding Language Models in Autonomous Loco-manipulation Tasks

Original Abstract:
Humanoid robots with behavioral autonomy have consistently been regarded as ideal collaborators in our daily lives and promising representations of embodied intelligence. Compared to fixed-based robotic arms, humanoid robots offer a larger operational space while significantly increasing the difficulty of control and planning. Despite the rapid progress towards general-purpose humanoid robots, most studies remain focused on locomotion ability with few investigations into whole-body coordination and tasks planning, thus limiting the potential to demonstrate long-horizon tasks involving both mobility and manipulation under open-ended verbal instructions. In this work, we propose a novel framework that learns, selects, and plans behaviors based on tasks in different scenarios. We combine reinforcement learning (RL) with whole-body optimization to generate robot motions and store them into a motion library. We further leverage the planning and reasoning features of the large language model (LLM), constructing a hierarchical task graph that comprises a series of motion primitives to bridge lower-level execution with higher-level planning. Experiments in simulation and real-world using the CENTAURO robot show that the language model based planner can efficiently adapt to new loco-manipulation tasks, demonstrating high autonomy from free-text commands in unstructured scenes.

Translated Abstract:
행동 자율성을 가진 인간형 로봇은 우리 일상에서 이상적인 협력자로 여겨지고, 구현된 지능의 유망한 대표로 간주돼. 고정형 로봇 팔과 비교했을 때, 인간형 로봇은 더 넓은 작업 공간을 제공하지만 제어와 계획의 난이도가 상당히 높아져. 일반 목적의 인간형 로봇에 대한 빠른 발전에도 불구하고, 대부분의 연구는 이동 능력에 집중하고 있고, 전체 몸의 조정이나 작업 계획에 대한 조사는 적어서 이동성과 조작을 포함한 장기 과제를 보여줄 가능성이 제한돼.

이 연구에서는 다양한 상황에서 작업에 따라 행동을 학습하고 선택하며 계획하는 새로운 프레임워크를 제안해. 우리는 강화 학습(RL)과 전체 몸 최적화를 결합해서 로봇의 움직임을 생성하고 이를 동작 라이브러리에 저장해. 또한 대형 언어 모델(LLM)의 계획과 추론 기능을 활용해 하위 실행과 상위 계획을 연결하는 일련의 동작 기본 요소로 구성된 계층적 작업 그래프를 만들어. CENTAURO 로봇을 사용한 시뮬레이션과 실제 실험에서 언어 모델 기반 계획자가 새로운 이동-조작 작업에 효과적으로 적응할 수 있음을 보여줬어. 이 과정에서 비구조적 장면에서 자유 텍스트 명령으로부터 높은 자율성을 발휘했어.

================================================================================

URL: https://arxiv.org/abs/2409.01319
Title: External Steering of Vine Robots via Magnetic Actuation

Original Abstract:
This paper explores the concept of external magnetic control for vine robots to enable their high curvature steering and navigation for use in endoluminal applications. Vine robots, inspired by natural growth and locomotion strategies, present unique shape adaptation capabilities that allow passive deformation around obstacles. However, without additional steering mechanisms, they lack the ability to actively select the desired direction of growth. The principles of magnetically steered growing robots are discussed, and experimental results showcase the effectiveness of the proposed magnetic actuation approach. We present a 25 mm diameter vine robot with integrated magnetic tip capsule, including 6 Degrees of Freedom (DOF) localization and camera and demonstrate a minimum bending radius of 3.85 cm with an internal pressure of 30 kPa. Furthermore, we evaluate the robot's ability to form tight curvature through complex navigation tasks, with magnetic actuation allowing for extended free-space navigation without buckling. The suspension of the magnetic tip was also validated using the 6 DOF localization system to ensure that the shear-free nature of vine robots was preserved. Additionally, by exploiting the magnetic wrench at the tip, we showcase preliminary results of vine retraction. The findings contribute to the development of controllable vine robots for endoluminal applications, providing high tip force and shear-free navigation.

Translated Abstract:
이 논문은 외부 자기 제어 개념을 이용해 덩굴 로봇이 높은 곡률로 방향을 조정하고 탐색할 수 있게 하는 방법을 다루고 있어. 덩굴 로봇은 자연의 성장 방식과 움직임에서 영감을 받아서 장애물 주변에서 자연스럽게 형태를 바꿀 수 있는 능력이 있어. 하지만 추가적인 조종 장치가 없으면 원하는 방향으로 성장하는 걸 적극적으로 선택할 수는 없어.

이 논문에서는 자기 유도 로봇의 원리에 대해 설명하고, 제안된 자기 작동 방식의 효과를 보여주는 실험 결과도 포함되어 있어. 우리는 25mm 지름의 덩굴 로봇을 소개하는데, 이 로봇은 자기 팁 캡슐이 통합되어 있고 6자유도(6 DOF) 위치 추적과 카메라를 포함하고 있어. 내부 압력 30kPa에서 최소 굽힘 반경이 3.85cm라는 걸 시연했어.

또한, 자기 작동을 이용해 복잡한 탐색 작업을 통해 로봇이 좁은 곡률을 형성할 수 있는 능력을 평가했어. 이 덕분에 로봇이 뒤틀리지 않고 자유 공간에서 길게 탐색할 수 있었어. 자기 팁의 서스펜션도 6 DOF 위치 추적 시스템을 사용해 검증해서 덩굴 로봇의 전단력이 없는 특성을 유지할 수 있었어.

마지막으로, 팁에서 자기 힘을 활용해 덩굴을 수축시키는 초기 결과도 보여줬어. 이 연구 결과는 덩굴 로봇을 조절할 수 있는 방향으로 발전시키는데 기여하며, 높은 팁 힘과 전단력 없는 탐색을 가능하게 해.

================================================================================

URL: https://arxiv.org/abs/2409.01277
Title: Adaptive Artificial Time Delay Control for Robotic Systems

Original Abstract:
Artificial time delay controller was conceptualised for nonlinear systems to reduce dependency on precise system modelling unlike the conventional adaptive and robust control strategies. In this approach unknown dynamics is compensated by using input and state measurements collected at immediate past time instant (i.e., artificially delayed). The advantage of this kind of approach lies in its simplicity and ease of implementation. However, the applications of artificial time delay controllers in robotics, which are also robust against unknown state-dependent uncertainty, are still missing at large. This thesis presents the study of this control approach toward two important classes of robotic systems, namely a fully actuated bipedal walking robot and an underactuated quadrotor system. In the first work, we explore the idea of a unified control design instead of multiple controllers for different walking phases in adaptive bipedal walking control while bypassing computing constraint forces, since they often lead to complex designs. The second work focuses on quadrotors employed for applications such as payload delivery, inspection and search-and-rescue. The effectiveness of this controller is validated using experimental results.

Translated Abstract:
비선형 시스템을 위한 인위적인 시간 지연 제어기가 개발되었어. 이 방법은 기존의 적응형 및 강인 제어 전략과는 달리, 정확한 시스템 모델링에 의존하지 않도록 만들어졌어. 이 접근법에서는 알 수 없는 동역학을 과거의 입력과 상태 측정값을 이용해 보상해. 이 방식의 장점은 간단하고 구현하기 쉽다는 거야.

하지만 인공지능 시간 지연 제어기가 로봇에 적용되는 사례는 아직 많지 않아. 이 논문에서는 이 제어 방법을 두 가지 중요한 로봇 시스템에 적용해봤어. 첫 번째는 완전 구동 이족 보행 로봇이고, 두 번째는 언더액추에이티드 쿼드로터 시스템이야.

첫 번째 연구에서는 적응형 이족 보행 제어에서 다양한 보행 단계에 대해 여러 개의 제어기를 사용하는 대신 통합된 제어 설계 아이디어를 탐구했어. 복잡한 설계를 유도하는 제약력을 계산하지 않고 진행했지. 두 번째 연구는 화물 운반, 검사, 수색 및 구조 작업 등에 사용되는 쿼드로터에 초점을 맞췄어. 이 제어기의 효과는 실험 결과를 통해 검증했어.

================================================================================

URL: https://arxiv.org/abs/2409.01242
Title: Saying goodbyes to rotating your phone: Magnetometer calibration during SLAM

Original Abstract:
While Wi-Fi positioning is still more common indoors, using magnetic field features has become widely known and utilized as an alternative or supporting source of information. Magnetometer bias presents significant challenge in magnetic field navigation and SLAM. Traditionally, magnetometers have been calibrated using standard sphere or ellipsoid fitting methods and by requiring manual user procedures, such as rotating a smartphone in a figure-eight shape. This is not always feasible, particularly when the magnetometer is attached to heavy or fast-moving platforms, or when user behavior cannot be reliably controlled. Recent research has proposed using map data for calibration during positioning. This paper takes a step further and verifies that a pre-collected map is not needed; instead, calibration can be done as part of a SLAM process. The presented solution uses a factorized particle filter that factors out calibration in addition to the magnetic field map. The method is validated using smartphone data from a shopping mall and mobile robotics data from an office environment. Results support the claim that magnetometer calibration can be achieved during SLAM with comparable accuracy to manual calibration. Furthermore, the method seems to slightly improve manual calibration when used on top of it, suggesting potential for integrating various calibration approaches.

Translated Abstract:
와이파이 위치 추적은 여전히 실내에서 더 흔하지만, 자기장 특징을 사용하는 것이 대안이나 보조 정보원으로 많이 알려지고 활용되고 있어. 자기계측기의 바이어스는 자기장 내비게이션과 SLAM에서 큰 도전 과제가 돼. 전통적으로 자기계측기는 표준 구형 또는 타원형 맞춤 방법을 사용해 보정되었고, 스마트폰을 숫자 8 모양으로 회전시키는 것 같은 수동 사용자 절차가 필요했어. 하지만 이 방법은 무거운 플랫폼이나 빠르게 움직이는 플랫폼에 부착된 자기계측기에는 항상 가능하지 않지. 또한 사용자 행동을 신뢰성 있게 제어할 수 없는 경우도 많고.

최근 연구에서는 위치 추적 중에 지도 데이터를 사용해 보정하는 방법을 제안했어. 이 논문은 한 단계 더 나아가서 미리 수집된 지도가 필요하지 않다는 걸 검증했어. 대신, SLAM 과정의 일부로 보정을 할 수 있다는 거야. 제안된 해결책은 자기장 지도 외에도 보정을 고려하는 분해된 입자 필터를 사용해. 이 방법은 쇼핑몰의 스마트폰 데이터와 사무실 환경의 모바일 로봇 데이터를 사용해 검증되었어. 결과는 SLAM 동안 자기계측기 보정이 수동 보정과 유사한 정확도로 이루어질 수 있다는 주장을 뒷받침해. 게다가 이 방법은 수동 보정 위에 사용될 경우 약간의 개선을 보여줘서 다양한 보정 접근 방법을 통합할 가능성을 제시하고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.01241
Title: CyberCortex.AI: An AI-based Operating System for Autonomous Robotics and Complex Automation

Original Abstract:
The underlying framework for controlling autonomous robots and complex automation applications are Operating Systems (OS) capable of scheduling perception-and-control tasks, as well as providing real-time data communication to other robotic peers and remote cloud computers. In this paper, we introduce this http URL, a robotics OS designed to enable heterogeneous AI-based robotics and complex automation applications. this http URL is a decentralized distributed OS which enables robots to talk to each other, as well as to High Performance Computers (HPC) in the cloud. Sensory and control data from the robots is streamed towards HPC systems with the purpose of training AI algorithms, which are afterwards deployed on the robots. Each functionality of a robot (e.g. sensory data acquisition, path planning, motion control, etc.) is executed within a so-called DataBlock of Filters shared through the internet, where each filter is computed either locally on the robot itself, or remotely on a different robotic system. The data is stored and accessed via a so-called \textit{Temporal Addressable Memory} (TAM), which acts as a gateway between each filter's input and output. this http URL has two main components: i) the CyberCortex.AI.inference system, which is a real-time implementation of the DataBlock running on the robots' embedded hardware, and ii) the CyberCortex.AI.dojo, which runs on an HPC computer in the cloud, and it is used to design, train and deploy AI algorithms. We present a quantitative and qualitative performance analysis of the proposed approach using two collaborative robotics applications: \textit{i}) a forest fires prevention system based on an Unitree A1 legged robot and an Anafi Parrot 4K drone, as well as \textit{ii}) an autonomous driving system which uses this http URL for collaborative perception and motion control.

Translated Abstract:
자율 로봇과 복잡한 자동화 애플리케이션을 제어하는 기본 프레임워크는 운영 체제(OS)야. 이 OS는 인식 및 제어 작업을 스케줄링하고, 다른 로봇이나 원격 클라우드 컴퓨터에 실시간으로 데이터를 통신할 수 있어야 해. 

이번 논문에서는 이 http URL이라는 로봇 OS를 소개해. 이 OS는 이질적인 AI 기반 로봇과 복잡한 자동화 애플리케이션을 가능하게 해주도록 설계됐어. 이 http URL은 분산형 OS로, 로봇들이 서로 소통할 수 있게 해주고, 클라우드의 고성능 컴퓨터(HPC)와도 연결할 수 있어. 로봇에서 나오는 감각 및 제어 데이터는 HPC 시스템으로 전송돼서 AI 알고리즘을 훈련시키는 데 사용되고, 이후 이 알고리즘이 로봇에 배포돼.

로봇의 각 기능(예: 감각 데이터 수집, 경로 계획, 모션 제어 등)은 인터넷을 통해 공유되는 '데이터 블록(DataBlock)'이라는 필터 내에서 실행돼. 여기서 각 필터는 로봇 자체에서 로컬로 계산되거나, 다른 로봇 시스템에서 원격으로 계산될 수 있어. 데이터는 '시간 주소 가능한 메모리(Temporal Addressable Memory, TAM)'를 통해 저장되고 접근돼. TAM은 각 필터의 입력과 출력을 연결해주는 역할을 해.

이 http URL에는 두 가지 주요 구성 요소가 있어: 첫째, CyberCortex.AI.inference 시스템은 로봇의 내장 하드웨어에서 실행되는 데이터 블록의 실시간 구현이야. 둘째, CyberCortex.AI.dojo는 클라우드의 HPC 컴퓨터에서 실행되며, AI 알고리즘을 설계하고 훈련시켜 배포하는 데 사용돼.

우리는 제안된 접근 방식의 정량적 및 정성적 성능 분석을 두 가지 협력 로봇 애플리케이션을 통해 보여줄 거야: 첫 번째는 Unitree A1 다리 로봇과 Anafi Parrot 4K 드론을 이용한 산불 예방 시스템이고, 두 번째는 이 http URL을 활용한 협력적인 인식 및 모션 제어를 사용하는 자율 주행 시스템이야.

================================================================================

URL: https://arxiv.org/abs/2409.01198
Title: Direct Kinematics, Inverse Kinematics, and Motion Planning of 1-DoF Rational Linkages

Original Abstract:
This study presents a set of algorithms that deal with trajectory planning of rational single-loop mechanisms with one degree-of-freedom (DoF). Benefiting from a dual quaternion representation of a rational motion, a formula for direct (forward) kinematics, a numerical inverse kinematics algorithm, and the generation of a driving-joint trajectory are provided. A novel approach using the Gauss-Newton search for the one-parameter inverse kinematics problem is presented. Additionally, a method for performing smooth equidistant travel of the tool is provided by applying arc-length reparameterization. This general approach can be applied to one-DoF mechanisms with four to seven joints characterized by a rational motion, without any additional geometrical analysis. An experiment was performed to demonstrate the usage in a laboratory setup.

Translated Abstract:
이 연구는 하나의 자유도(DoF)를 가진 합리적인 단일 루프 메커니즘의 경로 계획을 다루는 알고리즘 세트를 제시해. 

합리적인 운동의 이중 쿼터니언 표현을 활용하고, 직접(전방) 운동학에 대한 공식을 제공해. 또, 수치 역운동학 알고리즘과 구동 관절 경로 생성도 포함되어 있어. 

하나의 매개변수 역운동학 문제를 해결하기 위해 가우스-뉴턴 탐색을 사용하는 새로운 접근 방식도 소개돼. 게다가, 호 길이 재매개변화를 적용해서 도구의 부드러운 일정 거리 이동을 수행하는 방법도 제시해. 

이 일반적인 접근 방식은 추가적인 기하학적 분석 없이 합리적인 운동을 가진 네 개에서 일곱 개의 관절을 가진 하나의 자유도 메커니즘에 적용할 수 있어. 마지막으로, 실험을 통해 연구 결과를 실험실 환경에서 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.01174
Title: Development and Validation of a Modular Sensor-Based System for Gait Analysis and Control in Lower-Limb Exoskeletons

Original Abstract:
With rapid advancements in exoskeleton hardware technologies, successful assessment and accurate control remain challenging. This study introduces a modular sensor-based system to enhance biomechanical evaluation and control in lower-limb exoskeletons, utilizing advanced sensor technologies and fuzzy logic. We aim to surpass the limitations of current biomechanical evaluation methods confined to laboratories and to address the high costs and complexity of exoskeleton control systems. The system integrates inertial measurement units, force-sensitive resistors, and load cells into instrumented crutches and 3D-printed insoles. These components function both independently and collectively to capture comprehensive biomechanical data, including the anteroposterior center of pressure and crutch ground reaction forces. This data is processed through a central unit using fuzzy logic algorithms for real-time gait phase estimation and exoskeleton control. Validation experiments with three participants, benchmarked against gold-standard motion capture and force plate technologies, demonstrate our system's capability for reliable gait phase detection and precise biomechanical measurements. By offering our designs open-source and integrating cost-effective technologies, this study advances wearable robotics and promotes broader innovation and adoption in exoskeleton research.

Translated Abstract:
빠르게 발전하고 있는 외골격 하드웨어 기술에도 불구하고, 성공적인 평가와 정확한 제어는 여전히 어려운 과제야. 이 연구는 하체 외골격의 생체역학적 평가와 제어를 향상시키기 위한 모듈형 센서 기반 시스템을 소개해. 여기서는 최신 센서 기술과 퍼지 로직을 활용했어.

우리는 현재의 생체역학적 평가 방법이 실험실에만 국한되는 한계를 넘어서고, 외골격 제어 시스템의 높은 비용과 복잡성을 해결하려고 해. 이 시스템은 관성 측정 장치, 힘 감지 저항기, 로드 셀을 장착한 목발과 3D 프린팅한 인솔로 통합돼. 이 구성 요소들은 독립적으로도 작동하고 함께 작동해 포괄적인 생체역학적 데이터를 수집해. 여기에는 전후 압력 중심과 목발의 지면 반력 같은 데이터가 포함돼.

이 데이터는 중앙 장치를 통해 퍼지 로직 알고리즘으로 처리돼 실시간 보행 단계 추정과 외골격 제어에 사용돼. 세 명의 참가자를 대상으로 한 검증 실험에서는 금표준 모션 캡처와 힘 판 기술과 비교했을 때, 우리의 시스템이 신뢰할 수 있는 보행 단계 감지와 정확한 생체역학적 측정을 할 수 있음을 보여줬어.

우리의 설계를 오픈 소스로 제공하고 비용 효율적인 기술을 통합함으로써, 이 연구는 착용형 로봇 기술을 발전시키고 외골격 연구의 더 넓은 혁신과 채택을 촉진할 거야.

================================================================================

URL: https://arxiv.org/abs/2409.01159
Title: Remote telepresence over large distances via robot avatars: case studies

Original Abstract:
This paper discusses the necessary considerations and adjustments that allow a recently proposed avatar system architecture to be used with different robotic avatar morphologies (both wheeled and legged robots with various types of hands and kinematic structures) for the purpose of enabling remote (intercontinental) telepresence under communication bandwidth restrictions. The case studies reported involve robots using both position and torque control modes, independently of their software middleware.

Translated Abstract:
이 논문은 최근 제안된 아바타 시스템 구조를 다양한 로봇 아바타 형태(바퀴가 달린 로봇과 다리가 있는 로봇, 다양한 손과 운동 구조를 가진 로봇들)와 함께 사용할 때 고려해야 할 사항과 조정 방법에 대해 이야기해. 이 시스템은 통신 대역폭 제한 아래에서 원거리(대륙 간) 원격 존재감을 가능하게 해줘.

보고된 사례 연구에서는 로봇들이 소프트웨어 미들웨어와 상관없이 위치 제어 모드와 토크 제어 모드를 모두 사용하고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.01144
Title: Online Non-linear Centroidal MPC with Stability Guarantees for Robust Locomotion of Legged Robots

Original Abstract:
Nonlinear model predictive locomotion controllers based on the reduced centroidal dynamics are nowadays ubiquitous in legged robots. These schemes, even if they assume an inherent simplification of the robot's dynamics, were shown to endow robots with a step-adjustment capability in reaction to small pushes, and, moreover, in the case of uncertain parameters - as unknown payloads - they were shown to be able to provide some practical, albeit limited, robustness. In this work, we provide rigorous certificates of their closed loop stability via a reformulation of the centroidal MPC controller. This is achieved thanks to a systematic procedure inspired by the machinery of adaptive control, together with ideas coming from Control Lyapunov functions. Our reformulation, in addition, provides robustness for a class of unmeasured constant disturbances. To demonstrate the generality of our approach, we validated our formulation on a new generation of humanoid robots - the 56.7 kg ergoCub, as well as on a commercially available 21 kg quadruped robot, Aliengo.

Translated Abstract:
비선형 모델 예측 로코모션 컨트롤러는 요즘 다리 로봇에서 흔히 사용돼. 이 방식은 로봇의 동역학을 단순화한다고 가정하지만, 로봇이 작은 힘에 반응해서 걸음을 조정할 수 있게 해줘. 그리고 불확실한 파라미터, 예를 들어 알 수 없는 하중 같은 경우에도 일정 수준의 강인함을 제공하는 걸 보여줬어.

이번 연구에서는 센트로이드 MPC 컨트롤러의 재구성을 통해 이들의 폐쇄 루프 안정성에 대한 확실한 증거를 제시해. 이 과정은 적응 제어 기법에서 영감을 받은 체계적인 절차 덕분에 가능했고, 제어 리야푸노프 함수에서 나온 아이디어도 활용했어. 우리 재구성은 측정되지 않은 일정한 교란에 대해서도 강인함을 제공해.

우리 방법의 일반성을 보여주기 위해, 56.7kg의 초인간 로봇인 ergoCub와 상용 가능한 21kg의 사족 보행 로봇 Aliengo에서 우리의 공식을 검증했어.

================================================================================

URL: https://arxiv.org/abs/2409.01139
Title: Coverage Metrics for a Scenario Database for the Scenario-Based Assessment of Automated Driving Systems

Original Abstract:
Automated Driving Systems (ADSs) have the potential to make mobility services available and safe for all. A multi-pillar Safety Assessment Framework (SAF) has been proposed for the type-approval process of ADSs. The SAF requires that the test scenarios for the ADS adequately covers the Operational Design Domain (ODD) of the ADS. A common method for generating test scenarios involves basing them on scenarios identified and characterized from driving data.
This work addresses two questions when collecting scenarios from driving data. First, do the collected scenarios cover all relevant aspects of the ADS' ODD? Second, do the collected scenarios cover all relevant aspects that are in the driving data, such that no potentially important situations are missed? This work proposes coverage metrics that provide a quantitative answer to these questions.
The proposed coverage metrics are illustrated by means of an experiment in which over 200000 scenarios from 10 different scenario categories are collected from the HighD data set. The experiment demonstrates that a coverage of 100 % can be achieved under certain conditions, and it also identifies which data and scenarios could be added to enhance the coverage outcomes in case a 100 % coverage has not been achieved. Whereas this work presents metrics for the quantification of the coverage of driving data and the identified scenarios, this paper concludes with future research directions, including the quantification of the completeness of driving data and the identified scenarios.

Translated Abstract:
자동 운전 시스템(ADS)은 모두에게 이동 서비스를 제공하고 안전하게 만들 수 있는 잠재력을 가지고 있어. ADS의 형식 승인 과정을 위해 다중 기둥 안전 평가 프레임워크(SAF)가 제안되었어. SAF는 ADS의 운영 설계 도메인(ODD)을 충분히 포괄하는 테스트 시나리오가 필요하다고 해.

이 연구는 운전 데이터에서 시나리오를 수집할 때 두 가지 질문을 다뤄. 첫 번째로, 수집된 시나리오가 ADS의 ODD와 관련된 모든 측면을 포함하고 있는지? 두 번째로, 수집된 시나리오가 운전 데이터에서 중요한 상황을 놓치지 않도록 모든 관련 측면을 포함하고 있는지? 이 연구는 이러한 질문에 대해 정량적인 답을 제공하는 커버리지 메트릭을 제안해.

제안된 커버리지 메트릭은 10개 다른 시나리오 카테고리에서 수집된 20만 개 이상의 시나리오를 활용한 실험으로 설명돼. 이 실험은 특정 조건에서 100% 커버리지를 달성할 수 있음을 보여주고, 100% 커버리지를 달성하지 못한 경우 어떤 데이터와 시나리오를 추가해야 커버리지 결과를 향상시킬 수 있는지도 알려줘. 이 연구는 운전 데이터와 식별된 시나리오의 커버리지를 정량화하는 메트릭을 제시하며, 논문의 마지막 부분에서는 운전 데이터와 식별된 시나리오의 완전성을 정량화하는 미래 연구 방향에 대해서도 이야기해.

================================================================================

URL: https://arxiv.org/abs/2409.01117
Title: Scenario-based assessment of automated driving systems: How (not) to parameterize scenarios?

Original Abstract:
The development of Automated Driving Systems (ADSs) has advanced significantly. To enable their large-scale deployment, the United Nations Regulation 157 (UN R157) concerning the approval of Automated Lane Keeping Systems (ALKSs) has been approved in 2021. UN R157 requires an activated ALKS to avoid any collisions that are reasonably preventable and proposes a method to distinguish reasonably preventable collisions from unpreventable ones using "the simulated performance of a skilled and attentive human driver". With different driver models, benchmarks are set for ALKSs in three types of scenarios. The three types of scenarios considered in the proposed method in UN R157 assume a certain parameterization without any further consideration.
This work investigates the parameterization of these scenarios, showing that the choice of parameterization significantly affects the simulation outcomes. By comparing real-world and parameterized scenarios, we show that the influence of parameterization depends on the scenario type, driver model, and evaluation criterion. Alternative parameterizations are proposed, leading to results that are closer to the non-parameterized scenarios in terms of recall, precision, and F1 score. The study highlights the importance of careful scenario parameterization and suggests improvements to the current UN R157 approach.

Translated Abstract:
자동차 자동화 시스템(ADS)의 개발이 많이 발전했어. 이 시스템이 대규모로 사용되도록 하기 위해서, 2021년에 유엔의 규정 157(UN R157)이 승인됐어. 이 규정은 자동 차선 유지 시스템(ALKS)이 작동 중일 때, 합리적으로 피할 수 있는 충돌을 피해야 한다고 요구해. 그리고 "숙련된 주의 깊은 인간 운전자의 시뮬레이션 성능"을 사용해 합리적으로 피할 수 있는 충돌과 피할 수 없는 충돌을 구분하는 방법을 제안해.

이 연구는 세 가지 유형의 시나리오에서 ALKS의 기준을 설정해. UN R157에서 제안한 방법은 특정 파라미터를 가정하지만, 추가적인 고려 없이 진행돼.

우리는 이 시나리오의 파라미터 설정을 조사했어. 그 결과, 파라미터 설정의 선택이 시뮬레이션 결과에 큰 영향을 미친다는 걸 보여줬어. 실제 시나리오와 파라미터화된 시나리오를 비교해보니, 파라미터 설정의 영향은 시나리오 유형, 운전 모델, 평가 기준에 따라 달라졌어. 

대안적인 파라미터 설정을 제안했는데, 이걸 통해 리콜, 정밀도, F1 점수 면에서 비파라미터화된 시나리오에 더 가까운 결과를 얻을 수 있었어. 이 연구는 시나리오 파라미터 설정의 중요성을 강조하고, 현재 UN R157 접근 방식을 개선할 방법을 제안해.

================================================================================

URL: https://arxiv.org/abs/2409.01104
Title: AI Olympics challenge with Evolutionary Soft Actor Critic

Original Abstract:
In the following report, we describe the solution we propose for the AI Olympics competition held at IROS 2024. Our solution is based on a Model-free Deep Reinforcement Learning approach combined with an evolutionary strategy. We will briefly describe the algorithms that have been used and then provide details of the approach

Translated Abstract:
이번 보고서에서는 2024 IROS에서 열린 AI 올림픽 대회에 대한 우리의 해결책을 설명할 거야. 우리가 제안하는 방법은 모델이 필요 없는 딥 강화 학습 방식과 진화 전략을 결합한 거야.

우선 사용한 알고리즘에 대해 간단히 설명하고, 그 다음에 우리의 접근 방법에 대한 자세한 내용을 제공할게.

================================================================================

URL: https://arxiv.org/abs/2409.01091
Title: Online One-Dimensional Magnetic Field SLAM with Loop-Closure Detection

Original Abstract:
We present a lightweight magnetic field simultaneous localisation and mapping (SLAM) approach for drift correction in odometry paths, where the interest is purely in the odometry and not in map building. We represent the past magnetic field readings as a one-dimensional trajectory against which the current magnetic field observations are matched. This approach boils down to sequential loop-closure detection and decision-making, based on the current pose state estimate and the magnetic field. We combine this setup with a path estimation framework using an extended Kalman smoother which fuses the odometry increments with the detected loop-closure timings. We demonstrate the practical applicability of the model with several different real-world examples from a handheld iPad moving in indoor scenes.

Translated Abstract:
우리는 드리프트 보정을 위한 가벼운 자기장 동시 위치 측정 및 맵 생성(SLAM) 방법을 제안해. 이 방법은 맵을 만드는 것보다는 오도메트리에만 초점을 맞춰.

과거의 자기장 측정값을 일차원 궤적으로 나타내고, 현재의 자기장 관측값과 비교해. 이 과정은 현재 위치 추정과 자기장을 바탕으로 연속적인 루프 클로저 감지와 의사결정으로 이루어져.

우리는 이 설정을 확장 칼만 스무더를 이용한 경로 추정 프레임워크와 결합해, 오도메트리 증가값과 감지된 루프 클로저 타이밍을 융합해. 이 모델의 실제 적용 가능성을 여러 가지 실세계 예제를 통해 보여줬어. 예를 들어, 실내 장면에서 움직이는 핸드헬드 아이패드를 사용했어.

================================================================================

URL: https://arxiv.org/abs/2409.01083
Title: Affordance-based Robot Manipulation with Flow Matching

Original Abstract:
We present a framework for assistive robot manipulation, which focuses on two fundamental challenges: first, efficiently adapting large-scale models to downstream scene affordance understanding tasks, especially in daily living scenarios where gathering multi-task data involving humans requires strenuous effort; second, effectively learning robot trajectories by grounding the visual affordance model. We tackle the first challenge by employing a parameter-efficient prompt tuning method that prepends learnable text prompts to the frozen vision model to predict manipulation affordances in multi-task scenarios. Then we propose to learn robot trajectories guided by affordances in a supervised Flow Matching method. Flow matching represents a robot visuomotor policy as a conditional process of flowing random waypoints to desired robot trajectories. Finally, we introduce a real-world dataset with 10 tasks across Activities of Daily Living to test our framework. Our extensive evaluation highlights that the proposed prompt tuning method for learning manipulation affordance with language prompter achieves competitive performance and even outperforms other finetuning protocols across data scales, while satisfying parameter efficiency. Learning multi-task robot trajectories with a single flow matching policy also leads to consistently better performance than alternative behavior cloning methods, especially given multimodal robot action distributions. Our framework seamlessly unifies affordance model learning and trajectory generation with flow matching for robot manipulation.

Translated Abstract:
우리는 보조 로봇 조작을 위한 프레임워크를 제안해. 이 프레임워크는 두 가지 기본적인 문제에 초점을 맞추고 있어.

첫 번째 문제는 대규모 모델을 효율적으로 조정해서 일상 생활에서의 장면 이해 작업에 적용하는 건데, 특히 사람과 관련된 여러 작업 데이터를 모으는 게 힘들어. 두 번째 문제는 시각적 가능성 모델을 기반으로 로봇의 경로를 효과적으로 학습하는 거야.

첫 번째 문제를 해결하기 위해, 우리는 파라미터 효율적인 프롬프트 튜닝 방법을 사용해. 이 방법은 학습 가능한 텍스트 프롬프트를 고정된 비전 모델에 추가해서 여러 작업 시나리오에서 조작 가능성을 예측할 수 있게 해. 

그 다음, 우리는 가능성에 따라 로봇 경로를 학습하는 방법을 제안해. 이를 위해 'Flow Matching'이라는 감독 학습 방법을 사용해. Flow Matching은 로봇의 비주얼 모터 정책을 무작위 경로 지점들이 원하는 로봇 경로로 흐르는 조건부 과정으로 표현해.

마지막으로, 우리의 프레임워크를 테스트하기 위해 일상 생활의 10가지 작업을 포함한 실제 데이터셋을 소개해. 우리의 광범위한 평가 결과, 제안된 프롬프트 튜닝 방법이 언어 프롬프터를 통해 조작 가능성을 학습하는 데 경쟁력 있는 성능을 보여주고, 다른 미세 조정 프로토콜보다 더 나은 성능을 발휘하면서도 파라미터 효율성을 만족시켜.

단일 Flow Matching 정책으로 여러 작업의 로봇 경로를 학습하는 것도 대체 행동 복제 방법보다 일관되게 더 나은 성능을 보여줘, 특히 여러 모달리티의 로봇 행동 분포를 고려할 때. 우리의 프레임워크는 로봇 조작을 위한 가능성 모델 학습과 경로 생성을 Flow Matching으로 매끄럽게 통합해.

================================================================================

URL: https://arxiv.org/abs/2409.01080
Title: Flying a Quadrotor with Unknown Actuators and Sensor Configuration

Original Abstract:
Though control algorithms for multirotor Unmanned Air Vehicle (UAV) are well understood, the configuration, parameter estimation, and tuning of flight control algorithms takes quite some time and resources. In previous work, we have shown that it is possible to identify the control effectiveness and motor dynamics of a multirotor fast enough for it to recover to a stable hover after being thrown 4 meters in the air. In this paper, we extend this to include estimation of the position of the Inertial Measurement Unit (IMU) relative to the Center of Gravity (CoG), estimation of the IMU rotation, the thrust direction of all motors and the optimal combined thrust direction. In order to guarantee a correct IMU position estimation, two prior throw-and-catches of the vehicle with spin around different axes are required. For these throws, a height as low as 1 meter is sufficient. Quadrotor flight experimentation confirms the efficacy of the approach, and a simulation shows its applicability to fully-actuated crafts with multiple possible hover orientations.

Translated Abstract:
다중 로터 UAV의 제어 알고리즘은 잘 알려져 있지만, 비행 제어 알고리즘의 구성, 파라미터 추정 및 조정에는 시간과 자원이 많이 소모돼. 이전 연구에서는 UAV를 공중에서 4미터 던진 후 안정적으로 호버링 상태로 회복할 수 있도록 제어 효과성과 모터 동작을 빠르게 파악할 수 있다는 걸 보여줬어.

이번 논문에서는 관성 측정 장치(IMU)의 위치를 중심질량(CoG)과 비교해서 추정하는 것, IMU의 회전 추정, 모든 모터의 추력 방향과 최적의 결합 추력 방향을 포함하도록 연구를 확장했어. IMU의 위치 추정이 정확하게 이루어지려면, 차량을 서로 다른 축으로 회전시키면서 두 번 던지고 받는 과정이 필요해. 이때 던지는 높이는 1미터면 충분해.

쿼드로터 비행 실험은 이 방법의 효과를 확인했고, 시뮬레이션을 통해 여러 가지 호버링 방향을 가진 완전 구동 드론에도 적용 가능하다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.01046
Title: Accelerated Multi-objective Task Learning using Modified Q-learning Algorithm

Original Abstract:
Robots find extensive applications in industry. In recent years, the influence of robots has also increased rapidly in domestic scenarios. The Q-learning algorithm aims to maximise the reward for reaching the goal. This paper proposes a modified version of the Q-learning algorithm, known as Q-learning with scaled distance metric (Q-SD). This algorithm enhances task learning and makes task completion more meaningful. A robotic manipulator (agent) applies the Q-SD algorithm to the task of table cleaning. Using Q-SD, the agent acquires the sequence of steps necessary to accomplish the task while minimising the manipulator's movement distance. We partition the table into grids of different dimensions. The first has a grid count of 3 times 3, and the second has a grid count of 4 times 4. Using the Q-SD algorithm, the maximum success obtained in these two environments was 86% and 59% respectively. Moreover, Compared to the conventional Q-learning algorithm, the drop in average distance moved by the agent in these two environments using the Q-SD algorithm was 8.61% and 6.7% respectively.

Translated Abstract:
로봇은 산업에서 널리 사용되고 있어. 최근 몇 년 동안 로봇의 영향력이 가정에서도 빠르게 증가하고 있어. Q-러닝 알고리즘은 목표에 도달했을 때 보상을 최대화하려고 해. 이 논문에서는 Q-러닝의 수정된 버전인 Q-스케일드 거리 메트릭(Q-SD) 알고리즘을 제안해. 이 알고리즘은 작업 학습을 향상시키고 작업 완료를 더 의미 있게 만들어.

로봇 매니퓰레이터(에이전트)는 테이블 청소 작업에 Q-SD 알고리즘을 적용해. Q-SD를 사용하면 에이전트가 작업을 수행하는 데 필요한 단계의 순서를 배우면서 매니퓰레이터의 이동 거리를 최소화할 수 있어. 우리는 테이블을 서로 다른 크기의 격자로 나눴어. 첫 번째는 3x3 격자, 두 번째는 4x4 격자야. Q-SD 알고리즘을 사용했을 때, 이 두 환경에서 얻은 최대 성공률은 각각 86%와 59%였어. 게다가, 기존 Q-러닝 알고리즘과 비교했을 때, 이 두 환경에서 Q-SD 알고리즘을 사용한 에이전트의 평균 이동 거리가 각각 8.61%와 6.7% 줄어들었어.

================================================================================

URL: https://arxiv.org/abs/2409.01038
Title: Robust Vehicle Localization and Tracking in Rain using Street Maps

Original Abstract:
GPS-based vehicle localization and tracking suffers from unstable positional information commonly experienced in tunnel segments and in dense urban areas. Also, both Visual Odometry (VO) and Visual Inertial Odometry (VIO) are susceptible to adverse weather conditions that causes occlusions or blur on the visual input. In this paper, we propose a novel approach for vehicle localization that uses street network based map information to correct drifting odometry estimates and intermittent GPS measurements especially, in adversarial scenarios such as driving in rain and tunnels. Specifically, our approach is a flexible fusion algorithm that integrates intermittent GPS, drifting IMU and VO estimates together with 2D map information for robust vehicle localization and tracking. We refer to our approach as Map-Fusion. We robustly evaluate our proposed approach on four geographically diverse datasets from different countries ranging across clear and rain weather conditions. These datasets also include challenging visual segments in tunnels and underpasses. We show that with the integration of the map information, our Map-Fusion algorithm reduces the error of the state-of-the-art VO and VIO approaches across all datasets. We also validate our proposed algorithm in a real-world environment and in real-time on a hardware constrained mobile robot. Map-Fusion achieved 2.46m error in clear weather and 6.05m error in rain weather for a 150m route.

Translated Abstract:
GPS 기반 차량 위치 추적은 터널이나 밀집 도시 지역에서 흔히 겪는 불안정한 위치 정보로 어려움을 겪고 있어. 그리고 Visual Odometry (VO)나 Visual Inertial Odometry (VIO)도 나쁜 날씨에서 시각 입력이 가려지거나 흐릿해지는 문제에 취약해. 

이 논문에서는 차량 위치 추정에 대한 새로운 접근 방식을 제안해. 이 방법은 거리 네트워크 기반의 지도 정보를 사용해서 드리프트하는 오도메트리 추정치와 간헐적인 GPS 측정을 보정해. 특히 비 오는 날이나 터널을 주행할 때와 같은 어려운 상황에서 효과적이야. 

구체적으로, 우리 방법은 간헐적인 GPS, 드리프트하는 IMU, VO 추정치를 2D 지도 정보와 결합하는 유연한 융합 알고리즘이야. 우리는 이 방법을 Map-Fusion이라고 불러. 

우리는 제안한 방법을 다양한 날씨 조건에서 여러 나라의 네 개의 지리적으로 다양한 데이터셋을 통해 강력하게 평가했어. 이 데이터셋은 터널과 지하차도에서의 도전적인 시각 구간도 포함하고 있어. 지도 정보를 통합함으로써, Map-Fusion 알고리즘은 모든 데이터셋에서 최신 VO와 VIO 접근 방식의 오류를 줄였어. 

또한, 우리는 제안한 알고리즘을 실제 환경에서 하드웨어 제약이 있는 모바일 로봇을 사용해 실시간으로 검증했어. Map-Fusion은 맑은 날씨에서 150m 경로에 대해 2.46m의 오류를, 비 오는 날씨에서는 6.05m의 오류를 달성했어.

================================================================================

URL: https://arxiv.org/abs/2409.01036
Title: Upgrading Pepper Robot s Social Interaction with Advanced Hardware and Perception Enhancements

Original Abstract:
In this paper, we propose hardware and software enhancements for the Pepper robot to improve its human-robot interaction capabilities. This includes the integration of an NVIDIA Jetson GPU to enhance computational capabilities and execute real time algorithms, and a RealSense D435i camera to capture depth images, as well as the computer vision algorithms to detect and localize the humans around the robot and estimate their body orientation and gaze direction. The new stack is implemented on ROS and is running on the extended Pepper hardware, and the communication with the robot s firmware is done through the NAOqi ROS driver API. We have also collected a MoCap dataset of human activities in a controlled environment, together with the corresponding RGB-D data, to validate the proposed perception algorithms.

Translated Abstract:
이 논문에서는 Pepper 로봇의 인간-로봇 상호작용 능력을 향상시키기 위한 하드웨어와 소프트웨어 개선 내용을 제안해. 

우선, NVIDIA Jetson GPU를 통합해서 계산 능력을 높이고 실시간 알고리즘을 실행할 수 있도록 했어. 그리고 RealSense D435i 카메라를 사용해 깊이 이미지를 캡처하고, 주변의 사람들을 탐지하고 위치를 파악하는 컴퓨터 비전 알고리즘도 추가했어. 이를 통해 사람의 몸 방향과 시선 방향도 추정할 수 있어.

이 새로운 시스템은 ROS 위에서 구현됐고, 확장된 Pepper 하드웨어에서 작동해. 로봇의 펌웨어와의 통신은 NAOqi ROS 드라이버 API를 통해 이루어져. 

또한, 제안한 인식 알고리즘을 검증하기 위해 통제된 환경에서 인간 활동에 대한 MoCap 데이터셋을 수집하고, 그에 따른 RGB-D 데이터도 함께 모았어.

================================================================================

URL: https://arxiv.org/abs/2409.01002
Title: Kalman Filtering for Precise Indoor Position and Orientation Estimation Using IMU and Acoustics on Riemannian Manifolds

Original Abstract:
Indoor tracking and pose estimation, i.e., determining the position and orientation of a moving target, are increasingly important due to their numerous applications. While Inertial Navigation Systems (INS) provide high update rates, their positioning errors can accumulate rapidly over time. To mitigate this, it is common to integrate INS with complementary systems to correct drift and improve accuracy. This paper presents a novel approach that combines INS with an acoustic Riemannian-based localization system to enhance indoor positioning and orientation tracking. The proposed method employs both the Extended Kalman Filter (EKF) and the Unscented Kalman Filter (UKF) for fusing data from the two systems. The Riemannian-based localization system delivers high-accuracy estimates of the target's position and orientation, which are then used to correct the INS data. A new projection algorithm is introduced to map the EKF or UKF output onto the Riemannian manifold, further improving estimation accuracy. Our results show that the proposed methods significantly outperform benchmark algorithms in both position and orientation estimation. The effectiveness of the proposed methods was evaluated through extensive numerical simulations and testing using our in-house experimental setup. These evaluations confirm the superior performance of our approach in practical scenarios.

Translated Abstract:
실내 추적과 자세 추정, 즉 움직이는 대상의 위치와 방향을 알아내는 것은 여러 응용 분야 때문에 점점 더 중요해지고 있어. 관성 항법 시스템(INS)은 업데이트 속도가 빠르지만, 시간이 지남에 따라 위치 오류가 빠르게 쌓일 수 있어. 이를 해결하기 위해 INS와 다른 시스템을 통합해서 드리프트를 보정하고 정확도를 높이는 게 일반적이야.

이 논문에서는 INS와 음향 리만 기반 로컬라이제이션 시스템을 결합한 새로운 접근 방식을 제안해. 이 방법은 두 시스템의 데이터를 융합하기 위해 확장 칼만 필터(EKF)와 비선형 칼만 필터(UKF)를 모두 사용해. 리만 기반 로컬라이제이션 시스템은 대상의 위치와 방향을 매우 정확하게 추정하고, 이 정보를 사용해 INS 데이터를 보정해.

또한, EKF나 UKF의 출력을 리만 다양체에 매핑하기 위한 새로운 프로젝션 알고리즘도 도입했어. 이걸 통해 추정 정확도를 더 높일 수 있어. 우리 결과에 따르면, 제안한 방법이 위치와 방향 추정 모두에서 기준 알고리즘보다 훨씬 뛰어난 성능을 보였어. 제안한 방법의 효과는 여러 숫자 시뮬레이션과 우리 내부 실험 장비를 사용한 테스트를 통해 검증했어. 이 평가 결과는 실제 상황에서 우리 접근 방식이 뛰어난 성능을 보임을 확인해.

================================================================================

URL: https://arxiv.org/abs/2409.00992
Title: MFCalib: Single-shot and Automatic Extrinsic Calibration for LiDAR and Camera in Targetless Environments Based on Multi-Feature Edge

Original Abstract:
This paper presents MFCalib, an innovative extrinsic calibration technique for LiDAR and RGB camera that operates automatically in targetless environments with a single data capture. At the heart of this method is using a rich set of edge information, significantly enhancing calibration accuracy and robustness. Specifically, we extract both depth-continuous and depth-discontinuous edges, along with intensity-discontinuous edges on planes. This comprehensive edge extraction strategy ensures our ability to achieve accurate calibration with just one round of data collection, even in complex and varied settings. Addressing the uncertainty of depth-discontinuous edges, we delve into the physical measurement principles of LiDAR and develop a beam model, effectively mitigating the issue of edge inflation caused by the LiDAR beam. Extensive experiment results demonstrate that MFCalib outperforms the state-of-the-art targetless calibration methods across various scenes, achieving and often surpassing the precision of multi-scene calibrations in a single-shot collection. To support community development, we make our code available open-source on GitHub.

Translated Abstract:
이 논문은 MFCalib이라는 혁신적인 외부 보정 기술을 소개해. 이 기술은 목표물 없이도 자동으로 LiDAR와 RGB 카메라를 보정할 수 있고, 단 한 번의 데이터 수집으로 작동해.

이 방법의 핵심은 다양한 엣지 정보를 활용하는 건데, 이 덕분에 보정의 정확도와 견고함이 크게 향상돼. 구체적으로 말하자면, 우리는 깊이가 연속적인 엣지와 깊이가 불연속적인 엣지, 그리고 평면에서의 강도 불연속 엣지를 모두 추출해. 이런 포괄적인 엣지 추출 전략 덕분에 복잡하고 다양한 환경에서도 단 한 번의 데이터 수집으로 정확한 보정을 할 수 있어.

깊이가 불연속적인 엣지의 불확실성을 해결하기 위해 우리는 LiDAR의 물리적 측정 원리를 탐구하고, 빔 모델을 개발했어. 이렇게 함으로써 LiDAR 빔으로 인한 엣지 부풀림 문제를 효과적으로 완화했지. 

광범위한 실험 결과에 따르면, MFCalib은 다양한 장면에서 최신의 목표물 없는 보정 방법보다 뛰어난 성능을 보여주고, 단일 데이터 수집으로 여러 장면의 보정에서 정확도를 달성하거나 그 이상의 성능을 보여줘. 커뮤니티의 발전을 지원하기 위해, 우리는 GitHub에 코드를 오픈 소스로 공개했어.

================================================================================

URL: https://arxiv.org/abs/2409.00951
Title: Semantically Controllable Augmentations for Generalizable Robot Learning

Original Abstract:
Generalization to unseen real-world scenarios for robot manipulation requires exposure to diverse datasets during training. However, collecting large real-world datasets is intractable due to high operational costs. For robot learning to generalize despite these challenges, it is essential to leverage sources of data or priors beyond the robot's direct experience. In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data, can serve as such a data source. These generative models encompass a broad range of real-world scenarios beyond a robot's direct experience and can synthesize novel synthetic experiences that expose robotic agents to additional world priors aiding real-world generalization at no extra cost.
In particular, our approach leverages pre-trained generative models as an effective tool for data augmentation. We propose a generative augmentation framework for semantically controllable augmentations and rapidly multiplying robot datasets while inducing rich variations that enable real-world generalization. Based on diverse augmentations of robot data, we show how scalable robot manipulation policies can be trained and deployed both in simulation and in unseen real-world environments such as kitchens and table-tops. By demonstrating the effectiveness of image-text generative models in diverse real-world robotic applications, our generative augmentation framework provides a scalable and efficient path for boosting generalization in robot learning at no extra human cost.

Translated Abstract:
로봇 조작이 실제 세계의 보지 못한 상황에 잘 대응하려면, 다양한 데이터셋에 노출되어야 해. 하지만, 실제 데이터셋을 많이 수집하는 건 비용이 너무 많이 들어서 현실적으로 불가능해. 이런 문제에도 불구하고 로봇 학습이 일반화되려면, 로봇의 직접적인 경험 외의 데이터나 정보를 활용하는 게 중요해.

이 연구에서는 웹에서 수집한 대규모 데이터로 미리 학습된 이미지-텍스트 생성 모델이 이런 데이터 소스로 활용될 수 있다고 주장해. 이런 생성 모델은 로봇이 직접 경험하지 못한 다양한 실제 상황을 포함하고 있어서, 새로운 합성 경험을 만들어내고 로봇에게 추가적인 세계 정보를 제공할 수 있어. 이 과정에서 추가 비용 없이 실제 세계에서의 일반화를 도울 수 있어.

특히, 우리는 미리 학습된 생성 모델을 데이터 증강을 위한 효과적인 도구로 활용하는 방법을 제안해. 의미적으로 조절 가능한 증강을 위한 생성 증강 프레임워크를 만들어서 로봇 데이터셋을 빠르게 늘리고, 다양한 변화를 주면서 실제 세계에서 일반화할 수 있도록 해. 다양한 로봇 데이터의 증강을 기반으로, 우리는 확장 가능한 로봇 조작 정책을 어떻게 훈련하고 배포할 수 있는지 보여줘. 이건 시뮬레이션과 주방, 테이블 같은 보지 못한 실제 환경에서도 가능해.

이미지-텍스트 생성 모델이 다양한 실제 로봇 응용 프로그램에서 효과적이라는 걸 보여주면서, 우리의 생성 증강 프레임워크는 추가적인 인력 비용 없이 로봇 학습의 일반화를 높일 수 있는 확장 가능하고 효율적인 방법을 제공해.

================================================================================

URL: https://arxiv.org/abs/2409.00923
Title: Development of Occupancy Prediction Algorithm for Underground Parking Lots

Original Abstract:
The core objective of this study is to address the perception challenges faced by autonomous driving in adverse environments like basements. Initially, this paper commences with data collection in an underground garage. A simulated underground garage model is established within the CARLA simulation environment, and SemanticKITTI format occupancy ground truth data is collected in this simulated setting. Subsequently, the study integrates a Transformer-based Occupancy Network model to complete the occupancy grid prediction task within this scenario. A comprehensive BEV perception framework is designed to enhance the accuracy of neural network models in dimly lit, challenging autonomous driving environments. Finally, experiments validate the accuracy of the proposed solution's perception performance in basement scenarios. The proposed solution is tested on our self-constructed underground garage dataset, SUSTech-COE-ParkingLot, yielding satisfactory results.

Translated Abstract:
이 연구의 핵심 목표는 지하 같은 어려운 환경에서 자율주행이 겪는 인식 문제를 해결하는 거야. 우선, 이 논문은 지하 주차장에서 데이터를 수집하는 것부터 시작해.

CARLA 시뮬레이션 환경에서 지하 주차장 모델을 만들고, 이 환경에서 SemanticKITTI 형식의 점유 그라운드 트루스 데이터를 수집했어. 그 다음으로, Transformer 기반의 점유 네트워크 모델을 통합해서 이 시나리오에서 점유 그리드 예측 작업을 수행했어.

어두운 환경에서도 자율주행의 정확도를 높이기 위해 BEV(Top-Down View) 인식 프레임워크를 설계했어. 마지막으로, 실험을 통해 제안한 솔루션의 인식 성능이 지하 주차장 상황에서 얼마나 정확한지 검증했어. 제안한 솔루션은 우리가 직접 만든 지하 주차장 데이터셋인 SUSTech-COE-ParkingLot에서 테스트했는데, 만족스러운 결과를 얻었어.

================================================================================

URL: https://arxiv.org/abs/2409.00895
Title: Whole-Body Control Through Narrow Gaps From Pixels To Action

Original Abstract:
Flying through body-size narrow gaps in the environment is one of the most challenging moments for an underactuated multirotor. We explore a purely data-driven method to master this flight skill in simulation, where a neural network directly maps pixels and proprioception to continuous low-level control commands. This learned policy enables whole-body control through gaps with different geometries demanding sharp attitude changes (e.g., near-vertical roll angle). The policy is achieved by successive model-free reinforcement learning (RL) and online observation space distillation. The RL policy receives (virtual) point clouds of the gaps' edges for scalable simulation and is then distilled into the high-dimensional pixel space. However, this flight skill is fundamentally expensive to learn by exploring due to restricted feasible solution space. We propose to reset the agent as states on the trajectories by a model-based trajectory optimizer to alleviate this problem. The presented training pipeline is compared with baseline methods, and ablation studies are conducted to identify the key ingredients of our method. The immediate next step is to scale up the variation of gap sizes and geometries in anticipation of emergent policies and demonstrate the sim-to-real transformation.

Translated Abstract:
좁은 틈을 통과하는 것은 언더액추에이티드 멀티로터에겐 정말 어려운 순간 중 하나야. 우리는 이 비행 기술을 시뮬레이션에서 완전히 데이터 기반 방법으로 익히는 걸 탐구했어. 여기서 신경망이 픽셀과 자기 위치 감지를 연속적인 저수준 제어 명령으로 직접 매핑해.

이렇게 배운 정책 덕분에 다양한 형태의 틈을 통과할 때 몸 전체를 조절할 수 있어, 특히 급격한 자세 변화가 필요한 경우 (예: 거의 수직으로 기울어지는 경우)에도 말이야. 이 정책은 모델 없는 강화 학습(RL)을 통해 얻어지고, 온라인 관찰 공간 증류를 통해 이루어져. RL 정책은 틈의 경계에 대한 (가상) 포인트 클라우드를 받아서 시뮬레이션을 확장하고, 그 후 고차원 픽셀 공간으로 증류돼.

하지만 이 비행 기술은 탐색을 통해 배우기엔 기본적으로 비용이 많이 들어. 그래서 우리는 모델 기반 궤적 최적화기를 사용해 에이전트를 궤적의 상태로 리셋하는 방안을 제안했어. 이렇게 제안된 훈련 파이프라인은 기본 방법들과 비교하고, 우리의 방법의 핵심 요소를 파악하기 위해 절제 연구도 진행했어.

다음 단계는 다양한 틈 크기와 형태를 확대해 새로운 정책이 나타나길 기대하고, 시뮬레이션에서 실제로 전이되는 걸 보여주는 거야.

================================================================================

URL: https://arxiv.org/abs/2409.00867
Title: Kinematics & Dynamics Library for Baxter Arm

Original Abstract:
The Baxter robot is a standard research platform used widely in research tasks, supported with an SDK provided by the developers, Rethink Robotics. Despite the ubiquitous use of the robot, the official software support is sub-standard. Especially, the native IK service has a low success rate and is often inconsistent. This unreliable behavior makes Baxter difficult to use for experiments and the research community is in need of a more reliable software support to control the robot. We present our work towards creating a Python based software library supporting the kinematics and dynamics of the Baxter robot. Our toolbox contains implementation of pose and velocity kinematics with support for Jacobian operations for redundancy resolution. We present the implementation and performance of our library, along with a comparison with PyKDL. Keywords- Baxter Research Robot, Manipulator Kinematics, Iterative IK, Dynamical Model, Redundant Manipulator

Translated Abstract:
배서(Baxter) 로봇은 연구 작업에 널리 사용되는 표준 연구 플랫폼이야. 이 로봇은 Rethink Robotics에서 제공하는 SDK로 지원되는데, 공식 소프트웨어 지원이 그리 좋지 않아. 특히, 기본 IK 서비스의 성공률이 낮고 자주 불안정해. 이런 신뢰할 수 없는 행동 때문에 배서를 실험에 사용하기 어려워. 그래서 연구 커뮤니티는 로봇을 더 잘 제어할 수 있는 신뢰할 수 있는 소프트웨어 지원이 필요해.

우리는 배서 로봇의 운동학과 역학을 지원하는 파이썬 기반 소프트웨어 라이브러리를 만드는 작업을 소개해. 우리의 툴박스는 포즈와 속도 운동학을 구현하고, 중복 해소를 위한 야코비안 연산도 지원해. 라이브러리의 구현과 성능을 보여주고, PyKDL과의 비교도 함께 제시할 거야.

================================================================================

URL: https://arxiv.org/abs/2409.00866
Title: Vehicle-to-Everything (V2X) Communication: A Roadside Unit for Adaptive Intersection Control of Autonomous Electric Vehicles

Original Abstract:
Recent advances in autonomous vehicle technologies and cellular network speeds motivate developments in vehicle-to-everything (V2X) communications. Enhanced road safety features and improved fuel efficiency are some of the motivations behind V2X for future transportation systems. Adaptive intersection control systems have considerable potential to achieve these goals by minimizing idle times and predicting short-term future traffic conditions. Integrating V2X into traffic management systems introduces the infrastructure necessary to make roads safer for all users and initiates the shift towards more intelligent and connected cities. To demonstrate our solution, we implement both a simulated and real-world representation of a 4-way intersection and crosswalk scenario with 2 self-driving electric vehicles, a roadside unit (RSU), and traffic light. Our architecture minimizes fuel consumption through intersections by reducing acceleration and braking by up to 75.35%. We implement a cost-effective solution to intelligent and connected intersection control to serve as a proof-of-concept model suitable as the basis for continued research and development. Code for this project is available at this https URL.

Translated Abstract:
최근 자율주행차 기술과 이동통신 네트워크 속도가 발전하면서 차량-모든 것(V2X) 통신에 대한 개발이 활발해지고 있어. V2X는 도로 안전성을 높이고 연료 효율성을 개선하는 등 미래 교통 시스템을 위한 동기가 되고 있어.

적응형 교차로 제어 시스템은 대기 시간을 줄이고 단기적인 교통 상황을 예측함으로써 이런 목표를 달성할 수 있는 큰 잠재력을 가지고 있어. V2X를 교통 관리 시스템에 통합하면 모든 사용자에게 더 안전한 도로를 만들 수 있는 인프라가 마련되고, 더 스마트하고 연결된 도시로 나아가는 전환이 시작돼.

우리의 솔루션을 보여주기 위해서, 2대의 자율주행 전기차, 도로측 장치(RSU), 신호등이 있는 4방향 교차로와 횡단보도의 시뮬레이션과 실제 상황을 구현했어. 우리의 아키텍처는 교차로를 통과할 때 가속과 제동을 최대 75.35%까지 줄여 연료 소비를 최소화해.

이 프로젝트는 비용 효율적인 지능형 교차로 제어 솔루션을 제공하고, 앞으로의 연구 개발을 위한 모델로서의 역할을 해. 프로젝트 코드도 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.00864
Title: Automated Cinematography Motion Planning for UAVs

Original Abstract:
This project aimed to develop an automated cinematography platform using an unmanned aerial vehicle. Quadcopters are a great platform for shooting aerial scenes but are difficult to maneuver smoothly and can require expertise to pilot. We aim to design an algorithm to enable automated cinematography of a desired object of interest. Given the location of an object and other obstacles in the environment, the drone is able to plan its trajectory while simultaneously keeping the desired object in the video frame and avoiding obstacles. The high maneuverability of quadcopter platforms coupled with the desire for smooth movement and stability from camera platforms means a robust motion planning algorithm must be developed which can take advantage of the quadcopter's abilities while creating motion paths which satisfy the ultimate goal of capturing aerial video. This project aims to research, develop, simulate, and test such an algorithm.

Translated Abstract:
이 프로젝트는 무인 항공기를 이용한 자동 촬영 플랫폼을 개발하는 거야. 쿼드콥터는 공중 촬영에 좋은 플랫폼이지만, 부드럽게 조종하기가 어렵고 조종하기 위해서는 전문 지식이 필요할 수 있어.

우리는 관심 있는 물체를 자동으로 촬영할 수 있는 알고리즘을 설계하려고 해. 물체의 위치와 주변의 장애물을 고려해서 드론이 경로를 계획할 수 있도록 할 거야. 동시에 원하는 물체를 영상에 담고 장애물은 피해야 해.

쿼드콥터의 높은 기동성과 카메라 플랫폼에서 원하는 부드러운 움직임과 안정성을 고려할 때, 쿼드콥터의 능력을 활용하면서 공중 촬영을 제대로 할 수 있는 경로를 만드는 강력한 모션 계획 알고리즘이 필요해. 이 프로젝트는 그런 알고리즘을 연구하고 개발하며, 시뮬레이션하고 테스트하는 걸 목표로 하고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.00858
Title: Trustworthy Human-AI Collaboration: Reinforcement Learning with Human Feedback and Physics Knowledge for Safe Autonomous Driving

Original Abstract:
In the field of autonomous driving, developing safe and trustworthy autonomous driving policies remains a significant challenge. Recently, Reinforcement Learning with Human Feedback (RLHF) has attracted substantial attention due to its potential to enhance training safety and sampling efficiency. Nevertheless, existing RLHF-enabled methods often falter when faced with imperfect human demonstrations, potentially leading to training oscillations or even worse performance than rule-based approaches. Inspired by the human learning process, we propose Physics-enhanced Reinforcement Learning with Human Feedback (PE-RLHF). This novel framework synergistically integrates human feedback (e.g., human intervention and demonstration) and physics knowledge (e.g., traffic flow model) into the training loop of reinforcement learning. The key advantage of PE-RLHF is its guarantee that the learned policy will perform at least as well as the given physics-based policy, even when human feedback quality deteriorates, thus ensuring trustworthy safety improvements. PE-RLHF introduces a Physics-enhanced Human-AI (PE-HAI) collaborative paradigm for dynamic action selection between human and physics-based actions, employs a reward-free approach with a proxy value function to capture human preferences, and incorporates a minimal intervention mechanism to reduce the cognitive load on human mentors. Extensive experiments across diverse driving scenarios demonstrate that PE-RLHF significantly outperforms traditional methods, achieving state-of-the-art (SOTA) performance in safety, efficiency, and generalizability, even with varying quality of human feedback. The philosophy behind PE-RLHF not only advances autonomous driving technology but can also offer valuable insights for other safety-critical domains. Demo video and code are available at: \this https URL

Translated Abstract:
자율주행 분야에서 안전하고 신뢰할 수 있는 자율주행 정책을 개발하는 것은 여전히 큰 도전 과제야. 최근에 인간 피드백을 활용한 강화 학습(RLHF)이 훈련 안전성과 샘플링 효율성을 높일 수 있는 가능성 때문에 많은 주목을 받고 있어. 하지만 기존의 RLHF 방법들은 불완전한 인간 시연에 직면했을 때 잘 작동하지 않아서 훈련이 불안정해지거나 규칙 기반 접근법보다 더 나쁜 성능을 보일 수 있어.

우리는 인간의 학습 과정을 참고해서 물리학을 활용한 인간 피드백 강화 학습(PE-RLHF)을 제안해. 이 새로운 프레임워크는 인간 피드백(예: 인간의 개입과 시연)과 물리학 지식(예: 교통 흐름 모델)을 강화 학습의 훈련 루프에 통합해. PE-RLHF의 주요 장점은 학습된 정책이 주어진 물리 기반 정책만큼은 항상 성능을 발휘하도록 보장한다는 거야. 그래서 인간 피드백의 질이 떨어져도 신뢰할 수 있는 안전 개선이 보장돼.

PE-RLHF는 인간과 물리 기반 행동 간의 동적 행동 선택을 위한 물리학 강화 인간-인공지능(PE-HAI) 협업 패러다임을 도입하고, 인간의 선호를 포착하기 위해 대리 값 함수를 사용하는 보상 없는 접근 방식을 활용해. 또 최소한의 개입 메커니즘을 포함시켜 인간 멘토의 인지 부담을 줄여.

다양한 주행 시나리오에서 진행한 광범위한 실험 결과, PE-RLHF가 기존 방법들보다 훨씬 뛰어난 성능을 보이며, 안전성, 효율성, 일반화 면에서 최첨단(SOTA) 성과를 달성했어. PE-RLHF의 철학은 자율주행 기술을 발전시킬 뿐만 아니라 다른 안전-critical 분야에도 유용한 통찰을 제공할 수 있어. 데모 비디오와 코드는 이 URL에서 확인할 수 있어: \this https URL

================================================================================

URL: https://arxiv.org/abs/2409.00766
Title: Dynamic Subgoal based Path Formation and Task Allocation: A NeuroFleets Approach to Scalable Swarm Robotics

Original Abstract:
This paper addresses the challenges of exploration and navigation in unknown environments from the perspective of evolutionary swarm robotics. A key focus is on path formation, which is essential for enabling cooperative swarm robots to navigate effectively. We designed the task allocation and path formation process based on a finite state machine, ensuring systematic decision-making and efficient state transitions. The approach is decentralized, allowing each robot to make decisions independently based on local information, which enhances scalability and robustness. We present a novel subgoal-based path formation method that establishes paths between locations by leveraging visually connected subgoals. Simulation experiments conducted in the Argos simulator show that this method successfully forms paths in the majority of trials. However, inter-collision (traffic) among numerous robots during path formation can negatively impact performance. To address this issue, we propose a task allocation strategy that uses local communication protocols and light signal-based communication to manage robot deployment. This strategy assesses the distance between points and determines the optimal number of robots needed for the path formation task, thereby reducing unnecessary exploration and traffic congestion. The performance of both the subgoal-based path formation method and the task allocation strategy is evaluated by comparing the path length, time, and resource usage against the A* algorithm. Simulation results demonstrate the effectiveness of our approach, highlighting its scalability, robustness, and fault tolerance.

Translated Abstract:
이 논문은 진화적 군집 로봇 관점에서 미지의 환경에서 탐색과 내비게이션의 문제를 다루고 있어. 주요 초점은 협력하는 군집 로봇이 효과적으로 이동할 수 있도록 도와주는 경로 형성에 있어.

우리는 유한 상태 기계를 기반으로 작업 할당과 경로 형성 과정을 설계했어. 이렇게 하면 체계적인 의사결정과 효율적인 상태 전환이 가능해. 이 접근 방식은 분산형이라서 각 로봇이 지역 정보를 바탕으로 독립적으로 결정을 내릴 수 있어. 이 점이 확장성과 강인성을 높여줘.

우리는 시각적으로 연결된 하위 목표를 활용해서 위치 간의 경로를 설정하는 새로운 하위 목표 기반 경로 형성 방법을 제안해. Argos 시뮬레이터에서 실시한 시뮬레이션 실험 결과, 이 방법이 대부분의 시험에서 경로를 성공적으로 형성하는 걸 보여줬어. 하지만 경로 형성 중 여러 로봇 간의 충돌(교통)이 성능에 부정적인 영향을 줄 수 있어.

이 문제를 해결하기 위해 우리는 지역 통신 프로토콜과 빛 신호 기반 통신을 활용하는 작업 할당 전략을 제안해. 이 전략은 지점 간의 거리를 평가하고 경로 형성 작업에 필요한 최적의 로봇 수를 결정해. 이렇게 하면 불필요한 탐색과 교통 혼잡을 줄일 수 있어.

하위 목표 기반 경로 형성 방법과 작업 할당 전략의 성능은 A* 알고리즘과 비교해서 경로 길이, 시간, 자원 사용량으로 평가했어. 시뮬레이션 결과는 우리 접근 방식의 효과성을 보여주고, 확장성, 강인성, 고장 허용성을 강조해.

================================================================================

URL: https://arxiv.org/abs/2409.00705
Title: Antagonist Inhibition Control in Redundant Tendon-driven Structures Based on Human Reciprocal Innervation for Wide Range Limb Motion of Musculoskeletal Humanoids

Original Abstract:
The body structure of an anatomically correct tendon-driven musculoskeletal humanoid is complex, and the difference between its geometric model and the actual robot is very large because expressing the complex routes of tendon wires in a geometric model is very difficult. If we move a tendon-driven musculoskeletal humanoid by the tendon wire lengths of the geometric model, unintended muscle tension and slack will emerge. In some cases, this can lead to the wreckage of the actual robot. To solve this problem, we focused on reciprocal innervation in the human nervous system, and then implemented antagonist inhibition control (AIC) based on the reflex. This control makes it possible to avoid unnecessary internal muscle tension and slack of tendon wires caused by model error, and to perform wide range motion safely for a long time. To verify its effectiveness, we applied AIC to the upper limb of the tendon-driven musculoskeletal humanoid, Kengoro, and succeeded in dangling for 14 minutes and doing pull-ups.

Translated Abstract:
정확한 해부학적 구조를 가진 힘줄 구동 근골격 로봇의 몸은 복잡해. 기하학적 모델과 실제 로봇 사이의 차이가 크고, 힘줄의 복잡한 경로를 기하학적 모델로 표현하는 게 어렵거든. 기하학적 모델의 힘줄 길이로 로봇을 움직이면 원치 않는 근육 긴장이나 느슨함이 생길 수 있어. 어떤 경우에는 실제 로봇이 망가질 수도 있고. 

이 문제를 해결하기 위해 우리는 사람의 신경계에서의 상호 신경 자극에 주목했어. 그리고 반사 작용을 바탕으로 한 길항 억제 제어(AIC)를 구현했지. 이 제어 방식은 모델 오류로 인해 생기는 불필요한 내부 근육 긴장과 힘줄의 느슨함을 피할 수 있게 해줘. 그래서 오랜 시간 동안 안전하게 넓은 범위의 움직임을 할 수 있어. 

이 방법의 효과를 검증하기 위해 힘줄 구동 근골격 로봇인 켄고로의 팔에 AIC를 적용했어. 그 결과, 14분 동안 매달리기도 하고 턱걸이도 성공했어.

================================================================================

URL: https://arxiv.org/abs/2409.00678
Title: Automatic Grouping of Redundant Sensors and Actuators Using Functional and Spatial Connections: Application to Muscle Grouping for Musculoskeletal Humanoids

Original Abstract:
For a robot with redundant sensors and actuators distributed throughout its body, it is difficult to construct a controller or a neural network using all of them due to computational cost and complexity. Therefore, it is effective to extract functionally related sensors and actuators, group them, and construct a controller or a network for each of these groups. In this study, the functional and spatial connections among sensors and actuators are embedded into a graph structure and a method for automatic grouping is developed. Taking a musculoskeletal humanoid with a large number of redundant muscles as an example, this method automatically divides all the muscles into regions such as the forearm, upper arm, scapula, neck, etc., which has been done by humans based on a geometric model. The functional relationship among the muscles and the spatial relationship of the neural connections are calculated without a geometric model.

Translated Abstract:
로봇이 몸 전체에 분산된 여분의 센서와 액추에이터를 가지고 있을 때, 모든 센서와 액추에이터를 사용해서 컨트롤러나 신경망을 만드는 건 계산 비용과 복잡성 때문에 어려워. 그래서 기능적으로 관련된 센서와 액추에이터를 추출하고 그룹화해서 각 그룹에 대해 컨트롤러나 네트워크를 만드는 게 효과적이야.

이 연구에서는 센서와 액추에이터 사이의 기능적이고 공간적인 연결을 그래프 구조에 담고, 자동으로 그룹화하는 방법을 개발했어. 예를 들어, 여분의 근육이 많은 근골격형 휴머노이드를 가지고 이 방법을 적용해봤어. 이 방법은 모든 근육을 팔뚝, 위팔, 어깨, 목 등과 같은 영역으로 자동으로 나누는데, 이건 원래 사람이 기하학적 모델을 기반으로 했던 작업이야. 근육 사이의 기능적 관계와 신경 연결의 공간적 관계는 기하학적 모델 없이 계산되었어.

================================================================================

URL: https://arxiv.org/abs/2409.00643
Title: Learning to Singulate Objects in Packed Environments using a Dexterous Hand

Original Abstract:
Robotic object singulation, where a robot must isolate, grasp, and retrieve a target object in a cluttered environment, is a fundamental challenge in robotic manipulation. This task is difficult due to occlusions and how other objects act as obstacles for manipulation. A robot must also reason about the effect of object-object interactions as it tries to singulate the target. Prior work has explored object singulation in scenarios where there is enough free space to perform relatively long pushes to separate objects, in contrast to when space is tight and objects have little separation from each other. In this paper, we propose the Singulating Objects in Packed Environments (SOPE) framework. We propose a novel method that involves a displacement-based state representation and a multi-phase reinforcement learning procedure that enables singulation using the 16-DOF Allegro Hand. We demonstrate extensive experiments in Isaac Gym simulation, showing the ability of our system to singulate a target object in clutter. We directly transfer the policy trained in simulation to the real world. Over 250 physical robot manipulation trials, our method obtains success rates of 79.2%, outperforming alternative learning and non-learning methods.

Translated Abstract:
로봇 물체 분리 작업은 로봇이 복잡한 환경에서 목표 물체를 분리하고 잡아서 가져오는 걸 말해. 이건 로봇 조작에서 기본적인 도전 과제야. 이 작업이 어려운 이유는 물체가 서로 가리거나 다른 물체들이 방해가 되기 때문이야. 로봇은 목표 물체를 분리할 때 물체 간 상호작용이 미치는 영향도 고려해야 해.

이전 연구들은 물체가 충분한 공간에 있을 때 긴 밀어내기를 통해 물체를 분리하는 방법을 탐구했지만, 공간이 좁고 물체들이 서로 가까이 있을 때는 다뤄지지 않았어. 그래서 우리는 포장된 환경에서 물체를 분리하는 SOPE 프레임워크를 제안해. 

우리는 새로운 방법을 제안하는데, 이 방법은 이동 기반 상태 표현과 여러 단계의 강화 학습 절차를 포함해. 이 절차를 통해 16-DOF Allegro Hand를 사용해서 물체를 분리할 수 있어. 우리는 Isaac Gym 시뮬레이션에서 많은 실험을 보여줬고, 우리 시스템이 복잡한 환경에서 목표 물체를 분리할 수 있는 능력을 입증했어. 

그리고 시뮬레이션에서 훈련한 정책을 실제 세계로 바로 옮겼어. 250회 이상의 로봇 조작 시도에서 우리의 방법은 79.2%의 성공률을 기록했고, 다른 학습 방법이나 비학습 방법보다 더 나은 성과를 냈어.

================================================================================

URL: https://arxiv.org/abs/2409.00641
Title: Deep Probabilistic Traversability with Test-time Adaptation for Uncertainty-aware Planetary Rover Navigation

Original Abstract:
Traversability assessment of deformable terrain is vital for safe rover navigation on planetary surfaces. Machine learning (ML) is a powerful tool for traversability prediction but faces predictive uncertainty. This uncertainty leads to prediction errors, increasing the risk of wheel slips and immobilization for planetary rovers. To address this issue, we integrate principal approaches to uncertainty handling -- quantification, exploitation, and adaptation -- into a single learning and planning framework for rover navigation. The key concept is \emph{deep probabilistic traversability}, forming the basis of an end-to-end probabilistic ML model that predicts slip distributions directly from rover traverse observations. This probabilistic model quantifies uncertainties in slip prediction and exploits them as traversability costs in path planning. Its end-to-end nature also allows adaptation of pre-trained models with in-situ traverse experience to reduce uncertainties. We perform extensive simulations in synthetic environments that pose representative uncertainties in planetary analog terrains. Experimental results show that our method achieves more robust path planning under novel environmental conditions than existing approaches.

Translated Abstract:
변형 가능한 지형의 통과 가능성 평가가 행성 표면에서 로버가 안전하게 탐색하는 데 중요해. 머신러닝(ML)은 통과 가능성을 예측하는 데 강력한 도구지만, 예측에 불확실성이 있어. 이 불확실성은 예측 오류를 일으켜서 행성 로버의 바퀴가 미끄러지거나 정지할 위험을 높여.

이 문제를 해결하기 위해, 우리는 불확실성을 다루는 주요 접근 방식인 정량화, 활용, 적응을 로버 탐색을 위한 단일 학습 및 계획 프레임워크로 통합했어. 핵심 개념은 '딥 확률론적 통과 가능성'으로, 이게 로버의 탐색 관찰에서 직접 미끄러짐 분포를 예측하는 end-to-end 확률론적 ML 모델의 기초가 돼.

이 확률론적 모델은 미끄러짐 예측의 불확실성을 정량화하고, 이를 경로 계획에서 통과 가능성 비용으로 활용해. 또한, end-to-end 특성 덕분에 현장 탐색 경험으로 사전 훈련된 모델을 적응시켜 불확실성을 줄일 수 있어.

우리는 행성 유사 지형에서 대표적인 불확실성을 가진 합성 환경에서 광범위한 시뮬레이션을 수행했어. 실험 결과, 우리 방법이 기존 접근 방식보다 새로운 환경 조건에서 더 강력한 경로 계획을 달성하는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.00616
Title: Incorporating General Contact Surfaces in the Kinematics of Tendon-Driven Rolling-Contact Joint Mechanisms

Original Abstract:
This paper presents the first kinematic modeling of tendon-driven rolling-contact joint mechanisms with general contact surfaces subject to external loads. We derived the kinematics as a set of recursive equations and developed efficient iterative algorithms to solve for both tendon force actuation and tendon displacement actuation. The configuration predictions of the kinematics were experimentally validated using a prototype mechanism. Our MATLAB implementation of the proposed kinematic is available at this https URL.

Translated Abstract:
이 논문은 외부 하중이 가해지는 일반 접촉 면을 가진 힘줄 구동 롤링-접촉 관절 메커니즘의 첫 번째 운동학 모델링을 제안해. 우리는 운동학을 재귀 방정식의 집합으로 도출했으며, 힘줄 힘 작용과 힘줄 변위 작용을 해결하기 위해 효율적인 반복 알고리즘을 개발했어. 운동학의 구성 예측은 프로토타입 메커니즘을 사용해 실험적으로 검증했어. 제안된 운동학의 MATLAB 구현은 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.00593
Title: Online Temporal Fusion for Vectorized Map Construction in Mapless Autonomous Driving

Original Abstract:
To reduce the reliance on high-definition (HD) maps, a growing trend in autonomous driving is leveraging on-board sensors to generate vectorized maps online. However, current methods are mostly constrained by processing only single-frame inputs, which hampers their robustness and effectiveness in complex scenarios. To overcome this problem, we propose an online map construction system that exploits the long-term temporal information to build a consistent vectorized map. First, the system efficiently fuses all historical road marking detections from an off-the-shelf network into a semantic voxel map, which is implemented using a hashing-based strategy to exploit the sparsity of road elements. Then reliable voxels are found by examining the fused information and incrementally clustered into an instance-level representation of road markings. Finally, the system incorporates domain knowledge to estimate the geometric and topological structures of roads, which can be directly consumed by the planning and control (PnC) module. Through experiments conducted in complicated urban environments, we have demonstrated that the output of our system is more consistent and accurate than the network output by a large margin and can be effectively used in a closed-loop autonomous driving system.

Translated Abstract:
고해상도(HD) 지도에 대한 의존도를 줄이기 위해, 자율주행 분야에서는 차량에 장착된 센서를 이용해 온라인으로 벡터화된 지도를 만드는 추세가 늘고 있어. 하지만 현재 방법들은 대부분 단일 프레임 입력만 처리할 수 있어서 복잡한 상황에서의 강건성과 효과성이 떨어져. 

이 문제를 해결하기 위해 우리는 장기적인 시간 정보를 활용해 일관된 벡터화된 지도를 만드는 온라인 지도 구축 시스템을 제안해. 먼저, 이 시스템은 시중에 나와 있는 네트워크를 이용해 모든 역사적인 도로 표지 검출 결과를 효율적으로 융합해 의미 있는 복셀 맵을 만들고, 도로 요소의 희소성을 활용하기 위해 해싱 기반 전략을 적용해. 그런 다음 융합된 정보를 검토해서 신뢰할 수 있는 복셀을 찾고, 이를 점진적으로 도로 표지의 인스턴스 레벨 표현으로 클러스터링해. 

마지막으로, 이 시스템은 도메인 지식을 활용해 도로의 기하학적 및 위상적 구조를 추정하는데, 이 구조는 계획 및 제어(PnC) 모듈에서 직접 사용할 수 있어. 복잡한 도시 환경에서 진행한 실험을 통해, 우리의 시스템이 네트워크 출력보다 훨씬 더 일관되고 정확한 결과를 제공하며, 폐쇄 루프 자율주행 시스템에서 효과적으로 사용할 수 있다는 것을 입증했어.

================================================================================

URL: https://arxiv.org/abs/2409.00588
Title: Diffusion Policy Policy Optimization

Original Abstract:
We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG methods are ubiquitous in training RL policies with other policy parameterizations; nevertheless, they had been conjectured to be less efficient for diffusion-based policies. Surprisingly, we show that DPPO achieves the strongest overall performance and efficiency for fine-tuning in common benchmarks compared to other RL methods for diffusion-based policies and also compared to PG fine-tuning of other policy parameterizations. Through experimental investigation, we find that DPPO takes advantage of unique synergies between RL fine-tuning and the diffusion parameterization, leading to structured and on-manifold exploration, stable training, and strong policy robustness. We further demonstrate the strengths of DPPO in a range of realistic settings, including simulated robotic tasks with pixel observations, and via zero-shot deployment of simulation-trained policies on robot hardware in a long-horizon, multi-stage manipulation task. Website with code: this http URL

Translated Abstract:
우리는 DPPO(Diffusion Policy Policy Optimization)라는 알고리즘 프레임워크를 소개해. 이건 연속 제어와 로봇 학습 과제에서 확산 기반 정책(예: Diffusion Policy)을 미세 조정하는 데 필요한 최선의 방법들을 포함하고 있어. 이 과정에서 강화 학습(RL)에서 사용하는 정책 기울기(PG) 방법을 활용해.

PG 방법은 RL 정책을 훈련하는 데 많이 쓰이지만, 확산 기반 정책에는 덜 효율적일 거라는 추측이 있었어. 그런데 놀랍게도, DPPO가 일반적인 벤치마크에서 다른 RL 방법들보다도 확산 기반 정책의 미세 조정에서 가장 뛰어난 성능과 효율성을 보여준다는 걸 밝혀냈어. 또 다른 정책 매개변수화의 PG 미세 조정과 비교했을 때도 마찬가지야.

실험을 통해 DPPO가 RL 미세 조정과 확산 매개변수화 간의 독특한 시너지를 활용하고 있다는 걸 발견했어. 덕분에 구조화된 탐색과 안정적인 훈련, 강력한 정책 내구성을 이끌어내고 있어. 우리는 DPPO의 장점을 픽셀 관찰을 통한 시뮬레이션 로봇 작업과 로봇 하드웨어에서 시뮬레이션 훈련된 정책을 제로샷으로 배포하는 긴 시간의 다단계 조작 작업 등 다양한 현실적인 설정에서 보여주었어. 코드가 있는 웹사이트는 이 URL이야: this http URL

================================================================================

URL: https://arxiv.org/abs/2409.00572
Title: The Persistent Robot Charging Problem for Long-Duration Autonomy

Original Abstract:
This paper introduces a novel formulation aimed at determining the optimal schedule for recharging a fleet of $n$ heterogeneous robots, with the primary objective of minimizing resource utilization. This study provides a foundational framework applicable to Multi-Robot Mission Planning, particularly in scenarios demanding Long-Duration Autonomy (LDA) or other contexts that necessitate periodic recharging of multiple robots. A novel Integer Linear Programming (ILP) model is proposed to calculate the optimal initial conditions (partial charge) for individual robots, leading to the minimal utilization of charging stations. This formulation was further generalized to maximize the servicing time for robots given adequate charging stations. The efficacy of the proposed formulation is evaluated through a comparative analysis, measuring its performance against the thrift price scheduling algorithm documented in the existing literature. The findings not only validate the effectiveness of the proposed approach but also underscore its potential as a valuable tool in optimizing resource allocation for a range of robotic and engineering applications.

Translated Abstract:
이 논문은 다양한 로봇들로 구성된 로봇 군집의 최적 충전 스케줄을 정하는 새로운 방법을 제안해. 주된 목표는 자원 사용을 최소화하는 거야. 이 연구는 다중 로봇 임무 계획에 적용할 수 있는 기본 프레임워크를 제공하는데, 특히 장시간 자율 주행이 필요한 상황이나 여러 로봇이 주기적으로 충전해야 하는 경우에 유용해.

새로운 정수 선형 프로그래밍(ILP) 모델을 제안해서 각 로봇의 최적 초기 조건(부분 충전)을 계산해. 이렇게 하면 충전소의 사용을 최소화할 수 있어. 이 방법은 충분한 충전소가 있을 때 로봇의 서비스 시간을 최대화하는 방향으로도 일반화되었어.

제안된 방법의 효과는 기존 문헌에 있는 절약 가격 스케줄링 알고리즘과 비교 분석을 통해 평가했어. 결과는 제안된 접근 방식의 효과를 확인해줄 뿐만 아니라, 다양한 로봇 및 공학 응용 프로그램에서 자원 할당을 최적화하는 데 유용한 도구가 될 수 있음을 강조해.

================================================================================

URL: https://arxiv.org/abs/2409.00499
Title: DAP: Diffusion-based Affordance Prediction for Multi-modality Storage

Original Abstract:
Solving storage problem: where objects must be accurately placed into containers with precise orientations and positions, presents a distinct challenge that extends beyond traditional rearrangement tasks. These challenges are primarily due to the need for fine-grained 6D manipulation and the inherent multi-modality of solution spaces, where multiple viable goal configurations exist for the same storage container. We present a novel Diffusion-based Affordance Prediction (DAP) pipeline for the multi-modal object storage problem. DAP leverages a two-step approach, initially identifying a placeable region on the container and then precisely computing the relative pose between the object and that region. Existing methods either struggle with multi-modality issues or computation-intensive training. Our experiments demonstrate DAP's superior performance and training efficiency over the current state-of-the-art RPDiff, achieving remarkable results on the RPDiff benchmark. Additionally, our experiments showcase DAP's data efficiency in real-world applications, an advancement over existing simulation-driven approaches. Our contribution fills a gap in robotic manipulation research by offering a solution that is both computationally efficient and capable of handling real-world variability. Code and supplementary material can be found at: this https URL.

Translated Abstract:
물체를 정확하게 배치해야 하는 저장 문제는 전통적인 재배치 작업과는 다른 독특한 도전 과제를 제시해. 이 문제는 정밀한 6D 조작이 필요하고, 같은 저장 용기에 대해 여러 가지 가능한 목표 구성이 존재하는 복합적인 해결 공간이 있기 때문에 발생해.

우리는 다중 모드 물체 저장 문제를 해결하기 위해 새로운 확산 기반의 가능성 예측(DAP) 파이프라인을 제안해. DAP는 두 단계로 진행되는데, 먼저 용기에서 배치 가능한 영역을 찾고, 그 후에 물체와 그 영역 간의 상대적인 자세를 정확하게 계산해. 기존 방법들은 다중 모드 문제나 계산 집약적인 훈련에서 어려움을 겪고 있어.

우리의 실험 결과 DAP는 현재 최고 수준의 RPDiff보다 성능과 훈련 효율성이 뛰어난 것을 보여줬어. 특히 RPDiff 벤치마크에서 놀라운 결과를 달성했어. 또한, 우리의 실험은 DAP가 실제 세계 응용에서 데이터 효율성이 뛰어난 것을 보여줬고, 기존의 시뮬레이션 기반 접근 방식보다 발전된 거야.

우리가 제안하는 방법은 로봇 조작 연구에서 중요한 공백을 채우며, 계산 효율적이면서도 실제 세계의 변동성을 처리할 수 있는 솔루션을 제공해. 코드와 보충 자료는 이 링크에서 확인할 수 있어: 이 URL.

================================================================================

URL: https://arxiv.org/abs/2409.00303
Title: Rapid and Robust Trajectory Optimization for Humanoids

Original Abstract:
Performing trajectory design for humanoid robots with high degrees of freedom is computationally challenging. The trajectory design process also often involves carefully selecting various hyperparameters and requires a good initial guess which can further complicate the development process. This work introduces a generalized gait optimization framework that directly generates smooth and physically feasible trajectories. The proposed method demonstrates faster and more robust convergence than existing techniques and explicitly incorporates closed-loop kinematic constraints that appear in many modern humanoids. The method is implemented as an open-source C++ codebase which can be found at this https URL.

Translated Abstract:
고유한 자유도를 가진 휴머노이드 로봇의 경로 설계는 컴퓨터 자원 소모가 많아. 경로 설계 과정에서는 다양한 하이퍼파라미터를 신중하게 선택해야 하고, 좋은 초기 추정값이 필요해. 이게 개발 과정을 더 복잡하게 만들지.

이 연구에서는 부드럽고 물리적으로 가능한 경로를 직접 생성하는 일반화된 보행 최적화 프레임워크를 소개해. 제안된 방법은 기존 기술들보다 더 빠르고 강력하게 수렴하며, 많은 현대 휴머노이드에서 나타나는 폐쇄 루프 운동학적 제약을 명시적으로 포함하고 있어. 

이 방법은 오픈 소스 C++ 코드베이스로 구현되어 있고, 이 URL에서 찾을 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.00215
Title: Constraint-Aware Intent Estimation for Dynamic Human-Robot Object Co-Manipulation

Original Abstract:
Constraint-aware estimation of human intent is essential for robots to physically collaborate and interact with humans. Further, to achieve fluid collaboration in dynamic tasks intent estimation should be achieved in real-time. In this paper, we present a framework that combines online estimation and control to facilitate robots in interpreting human intentions, and dynamically adjust their actions to assist in dynamic object co-manipulation tasks while considering both robot and human constraints. Central to our approach is the adoption of a Dynamic Systems (DS) model to represent human intent. Such a low-dimensional parameterized model, along with human manipulability and robot kinematic constraints, enables us to predict intent using a particle filter solely based on past motion data and tracking errors. For safe assistive control, we propose a variable impedance controller that adapts the robot's impedance to offer assistance based on the intent estimation confidence from the DS particle filter. We validate our framework on a challenging real-world human-robot co-manipulation task and present promising results over baselines. Our framework represents a significant step forward in physical human-robot collaboration (pHRC), ensuring that robot cooperative interactions with humans are both feasible and effective.

Translated Abstract:
인간의 의도를 제약을 고려해서 추정하는 건 로봇이 사람과 함께 물리적으로 협력하고 상호작용하는 데 정말 중요해. 그리고 동적인 작업에서 부드럽게 협력하려면 의도 추정이 실시간으로 이루어져야 해.

이 논문에서는 로봇이 인간의 의도를 해석하고, 동적인 물체 공동 조작 작업을 도와주기 위해 행동을 동적으로 조정할 수 있도록 온라인 추정과 제어를 결합한 프레임워크를 제시해. 이때 로봇과 인간의 제약도 고려해.

우리가 사용하는 방법의 핵심은 인간의 의도를 나타내기 위해 동적 시스템(DS) 모델을 채택하는 거야. 이런 낮은 차원의 매개변수화된 모델과 인간의 조작 가능성, 로봇의 운동학적 제약 덕분에, 우리는 과거의 움직임 데이터와 추적 오류만을 기반으로 입자 필터를 사용해 의도를 예측할 수 있어.

안전한 보조 제어를 위해, 우리는 DS 입자 필터의 의도 추정 신뢰도에 따라 로봇의 임피던스를 조정하는 가변 임피던스 제어기를 제안해. 우리는 이 프레임워크를 실제 인간-로봇 공동 조작 작업에 적용해 보고, 기준선보다도 좋은 결과를 얻었어.

우리의 프레임워크는 물리적 인간-로봇 협력(pHRC)에서 중요한 진전을 의미해. 이로 인해 로봇과 인간 간의 협력적 상호작용이 더 가능하고 효과적으로 이루어질 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.01974
Title: Planning to avoid ambiguous states through Gaussian approximations to non-linear sensors in active inference agents

Original Abstract:
In nature, active inference agents must learn how observations of the world represent the state of the agent. In engineering, the physics behind sensors is often known reasonably accurately and measurement functions can be incorporated into generative models. When a measurement function is non-linear, the transformed variable is typically approximated with a Gaussian distribution to ensure tractable inference. We show that Gaussian approximations that are sensitive to the curvature of the measurement function, such as a second-order Taylor approximation, produce a state-dependent ambiguity term. This induces a preference over states, based on how accurately the state can be inferred from the observation. We demonstrate this preference with a robot navigation experiment where agents plan trajectories.

Translated Abstract:
자연에서는 능동적 추론 에이전트가 세상의 관찰이 에이전트의 상태를 어떻게 나타내는지를 배워야 해. 공학에서는 센서의 물리적 원리가 대체로 정확하게 알려져 있고, 측정 함수는 생성 모델에 포함될 수 있어. 

측정 함수가 비선형일 때는, 변환된 변수를 가우시안 분포로 근사하는 경우가 많아. 이렇게 해야 추론이 쉽게 진행될 수 있거든. 우리는 측정 함수의 곡률에 민감한 가우시안 근사가 상태에 따라 모호함을 나타내는 항을 생성한다는 걸 보여줬어. 이건 관찰을 통해 상태를 얼마나 정확하게 추론할 수 있는지에 따라 상태에 대한 선호를 유도해. 

우리는 로봇 내비게이션 실험을 통해 이 선호를 보여줬는데, 여기서 에이전트들이 경로를 계획하는 방식으로 나타났어.

================================================================================

URL: https://arxiv.org/abs/2409.01427
Title: Enhancing Sample Efficiency and Exploration in Reinforcement Learning through the Integration of Diffusion Models and Proximal Policy Optimization

Original Abstract:
Recent advancements in reinforcement learning (RL) have been fueled by large-scale data and deep neural networks, particularly for high-dimensional and complex tasks. Online RL methods like Proximal Policy Optimization (PPO) are effective in dynamic scenarios but require substantial real-time data, posing challenges in resource-constrained or slow simulation environments. Offline RL addresses this by pre-learning policies from large datasets, though its success depends on the quality and diversity of the data. This work proposes a framework that enhances PPO algorithms by incorporating a diffusion model to generate high-quality virtual trajectories for offline datasets. This approach improves exploration and sample efficiency, leading to significant gains in cumulative rewards, convergence speed, and strategy stability in complex tasks. Our contributions are threefold: we explore the potential of diffusion models in RL, particularly for offline datasets, extend the application of online RL to offline environments, and experimentally validate the performance improvements of PPO with diffusion models. These findings provide new insights and methods for applying RL to high-dimensional, complex tasks. Finally, we open-source our code at this https URL

Translated Abstract:
최근 강화 학습(RL) 분야는 대규모 데이터와 딥 뉴럴 네트워크 덕분에 발전해왔어. 특히 고차원 복잡한 작업에서 효과적이지. Proximal Policy Optimization(PPO) 같은 온라인 RL 방법은 동적 상황에서 잘 작동하지만, 실시간 데이터가 많이 필요해. 이 때문에 자원이 제한된 환경이나 느린 시뮬레이션에서는 어려움이 있어.

오프라인 RL은 대규모 데이터셋에서 미리 정책을 학습함으로써 이 문제를 해결해. 하지만 성공 여부는 데이터의 품질과 다양성에 따라 달라져. 이 연구에서는 PPO 알고리즘을 개선하기 위해 확산 모델을 도입해서 고품질 가상 궤적을 생성하는 프레임워크를 제안해. 이 방법은 탐색 및 샘플 효율성을 높여주고, 복잡한 작업에서 누적 보상, 수렴 속도, 전략 안정성을 크게 향상시켜.

우리의 기여는 세 가지야: 첫째, 오프라인 데이터셋에서 RL에 대한 확산 모델의 잠재력을 탐구하고, 둘째, 온라인 RL의 적용을 오프라인 환경으로 확장하고, 셋째, 확산 모델을 적용한 PPO의 성능 향상을 실험적으로 검증했어. 이 발견들은 RL을 고차원 복잡한 작업에 적용하는 데 새로운 통찰과 방법을 제공해. 마지막으로, 우리의 코드는 이 URL에서 오픈 소스로 제공해.

================================================================================

URL: https://arxiv.org/abs/2409.01411
Title: Performance-Aware Self-Configurable Multi-Agent Networks: A Distributed Submodular Approach for Simultaneous Coordination and Network Design

Original Abstract:
We introduce the first, to our knowledge, rigorous approach that enables multi-agent networks to self-configure their communication topology to balance the trade-off between scalability and optimality during multi-agent planning. We are motivated by the future of ubiquitous collaborative autonomy where numerous distributed agents will be coordinating via agent-to-agent communication to execute complex tasks such as traffic monitoring, event detection, and environmental exploration. But the explosion of information in such large-scale networks currently curtails their deployment due to impractical decision times induced by the computational and communication requirements of the existing near-optimal coordination algorithms. To overcome this challenge, we present the AlterNAting COordination and Network-Design Algorithm (Anaconda), a scalable algorithm that also enjoys near-optimality guarantees. Subject to the agents' bandwidth constraints, Anaconda enables the agents to optimize their local communication neighborhoods such that the action-coordination approximation performance of the network is maximized. Compared to the state of the art, Anaconda is an anytime self-configurable algorithm that quantifies its suboptimality guarantee for any type of network, from fully disconnected to fully centralized, and that, for sparse networks, is one order faster in terms of decision speed. To develop the algorithm, we quantify the suboptimality cost due to decentralization, i.e., due to communication-minimal distributed coordination. We also employ tools inspired by the literature on multi-armed bandits and submodular maximization subject to cardinality constraints. We demonstrate Anaconda in simulated scenarios of area monitoring and compare it with a state-of-the-art algorithm.

Translated Abstract:
우리는 다중 에이전트 네트워크가 통신 구조를 스스로 설정할 수 있게 해주는 첫 번째 엄격한 접근 방식을 소개해. 이 방식은 다중 에이전트 계획 시 확장성과 최적성 간의 균형을 맞추는 데 도움이 돼.

우리가 이 연구를 하게 된 이유는, 미래의 협력 자율 시스템에서 많은 분산된 에이전트들이 서로 통신하며 복잡한 작업을 수행할 것이라는 점이야. 예를 들어, 교통 모니터링, 사건 탐지, 환경 탐사 같은 것들이지. 하지만, 대규모 네트워크에서 정보가 폭발적으로 늘어나면 기존의 근접 최적 협동 알고리즘의 계산 및 통신 요구로 인해 실용적인 결정 시간이 줄어들어 배치가 어려워져.

이 문제를 해결하기 위해 우리는 AlterNAting COordination and Network-Design Algorithm, 줄여서 Anaconda라는 스케일러블한 알고리즘을 제안해. 이 알고리즘은 거의 최적의 성능을 보장해. Anaconda는 에이전트의 대역폭 제약에 따라, 에이전트들이 지역 통신 이웃을 최적화하게 해줘서 네트워크의 행동 조정 성능을 극대화할 수 있어.

최신 기술과 비교했을 때, Anaconda는 언제든지 스스로 구성할 수 있는 알고리즘이고, 완전히 분리된 네트워크부터 완전히 중앙화된 네트워크까지 어떤 유형의 네트워크에서도 하위 최적성 보장을 정량화해. 특히 희소한 네트워크에서는 결정 속도 면에서 한 단계 더 빠르지.

이 알고리즘을 개발하기 위해 우리는 분산화로 인한 하위 최적성 비용을 정량화했어. 즉, 통신 최소화된 분산 조정에서 오는 비용이야. 또한, 다중 무장 강도 문제와 하위 모듈 최대화에 관한 문헌에서 영감을 받은 도구들도 활용했어. 마지막으로, Anaconda를 지역 모니터링의 시뮬레이션 시나리오에서 시연하고, 최신 알고리즘과 비교할 거야.

================================================================================

URL: https://arxiv.org/abs/2409.01324
Title: An Investigation of Denial of Service Attacks on Autonomous Driving Software and Hardware in Operation

Original Abstract:
This research investigates the impact of Denial of Service (DoS) attacks, specifically Internet Control Message Protocol (ICMP) flood attacks, on Autonomous Driving (AD) systems, focusing on their control modules. Two experimental setups were created: the first involved an ICMP flood attack on a Raspberry Pi running an AD software stack, and the second examined the effects of single and double ICMP flood attacks on a Global Navigation Satellite System Real-Time Kinematic (GNSS-RTK) device for high-accuracy localization of an autonomous vehicle that is available on the market. The results indicate a moderate impact of DoS attacks on the AD stack, where the increase in median computation time was marginal, suggesting a degree of resilience to these types of attacks. In contrast, the GNSS device demonstrated significant vulnerability: during DoS attacks, the sample rate dropped drastically to approximately 50% and 5% of the nominal rate for single and double attacker configurations, respectively. Additionally, the longest observed time increments were in the range of seconds during the attacks. These results underscore the vulnerability of AD systems to DoS attacks and the critical need for robust cybersecurity measures. This work provides valuable insights into the design requirements of AD software stacks and highlights that external hardware and modules can be significant attack surfaces.

Translated Abstract:
이 연구는 서비스 거부 공격(DoS), 특히 인터넷 제어 메시지 프로토콜(ICMP) 플러드 공격이 자율주행(AD) 시스템에 미치는 영향을 조사해. 주로 제어 모듈에 초점을 맞췄어. 

두 가지 실험 세트를 만들었고, 첫 번째는 AD 소프트웨어 스택을 실행하는 라즈베리 파이에 ICMP 플러드 공격을 가하는 거였고, 두 번째는 자율주행차의 고정밀 위치지정을 위한 글로벌 네비게이션 위성 시스템 실시간 운동학(GNSS-RTK) 장치에 단일 및 이중 ICMP 플러드 공격을 가하는 거였어. 

결과를 보면, AD 스택에 대한 DoS 공격의 영향은 보통 정도였고, 중간 계산 시간이 조금 늘어나긴 했지만 이런 공격에 대한 어느 정도의 저항력을 보여줬어. 반면, GNSS 장치는 상당한 취약성을 보였어. DoS 공격 중에 샘플 비율이 단일 공격자 설정에서는 약 50%, 이중 공격자 설정에서는 5%로 급격히 떨어졌어. 게다가 공격 동안 가장 긴 시간 증가가 몇 초에 이르렀어. 

이 결과들은 AD 시스템이 DoS 공격에 얼마나 취약한지를 강조해 주고, 강력한 사이버 보안 조치가 필요하다는 점을 알려줘. 이 연구는 AD 소프트웨어 스택의 설계 요구사항에 대한 귀중한 통찰을 제공하고, 외부 하드웨어와 모듈이 중요한 공격 표면이 될 수 있음을 강조하고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.01245
Title: Revisiting Safe Exploration in Safe Reinforcement learning

Original Abstract:
Safe reinforcement learning (SafeRL) extends standard reinforcement learning with the idea of safety, where safety is typically defined through the constraint of the expected cost return of a trajectory being below a set limit. However, this metric fails to distinguish how costs accrue, treating infrequent severe cost events as equal to frequent mild ones, which can lead to riskier behaviors and result in unsafe exploration. We introduce a new metric, expected maximum consecutive cost steps (EMCC), which addresses safety during training by assessing the severity of unsafe steps based on their consecutive occurrence. This metric is particularly effective for distinguishing between prolonged and occasional safety violations. We apply EMMC in both on- and off-policy algorithm for benchmarking their safe exploration capability. Finally, we validate our metric through a set of benchmarks and propose a new lightweight benchmark task, which allows fast evaluation for algorithm design.

Translated Abstract:
안전 강화 학습(SafeRL)은 표준 강화 학습에 안전 개념을 추가한 거야. 여기서 안전은 보통 예상 비용이 정해진 한계 아래로 유지되는 걸로 정의돼. 하지만, 이 방식은 비용이 어떻게 발생하는지를 구분하지 못해. 가끔 발생하는 큰 비용 사건과 자주 발생하는 작은 비용 사건을 똑같이 취급하게 되는데, 이건 더 위험한 행동으로 이어질 수 있고 안전하지 않은 탐색을 초래할 수 있어.

그래서 우리는 새로운 지표인 예상 최대 연속 비용 단계(EMCC)를 도입했어. 이 지표는 훈련 중에 안전성을 평가하는 데 도움이 되는데, 위험한 단계들이 얼마나 연속적으로 발생했는지를 기반으로 심각도를 판단해. 이 지표는 장기간 발생하는 안전 위반과 가끔 발생하는 안전 위반을 구분하는 데 특히 효과적이야.

우리는 EMMC를 온정책과 오프정책 알고리즘에 적용해서 안전 탐색 능력을 비교했어. 마지막으로, 이 지표를 여러 벤치마크를 통해 검증하고, 알고리즘 설계를 위한 빠른 평가를 가능하게 하는 새로운 경량 벤치마크 작업도 제안했어.

================================================================================

URL: https://arxiv.org/abs/2409.01178
Title: Integrating End-to-End and Modular Driving Approaches for Online Corner Case Detection in Autonomous Driving

Original Abstract:
Online corner case detection is crucial for ensuring safety in autonomous driving vehicles. Current autonomous driving approaches can be categorized into modular approaches and end-to-end approaches. To leverage the advantages of both, we propose a method for online corner case detection that integrates an end-to-end approach into a modular system. The modular system takes over the primary driving task and the end-to-end network runs in parallel as a secondary one, the disagreement between the systems is then used for corner case detection. We implement this method on a real vehicle and evaluate it qualitatively. Our results demonstrate that end-to-end networks, known for their superior situational awareness, as secondary driving systems, can effectively contribute to corner case detection. These findings suggest that such an approach holds potential for enhancing the safety of autonomous vehicles.

Translated Abstract:
온라인 코너 케이스 감지는 자율주행 차량의 안전을 보장하는 데 아주 중요해. 현재 자율주행 방식은 모듈형 접근법과 엔드 투 엔드 접근법으로 나눌 수 있어. 두 가지의 장점을 모두 활용하기 위해, 우리는 엔드 투 엔드 방식을 모듈 시스템에 통합한 온라인 코너 케이스 감지 방법을 제안해.

모듈 시스템이 기본 주행 작업을 맡고, 엔드 투 엔드 네트워크는 보조적으로 병행해서 작동해. 이 두 시스템 간의 불일치 점을 이용해 코너 케이스를 감지하는 거야. 이 방법을 실제 차량에 구현하고, 질적으로 평가해봤어.

결과적으로, 상황 인식이 뛰어난 엔드 투 엔드 네트워크가 보조 주행 시스템으로서 코너 케이스 감지에 효과적으로 기여할 수 있음을 보여줬어. 이런 방식을 통해 자율주행 차량의 안전성을 높일 가능성이 있다는 걸 발견했어.

================================================================================

URL: https://arxiv.org/abs/2409.00869
Title: Detection, Recognition and Pose Estimation of Tabletop Objects

Original Abstract:
The problem of cleaning a messy table using Deep Neural Networks is a very interesting problem in both social and industrial robotics. This project focuses on the social application of this technology. A neural network model that is capable of detecting and recognizing common tabletop objects, such as a mug, mouse, or stapler is developed. The model also predicts the angle at which these objects are placed on a table,with respect to some reference. Assuming each object has a fixed intended position and orientation on the tabletop, the orientation of a particular object predicted by the deep learning model can be used to compute the transformation matrix to move the object from its initial position to the intended position. This can be fed to a pick and place robot to carry out the transfer.This paper talks about the deep learning approaches used in this project for object detection and orientation estimation.

Translated Abstract:
지저분한 테이블을 청소하는 문제는 소셜 로봇과 산업 로봇 둘 다에서 아주 흥미로운 주제야. 이 프로젝트는 이 기술의 사회적 응용에 초점을 맞추고 있어. 

여기서는 머그컵, 마우스, 스테이플러 같은 일반적인 테이블 위 물체를 감지하고 인식할 수 있는 신경망 모델을 개발했어. 이 모델은 물체가 테이블 위에 놓인 각도도 예측할 수 있어. 각 물체는 고정된 위치와 방향이 있다고 가정할 때, 딥러닝 모델이 예측한 특정 물체의 방향을 이용해 물체를 초기 위치에서 원하는 위치로 이동시키기 위한 변환 행렬을 계산할 수 있어. 이 정보를 픽 앤 플레이스 로봇에 전달하면 물체를 옮길 수 있지.

이 논문에서는 이 프로젝트에서 물체 감지와 방향 추정을 위해 사용된 딥러닝 접근법에 대해 이야기하고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.00744
Title: DSLO: Deep Sequence LiDAR Odometry Based on Inconsistent Spatio-temporal Propagation

Original Abstract:
This paper introduces a 3D point cloud sequence learning model based on inconsistent spatio-temporal propagation for LiDAR odometry, termed DSLO. It consists of a pyramid structure with a spatial information reuse strategy, a sequential pose initialization module, a gated hierarchical pose refinement module, and a temporal feature propagation module. First, spatial features are encoded using a point feature pyramid, with features reused in successive pose estimations to reduce computational overhead. Second, a sequential pose initialization method is introduced, leveraging the high-frequency sampling characteristic of LiDAR to initialize the LiDAR pose. Then, a gated hierarchical pose refinement mechanism refines poses from coarse to fine by selectively retaining or discarding motion information from different layers based on gate estimations. Finally, temporal feature propagation is proposed to incorporate the historical motion information from point cloud sequences, and address the spatial inconsistency issue when transmitting motion information embedded in point clouds between frames. Experimental results on the KITTI odometry dataset and Argoverse dataset demonstrate that DSLO outperforms state-of-the-art methods, achieving at least a 15.67\% improvement on RTE and a 12.64\% improvement on RRE, while also achieving a 34.69\% reduction in runtime compared to baseline methods. Our implementation will be available at this https URL.

Translated Abstract:
이 논문에서는 LiDAR 오도메트리를 위한 불일치적인 시공간 전파 기반의 3D 포인트 클라우드 시퀀스 학습 모델인 DSLO를 소개해. 이 모델은 피라미드 구조와 공간 정보를 재사용하는 전략, 순차 포즈 초기화 모듈, 게이트 계층 포즈 정제 모듈, 그리고 시계열 특징 전파 모듈로 구성돼.

먼저, 포인트 피처 피라미드를 사용해 공간 특징을 인코딩하고, 다음 포즈 추정에서 특징을 재사용해서 계산 부담을 줄여. 두 번째로, LiDAR의 고주파 샘플링 특성을 활용해 LiDAR 포즈를 초기화하는 순차 포즈 초기화 방법이 도입돼. 그 다음에는 게이트 계층 포즈 정제 메커니즘이 도입되는데, 이는 서로 다른 레이어에서 모션 정보를 선택적으로 유지하거나 버려서 포즈를 거칠게부터 세밀하게 다듬어.

마지막으로, 시계열 특징 전파를 통해 포인트 클라우드 시퀀스의 과거 모션 정보를 포함하고, 프레임 간에 포인트 클라우드에 내장된 모션 정보를 전송할 때의 공간 불일치 문제를 해결하려고 해. KITTI 오도메트리 데이터셋과 Argoverse 데이터셋에서의 실험 결과, DSLO가 최신 기술을 능가하며 RTE에서 최소 15.67% 향상, RRE에서 12.64% 향상을 보여주었고, 기본 방법들에 비해 런타임도 34.69% 줄였어. 우리의 구현은 이 https URL에서 사용할 수 있을 거야.

================================================================================

URL: https://arxiv.org/abs/2409.00536
Title: Formal Verification and Control with Conformal Prediction

Original Abstract:
In this survey, we design formal verification and control algorithms for autonomous systems with practical safety guarantees using conformal prediction (CP), a statistical tool for uncertainty quantification. We focus on learning-enabled autonomous systems (LEASs) in which the complexity of learning-enabled components (LECs) is a major bottleneck that hampers the use of existing model-based verification and design techniques. Instead, we advocate for the use of CP, and we will demonstrate its use in formal verification, systems and control theory, and robotics. We argue that CP is specifically useful due to its simplicity (easy to understand, use, and modify), generality (requires no assumptions on learned models and data distributions, i.e., is distribution-free), and efficiency (real-time capable and accurate).
We pursue the following goals with this survey. First, we provide an accessible introduction to CP for non-experts who are interested in using CP to solve problems in autonomy. Second, we show how to use CP for the verification of LECs, e.g., for verifying input-output properties of neural networks. Third and fourth, we review recent articles that use CP for safe control design as well as offline and online verification of LEASs. We summarize their ideas in a unifying framework that can deal with the complexity of LEASs in a computationally efficient manner. In our exposition, we consider simple system specifications, e.g., robot navigation tasks, as well as complex specifications formulated in temporal logic formalisms. Throughout our survey, we compare to other statistical techniques (e.g., scenario optimization, PAC-Bayes theory, etc.) and how these techniques have been used in verification and control. Lastly, we point the reader to open problems and future research directions.

Translated Abstract:
이 설문조사에서는 불확실성을 정량화하는 통계 도구인 적합 예측(conformal prediction, CP)을 사용해 실제 안전 보장이 있는 자율 시스템에 대한 형식 검증 및 제어 알고리즘을 설계했어. 우리는 학습 기능이 있는 자율 시스템(LEASs)에 집중하는데, 여기서 학습 기능이 있는 구성 요소(LECs)의 복잡성이 기존 모델 기반 검증 및 설계 기술의 사용을 방해하는 주요 병목 현상이야. 그래서 우리는 CP의 사용을 권장하고, 형식 검증, 시스템 및 제어 이론, 로봇 공학에서의 활용을 보여줄 거야.

CP는 이해하기 쉽고 사용하기 간편하며 수정도 쉬워서 특히 유용해. 또한, CP는 학습된 모델이나 데이터 분포에 대한 가정을 요구하지 않아서 일반성이 뛰어나고, 실시간으로 정확하게 작동할 수 있어서 효율적이야.

이 설문조사를 통해 다음과 같은 목표를 추구해. 첫째, CP를 사용하고 싶지만 전문가가 아닌 사람들을 위해 CP에 대한 접근 가능한 소개를 제공해. 둘째, CP를 사용해 LEC의 검증, 예를 들어 신경망의 입력-출력 속성을 검증하는 방법을 보여줄 거야. 셋째, 넷째로, CP를 안전한 제어 설계와 LEAS의 오프라인 및 온라인 검증에 사용하는 최근 기사를 리뷰할 거야. 이 아이디어들을 통합된 프레임워크로 요약해서 LEAS의 복잡성을 효율적으로 처리할 수 있게 할 거야.

우리는 로봇 내비게이션 같은 간단한 시스템 사양과 시간 논리 형식으로 표현된 복잡한 사양 모두를 고려할 거야. 설문조사 전반에 걸쳐 다른 통계 기법(예: 시나리오 최적화, PAC-Bayes 이론 등)과 이 기법들이 검증 및 제어에 어떻게 사용되었는지도 비교할 거야. 마지막으로, 독자에게 열린 문제와 미래 연구 방향을 제시할 거야.

================================================================================

URL: https://arxiv.org/abs/2409.00362
Title: UDGS-SLAM : UniDepth Assisted Gaussian Splatting for Monocular SLAM

Original Abstract:
Recent advancements in monocular neural depth estimation, particularly those achieved by the UniDepth network, have prompted the investigation of integrating UniDepth within a Gaussian splatting framework for monocular SLAM.This study presents UDGS-SLAM, a novel approach that eliminates the necessity of RGB-D sensors for depth estimation within Gaussian splatting framework. UDGS-SLAM employs statistical filtering to ensure local consistency of the estimated depth and jointly optimizes camera trajectory and Gaussian scene representation parameters. The proposed method achieves high-fidelity rendered images and low ATERMSE of the camera trajectory. The performance of UDGS-SLAM is rigorously evaluated using the TUM RGB-D dataset and benchmarked against several baseline methods, demonstrating superior performance across various scenarios. Additionally, an ablation study is conducted to validate design choices and investigate the impact of different network backbone encoders on system performance.

Translated Abstract:
최근 단안 신경망 깊이 추정 기술, 특히 UniDepth 네트워크의 발전 덕분에 UniDepth를 가우시안 스플래팅 프레임워크에 통합하는 연구가 진행되고 있어. 이 연구에서는 UDGS-SLAM이라는 새로운 접근 방식을 제안하는데, 이 방법은 가우시안 스플래팅 프레임워크에서 깊이 추정을 위해 RGB-D 센서를 사용할 필요가 없어.

UDGS-SLAM은 통계적 필터링을 사용해서 추정된 깊이의 지역적 일관성을 보장하고, 카메라 경로와 가우시안 장면 표현 매개변수를 함께 최적화해. 제안된 방법은 고해상도 렌더링 이미지와 낮은 ATERMSE를 가진 카메라 경로를 달성해.

UDGS-SLAM의 성능은 TUM RGB-D 데이터셋을 사용해서 엄격하게 평가되었고, 여러 기준 방법과 비교했을 때 다양한 상황에서 우수한 성능을 보여줬어. 또한, 설계 선택을 검증하고 다른 네트워크 백본 인코더가 시스템 성능에 미치는 영향을 조사하기 위해 아블레이션 연구도 진행했어.

================================================================================

URL: https://arxiv.org/abs/2409.00206
Title: RING#: PR-by-PE Global Localization with Roto-translation Equivariant Gram Learning

Original Abstract:
Global localization using onboard perception sensors, such as cameras and LiDARs, is crucial in autonomous driving and robotics applications when GPS signals are unreliable. Most approaches achieve global localization by sequential place recognition and pose estimation. Some of them train separate models for each task, while others employ a single model with dual heads, trained jointly with separate task-specific losses. However, the accuracy of localization heavily depends on the success of place recognition, which often fails in scenarios with significant changes in viewpoint or environmental appearance. Consequently, this renders the final pose estimation of localization ineffective. To address this, we propose a novel paradigm, PR-by-PE localization, which improves global localization accuracy by deriving place recognition directly from pose estimation. Our framework, RING#, is an end-to-end PR-by-PE localization network operating in the bird's-eye view (BEV) space, designed to support both vision and LiDAR sensors. It introduces a theoretical foundation for learning two equivariant representations from BEV features, which enables globally convergent and computationally efficient pose estimation. Comprehensive experiments on the NCLT and Oxford datasets across both vision and LiDAR modalities demonstrate that our method outperforms state-of-the-art approaches. Furthermore, we provide extensive analyses to confirm the effectiveness of our method. The code will be publicly released.

Translated Abstract:
자율주행과 로봇 응용에서 GPS 신호가 불안정할 때, 카메라와 LiDAR 같은 탑재 센서를 이용한 글로벌 로컬라이제이션이 정말 중요해. 대부분의 방법은 장소 인식과 자세 추정을 순차적으로 수행해서 글로벌 로컬라이제이션을 달성해. 어떤 방법은 각 작업에 대해 별도의 모델을 훈련시키고, 다른 방법은 두 개의 헤드를 가진 단일 모델을 사용해서 각각의 작업에 맞는 손실 함수로 공동 훈련해. 

하지만 로컬라이제이션의 정확도는 장소 인식의 성공 여부에 많이 의존해. 특히 시점이나 환경이 많이 바뀌는 경우에는 장소 인식이 잘 안 되는 경우가 많아. 이 때문에 최종적으로 로컬라이제이션의 자세 추정이 효과적이지 않게 되는 거야. 

이 문제를 해결하기 위해 우리는 PR-by-PE 로컬라이제이션이라는 새로운 패러다임을 제안해. 이 방법은 자세 추정에서 직접 장소 인식을 도출함으로써 글로벌 로컬라이제이션의 정확성을 높여. 우리의 프레임워크인 RING#는 조감도(BEV) 공간에서 작동하는 엔드 투 엔드 PR-by-PE 로컬라이제이션 네트워크야. 이 네트워크는 비전과 LiDAR 센서를 모두 지원하도록 설계되었어. 

우리는 BEV 특징에서 두 개의 동형 표현을 학습하는 이론적 기초를 소개해서, 전역적으로 수렴하고 계산적으로 효율적인 자세 추정을 가능하게 해. NCLT와 옥스포드 데이터셋을 이용한 포괄적인 실험 결과, 우리의 방법이 최신 기술들을 능가한다는 걸 보여줬어. 게다가 우리의 방법의 효과성을 확인하기 위한 자세한 분석도 제공할 거야. 코드도 공개할 예정이야.

================================================================================

URL: https://arxiv.org/abs/2408.17435
Title: Information-Based Trajectory Planning for Autonomous Absolute Tracking in Cislunar Space

Original Abstract:
The resurgence of lunar operations requires advancements in cislunar navigation and Space Situational Awareness (SSA). Challenges associated to these tasks have created an interest in autonomous planning, navigation, and tracking technologies that operate with little ground-based intervention. This research introduces a trajectory planning tool for a low-thrust mobile observer, aimed at maximizing navigation and tracking performance with satellite-to-satellite relative measurements. We formulate an expression for the information gathered over an observation period based on the mutual information between augmented observer/target states and the associated measurement set collected. We then develop an optimal trajectory design problem for a mobile observer, balancing information gain and control effort, and solve this problem with a Sequential Convex Programming (SCP) approach. The developed methods are demonstrated in scenarios involving spacecraft in the cislunar regime, demonstrating the potential for improved autonomous navigation and tracking.

Translated Abstract:
달 탐사가 다시 활성화되면서, 달 사이의 항법(cislunar navigation)과 우주 상황 인식(Space Situational Awareness, SSA)에 대한 발전이 필요해졌어. 이런 작업에서의 도전 과제로 인해, 지상에서의 개입이 거의 없는 자율 계획, 항법, 추적 기술에 대한 관심이 커지고 있어.

이 연구에서는 저항력(낮은 추진력)을 가진 이동 관측자를 위한 궤적 계획 도구를 소개해. 이 도구는 위성 간 상대 측정을 통해 항법과 추적 성능을 극대화하는 게 목표야. 관측 기간 동안 수집된 상호 정보(mutual information)를 기반으로 관측자와 목표의 상태 간의 정보를 나타내는 식을 만들어냈어.

그리고 나서 이동 관측자를 위한 최적 궤적 설계 문제를 개발했어. 이 문제는 정보 획득과 제어 노력을 균형 있게 맞추는 거야. 이 문제는 순차적 볼록 프로그래밍(Sequential Convex Programming, SCP) 방법으로 해결했어. 개발된 방법들은 달 사이의 영역에서 우주선이 포함된 시나리오에서 시연되었고, 개선된 자율 항법과 추적 가능성을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2408.17379
Title: EMPOWER: Embodied Multi-role Open-vocabulary Planning with Online Grounding and Execution

Original Abstract:
Task planning for robots in real-life settings presents significant challenges. These challenges stem from three primary issues: the difficulty in identifying grounded sequences of steps to achieve a goal; the lack of a standardized mapping between high-level actions and low-level commands; and the challenge of maintaining low computational overhead given the limited resources of robotic hardware. We introduce EMPOWER, a framework designed for open-vocabulary online grounding and planning for embodied agents aimed at addressing these issues. By leveraging efficient pre-trained foundation models and a multi-role mechanism, EMPOWER demonstrates notable improvements in grounded planning and execution. Quantitative results highlight the effectiveness of our approach, achieving an average success rate of 0.73 across six different real-life scenarios using a TIAGo robot.

Translated Abstract:
로봇의 실제 환경에서 작업 계획을 세우는 건 여러 가지 어려움이 있어. 이 어려움은 크게 세 가지 문제에서 비롯돼.

첫 번째는 목표를 달성하기 위한 단계들을 구체적으로 파악하기 어렵다는 거야. 두 번째는 고수준의 행동과 저수준 명령 사이에 표준화된 매핑이 부족하다는 거고, 세 번째는 로봇 하드웨어의 한정된 자원으로 인해 계산 부담을 줄이는 게 힘들다는 거야.

우리는 EMPOWER라는 프레임워크를 소개할 건데, 이건 이러한 문제들을 해결하기 위해 만들어졌어. EMPOWER는 효율적인 미리 훈련된 기본 모델과 다중 역할 메커니즘을 활용해서 구체적인 계획과 실행에서 눈에 띄는 개선을 보여줘.

정량적인 결과를 보면, 우리가 제안한 방법이 효과적이라는 걸 알 수 있어. TIAGo 로봇을 사용해서 여섯 가지 다른 실제 시나리오에서 평균 0.73의 성공률을 달성했어.

================================================================================

URL: https://arxiv.org/abs/2408.17373
Title: Augmented Reality without Borders: Achieving Precise Localization Without Maps

Original Abstract:
Visual localization is crucial for Computer Vision and Augmented Reality (AR) applications, where determining the camera or device's position and orientation is essential to accurately interact with the physical environment. Traditional methods rely on detailed 3D maps constructed using Structure from Motion (SfM) or Simultaneous Localization and Mapping (SLAM), which is computationally expensive and impractical for dynamic or large-scale environments. We introduce MARLoc, a novel localization framework for AR applications that uses known relative transformations within image sequences to perform intra-sequence triangulation, generating 3D-2D correspondences for pose estimation and refinement. MARLoc eliminates the need for pre-built SfM maps, providing accurate and efficient localization suitable for dynamic outdoor environments. Evaluation with benchmark datasets and real-world experiments demonstrates MARLoc's state-of-the-art performance and robustness. By integrating MARLoc into an AR device, we highlight its capability to achieve precise localization in real-world outdoor scenarios, showcasing its practical effectiveness and potential to enhance visual localization in AR applications.

Translated Abstract:
비주얼 로컬라이제이션은 컴퓨터 비전과 증강 현실(AR)에서 정말 중요해. 카메라나 기기의 위치와 방향을 정확하게 파악해야 물리적 환경과 잘 상호작용할 수 있거든. 전통적인 방법들은 구조에서 움직임을 이용한 방법(SfM)이나 동시 위치 추정 및 맵핑(SLAM) 같은 걸로 세밀한 3D 맵을 만드는 데 의존하는데, 이건 계산이 복잡하고 동적인 환경이나 대규모 환경에서는 비현실적이야.

우리는 MARLoc이라는 새로운 로컬라이제이션 프레임워크를 소개해. 이건 이미지 시퀀스 안에서 알려진 상대 변환을 사용해서 시퀀스 내 삼각측량을 수행해. 이렇게 해서 포즈 추정과 정제를 위한 3D-2D 대응 관계를 만들어내. MARLoc은 미리 만들어진 SfM 맵이 필요 없어서 동적인 야외 환경에서도 정확하고 효율적인 로컬라이제이션을 제공해.

벤치마크 데이터셋과 실제 실험으로 평가했을 때, MARLoc은 최신 기술 수준의 성능과 강인성을 보여줬어. MARLoc을 AR 기기에 통합하면, 실제 야외 상황에서 정밀한 로컬라이제이션을 이룰 수 있다는 점을 강조할 수 있어. 이로 인해 AR 애플리케이션에서 비주얼 로컬라이제이션을 향상시킬 수 있는 가능성을 보여주고 있어.

================================================================================

URL: https://arxiv.org/abs/2408.17355
Title: Bidirectional Decoding: Improving Action Chunking via Closed-Loop Resampling

Original Abstract:
Predicting and executing a sequence of actions without intermediate replanning, known as action chunking, is increasingly used in robot learning from human demonstrations. However, its effects on learned policies remain puzzling: some studies highlight its importance for achieving strong performance, while others observe detrimental effects. In this paper, we first dissect the role of action chunking by analyzing the divergence between the learner and the demonstrator. We find that longer action chunks enable a policy to better capture temporal dependencies by taking into account more past states and actions within the chunk. However, this advantage comes at the cost of exacerbating errors in stochastic environments due to fewer observations of recent states. To address this, we propose Bidirectional Decoding (BID), a test-time inference algorithm that bridges action chunking with closed-loop operations. BID samples multiple predictions at each time step and searches for the optimal one based on two criteria: (i) backward coherence, which favors samples aligned with previous decisions, (ii) forward contrast, which favors samples close to outputs of a stronger policy and distant from those of a weaker policy. By coupling decisions within and across action chunks, BID enhances temporal consistency over extended sequences while enabling adaptive replanning in stochastic environments. Experimental results show that BID substantially outperforms conventional closed-loop operations of two state-of-the-art generative policies across seven simulation benchmarks and two real-world tasks.

Translated Abstract:
액션 청킹(action chunking)은 중간에 재계획 없이 행동 순서를 예측하고 실행하는 방법으로, 로봇이 인간의 시연을 통해 배우는 데 점점 더 많이 사용되고 있어. 하지만 이 방법이 학습된 정책에 미치는 영향은 여전히 헷갈려. 어떤 연구는 강력한 성능을 얻는 데 중요하다고 강조하는 반면, 다른 연구는 오히려 부정적인 영향을 미친다고 보고해.

이 논문에서는 먼저 액션 청킹의 역할을 분석하면서 학습자와 시연자 간의 차이를 살펴봤어. 우리가 발견한 건, 더 긴 액션 청크가 정책이 과거의 상태와 행동을 더 많이 고려할 수 있게 해줘서 시간적 의존성을 더 잘 잡을 수 있다는 거야. 하지만 이 장점은 최근 상태에 대한 관측이 줄어들기 때문에 확률적 환경에서의 오류를 악화시키는 단점이 있어.

이 문제를 해결하기 위해 우리는 Bidirectional Decoding (BID)이라는 테스트 시간 추론 알고리즘을 제안해. BID는 액션 청킹과 폐쇄 루프 작업을 연결해줘. 각 시간 단계에서 여러 예측을 샘플링하고, 두 가지 기준에 따라 최적의 예측을 찾는 방식이야. 첫째, 이전 결정과 일치하는 샘플을 선호하는 '역방향 일관성(backward coherence)'과, 둘째, 더 강한 정책의 출력에 가까우면서 더 약한 정책의 출력에서 멀리 있는 샘플을 선호하는 '전방향 대비(forward contrast)'가 있어.

BID는 액션 청크 내외의 결정을 연결함으로써 긴 시퀀스에서 시간적 일관성을 높이고, 확률적 환경에서도 적응형 재계획을 가능하게 해. 실험 결과, BID는 7개의 시뮬레이션 벤치마크와 2개의 실제 작업에서 두 가지 최첨단 생성 정책의 기존 폐쇄 루프 작업보다 상당히 뛰어난 성능을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2408.17287
Title: Optimizing Interaction Space: Enlarging the Capture Volume for Multiple Portable Motion Capture Devices

Original Abstract:
Markerless motion capture devices such as the Leap Motion Controller (LMC) have been extensively used for tracking hand, wrist, and forearm positions as an alternative to Marker-based Motion Capture (MMC). However, previous studies have highlighted the subpar performance of LMC in reliably recording hand kinematics. In this study, we employ four LMC devices to optimize their collective tracking volume, aiming to enhance the accuracy and precision of hand kinematics. Through Monte Carlo simulation, we determine an optimized layout for the four LMC devices and subsequently conduct reliability and validity experiments encompassing 1560 trials across ten subjects. The combined tracking volume is validated against an MMC system, particularly for kinematic movements involving wrist, index, and thumb flexion. Utilizing calculation resources in one computer, our result of the optimized configuration has a better visibility rate with a value of 0.05 $\pm$ 0.55 compared to the initial configuration with -0.07 $\pm$ 0.40. Multiple Leap Motion Controllers (LMCs) have proven to increase the interaction space of capture volume but are still unable to give agreeable measurements from dynamic movement.

Translated Abstract:
마커 없이 움직임을 포착하는 장치인 리프 모션 컨트롤러(LMC)는 손, 손목, 팔꿈치 위치 추적에 많이 사용되고 있어. 하지만 이전 연구에서는 LMC가 손의 움직임을 정확하게 기록하는 데 한계가 있다고 지적했어. 

이번 연구에서는 네 개의 LMC를 사용해서 이들을 최적화하여 손의 움직임을 더 정확하고 정밀하게 추적하려고 했어. 몬테카를로 시뮬레이션을 통해 네 개의 LMC 장치에 대한 최적 배치를 찾았고, 그 후에 10명의 피험자를 대상으로 1560번의 실험을 통해 신뢰성과 유효성을 검증했어. 

결과적으로, 최적화된 추적 볼륨은 마커 기반 시스템과 비교했을 때 손목, 검지, 엄지의 움직임에서 더 나은 결과를 보여줬어. 특히, 최적화된 구성은 가시성 비율이 0.05 $\pm$ 0.55로 초기 구성의 -0.07 $\pm$ 0.40보다 훨씬 나았어. 여러 개의 리프 모션 컨트롤러를 사용하면 캡처 볼륨의 상호작용 공간이 늘어나지만, 여전히 동적인 움직임에 대한 정확한 측정은 어려운 상태야.

================================================================================

URL: https://arxiv.org/abs/2408.17066
Title: Non-verbal Interaction and Interface with a Quadruped Robot using Body and Hand Gestures: Design and User Experience Evaluation

Original Abstract:
In recent years, quadruped robots have attracted significant attention due to their practical advantages in maneuverability, particularly when navigating rough terrain and climbing stairs. As these robots become more integrated into various industries, including construction and healthcare, researchers have increasingly focused on developing intuitive interaction methods such as speech and gestures that do not require separate devices such as keyboards or joysticks. This paper aims at investigating a comfortable and efficient interaction method with quadruped robots that possess a familiar form factor. To this end, we conducted two preliminary studies to observe how individuals naturally interact with a quadruped robot in natural and controlled settings, followed by a prototype experiment to examine human preferences for body-based and hand-based gesture controls using a Unitree Go1 Pro quadruped robot. We assessed the user experience of 13 participants using the User Experience Questionnaire and measured the time taken to complete specific tasks. The findings of our preliminary results indicate that humans have a natural preference for communicating with robots through hand and body gestures rather than speech. In addition, participants reported higher satisfaction and completed tasks more quickly when using body gestures to interact with the robot. This contradicts the fact that most gesture-based control technologies for quadruped robots are hand-based. The video is available at this https URL.

Translated Abstract:
최근 몇 년간, 4족 로봇이 기동성 때문에 많은 관심을 받고 있어. 특히 험한 지형이나 계단을 오를 때 유용하거든. 이런 로봇들이 건설이나 의료 같은 다양한 산업에 더 많이 사용되면서, 연구자들은 별도의 장치 없이 음성이나 제스처로 자연스럽게 상호작용할 수 있는 방법을 개발하는 데 집중하고 있어.

이 논문은 사람들이 익숙한 형태의 4족 로봇과 편안하고 효율적으로 상호작용하는 방법을 조사하는 게 목표야. 그래서 두 가지 예비 연구를 진행했는데, 하나는 자연스러운 환경에서, 다른 하나는 통제된 환경에서 사람들이 4족 로봇과 어떻게 상호작용하는지 관찰했어. 그 후, Unitree Go1 Pro 4족 로봇을 사용해서 몸 기반과 손 기반 제스처 조작에 대한 인간의 선호도를 조사하는 프로토타입 실험도 했어.

13명의 참가자를 대상으로 사용자 경험 설문지를 통해 사용자 경험을 평가했고, 특정 작업을 완료하는 데 걸린 시간도 측정했어. 예비 결과에 따르면, 사람들은 로봇과의 소통에서 음성보다 손과 몸의 제스처를 더 선호하는 것으로 나타났어. 또 참가자들은 로봇과 상호작용할 때 몸 제스처를 사용할 때 더 높은 만족도를 느끼고 작업을 더 빨리 완료했다고 해. 이건 대부분의 4족 로봇 제스처 기반 제어 기술이 손 기반이라는 사실과는 상반되는 결과야. 비디오는 이 URL에서 볼 수 있어.

================================================================================

URL: https://arxiv.org/abs/2408.17061
Title: Robotic Object Insertion with a Soft Wrist through Sim-to-Real Privileged Training

Original Abstract:
This study addresses contact-rich object insertion tasks under unstructured environments using a robot with a soft wrist, enabling safe contact interactions. For the unstructured environments, we assume that there are uncertainties in object grasp and hole pose and that the soft wrist pose cannot be directly measured. Recent methods employ learning approaches and force/torque sensors for contact localization; however, they require data collection in the real world. This study proposes a sim-to-real approach using a privileged training strategy. This method has two steps. 1) The teacher policy is trained to complete the task with sensor inputs and ground truth privileged information such as the peg pose, and then 2) the student encoder is trained with data produced from teacher policy rollouts to estimate the privileged information from sensor history. We performed sim-to-real experiments under grasp and hole pose uncertainties. This resulted in 100\%, 95\%, and 80\% success rates for circular peg insertion with 0, +5, and -5 degree peg misalignments, respectively, and start positions randomly shifted $\pm$ 10 mm from a default position. Also, we tested the proposed method with a square peg that was never seen during training. Additional simulation evaluations revealed that using the privileged strategy improved success rates compared to training with only simulated sensor data. Our results demonstrate the advantage of using sim-to-real privileged training for soft robots, which has the potential to alleviate human engineering efforts for robotic assembly.

Translated Abstract:
이 연구는 부드러운 손목을 가진 로봇을 사용해 비구조적인 환경에서 물체를 삽입하는 작업을 다루고 있어. 이 작업은 안전한 접촉 상호작용을 가능하게 해. 비구조적인 환경에서는 물체를 잡거나 구멍의 위치에 대한 불확실성이 있고, 부드러운 손목의 위치를 직접 측정할 수 없다고 가정해.

최근 방법들은 접촉 위치를 찾기 위해 학습 방법과 힘/토크 센서를 사용하는데, 이건 실제 세계에서 데이터 수집이 필요해. 이 연구는 특권 훈련 전략을 사용하는 시뮬레이션-실제(Sim-to-Real) 접근 방식을 제안해. 이 방법은 두 단계가 있어. 

1) 교사 정책을 훈련시켜 센서 입력과 핀 위치 같은 실제 정보를 사용해 작업을 완료하게 해. 
2) 그 다음, 학생 인코더를 교사 정책의 데이터를 가지고 훈련시켜 센서 히스토리로부터 실제 정보를 추정하도록 해.

우리는 잡기와 구멍 위치의 불확실성이 있는 상황에서 시뮬레이션-실제 실험을 진행했어. 그 결과, 0도, +5도, -5도 핀 정렬 오차에서 원형 핀 삽입 성공률이 각각 100%, 95%, 80%였고, 시작 위치는 기본 위치에서 ±10mm 랜덤으로 이동했어. 또, 훈련 중에 본 적이 없는 사각 핀으로도 방법을 시험해봤어. 추가 시뮬레이션 평가에서는 특권 전략을 사용할 때 성공률이 시뮬레이션 센서 데이터만으로 훈련했을 때보다 나아졌다는 결과를 보여줬어. 

우리 결과는 부드러운 로봇에 대한 시뮬레이션-실제 특권 훈련의 장점을 보여주고, 이는 로봇 조립을 위한 인간의 엔지니어링 노력을 줄일 수 있는 가능성이 있어.

================================================================================

URL: https://arxiv.org/abs/2408.17041
Title: Generative Modeling Perspective for Control and Reasoning in Robotics

Original Abstract:
Heralded by the initial success in speech recognition and image classification, learning-based approaches with neural networks, commonly referred to as deep learning, have spread across various fields. A primitive form of a neural network functions as a deterministic mapping from one vector to another, parameterized by trainable weights. This is well suited for point estimation in which the model learns a one-to-one mapping (e.g., mapping a front camera view to a steering angle) that is required to solve the task of interest. Although learning such a deterministic, one-to-one mapping is effective, there are scenarios where modeling \emph{multimodal} data distributions, namely learning one-to-many relationships, is helpful or even necessary.
In this thesis, we adopt a generative modeling perspective on robotics problems. Generative models learn and produce samples from multimodal distributions, rather than performing point estimation. We will explore the advantages this perspective offers for three topics in robotics.

Translated Abstract:
음성 인식과 이미지 분류에서의 초기 성공 덕분에, 신경망을 기반으로 한 학습 방법인 딥러닝이 다양한 분야로 퍼졌어. 기본적인 신경망은 하나의 벡터를 다른 벡터로 변환하는 정해진 방식으로 작동하고, 훈련 가능한 가중치로 매개변수를 조정해. 이 방식은 모델이 1:1 매핑을 배우는 데 적합해 (예를 들어, 앞쪽 카메라의 시점을 조향 각도로 매핑하는 것처럼) 관심 있는 작업을 해결하는 데 필요해.

하지만 이런 정해진 1:1 매핑을 학습하는 게 효과적이긴 하지만, 여러 개의 모드가 있는 데이터 분포를 모델링하는 게 필요할 때도 있어. 즉, 하나의 입력에 대해 여러 개의 출력을 학습하는 게 도움이 될 때가 있다는 거야.

이번 논문에서는 로봇 문제에 대해 생성 모델 관점에서 접근할 거야. 생성 모델은 점 추정이 아니라 다중 모드 분포에서 샘플을 학습하고 만들어내는 방식이야. 이 관점이 로봇 분야의 세 가지 주제에 어떤 장점을 제공하는지 살펴볼 거야.

================================================================================

URL: https://arxiv.org/abs/2408.17034
Title: MakeWay: Object-Aware Costmaps for Proactive Indoor Navigation Using LiDAR

Original Abstract:
In this paper, we introduce a LiDAR-based robot navigation system, based on novel object-aware affordance-based costmaps. Utilizing a 3D object detection network, our system identifies objects of interest in LiDAR keyframes, refines their 3D poses with the Iterative Closest Point (ICP) algorithm, and tracks them via Kalman filters and the Hungarian algorithm for data association. It then updates existing object poses with new associated detections and creates new object maps for unmatched detections. Using the maintained object-level mapping system, our system creates affordance-driven object costmaps for proactive collision avoidance in path planning. Additionally, we address the scarcity of indoor semantic LiDAR data by introducing an automated labeling technique. This method utilizes a CAD model database for accurate ground-truth annotations, encompassing bounding boxes, positions, orientations, and point-wise semantics of each object in LiDAR sequences. Our extensive evaluations, conducted in both simulated and real-world robot platforms, highlights the effectiveness of proactive object avoidance by using object affordance costmaps, enhancing robotic navigation safety and efficiency. The system can operate in real-time onboard and we intend to release our code and data for public use.

Translated Abstract:
이 논문에서는 새로운 객체 인식 기반의 비용 맵을 활용한 LiDAR 기반 로봇 내비게이션 시스템을 소개해. 3D 객체 감지 네트워크를 사용해서, LiDAR 키프레임에서 관심 있는 객체를 찾아내고, ICP 알고리즘으로 3D 자세를 정교하게 조정해. 그리고 칼만 필터와 헝가리안 알고리즘으로 객체를 추적해. 기존 객체의 자세는 새로운 감지 결과로 업데이트하고, 일치하지 않는 감지에 대해서는 새로운 객체 맵을 생성해.

이런 객체 수준 맵핑 시스템을 통해, 우리는 경로 계획에서 충돌을 피하기 위해 객체 기반의 비용 맵을 만들어. 또한, 실내에서의 의미론적 LiDAR 데이터가 부족한 문제를 해결하기 위해 자동 라벨링 기법을 도입했어. 이 방법은 CAD 모델 데이터베이스를 활용해 정확한 실제 주석을 생성하고, LiDAR 시퀀스의 각 객체에 대한 경계 상자, 위치, 방향, 점별 의미론을 포함해.

시뮬레이션과 실제 로봇 플랫폼에서 진행한 광범위한 평가 결과, 객체 기반 비용 맵을 사용한 능동적인 객체 회피의 효과가 강조됐어. 이 시스템은 실시간으로 작동할 수 있고, 우리는 코드와 데이터를 공개할 계획이야.

================================================================================

URL: https://arxiv.org/abs/2408.17005
Title: Efficient Camera Exposure Control for Visual Odometry via Deep Reinforcement Learning

Original Abstract:
The stability of visual odometry (VO) systems is undermined by degraded image quality, especially in environments with significant illumination changes. This study employs a deep reinforcement learning (DRL) framework to train agents for exposure control, aiming to enhance imaging performance in challenging conditions. A lightweight image simulator is developed to facilitate the training process, enabling the diversification of image exposure and sequence trajectory. This setup enables completely offline training, eliminating the need for direct interaction with camera hardware and the real environments. Different levels of reward functions are crafted to enhance the VO systems, equipping the DRL agents with varying intelligence. Extensive experiments have shown that our exposure control agents achieve superior efficiency-with an average inference duration of 1.58 ms per frame on a CPU-and respond more quickly than traditional feedback control schemes. By choosing an appropriate reward function, agents acquire an intelligent understanding of motion trends and anticipate future illumination changes. This predictive capability allows VO systems to deliver more stable and precise odometry results. The codes and datasets are available at this https URL.

Translated Abstract:
비주얼 오도메트리(VO) 시스템의 안정성은 이미지 품질이 떨어질 때, 특히 조명이 많이 변하는 환경에서 영향을 받는다. 이 연구에서는 깊은 강화 학습(DRL) 프레임워크를 사용해서 에이전트를 노출 제어를 위해 훈련시키고, 어려운 조건에서 이미징 성능을 향상시키는 것을 목표로 한다.

훈련 과정을 쉽게 하기 위해 가벼운 이미지 시뮬레이터를 개발했어. 이 시뮬레이터 덕분에 이미지 노출과 시퀀스 경로를 다양하게 할 수 있다. 그래서 카메라 하드웨어나 실제 환경과 직접적으로 상호작용할 필요 없이 완전히 오프라인에서 훈련이 가능해.

보상 함수의 다양한 수준을 설계해서 VO 시스템을 개선하고, DRL 에이전트에 다양한 지능을 부여했어. 여러 실험을 통해 우리의 노출 제어 에이전트가 평균 1.58ms의 프레임 당 추론 시간을 기록하며, 전통적인 피드백 제어 방식보다 더 빠르게 반응한다는 것을 보여줬어. 적절한 보상 함수를 선택하면 에이전트가 움직임의 경향을 지능적으로 이해하고 미래의 조명 변화를 예측할 수 있어. 이 예측 능력 덕분에 VO 시스템이 더 안정적이고 정확한 오도메트리 결과를 제공할 수 있게 돼. 

코드와 데이터셋은 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2408.16949
Title: From "Made In" to Mukokuseki: Exploring the Visual Perception of National Identity in Robots

Original Abstract:
People read human characteristics into the design of social robots, a visual process with socio-cultural implications. One factor may be nationality, a complex social characteristic that is linked to ethnicity, culture, and other factors of identity that can be embedded in the visual design of robots. Guided by social identity theory (SIT), we explored the notion of "mukokuseki," a visual design characteristic defined by the absence of visual cues to national and ethnic identity in Japanese cultural exports. In a two-phase categorization study (n=212), American (n=110) and Japanese (n=92) participants rated a random selection of nine robot stimuli from America and Japan, plus multinational Pepper. We found evidence of made-in and two kinds of mukokuseki effects. We offer suggestions for the visual design of mukokuseki robots that may interact with people from diverse backgrounds. Our findings have implications for robots and social identity, the viability of robotic exports, and the use of robots internationally.

Translated Abstract:
사람들은 소셜 로봇의 디자인에 인간의 특성을 반영하게 되는데, 이건 사회문화적인 의미를 가지고 있어. 이 중 하나는 국적일 수 있는데, 국적은 민족, 문화, 그리고 다른 정체성과 관련된 복잡한 사회적 특성이야. 이런 것들은 로봇의 시각적 디자인에 포함될 수 있어.

우리는 사회 정체성 이론(SIT)을 바탕으로 "무코쿠세키"라는 개념을 살펴봤어. 이건 일본 문화 수출에서 국적이나 민족 정체성을 나타내는 시각적 단서가 없는 디자인 특성을 말해. 두 단계로 나뉜 분류 연구(n=212)에서, 미국 참가자(n=110)와 일본 참가자(n=92)가 미국과 일본의 아홉 개 로봇 자극을 무작위로 평가했어. 여기에는 다국적 로봇 페퍼도 포함됐고.

우리는 '메이드 인'과 두 가지 종류의 무코쿠세키 효과에 대한 증거를 발견했어. 다양한 배경을 가진 사람들과 상호작용할 수 있는 무코쿠세키 로봇의 시각적 디자인에 대한 제안도 했어. 우리의 발견은 로봇과 사회 정체성, 로봇 수출의 가능성, 그리고 로봇의 국제적인 사용에 대한 의미를 가지고 있어.

================================================================================

URL: https://arxiv.org/abs/2408.16944
Title: FlowRetrieval: Flow-Guided Data Retrieval for Few-Shot Imitation Learning

Original Abstract:
Few-shot imitation learning relies on only a small amount of task-specific demonstrations to efficiently adapt a policy for a given downstream tasks. Retrieval-based methods come with a promise of retrieving relevant past experiences to augment this target data when learning policies. However, existing data retrieval methods fall under two extremes: they either rely on the existence of exact behaviors with visually similar scenes in the prior data, which is impractical to assume; or they retrieve based on semantic similarity of high-level language descriptions of the task, which might not be that informative about the shared low-level behaviors or motions across tasks that is often a more important factor for retrieving relevant data for policy learning. In this work, we investigate how we can leverage motion similarity in the vast amount of cross-task data to improve few-shot imitation learning of the target task. Our key insight is that motion-similar data carries rich information about the effects of actions and object interactions that can be leveraged during few-shot adaptation. We propose FlowRetrieval, an approach that leverages optical flow representations for both extracting similar motions to target tasks from prior data, and for guiding learning of a policy that can maximally benefit from such data. Our results show FlowRetrieval significantly outperforms prior methods across simulated and real-world domains, achieving on average 27% higher success rate than the best retrieval-based prior method. In the Pen-in-Cup task with a real Franka Emika robot, FlowRetrieval achieves 3.7x the performance of the baseline imitation learning technique that learns from all prior and target data. Website: this https URL

Translated Abstract:
Few-shot 모방 학습은 주어진 작업을 위해 정책을 효율적으로 조정하기 위해 적은 양의 작업별 시연에 의존해. 검색 기반 방법은 학습할 때 관련된 과거 경험을 찾아서 이 데이터를 보강할 수 있다는 장점이 있어. 하지만 기존 데이터 검색 방법은 두 가지 극단적인 방식에 속해: 하나는 이전 데이터에서 시각적으로 비슷한 장면의 정확한 행동이 존재해야 한다는 가정에 의존하고, 다른 하나는 작업의 고수준 언어 설명의 의미적 유사성을 기반으로 검색하는데, 이는 종종 정책 학습에 필요한 관련 데이터를 찾는 데 더 중요한 저수준 행동이나 동작에 대한 정보가 부족할 수 있어.

이 연구에서는 다양한 작업의 데이터를 활용해 목표 작업의 few-shot 모방 학습을 개선하는 방법을 알아봤어. 우리의 주요 통찰력은 동작이 유사한 데이터가 행동의 효과와 객체 상호작용에 대한 풍부한 정보를 담고 있다는 거야. 이 정보를 이용해서 few-shot 적응을 할 수 있어. 우리는 FlowRetrieval이라는 방법을 제안하는데, 이 방법은 광학 흐름 표현을 활용해서 이전 데이터에서 목표 작업과 유사한 동작을 추출하고, 그런 데이터를 최대한 활용할 수 있는 정책 학습을 안내해.

우리의 결과는 FlowRetrieval이 시뮬레이션과 실제 세계 도메인에서 이전 방법들보다 훨씬 더 우수하다는 걸 보여줘. 평균 27% 더 높은 성공률을 달성했어. 실제 Franka Emika 로봇을 사용한 Pen-in-Cup 작업에서는, FlowRetrieval이 모든 이전 및 목표 데이터에서 배우는 기본 모방 학습 기술의 3.7배 성능을 달성했어.

================================================================================

URL: https://arxiv.org/abs/2408.16938
Title: Autonomous Image-to-Grasp Robotic Suturing Using Reliability-Driven Suture Thread Reconstruction

Original Abstract:
Automating suturing during robotically-assisted surgery reduces the burden on the operating surgeon, enabling them to focus on making higher-level decisions rather than fatiguing themselves in the numerous intricacies of a surgical procedure. Accurate suture thread reconstruction and grasping are vital prerequisites for suturing, particularly for avoiding entanglement with surgical tools and performing complex thread manipulation. However, such methods must be robust to heavy perceptual degradation resulting from heavy noise and thread feature sparsity from endoscopic images. We develop a reconstruction algorithm that utilizes quadratic programming optimization to fit smooth splines to thread observations, satisfying reliability bounds estimated from measured observation noise. Additionally, we craft a grasping policy that generates gripper trajectories that maximize the probability of a successful grasp. Our full image-to-grasp pipeline is rigorously evaluated with over 400 grasping trials, exhibiting state-of-the-art accuracy. We show that this strategy can be applied to the various techniques in autonomous suture needle manipulation to achieve autonomous surgery in a generalizable way.

Translated Abstract:
로봇 보조 수술에서 봉합을 자동화하면 수술하는 의사의 부담이 줄어들고, 의사는 수술 절차의 복잡한 부분에 지치지 않고 더 높은 수준의 결정에 집중할 수 있게 돼. 정확한 봉합 실 재구성과 잡기는 봉합을 위해서 꼭 필요해. 이 과정에서 수술 도구와 얽히지 않도록 하고, 복잡한 실 조작을 수행하는 게 중요해. 하지만 이런 방법은 내시경 이미지에서 발생하는 큰 소음과 실 특징의 희소성으로 인해 강인해야 해.

우리는 실 관측치에 부드러운 스플라인을 맞추는 이차 프로그래밍 최적화 알고리즘을 개발했어. 이 알고리즘은 측정된 관측 소음으로부터 추정된 신뢰성 경계를 만족시켜. 또, 성공적인 잡기를 극대화하는 그리퍼 경로를 생성하는 잡기 정책도 만들었어. 우리의 전체 이미지-잡기 파이프라인은 400회 이상의 잡기 실험을 통해 엄격하게 평가되었고, 최신 기술의 정확도를 보여줬어.

이 전략은 자율 봉합 바늘 조작의 다양한 기술에 적용될 수 있어, 일반화된 방식으로 자율 수술을 달성할 수 있다는 걸 보여줘.

================================================================================

URL: https://arxiv.org/abs/2408.16890
Title: Robotic warehousing operations: a learn-then-optimize approach to large-scale neighborhood search

Original Abstract:
The rapid deployment of robotics technologies requires dedicated optimization algorithms to manage large fleets of autonomous agents. This paper supports robotic parts-to-picker operations in warehousing by optimizing order-workstation assignments, item-pod assignments and the schedule of order fulfillment at workstations. The model maximizes throughput, while managing human workload at the workstations and congestion in the facility. We solve it via large-scale neighborhood search, with a novel learn-then-optimize approach to subproblem generation. The algorithm relies on an offline machine learning procedure to predict objective improvements based on subproblem features, and an online optimization model to generate a new subproblem at each iteration. In collaboration with Amazon Robotics, we show that our model and algorithm generate much stronger solutions for practical problems than state-of-the-art approaches. In particular, our solution enhances the utilization of robotic fleets by coordinating robotic tasks for human operators to pick multiple items at once, and by coordinating robotic routes to avoid congestion in the facility.

Translated Abstract:
로봇 기술이 빠르게 발전하면서, 많은 자율 로봇을 관리하기 위한 최적화 알고리즘이 필요해졌어. 이 논문은 창고에서 로봇 부품을 픽업하는 작업을 돕기 위해, 주문-작업대 할당, 물품-포드 할당, 그리고 작업대에서의 주문 이행 일정을 최적화해. 모델은 처리량을 최대화하면서, 작업대에서 사람의 작업량과 시설의 혼잡함도 관리해.

우리는 대규모 이웃 검색을 통해 문제를 해결하고, 서브 문제 생성을 위한 새로운 학습 후 최적화 접근 방식을 사용해. 이 알고리즘은 오프라인 머신러닝 절차를 통해 서브 문제의 특징에 따른 목표 개선을 예측하고, 각 반복에서 새로운 서브 문제를 생성하기 위한 온라인 최적화 모델을 활용해.

아마존 로보틱스와 협력하여, 우리의 모델과 알고리즘이 최첨단 접근 방식보다 실제 문제에 대해 훨씬 더 강력한 해결책을 제공한다는 것을 보여줘. 특히, 우리의 해결책은 로봇 작업을 조정해서 사람 작업자가 여러 물품을 동시에 픽업할 수 있게 하고, 로봇 경로를 조정해서 시설 내 혼잡을 피하게 해.

================================================================================

URL: https://arxiv.org/abs/2408.16875
Title: Learning Multi-agent Multi-machine Tending by Mobile Robots

Original Abstract:
Robotics can help address the growing worker shortage challenge of the manufacturing industry. As such, machine tending is a task collaborative robots can tackle that can also highly boost productivity. Nevertheless, existing robotics systems deployed in that sector rely on a fixed single-arm setup, whereas mobile robots can provide more flexibility and scalability. In this work, we introduce a multi-agent multi-machine tending learning framework by mobile robots based on Multi-agent Reinforcement Learning (MARL) techniques with the design of a suitable observation and reward. Moreover, an attention-based encoding mechanism is developed and integrated into Multi-agent Proximal Policy Optimization (MAPPO) algorithm to boost its performance for machine tending scenarios. Our model (AB-MAPPO) outperformed MAPPO in this new challenging scenario in terms of task success, safety, and resources utilization. Furthermore, we provided an extensive ablation study to support our various design decisions.

Translated Abstract:
로봇 기술은 제조업에서 점점 심각해지는 인력 부족 문제를 해결하는 데 도움을 줄 수 있어. 특히, 협동 로봇이 기계 작업을 맡으면 생산성을 크게 높일 수 있어. 하지만 지금 사용되고 있는 로봇 시스템은 고정된 단일 팔 구조에 의존하고 있어. 반면에, 이동 로봇은 더 많은 유연성과 확장성을 제공할 수 있어.

이번 연구에서는 이동 로봇을 기반으로 한 다중 에이전트 다중 기계 작업 학습 프레임워크를 소개할 거야. 이건 다중 에이전트 강화 학습(MARL) 기법을 사용해서 적절한 관찰과 보상을 설계했어. 게다가, 주의 기반 인코딩 메커니즘을 개발해서 다중 에이전트 근접 정책 최적화(MAPPO) 알고리즘에 통합했어. 이걸 통해 기계 작업 시나리오에서 성능을 높였어.

우리 모델(AB-MAPPO)은 이 새로운 도전적인 상황에서 작업 성공률, 안전성, 자원 활용 측면에서 MAPPO보다 더 좋은 성과를 냈어. 또한, 다양한 설계 결정을 뒷받침하기 위해 포괄적인 제거 연구도 진행했어.

================================================================================

URL: https://arxiv.org/abs/2408.16867
Title: CalTag: Robust calibration of mmWave Radar and LiDAR using backscatter tags

Original Abstract:
The rise of automation in robotics necessitates the use of high-quality perception systems, often through the use of multiple sensors. A crucial aspect of a successfully deployed multi-sensor systems is the calibration with a known object typically named fiducial. In this work, we propose a novel fiducial system for millimeter wave radars, termed as \name. \name addresses the limitations of traditional corner reflector-based calibration methods in extremely cluttered environments. \name leverages millimeter wave backscatter technology to achieve more reliable calibration than corner reflectors, enhancing the overall performance of multi-sensor perception systems. We compare the performance in several real-world environments and show the improvement achieved by using \name as the radar fiducial over a corner reflector.

Translated Abstract:
로봇 자동화가 발전하면서, 고품질의 인식 시스템이 필요해졌어. 보통 여러 센서를 사용해서 이걸 해결하는데, 여러 센서 시스템을 잘 운영하기 위해서는 '피듀셜'이라고 하는 알려진 물체와의 보정이 정말 중요해.

이번 연구에서는 밀리미터파 레이더를 위한 새로운 피듀셜 시스템, 이름은 \name을 제안해. \name은 전통적인 코너 반사기 기반의 보정 방법이 복잡한 환경에서 한계를 가지는 문제를 해결해. \name은 밀리미터파 백스캐터 기술을 활용해서 코너 반사기보다 더 신뢰할 수 있는 보정을 달성해, 여러 센서 인식 시스템의 전반적인 성능을 향상시켜.

우리는 여러 실제 환경에서 성능을 비교해봤고, \name을 레이더 피듀셜로 사용했을 때 코너 반사기보다 어떻게 개선되는지 보여줄 거야.

================================================================================

URL: https://arxiv.org/abs/2408.16865
Title: Measuring Transparency in Intelligent Robots

Original Abstract:
As robots become increasingly integrated into our daily lives, the need to make them transparent has never been more critical. Yet, despite its importance in human-robot interaction, a standardized measure of robot transparency has been missing until now. This paper addresses this gap by presenting the first comprehensive scale to measure perceived transparency in robotic systems, available in English, German, and Italian languages. Our approach conceptualizes transparency as a multidimensional construct, encompassing explainability, legibility, predictability, and meta-understanding. The proposed scale was a product of a rigorous three-stage process involving 1,223 participants. Firstly, we generated the items of our scale, secondly, we conducted an exploratory factor analysis, and thirdly, a confirmatory factor analysis served to validate the factor structure of the newly developed TOROS scale. The final scale encompasses 26 items and comprises three factors: Illegibility, Explainability, and Predictability. TOROS demonstrates high cross-linguistic reliability, inter-factor correlation, model fit, internal consistency, and convergent validity across the three cross-national samples. This empirically validated tool enables the assessment of robot transparency and contributes to the theoretical understanding of this complex construct. By offering a standardized measure, we facilitate consistent and comparable research in human-robot interaction in which TOROS can serve as a benchmark.

Translated Abstract:
로봇이 우리 일상 생활에 점점 더 많이 통합됨에 따라, 로봇의 투명성을 높이는 것이 매우 중요해지고 있어. 그런데 인간과 로봇 간의 상호작용에서 이 투명성을 측정할 수 있는 기준이 지금까지 없었어. 이 논문은 그런 문제를 해결하기 위해 로봇 시스템의 인지된 투명성을 측정할 수 있는 첫 번째 종합적인 기준을 제시해. 이 기준은 영어, 독일어, 이탈리아어로 제공돼.

우리의 접근 방식은 투명성을 여러 차원으로 구성된 개념으로 보고, 설명 가능성, 가독성, 예측 가능성, 그리고 메타 이해를 포함해. 제안된 기준은 1,223명의 참여자가 포함된 철저한 세 단계 과정을 통해 만들어졌어. 첫 번째로, 기준의 항목을 생성했고, 두 번째로 탐색적 요인 분석을 했어. 세 번째로는 확인적 요인 분석을 통해 새롭게 개발된 TOROS 기준의 요인 구조를 검증했지. 최종 기준은 26개의 항목으로 구성되고, Illegibility, Explainability, Predictability의 세 가지 요인으로 나뉘어.

TOROS는 세 개의 국가 샘플에서 높은 언어 간 신뢰성, 요인 간 상관 관계, 모델 적합도, 내부 일관성, 그리고 수렴 타당성을 보여줘. 이 empirically validated 도구는 로봇의 투명성을 평가할 수 있게 해주고, 이 복잡한 개념에 대한 이론적 이해에도 기여해. 표준화된 측정을 제공함으로써, 우리는 인간-로봇 상호작용 연구에서 일관되고 비교 가능한 연구를 촉진할 수 있어, TOROS가 기준 역할을 할 수 있을 거야.

================================================================================

URL: https://arxiv.org/abs/2408.16844
Title: A framework for training and benchmarking algorithms that schedule robot tasks

Original Abstract:
Service robots work in a changing environment habited by exogenous agents like humans. In the service robotics domain, lots of uncertainties result from exogenous actions and inaccurate localisation of objects and the robot itself. This makes the robot task scheduling problem incredibly challenging. In this article, we propose a benchmarking system for systematically assessing the performance of algorithms scheduling robot tasks. The robot environment incorporates a room map, furniture, transportable objects, and moving humans; the system defines interfaces for the algorithms, tasks to be executed, and evaluation methods. The system consists of several tools, easing testing scenario generation for training AI-based scheduling algorithms and statistical testing. For benchmarking purposes, a set of scenarios is chosen, and the performance of several scheduling algorithms is assessed. The system source is published to serve the community for tuning and comparable assessment of robot task scheduling algorithms for service robots.

Translated Abstract:
서비스 로봇은 인간 같은 외부 요인이 있는 변화하는 환경에서 움직여요. 서비스 로봇 분야에서는 외부 행동이나 로봇과 물체의 위치가 정확하지 않아서 많은 불확실성이 생겨요. 이 때문에 로봇의 작업 스케줄링 문제는 굉장히 어려워져요.

이 논문에서는 로봇 작업 스케줄링 알고리즘의 성능을 체계적으로 평가할 수 있는 벤치마킹 시스템을 제안해요. 로봇 환경은 방 지도, 가구, 이동 가능한 물체, 그리고 움직이는 사람들로 구성되어 있어요. 이 시스템은 알고리즘, 실행할 작업, 평가 방법을 정의하는 인터페이스를 제공해요.

시스템은 여러 가지 도구로 구성되어 있어서 AI 기반 스케줄링 알고리즘을 훈련시키기 위한 테스트 시나리오 생성을 쉽게 해줘요. 벤치마킹을 위해 여러 시나리오를 선택하고, 여러 스케줄링 알고리즘의 성능을 평가해요. 이 시스템의 소스 코드는 로봇 작업 스케줄링 알고리즘을 조정하고 비교할 수 있도록 커뮤니티에 공개돼요.

================================================================================

URL: https://arxiv.org/abs/2408.16776
Title: Online Behavior Modification for Expressive User Control of RL-Trained Robots

Original Abstract:
Reinforcement Learning (RL) is an effective method for robots to learn tasks. However, in typical RL, end-users have little to no control over how the robot does the task after the robot has been deployed. To address this, we introduce the idea of online behavior modification, a paradigm in which users have control over behavior features of a robot in real time as it autonomously completes a task using an RL-trained policy. To show the value of this user-centered formulation for human-robot interaction, we present a behavior diversity based algorithm, Adjustable Control Of RL Dynamics (ACORD), and demonstrate its applicability to online behavior modification in simulation and a user study. In the study (n=23) users adjust the style of paintings as a robot traces a shape autonomously. We compare ACORD to RL and Shared Autonomy (SA), and show ACORD affords user-preferred levels of control and expression, comparable to SA, but with the potential for autonomous execution and robustness of RL.

Translated Abstract:
강화 학습(RL)은 로봇이 작업을 배우는 데 효과적인 방법이야. 하지만 일반적인 RL에서는 로봇이 배포된 후에 사용자가 로봇이 작업을 수행하는 방식에 거의 영향을 미칠 수 없어. 그래서 우리는 온라인 행동 수정이라는 개념을 도입했어. 이건 사용자가 로봇이 RL로 훈련된 정책을 사용해 작업을 자동으로 수행할 때, 행동의 특정 특징을 실시간으로 조정할 수 있는 패러다임이야.

우리는 이 사용자 중심의 접근 방식이 인간-로봇 상호작용에 얼마나 유용한지를 보여주기 위해, 행동 다양성 기반 알고리즘인 조정 가능한 RL 동적 제어(ACORD)를 소개해. 그리고 이 알고리즘이 시뮬레이션과 사용자 연구에서 온라인 행동 수정에 어떻게 적용될 수 있는지를 보여줄 거야. 

연구에서는 23명의 사용자가 로봇이 자율적으로 도형을 그릴 때 그림 스타일을 조정했어. 우리는 ACORD를 RL과 공유 자율성(SA)과 비교했는데, ACORD는 사용자들이 선호하는 수준의 제어와 표현을 제공하면서도, 자율적인 실행과 RL의 강인성도 갖추고 있다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2408.17422
Title: Open-vocabulary Temporal Action Localization using VLMs

Original Abstract:
Video action localization aims to find timings of a specific action from a long video. Although existing learning-based approaches have been successful, those require annotating videos that come with a considerable labor cost. This paper proposes a learning-free, open-vocabulary approach based on emerging off-the-shelf vision-language models (VLM). The challenge stems from the fact that VLMs are neither designed to process long videos nor tailored for finding actions. We overcome these problems by extending an iterative visual prompting technique. Specifically, we sample video frames into a concatenated image with frame index labels, making a VLM guess a frame that is considered to be closest to the start/end of the action. Iterating this process by narrowing a sampling time window results in finding a specific frame of start and end of an action. We demonstrate that this sampling technique yields reasonable results, illustrating a practical extension of VLMs for understanding videos. A sample code is available at this https URL.

Translated Abstract:
비디오 액션 로컬라이제이션은 긴 비디오에서 특정 액션의 타이밍을 찾는 작업이야. 기존의 학습 기반 방법들은 꽤 성공적이긴 했지만, 비디오에 주석을 달아야 해서 많은 노동력이 필요해. 이 논문에서는 새로운 오픈 어휘 접근법을 제안하는데, 이건 최근에 나온 비전-언어 모델(VLM)을 기반으로 해. 

문제는 VLM이 긴 비디오를 처리하도록 설계되지 않았고, 액션을 찾는 데에 맞춰져 있지 않다는 거야. 우리는 반복적인 비주얼 프롬프팅 기법을 확장해서 이 문제를 해결해. 구체적으로, 비디오 프레임을 연결된 이미지로 샘플링하고 프레임 인덱스 레이블을 붙여서, VLM이 액션의 시작이나 끝과 가장 가까운 프레임을 추측하게 해. 샘플링 시간 창을 좁혀가며 이 과정을 반복하면 액션의 시작과 끝 프레임을 찾을 수 있어.

우리는 이 샘플링 기법이 합리적인 결과를 낳는다는 걸 보여주고, VLM을 활용한 비디오 이해의 실용적인 확장을 설명했어. 샘플 코드는 이 https URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2408.17207
Title: NanoMVG: USV-Centric Low-Power Multi-Task Visual Grounding based on Prompt-Guided Camera and 4D mmWave Radar

Original Abstract:
Recently, visual grounding and multi-sensors setting have been incorporated into perception system for terrestrial autonomous driving systems and Unmanned Surface Vehicles (USVs), yet the high complexity of modern learning-based visual grounding model using multi-sensors prevents such model to be deployed on USVs in the real-life. To this end, we design a low-power multi-task model named NanoMVG for waterway embodied perception, guiding both camera and 4D millimeter-wave radar to locate specific object(s) through natural language. NanoMVG can perform both box-level and mask-level visual grounding tasks simultaneously. Compared to other visual grounding models, NanoMVG achieves highly competitive performance on the WaterVG dataset, particularly in harsh environments and boasts ultra-low power consumption for long endurance.

Translated Abstract:
최근에 지상 자율주행 시스템과 무인 수상 차량(USV)의 인식 시스템에 시각적 기초와 다중 센서 설정이 포함되었지만, 다중 센서를 사용하는 현대의 학습 기반 시각적 기초 모델의 복잡함 때문에 실제 USV에 배포하기가 어렵습니다. 

이 문제를 해결하기 위해, 우리는 물길 내재 인식을 위한 낮은 전력 소모의 다중 작업 모델인 NanoMVG를 설계했습니다. 이 모델은 카메라와 4D 밀리미터파 레이더를 활용해 자연어를 통해 특정 객체를 찾도록 안내합니다. NanoMVG는 박스 수준과 마스크 수준의 시각적 기초 작업을 동시에 수행할 수 있습니다. 

다른 시각적 기초 모델들과 비교했을 때, NanoMVG는 WaterVG 데이터셋에서 매우 경쟁력 있는 성능을 보여줍니다. 특히, 가혹한 환경에서도 잘 작동하며, 매우 낮은 전력 소모로 긴 지속 시간을 자랑합니다.

================================================================================

URL: https://arxiv.org/abs/2408.16754
Title: A compact neuromorphic system for ultra energy-efficient, on-device robot localization

Original Abstract:
Neuromorphic computing offers a transformative pathway to overcome the computational and energy challenges faced in deploying robotic localization and navigation systems at the edge. Visual place recognition, a critical component for navigation, is often hampered by the high resource demands of conventional systems, making them unsuitable for small-scale robotic platforms which still require to perform complex, long-range tasks. Although neuromorphic approaches offer potential for greater efficiency, real-time edge deployment remains constrained by the complexity and limited scalability of bio-realistic networks. Here, we demonstrate a neuromorphic localization system that performs accurate place recognition in up to 8km of traversal using models as small as 180 KB with 44k parameters, while consuming less than 1% of the energy required by conventional methods. Our Locational Encoding with Neuromorphic Systems (LENS) integrates spiking neural networks, an event-based dynamic vision sensor, and a neuromorphic processor within a single SPECK(TM) chip, enabling real-time, energy-efficient localization on a hexapod robot. LENS represents the first fully neuromorphic localization system capable of large-scale, on-device deployment, setting a new benchmark for energy efficient robotic place recognition.

Translated Abstract:
신경형 컴퓨팅은 로봇의 위치 인식 및 내비게이션 시스템을 구현할 때 마주치는 계산 및 에너지 문제를 해결하는 혁신적인 방법을 제공합니다. 내비게이션에 중요한 시각적 장소 인식은 전통적인 시스템의 높은 자원 요구 때문에 자주 방해받는데, 이로 인해 소형 로봇 플랫폼에서는 복잡하고 긴 거리의 작업을 수행하기에 적합하지 않습니다. 신경형 접근 방식은 더 높은 효율성을 제공할 가능성이 있지만, 실제로 에지에서 배치하는 데는 생물학적으로 현실적인 네트워크의 복잡성과 제한된 확장성 때문에 어려움이 있습니다.

이 연구에서는 180KB 크기의 모델과 44,000개의 파라미터로 최대 8km의 이동 거리에서 정확한 장소 인식을 수행하는 신경형 위치 인식 시스템을 보여줍니다. 이 시스템은 기존 방법의 1%도 안 되는 에너지를 소비합니다. 우리의 신경형 시스템을 이용한 위치 인코딩(LENS)은 스파이킹 신경망, 이벤트 기반 동적 비전 센서, 신경형 프로세서를 하나의 SPECK(TM) 칩 안에 통합하여 헥사포드 로봇에서 실시간으로 에너지 효율적인 위치 인식을 가능하게 합니다. LENS는 대규모로 장치 내에서 배치할 수 있는 최초의 완전 신경형 위치 인식 시스템으로, 에너지 효율적인 로봇 장소 인식의 새로운 기준을 설정합니다.

================================================================================

URL: https://arxiv.org/abs/2408.16726
Title: Bipedal locomotion using geometric techniques

Original Abstract:
This article describes a bipedal walking algorithm with inverse kinematics resolution based solely on geometric methods, so that all mathematical concepts are explained from the base, in order to clarify the reason for this solution. To do so, it has been necessary to simplify the problem and carry out didactic work to distribute content. In general, the articles related to this topic use matrix systems to solve both direct and inverse kinematics, using complex techniques such as decoupling or the Jacobian calculation. By simplifying the walking process, its resolution has been proposed in a simple way using only geometric techniques.

Translated Abstract:
이 논문은 기하학적인 방법만으로 해결하는 역운동학을 이용한 이족 보행 알고리즘에 대해 설명해. 모든 수학적 개념을 처음부터 쉽게 풀어서 이 해결 방법의 이유를 분명히 하려고 했어.

그래서 문제를 간단하게 만들고, 내용을 잘 전달하기 위해 교육적인 작업이 필요했어. 일반적으로 이 주제와 관련된 논문들은 행렬 시스템을 사용해서 직접 및 역운동학을 해결하는데, 복잡한 기술인 분리 또는 야코비안 계산 같은 걸 많이 써. 

하지만 보행 과정을 간소화해서, 기하학적인 기술만으로 간단하게 해결할 수 있는 방법을 제안했어.

================================================================================

URL: https://arxiv.org/abs/2408.16703
Title: RoboMNIST: A Multimodal Dataset for Multi-Robot Activity Recognition Using WiFi Sensing, Video, and Audio

Original Abstract:
We introduce a novel dataset for multi-robot activity recognition (MRAR) using two robotic arms integrating WiFi channel state information (CSI), video, and audio data. This multimodal dataset utilizes signals of opportunity, leveraging existing WiFi infrastructure to provide detailed indoor environmental sensing without additional sensor deployment. Data were collected using two Franka Emika robotic arms, complemented by three cameras, three WiFi sniffers to collect CSI, and three microphones capturing distinct yet complementary audio data streams. The combination of CSI, visual, and auditory data can enhance robustness and accuracy in MRAR. This comprehensive dataset enables a holistic understanding of robotic environments, facilitating advanced autonomous operations that mimic human-like perception and interaction. By repurposing ubiquitous WiFi signals for environmental sensing, this dataset offers significant potential aiming to advance robotic perception and autonomous systems. It provides a valuable resource for developing sophisticated decision-making and adaptive capabilities in dynamic environments.

Translated Abstract:
우리는 두 개의 로봇 팔을 사용해서 WiFi 채널 상태 정보(CSI), 비디오, 오디오 데이터를 통합한 새로운 다중 로봇 활동 인식(MRAR)을 위한 데이터셋을 소개해. 이 데이터셋은 기존의 WiFi 인프라를 활용해서 추가 센서 없이도 실내 환경을 자세히 감지할 수 있는 기회를 제공해.

데이터는 두 개의 Franka Emika 로봇 팔과 세 개의 카메라, 세 개의 WiFi 스니퍼(CSI 수집용), 그리고 세 개의 마이크로폰을 사용해서 수집했어. 이 마이크로폰은 서로 다른 오디오 데이터를 캡처하면서 보완적인 역할을 해. CSI, 시각, 청각 데이터를 조합하면 MRAR의 견고성과 정확성을 높일 수 있어.

이 포괄적인 데이터셋은 로봇 환경을 전체적으로 이해하는 데 도움을 주고, 사람처럼 인식하고 상호작용하는 고급 자율 작동을 가능하게 해. 흔히 있는 WiFi 신호를 환경 감지에 재활용함으로써, 이 데이터셋은 로봇 인식과 자율 시스템을 발전시킬 수 있는 큰 잠재력을 제공해. 동적인 환경에서 복잡한 의사결정과 적응 능력을 개발하는 데 유용한 자원이 될 거야.

================================================================================

URL: https://arxiv.org/abs/2408.16633
Title: Optimizing Automated Picking Systems in Warehouse Robots Using Machine Learning

Original Abstract:
With the rapid growth of global e-commerce, the demand for automation in the logistics industry is increasing. This study focuses on automated picking systems in warehouses, utilizing deep learning and reinforcement learning technologies to enhance picking efficiency and accuracy while reducing system failure rates. Through empirical analysis, we demonstrate the effectiveness of these technologies in improving robot picking performance and adaptability to complex environments. The results show that the integrated machine learning model significantly outperforms traditional methods, effectively addressing the challenges of peak order processing, reducing operational errors, and improving overall logistics efficiency. Additionally, by analyzing environmental factors, this study further optimizes system design to ensure efficient and stable operation under variable conditions. This research not only provides innovative solutions for logistics automation but also offers a theoretical and empirical foundation for future technological development and application.

Translated Abstract:
전 세계 전자상거래가 빠르게 성장하면서 물류 산업에서 자동화에 대한 수요가 증가하고 있어. 이 연구는 창고의 자동 피킹 시스템에 초점을 맞추고 있어. 딥러닝과 강화 학습 기술을 활용해 피킹 효율성과 정확성을 높이고 시스템 실패율을 줄이려고 해.

경험적 분석을 통해 이 기술들이 로봇 피킹 성능을 개선하고 복잡한 환경에 적응하는 데 효과적이라는 걸 보여주고 있어. 결과적으로 통합된 머신러닝 모델이 전통적인 방법보다 훨씬 뛰어난 성능을 보이며, 주문 처리의 피크 시점에서의 문제를 효과적으로 해결하고 운영 오류를 줄이며 전반적인 물류 효율성을 높이는 데 기여하고 있어.

또한 환경 요인을 분석함으로써 이 연구는 변동적인 조건에서도 효율적이고 안정적인 운영을 보장하는 시스템 디자인을 추가로 최적화하고 있어. 이 연구는 물류 자동화에 대한 혁신적인 솔루션을 제공할 뿐만 아니라, 미래의 기술 개발과 응용을 위한 이론적 및 경험적 기초를 제공하고 있어.

================================================================================

URL: https://arxiv.org/abs/2408.16567
Title: Identifying Terrain Physical Parameters from Vision -- Towards Physical-Parameter-Aware Locomotion and Navigation

Original Abstract:
Identifying the physical properties of the surrounding environment is essential for robotic locomotion and navigation to deal with non-geometric hazards, such as slippery and deformable terrains. It would be of great benefit for robots to anticipate these extreme physical properties before contact; however, estimating environmental physical parameters from vision is still an open challenge. Animals can achieve this by using their prior experience and knowledge of what they have seen and how it felt. In this work, we propose a cross-modal self-supervised learning framework for vision-based environmental physical parameter estimation, which paves the way for future physical-property-aware locomotion and navigation. We bridge the gap between existing policies trained in simulation and identification of physical terrain parameters from vision. We propose to train a physical decoder in simulation to predict friction and stiffness from multi-modal input. The trained network allows the labeling of real-world images with physical parameters in a self-supervised manner to further train a visual network during deployment, which can densely predict the friction and stiffness from image data. We validate our physical decoder in simulation and the real world using a quadruped ANYmal robot, outperforming an existing baseline method. We show that our visual network can predict the physical properties in indoor and outdoor experiments while allowing fast adaptation to new environments.

Translated Abstract:
주변 환경의 물리적 특성을 파악하는 것은 로봇이 미끄럽거나 변형 가능한 지형 같은 비기하학적 위험을 피하는 데 중요해. 로봇이 접촉하기 전에 이런 극단적인 물리적 특성을 미리 예측할 수 있다면 큰 도움이 될 거야. 하지만 환경의 물리적 매개변수를 시각적으로 추정하는 건 여전히 해결해야 할 문제야. 동물들은 자신의 경험과 과거에 본 것, 느낀 것들을 바탕으로 이걸 잘 해내지.

이 연구에서는 시각 기반의 환경 물리적 매개변수를 추정하기 위한 교차 모달 자기 지도 학습 프레임워크를 제안해. 이건 앞으로 물리적 특성을 인식하는 로봇의 이동과 탐색을 가능하게 해. 기존의 시뮬레이션에서 훈련된 정책과 시각을 통한 물리적 지형 매개변수 식별 사이의 격차를 메우는 거야. 우리는 시뮬레이션에서 물리적 디코더를 훈련시켜 다양한 입력으로부터 마찰력과 강성을 예측할 수 있도록 해.

훈련된 네트워크는 실제 이미지에 물리적 매개변수를 자기 지도 방식으로 라벨링할 수 있게 해줘. 이걸 통해 배포 중에 시각 네트워크를 추가로 훈련시킬 수 있고, 이미지 데이터에서 마찰력과 강성을 밀집하게 예측할 수 있어. 우리는 쿼드러펑 ANYmal 로봇을 사용해 시뮬레이션과 실제 환경에서 우리의 물리적 디코더를 검증했는데, 기존의 방법보다 성능이 뛰어났어.

우리의 시각 네트워크는 실내외 실험에서 물리적 특성을 예측할 수 있으며, 새로운 환경에 빠르게 적응할 수 있다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2408.16420
Title: Time-Optimized Trajectory Planning for Non-Prehensile Object Transportation in 3D

Original Abstract:
Non-prehensile object transportation offers a way to enhance robotic performance in object manipulation tasks, especially with unstable objects. Effective trajectory planning requires simultaneous consideration of robot motion constraints and object stability. Here, we introduce a physical model for object stability and propose a novel trajectory planning approach for non-prehensile transportation along arbitrary straight lines in 3D space. Validation with a 7-DoF Franka Panda robot confirms improved transportation speed via tray rotation integration while ensuring object stability and robot motion constraints.

Translated Abstract:
비접촉 방식으로 물체를 운반하는 방법은 로봇이 물체를 다루는 데 더 나은 성능을 발휘할 수 있게 해줘. 특히 불안정한 물체를 다룰 때 그렇지. 효과적인 경로 계획은 로봇의 움직임 제약과 물체의 안정성을 동시에 고려해야 해.

여기서는 물체의 안정성을 위한 물리 모델을 소개하고, 3D 공간에서 임의의 직선으로 비접촉 운반을 위한 새로운 경로 계획 방법을 제안해. 7자유도 프랑카 판다 로봇으로 검증해본 결과, 트레이 회전을 통합했을 때 물체의 안정성과 로봇의 움직임 제약을 유지하면서 운반 속도가 향상된 것을 확인했어.

================================================================================

URL: https://arxiv.org/abs/2408.16375
Title: EasyChauffeur: A Baseline Advancing Simplicity and Efficiency on Waymax

Original Abstract:
Recent advancements in deep-learning-based driving planners have primarily focused on elaborate network engineering, yielding limited improvements. This paper diverges from conventional approaches by exploring three fundamental yet underinvestigated aspects: training policy, data efficiency, and evaluation robustness. We introduce EasyChauffeur, a reproducible and effective planner for both imitation learning (IL) and reinforcement learning (RL) on Waymax, a GPU-accelerated simulator. Notably, our findings indicate that the incorporation of on-policy RL significantly boosts performance and data efficiency. To further enhance this efficiency, we propose SNE-Sampling, a novel method that selectively samples data from the encoder's latent space, substantially improving EasyChauffeur's performance with RL. Additionally, we identify a deficiency in current evaluation methods, which fail to accurately assess the robustness of different planners due to significant performance drops from minor changes in the ego vehicle's initial state. In response, we propose Ego-Shifting, a new evaluation setting for assessing planners' robustness. Our findings advocate for a shift from a primary focus on network architectures to adopting a holistic approach encompassing training strategies, data efficiency, and robust evaluation methods.

Translated Abstract:
최근 딥러닝 기반의 주행 계획자들이 주로 복잡한 네트워크 설계에 집중하면서 큰 개선이 없었어. 이 논문은 전통적인 접근 방식과는 다르게, 훈련 정책, 데이터 효율성, 평가 강인성이라는 세 가지 기본적인 주제를 탐구해. 

우리는 EasyChauffeur라는 reproducible하고 효과적인 계획자를 소개하는데, 이건 Waymax라는 GPU 가속 시뮬레이터에서 모방 학습(IL)과 강화 학습(RL) 모두에 사용할 수 있어. 특히, 우리의 연구 결과는 온-정책 RL을 포함하면 성능과 데이터 효율성이 크게 향상된다는 것을 보여줘. 

또한, 이 효율성을 더 높이기 위해 SNE-Sampling이라는 새로운 방법을 제안하는데, 이건 인코더의 잠재 공간에서 데이터를 선택적으로 샘플링해서 EasyChauffur의 RL 성능을 크게 개선해. 

우리는 현재 평가 방법에서의 결점을 발견했어. 기존 평가는 자율주행 차량의 초기 상태에 약간의 변화가 생기면 성능이 크게 떨어지기 때문에 다양한 계획자의 강인성을 정확히 평가하지 못해. 그래서 우리는 Ego-Shifting이라는 새로운 평가 방식을 제안해. 

이 연구는 네트워크 아키텍처에만 집중하는 대신, 훈련 전략, 데이터 효율성, 강인한 평가 방법을 포함한 전체적인 접근 방식을 채택할 필요가 있다는 것을 강조해.

================================================================================

URL: https://arxiv.org/abs/2408.16370
Title: Efficient Multi-agent Navigation with Lightweight DRL Policy

Original Abstract:
In this article, we present an end-to-end collision avoidance policy based on deep reinforcement learning (DRL) for multi-agent systems, demonstrating encouraging outcomes in real-world applications. In particular, our policy calculates the control commands of the agent based on the raw LiDAR observation. In addition, the number of parameters of the proposed basic model is 140,000, and the size of the parameter file is 3.5 MB, which allows the robot to calculate the actions from the CPU alone. We propose a multi-agent training platform based on a physics-based simulator to further bridge the gap between simulation and the real world. The policy is trained on a policy-gradients-based RL algorithm in a dense and messy training environment. A novel reward function is introduced to address the issue of agents choosing suboptimal actions in some common scenarios. Although the data used for training is exclusively from the simulation platform, the policy can be successfully transferred and deployed in real-world robots. Finally, our policy effectively responds to intentional obstructions and avoids collisions. The website is available at \url{this https URL}.

Translated Abstract:
이 논문에서는 다중 에이전트 시스템을 위한 딥 강화 학습(DRL) 기반의 충돌 회피 정책을 제안해. 실제 응용에서 좋은 결과를 보여줬어. 

특히, 이 정책은 원시 LiDAR 관측 데이터를 바탕으로 에이전트의 제어 명령을 계산해. 제안한 기본 모델의 파라미터 수는 14만 개이고, 파라미터 파일 크기는 3.5MB로, 로봇이 CPU만으로도 동작을 계산할 수 있게 해. 

우리는 시뮬레이션과 실제 세계의 간극을 좁히기 위해 물리 기반 시뮬레이터를 기반으로 한 다중 에이전트 훈련 플랫폼을 제안해. 이 정책은 복잡하고 혼잡한 훈련 환경에서 정책 기울기 기반 RL 알고리즘으로 학습돼. 에이전트가 일반적인 상황에서 최적이 아닌 행동을 선택하는 문제를 해결하기 위해 새로운 보상 함수를 도입했어. 

훈련에 사용된 데이터는 오직 시뮬레이션 플랫폼에서 나온 것인데도, 이 정책은 실제 로봇에 잘 적용되고 배포될 수 있어. 마지막으로, 우리의 정책은 의도적인 방해에 효과적으로 대응하고 충돌을 피할 수 있어. 웹사이트는 \url{this https URL}에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2408.16354
Title: An Accurate Filter-based Visual Inertial External Force Estimator via Instantaneous Accelerometer Update

Original Abstract:
Accurate disturbance estimation is crucial for reliable robotic physical interaction. To estimate environmental interference in a low-cost and sensorless way (without force sensor), a variety of tightly-coupled visual inertial external force estimators are proposed in the literature. However, existing solutions may suffer from relatively low-frequency preintegration. In this paper, a novel estimator is designed to overcome this issue via high-frequency instantaneous accelerometer update.

Translated Abstract:
정확한 방해 추정은 로봇의 물리적 상호작용에서 정말 중요해. 환경의 간섭을 저렴하고 센서 없이 추정하기 위해, 여러 가지 시각-관성 외부 힘 추정기들이 제안됐어. 그런데 기존 방법들은 주로 상대적으로 낮은 주파수로 사전 통합되는 문제가 있어.

이 논문에서는 이 문제를 해결하기 위해 고주파의 순간 가속도계 업데이트를 사용하는 새로운 추정기를 설계했어.

================================================================================

URL: https://arxiv.org/abs/2408.16307
Title: Safe Bayesian Optimization for High-Dimensional Control Systems via Additive Gaussian Processes

Original Abstract:
Controller tuning and optimization have been among the most fundamental problems in robotics and mechatronic systems. The traditional methodology is usually model-based, but its performance heavily relies on an accurate mathematical model of the system. In control applications with complex dynamics, obtaining a precise model is often challenging, leading us towards a data-driven approach. While optimizing a single controller has been explored by various researchers, it remains a challenge to obtain the optimal controller parameters safely and efficiently when multiple controllers are involved. In this paper, we propose a high-dimensional safe Bayesian optimization method based on additive Gaussian processes to optimize multiple controllers simultaneously and safely. Additive Gaussian kernels replace the traditional squared-exponential kernels or Matérn kernels, enhancing the efficiency with which Gaussian processes update information on unknown functions. Experimental results on a permanent magnet synchronous motor (PMSM) demonstrate that compared to existing safe Bayesian optimization algorithms, our method can obtain optimal parameters more efficiently while ensuring safety.

Translated Abstract:
로봇과 메카트로닉 시스템에서 컨트롤러 조정과 최적화는 가장 기본적인 문제 중 하나야. 전통적으로 모델 기반의 방법을 사용하는데, 이 방법은 시스템의 정확한 수학적 모델에 크게 의존해. 복잡한 동역학을 가진 제어 응용에서는 정확한 모델을 얻는 게 어렵기 때문에 데이터 기반 접근 방식으로 가게 되는 거지.

많은 연구자들이 단일 컨트롤러의 최적화를 다뤄봤지만, 여러 개의 컨트롤러가 있을 때 최적의 파라미터를 안전하고 효율적으로 찾는 건 여전히 도전적인 문제야. 이 논문에서는 여러 개의 컨트롤러를 동시에 안전하게 최적화할 수 있는 고차원 안전 베이지안 최적화 방법을 제안해. 여기서 가법 가우시안 커널을 사용하는데, 이건 전통적인 제곱지수 커널이나 마테른 커널을 대체하면서 가우시안 프로세스가 알려지지 않은 함수에 대한 정보를 업데이트하는 효율을 높여줘.

영구 자석 동기 모터(PMSM)에 대한 실험 결과를 보면, 기존의 안전 베이지안 최적화 알고리즘과 비교했을 때, 우리의 방법이 더 효율적으로 최적의 파라미터를 찾으면서도 안전성을 보장할 수 있다는 걸 보여줘.

================================================================================

URL: https://arxiv.org/abs/2408.16228
Title: Policy Adaptation via Language Optimization: Decomposing Tasks for Few-Shot Imitation

Original Abstract:
Learned language-conditioned robot policies often struggle to effectively adapt to new real-world tasks even when pre-trained across a diverse set of instructions. We propose a novel approach for few-shot adaptation to unseen tasks that exploits the semantic understanding of task decomposition provided by vision-language models (VLMs). Our method, Policy Adaptation via Language Optimization (PALO), combines a handful of demonstrations of a task with proposed language decompositions sampled from a VLM to quickly enable rapid nonparametric adaptation, avoiding the need for a larger fine-tuning dataset. We evaluate PALO on extensive real-world experiments consisting of challenging unseen, long-horizon robot manipulation tasks. We find that PALO is able of consistently complete long-horizon, multi-tier tasks in the real world, outperforming state of the art pre-trained generalist policies, and methods that have access to the same demonstrations.

Translated Abstract:
학습된 언어 기반 로봇 정책들은 다양한 지침을 통해 사전 훈련을 받았음에도 불구하고 새로운 현실 세계의 작업에 잘 적응하지 못하는 경우가 많아. 우리는 비전-언어 모델(VLM)이 제공하는 작업 분해에 대한 의미적 이해를 활용해서 새로운 작업에 대한 몇 가지 샷 적응을 위한 새로운 접근 방식을 제안해.

우리 방법은 '언어 최적화를 통한 정책 적응(Policy Adaptation via Language Optimization, PALO)'이라고 부르며, 특정 작업의 몇 가지 시연과 VLM에서 샘플링된 언어 분해를 결합해서 빠르게 비모수적 적응을 가능하게 해. 이렇게 하면 더 큰 미세 조정 데이터셋이 필요하지 않아.

우리는 PALO를 다양한 도전적인 새로운 장기 로봇 조작 작업이 포함된 실제 실험에서 평가했어. 그 결과, PALO는 실제 세계에서 긴 시간 동안 여러 단계에 걸친 작업을 꾸준히 완수할 수 있었고, 최신의 사전 훈련된 일반 정책들과 같은 시연에 접근할 수 있는 방법들보다 더 나은 성능을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2408.16206
Title: RMMI: Enhanced Obstacle Avoidance for Reactive Mobile Manipulation using an Implicit Neural Map

Original Abstract:
We introduce RMMI, a novel reactive control framework for mobile manipulators operating in complex, static environments. Our approach leverages a neural Signed Distance Field (SDF) to model intricate environment details and incorporates this representation as inequality constraints within a Quadratic Program (QP) to coordinate robot joint and base motion. A key contribution is the introduction of an active collision avoidance cost term that maximises the total robot distance to obstacles during the motion. We first evaluate our approach in a simulated reaching task, outperforming previous methods that rely on representing both the robot and the scene as a set of primitive geometries. Compared with the baseline, we improved the task success rate by 25% in total, which includes increases of 10% by using the active collision cost. We also demonstrate our approach on a real-world platform, showing its effectiveness in reaching target poses in cluttered and confined spaces using environment models built directly from sensor data. For additional details and experiment videos, visit this https URL.

Translated Abstract:
우리는 복잡한 고정 환경에서 작동하는 모바일 조작기를 위한 새로운 반응형 제어 프레임워크인 RMMI를 소개해. 이 방법은 신경망을 이용한 서명 거리 필드(SDF)를 사용해서 복잡한 환경 세부 사항을 모델링하고, 이 표현을 불평등 제약조건으로 사용해서 로봇의 관절과 바닥 움직임을 조정해.

주요 기여 중 하나는 로봇이 움직일 때 장애물과의 거리를 최대화하는 능동적 충돌 회피 비용 항을 도입한 거야. 우리는 먼저 시뮬레이션된 도달 작업에서 이 방법을 평가했는데, 로봇과 장면을 원시 기하학의 집합으로 표현하는 이전 방법들보다 성능이 더 좋았어. 기준선과 비교했을 때, 전체 작업 성공률이 25% 향상되었고, 여기에는 능동적 충돌 비용을 사용해서 10%가 증가한 것도 포함돼.

또한 실제 플랫폼에서도 이 방법을 보여줬는데, 센서 데이터를 직접 사용해 만든 환경 모델을 이용해서 복잡하고 좁은 공간에서 목표 자세에 도달하는 데 효과적임을 보여줬어. 추가적인 세부 사항과 실험 비디오는 이 링크에서 확인해.

================================================================================

URL: https://arxiv.org/abs/2408.16125
Title: DECAF: a Discrete-Event based Collaborative Human-Robot Framework for Furniture Assembly

Original Abstract:
This paper proposes a task planning framework for collaborative Human-Robot scenarios, specifically focused on assembling complex systems such as furniture. The human is characterized as an uncontrollable agent, implying for example that the agent is not bound by a pre-established sequence of actions and instead acts according to its own preferences. Meanwhile, the task planner computes reactively the optimal actions for the collaborative robot to efficiently complete the entire assembly task in the least time possible. We formalize the problem as a Discrete Event Markov Decision Problem (DE-MDP), a comprehensive framework that incorporates a variety of asynchronous behaviors, human change of mind and failure recovery as stochastic events. Although the problem could theoretically be addressed by constructing a graph of all possible actions, such an approach would be constrained by computational limitations. The proposed formulation offers an alternative solution utilizing Reinforcement Learning to derive an optimal policy for the robot. Experiments where conducted both in simulation and on a real system with human subjects assembling a chair in collaboration with a 7-DoF manipulator.

Translated Abstract:
이 논문은 협력적인 인간-로봇 시나리오를 위한 작업 계획 프레임워크를 제안해. 특히 가구 같은 복잡한 시스템을 조립하는 데 초점을 맞추고 있어. 여기서 인간은 통제할 수 없는 존재로 묘사되는데, 예를 들어 사전에 정해진 행동 순서에 얽매이지 않고 자신의 선호에 따라 행동한다는 뜻이야.

한편, 작업 계획자는 협력 로봇이 전체 조립 작업을 가능한 가장 짧은 시간 안에 효율적으로 완료할 수 있도록 최적의 행동을 즉각 계산해. 이 문제를 이산 사건 마르코프 결정 문제(Discrete Event Markov Decision Problem, DE-MDP)로 공식화했어. 이 프레임워크는 다양한 비동기 행동, 인간의 마음 변화, 실패 복구를 확률적 사건으로 포함하고 있어.

이론적으로는 모든 가능한 행동의 그래프를 만들어서 문제를 해결할 수 있지만, 그런 방법은 계산적인 한계가 있어. 그래서 제안된 방식은 강화 학습을 활용해 로봇의 최적 정책을 도출하는 대안을 제공해. 실험은 시뮬레이션과 실제 시스템에서 사람과 함께 7자유도 조작기를 사용해 의자를 조립하는 방식으로 진행했어.

================================================================================

URL: https://arxiv.org/abs/2408.16076
Title: Path planning for autonomous vehicles with minimal collision severity

Original Abstract:
This paper proposes a path planning algorithm for autonomous vehicles, evaluating collision severity with respect to both static and dynamic obstacles. A collision severity map is generated from ratings, quantifying the severity of collisions. A two-level optimal control problem is designed. At the first level, the objective is to identify paths with the lowest collision severity. Subsequently, at the second level, among the paths with lowest collision severity, the one requiring the minimum steering effort is determined. Finally, numerical simulations were conducted using the optimal control software OCPID-DAE1. The study focuses on scenarios where collisions are unavoidable. Results demonstrate the effectiveness and significance of this approach in finding a path with minimum collision severity for autonomous vehicles. Furthermore, this paper illustrates how the ratings for collision severity influence the behaviour of the automated vehicle.

Translated Abstract:
이 논문은 자율주행차를 위한 경로 계획 알고리즘을 제안해. 이 알고리즘은 정적 장애물과 동적 장애물에 대한 충돌 심각성을 평가해. 충돌 심각도 맵이 만들어지는데, 이건 충돌의 심각성을 정량화하는 데 사용돼.

먼저, 두 단계의 최적 제어 문제를 설계했어. 첫 번째 단계에서는 충돌 심각도가 가장 낮은 경로를 찾는 게 목표야. 그 다음 두 번째 단계에서는 충돌 심각도가 낮은 경로들 중에서 조향 노력이 가장 적게 드는 경로를 결정해.

마지막으로 OCPID-DAE1이라는 최적 제어 소프트웨어를 사용해 수치 시뮬레이션을 했어. 이 연구는 충돌이 피할 수 없는 상황에 초점을 맞추고 있어. 결과적으로 이 방법이 자율주행차의 충돌 심각도가 가장 낮은 경로를 찾는 데 효과적이고 중요하다는 걸 보여줬어. 

또한, 이 논문은 충돌 심각도에 대한 평가가 자동화된 차량의 행동에 어떻게 영향을 미치는지도 설명하고 있어.

================================================================================

URL: https://arxiv.org/abs/2408.16069
Title: Hitting the Gym: Reinforcement Learning Control of Exercise-Strengthened Biohybrid Robots in Simulation

Original Abstract:
Animals can accomplish many incredible behavioral feats across a wide range of operational environments and scales that current robots struggle to match. One explanation for this performance gap is the extraordinary properties of the biological materials that comprise animals, such as muscle tissue. Using living muscle tissue as an actuator can endow robotic systems with highly desirable properties such as self-healing, compliance, and biocompatibility. Unlike traditional soft robotic actuators, living muscle biohybrid actuators exhibit unique adaptability, growing stronger with use. The dependency of a muscle's force output on its use history endows muscular organisms the ability to dynamically adapt to their environment, getting better at tasks over time. While muscle adaptability is a benefit to muscular organisms, it currently presents a challenge for biohybrid researchers: how does one design and control a robot whose actuators' force output changes over time? Here, we incorporate muscle adaptability into a many-muscle biohybrid robot design and modeling tool, leveraging reinforcement learning as both a co-design partner and system controller. As a controller, our learning agents coordinated the independent contraction of 42 muscles distributed on a lattice worm structure to successfully steer it towards eight distinct targets while incorporating muscle adaptability. As a co-design tool, our agents enable users to identify which muscles are important to accomplishing a given task. Our results show that adaptive agents outperform non-adaptive agents in terms of maximum rewards and training time. Together, these contributions can both enable the elucidation of muscle actuator adaptation and inform the design and modeling of adaptive, performant, many-muscle robots.

Translated Abstract:
동물들은 다양한 환경과 상황에서 놀라운 행동을 할 수 있는데, 현재 로봇들은 이걸 따라가기 힘들어. 그 이유 중 하나는 동물들을 구성하는 생물학적 물질, 예를 들어 근육 조직의 특별한 성질 때문이야. 살아있는 근육 조직을 작동기로 쓰면 로봇 시스템에 자가 치유, 순응성, 생체 적합성 같은 매우 바람직한 특성을 부여할 수 있어.

전통적인 소프트 로봇 작동기와는 달리, 살아있는 근육 바이오하이브리드 작동기는 독특한 적응력을 보여줘. 사용하면서 점점 더 강해지는 거지. 근육의 힘 출력이 사용 이력에 따라 달라지기 때문에 근육이 있는 생물들은 환경에 맞춰 동적으로 적응할 수 있어, 시간이 지날수록 작업을 잘하게 되는 거야.

근육의 적응력은 근육 생물에게는 장점이지만, 바이오하이브리드 연구자들에게는 도전 과제가 돼. 어떻게 작동기의 힘 출력이 시간이 지남에 따라 변하는 로봇을 설계하고 제어할 수 있을까? 여기서 우리는 근육의 적응력을 많은 근육을 가진 바이오하이브리드 로봇 설계와 모델링 도구에 통합했어. 강화 학습을 설계 파트너이자 시스템 제어기로 활용했지.

제어기로서, 우리의 학습 에이전트는 격자 모양의 벌레 구조에 분포된 42개의 근육을 독립적으로 수축시켜서 8개의 다른 목표로 성공적으로 조종했어. 또, 설계 도구로서, 우리의 에이전트는 사용자들이 특정 작업을 수행하는 데 중요한 근육이 무엇인지 파악할 수 있게 도와줘. 결과적으로, 적응형 에이전트는 최대 보상과 훈련 시간 면에서 비적응형 에이전트보다 더 나은 성과를 보여줬어. 

이런 기여들은 근육 작동기의 적응력을 이해하는 데 도움을 주고, 적응형이고 성능이 좋은 많은 근육 로봇의 설계와 모델링에도 유용할 거야.

================================================================================

URL: https://arxiv.org/abs/2408.16770
Title: 3D Whole-body Grasp Synthesis with Directional Controllability

Original Abstract:
Synthesizing 3D whole-bodies that realistically grasp objects is useful for animation, mixed reality, and robotics. This is challenging, because the hands and body need to look natural w.r.t. each other, the grasped object, as well as the local scene (i.e., a receptacle supporting the object). Only recent work tackles this, with a divide-and-conquer approach; it first generates a "guiding" right-hand grasp, and then searches for bodies that match this. However, the guiding-hand synthesis lacks controllability and receptacle awareness, so it likely has an implausible direction (i.e., a body can't match this without penetrating the receptacle) and needs corrections through major post-processing. Moreover, the body search needs exhaustive sampling and is expensive. These are strong limitations. We tackle these with a novel method called CWGrasp. Our key idea is that performing geometry-based reasoning "early on," instead of "too late," provides rich "control" signals for inference. To this end, CWGrasp first samples a plausible reaching-direction vector (used later for both the arm and hand) from a probabilistic model built via raycasting from the object and collision checking. Then, it generates a reaching body with a desired arm direction, as well as a "guiding" grasping hand with a desired palm direction that complies with the arm's one. Eventually, CWGrasp refines the body to match the "guiding" hand, while plausibly contacting the scene. Notably, generating already-compatible "parts" greatly simplifies the "whole." Moreover, CWGrasp uniquely tackles both right- and left-hand grasps. We evaluate on the GRAB and ReplicaGrasp datasets. CWGrasp outperforms baselines, at lower runtime and budget, while all components help performance. Code and models will be released.

Translated Abstract:
3D 전체 몸체를 실제로 물체를 잡는 모습으로 합성하는 건 애니메이션, 혼합 현실, 로봇 공학에 유용해. 하지만 이게 쉽지 않아, 왜냐하면 손과 몸이 서로, 그리고 잡고 있는 물체와 지역 상황(즉, 물체를 지지하는 용기)에 대해 자연스러워야 하거든. 최근에야 이런 문제를 다룬 연구가 나왔는데, 나눠서 해결하는 방식을 사용해. 먼저 "가이드" 역할을 하는 오른손의 그립을 생성하고, 그 다음 이와 맞는 몸체를 찾는 방식이야. 

하지만 가이드 손 합성은 조절 가능성이 부족하고, 용기에 대한 인식이 없어서 비현실적인 방향이 나올 가능성이 높아(즉, 몸체가 용기를 뚫지 않고는 맞출 수 없는 경우) 그래서 큰 후처리를 통해 수정이 필요해. 게다가 몸체를 찾는 과정은 exhaustive sampling이 필요하고 비용도 많이 들어. 이런 점들이 큰 제한이야.

우리는 CWGrasp라는 새로운 방법으로 이 문제를 해결해. 우리의 핵심 아이디어는 "너무 늦게"가 아니라 "일찍" 기하학 기반의 추론을 수행하는 게 풍부한 "제어" 신호를 제공한다는 거야. 이를 위해 CWGrasp는 먼저 물체에서 레이캐스팅으로 만든 확률적 모델에서 그럴듯한 도달 방향 벡터를 샘플링해(이건 나중에 팔과 손 모두에 사용돼). 그리고 원하는 팔 방향을 가진 도달 가능한 몸체를 생성하고, 팔 방향과 일치하는 원하는 손바닥 방향을 가진 "가이드" 그립 손을 만들어. 마지막으로 CWGrasp는 "가이드" 손에 맞게 몸체를 다듬으면서 자연스럽게 장면과 접촉하도록 해. 특히, 이미 호환 가능한 "부분"을 생성하는 게 "전체"를 훨씬 간단하게 만들어. 

또한 CWGrasp는 오른손과 왼손의 그립을 모두 다룬다는 점이 독특해. 우리는 GRAB과 ReplicaGrasp 데이터셋에서 평가했어. CWGrasp는 기준 모델들보다 성능이 뛰어나면서도 실행 시간과 비용이 적고, 모든 구성 요소가 성능 향상에 도움이 돼. 코드와 모델은 곧 공개할 예정이야.

================================================================================

URL: https://arxiv.org/abs/2408.16755
Title: Auricular Vagus Nerve Stimulation for Enhancing Remote Pilot Training and Operations

Original Abstract:
The rapid growth of the drone industry, particularly in the use of small unmanned aerial systems (sUAS) and unmanned aerial vehicles (UAVs), requires the development of advanced training protocols for remote pilots. Remote pilots must develop a combination of technical and cognitive skills to manage the complexities of modern drone operations. This paper explores the integration of neurotechnology, specifically auricular vagus nerve stimulation (aVNS), as a method to enhance remote pilot training and performance. The scientific literature shows aVNS can safely improve cognitive functions such as attention, learning, and memory. It has also been shown useful to manage stress responses. For safe and efficient sUAS/UAV operation, it is essential for pilots to maintain high levels of vigilance and decision-making under pressure. By modulating sympathetic stress and cortical arousal, aVNS can prime cognitive faculties before training, help maintain focus during training and improve stress recovery post-training. Furthermore, aVNS has demonstrated the potential to enhance multitasking and cognitive control. This may help remote pilots during complex sUAS operations by potentially reducing the risk of impulsive decision-making or cognitive errors. This paper advocates for the inclusion of aVNS in remote pilot training programs by proposing that it can provide significant benefits in improving cognitive readiness, skill and knowledge acquisition, as well as operational safety and efficiency. Future research should focus on optimizing aVNS protocols for drone pilots while assessing long-term benefits to industrial safety and workforce readiness in real-world scenarios.

Translated Abstract:
드론 산업이 빠르게 성장하면서, 특히 소형 무인 항공 시스템(sUAS)과 무인 항공기(UAV)의 사용이 늘어나고 있어 원격 조종사를 위한 고급 훈련 프로토콜이 필요해지고 있어. 원격 조종사는 현대 드론 운영의 복잡성을 관리하기 위해 기술적이고 인지적인 능력 두 가지를 모두 개발해야 해.

이 논문에서는 신경 기술, 특히 귀 미주 신경 자극(aVNS)을 활용해 원격 조종사의 훈련과 성능을 향상시키는 방법을 살펴보고 있어. 여러 연구에 따르면, aVNS는 주의력, 학습, 기억 같은 인지 기능을 안전하게 개선할 수 있어. 스트레스 반응을 관리하는 데도 효과적이라는 결과가 나왔어.

안전하고 효율적인 sUAS/UAV 운영을 위해서는 조종사가 압박 속에서도 높은 경계심과 의사결정을 유지하는 게 중요해. aVNS는 교감 신경의 스트레스와 피질 각성을 조절해 훈련 전에 인지 기능을 활성화하고, 훈련 중 집중력을 유지하며, 훈련 후 스트레스 회복을 도와줄 수 있어. 또, aVNS는 멀티태스킹과 인지 통제를 향상시킬 가능성도 보여. 그래서 복잡한 sUAS 운영 중 원격 조종사가 충동적인 의사결정이나 인지 오류의 위험을 줄이는 데 도움이 될 수 있어.

이 논문은 원격 조종사 훈련 프로그램에 aVNS를 포함할 것을 권장하고 있어. aVNS가 인지 준비 상태, 기술과 지식 습득, 운영 안전성 및 효율성을 크게 향상시킬 수 있다고 제안하고 있어. 앞으로의 연구는 드론 조종사를 위한 aVNS 프로토콜을 최적화하고, 실제 상황에서 산업 안전과 인력 준비성에 대한 장기적인 이점을 평가하는 데 초점을 맞춰야 해.

================================================================================

URL: https://arxiv.org/abs/2408.16559
Title: DroneWiS: Automated Simulation Testing of small Unmanned Aerial Systems in Realistic Windy Conditions

Original Abstract:
The continuous evolution of small Unmanned Aerial Systems (sUAS) demands advanced testing methodologies to ensure their safe and reliable operations in the real-world. To push the boundaries of sUAS simulation testing in realistic environments, we previously developed the DroneReqValidator (DRV) platform, allowing developers to automatically conduct simulation testing in digital twin of earth. In this paper, we present DRV 2.0, which introduces a novel component called DroneWiS (Drone Wind Simulation). DroneWiS allows sUAS developers to automatically simulate realistic windy conditions and test the resilience of sUAS against wind. Unlike current state-of-the-art simulation tools such as Gazebo and AirSim that only simulate basic wind conditions, DroneWiS leverages Computational Fluid Dynamics (CFD) to compute the unique wind flows caused by the interaction of wind with the objects in the environment such as buildings and uneven terrains. This simulation capability provides deeper insights to developers about the navigation capability of sUAS in challenging and realistic windy conditions. DroneWiS equips sUAS developers with a powerful tool to test, debug, and improve the reliability and safety of sUAS in real-world. A working demonstration is available at this https URL

Translated Abstract:
소형 무인 항공 시스템(sUAS)의 지속적인 발전은 안전하고 신뢰할 수 있는 운영을 보장하기 위해 고급 테스트 방법론을 필요로 해. 현실적인 환경에서 sUAS 시뮬레이션 테스트의 한계를 넘기 위해, 우리는 드론 요구 검증기(DroneReqValidator, DRV) 플랫폼을 개발했어. 이 플랫폼은 개발자들이 지구의 디지털 트윈에서 자동으로 시뮬레이션 테스트를 할 수 있게 해줘.

이번 논문에서는 DRV 2.0을 소개하는데, 여기에는 드론 바람 시뮬레이션(DroneWiS)이라는 새로운 컴포넌트가 포함되어 있어. DroneWiS는 sUAS 개발자들이 실제 바람이 부는 상황을 자동으로 시뮬레이션하고, 바람에 대한 sUAS의 저항력을 테스트할 수 있게 해줘. 현재의 최첨단 시뮬레이션 도구인 가제보(Gazebo)나 에어심(AirSim)은 기본적인 바람 조건만 시뮬레이션하는 반면, DroneWiS는 계산 유체 역학(CFD)을 활용해 바람과 건물, 울퉁불퉁한 지형 같은 환경 내의 객체 간 상호작용으로 발생하는 독특한 바람 흐름을 계산해.

이런 시뮬레이션 기능은 개발자들에게 도전적이고 현실적인 바람 조건에서 sUAS의 항법 능력에 대한 더 깊은 통찰을 제공해. DroneWiS는 sUAS 개발자들에게 실제 환경에서 sUAS의 신뢰성과 안전성을 테스트하고 디버깅하며 개선할 수 있는 강력한 도구를 제공해. 작동하는 시연은 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2408.16501
Title: UAV-Based Human Body Detector Selection and Fusion for Geolocated Saliency Map Generation

Original Abstract:
The problem of reliably detecting and geolocating objects of different classes in soft real-time is essential in many application areas, such as Search and Rescue performed using Unmanned Aerial Vehicles (UAVs). This research addresses the complementary problems of system contextual vision-based detector selection, allocation, and execution, in addition to the fusion of detection results from teams of UAVs for the purpose of accurately and reliably geolocating objects of interest in a timely manner. In an offline step, an application-independent evaluation of vision-based detectors from a system perspective is first performed. Based on this evaluation, the most appropriate algorithms for online object detection for each platform are selected automatically before a mission, taking into account a number of practical system considerations, such as the available communication links, video compression used, and the available computational resources. The detection results are fused using a method for building maps of salient locations which takes advantage of a novel sensor model for vision-based detections for both positive and negative observations. A number of simulated and real flight experiments are also presented, validating the proposed method.

Translated Abstract:
다양한 클래스의 객체를 신뢰성 있게 감지하고 지리적으로 위치를 파악하는 문제는 무인 항공기(UAV)를 이용한 수색 및 구조와 같은 여러 응용 분야에서 매우 중요해. 이 연구는 시스템의 맥락을 고려한 비전 기반 감지기 선택, 할당, 실행 문제와 UAV 팀의 감지 결과 융합 문제를 다루고 있어. 목표는 관심 있는 객체를 정확하고 신뢰할 수 있게 제때에 지리적으로 위치를 파악하는 거야.

오프라인 단계에서는 시스템 관점에서 비전 기반 감지기를 평가하는 작업이 먼저 이루어져. 이 평가를 바탕으로, 각 플랫폼에 대해 온라인 객체 감지를 위한 가장 적합한 알고리즘이 자동으로 선택돼. 이때 고려하는 요소로는 사용 가능한 통신 링크, 비디오 압축 방식, 그리고 가용한 컴퓨팅 자원 등이 있어.

감지 결과는 긍정적 및 부정적 관측을 위한 새로운 센서 모델을 활용해 주목할 만한 위치의 지도를 만드는 방법으로 융합돼. 이 연구에서는 제안된 방법을 검증하기 위해 여러 번의 시뮬레이션 및 실제 비행 실험도 소개하고 있어.

================================================================================

URL: https://arxiv.org/abs/2408.16442
Title: Integrating Features for Recognizing Human Activities through Optimized Parameters in Graph Convolutional Networks and Transformer Architectures

Original Abstract:
Human activity recognition is a major field of study that employs computer vision, machine vision, and deep learning techniques to categorize human actions. The field of deep learning has made significant progress, with architectures that are extremely effective at capturing human dynamics. This study emphasizes the influence of feature fusion on the accuracy of activity recognition. This technique addresses the limitation of conventional models, which face difficulties in identifying activities because of their limited capacity to understand spatial and temporal features. The technique employs sensory data obtained from four publicly available datasets: HuGaDB, PKU-MMD, LARa, and TUG. The accuracy and F1-score of two deep learning models, specifically a Transformer model and a Parameter-Optimized Graph Convolutional Network (PO-GCN), were evaluated using these datasets. The feature fusion technique integrated the final layer features from both models and inputted them into a classifier. Empirical evidence demonstrates that PO-GCN outperforms standard models in activity recognition. HuGaDB demonstrated a 2.3% improvement in accuracy and a 2.2% increase in F1-score. TUG showed a 5% increase in accuracy and a 0.5% rise in F1-score. On the other hand, LARa and PKU-MMD achieved lower accuracies of 64% and 69% respectively. This indicates that the integration of features enhanced the performance of both the Transformer model and PO-GCN.

Translated Abstract:
인간 활동 인식은 컴퓨터 비전, 머신 비전, 그리고 딥러닝 기술을 사용해서 사람의 행동을 분류하는 중요한 연구 분야야. 딥러닝 분야는 인간의 동적 행동을 잘 포착할 수 있는 매우 효과적인 구조들이 많이 발전했어. 

이 연구는 활동 인식의 정확도에 영향을 주는 특징 융합 기술에 대해 강조하고 있어. 이 기술은 기존 모델의 한계를 극복하는데, 기존 모델은 공간적이고 시간적인 특징을 이해하는 데 어려움을 겪기 때문이야. 이 방법은 네 개의 공개 데이터셋인 HuGaDB, PKU-MMD, LARa, TUG에서 얻은 센서 데이터를 사용했어. 

이 데이터셋을 이용해서 두 개의 딥러닝 모델, 즉 Transformer 모델과 파라미터 최적화 그래프 합성곱 네트워크(PO-GCN)의 정확도와 F1 점수를 평가했어. 특징 융합 기술은 두 모델의 마지막 층에서 나온 특징을 통합해서 분류기에 입력했어. 실험 결과, PO-GCN이 활동 인식에서 표준 모델보다 더 뛰어난 성능을 보였어. 

HuGaDB에서는 정확도가 2.3% 향상되었고, F1 점수가 2.2% 증가했어. TUG에서는 정확도가 5% 올라가고, F1 점수는 0.5% 상승했어. 반면에, LARa와 PKU-MMD는 각각 64%와 69%의 낮은 정확도를 기록했어. 이 결과는 특징을 통합함으로써 Transformer 모델과 PO-GCN의 성능이 향상되었다는 것을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2408.16322
Title: BEVal: A Cross-dataset Evaluation Study of BEV Segmentation Models for Autononomous Driving

Original Abstract:
Current research in semantic bird's-eye view segmentation for autonomous driving focuses solely on optimizing neural network models using a single dataset, typically nuScenes. This practice leads to the development of highly specialized models that may fail when faced with different environments or sensor setups, a problem known as domain shift. In this paper, we conduct a comprehensive cross-dataset evaluation of state-of-the-art BEV segmentation models to assess their performance across different training and testing datasets and setups, as well as different semantic categories. We investigate the influence of different sensors, such as cameras and LiDAR, on the models' ability to generalize to diverse conditions and scenarios. Additionally, we conduct multi-dataset training experiments that improve models' BEV segmentation performance compared to single-dataset training. Our work addresses the gap in evaluating BEV segmentation models under cross-dataset validation. And our findings underscore the importance of enhancing model generalizability and adaptability to ensure more robust and reliable BEV segmentation approaches for autonomous driving applications. The code for this paper available at this https URL .

Translated Abstract:
현재 자율주행을 위한 의미적 새 시점 분할 연구는 주로 nuScenes라는 단일 데이터셋을 사용해 신경망 모델을 최적화하는 데 집중하고 있어. 이러면 특정 환경이나 센서 설정에서 문제가 발생할 수 있는데, 이를 도메인 시프트라고 해.

이 논문에서는 최신 BEV 분할 모델들을 다양한 훈련 및 테스트 데이터셋과 설정, 그리고 다양한 의미적 카테고리에서 평가해봤어. 카메라나 LiDAR 같은 다른 센서가 모델의 일반화 능력에 어떤 영향을 미치는지도 살펴봤어. 

또한, 여러 데이터셋으로 훈련하는 실험을 통해 단일 데이터셋으로 훈련할 때보다 BEV 분할 성능을 향상시켰어. 우리는 교차 데이터셋 검증에서 BEV 분할 모델을 평가하는 데 필요한 부분을 다뤘고, 모델의 일반화 가능성과 적응력을 높이는 게 얼마나 중요한지를 강조했어. 이 연구는 자율주행 애플리케이션에서 더 강력하고 신뢰할 수 있는 BEV 분할 접근 방식을 보장하기 위해 필요해. 

이 논문에 대한 코드는 이 URL에서 확인할 수 있어.

================================================================================

