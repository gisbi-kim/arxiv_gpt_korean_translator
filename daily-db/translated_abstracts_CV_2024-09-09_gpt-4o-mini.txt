URL: https://arxiv.org/abs/2409.03761
Title: Efficient Scene Appearance Aggregation for Level-of-Detail Rendering

Original Abstract:
Creating an appearance-preserving level-of-detail (LoD) representation for arbitrary 3D scenes is a challenging problem. The appearance of a scene is an intricate combination of both geometry and material models, and is further complicated by correlation due to the spatial configuration of scene elements. We present a novel volumetric representation for the aggregated appearance of complex scenes and an efficient pipeline for LoD generation and rendering. The core of our representation is the Aggregated Bidirectional Scattering Distribution Function (ABSDF) that summarizes the far-field appearance of all surfaces inside a voxel. We propose a closed-form factorization of the ABSDF that accounts for spatially varying and orientation-varying material parameters. We tackle the challenge of capturing the correlation existing locally within a voxel and globally across different parts of the scene. Our method faithfully reproduces appearance and achieves higher quality than existing scene filtering methods while being inherently efficient to render. The memory footprint and rendering cost of our representation are independent of the original scene complexity.

Translated Abstract:
임의의 3D 장면에 대해 외관을 보존하는 상세 수준(Level-of-Detail, LoD) 표현을 만드는 것은 어려운 문제야. 장면의 외관은 기하학과 재질 모델의 복잡한 조합으로 이루어져 있고, 장면 요소의 공간적 배치로 인해 더 복잡해져. 

우리는 복잡한 장면의 집합된 외관을 위한 새로운 볼륨 표현과 효율적인 LoD 생성 및 렌더링 파이프라인을 제안해. 우리의 표현의 핵심은 모든 표면의 원거리 외관을 요약한 Aggregated Bidirectional Scattering Distribution Function (ABSDF)이야. 

우리는 공간적으로 변화하는 재질 파라미터와 방향에 따라 변화하는 파라미터를 고려한 ABSDF의 닫힌 형태의 인수를 제안해. 그리고 우리는 복셀 내에서 그리고 장면의 서로 다른 부분 사이에서 존재하는 상관관계를 잡아내는 문제를 해결해. 

우리 방법은 외관을 충실하게 재현하고, 기존의 장면 필터링 방법보다 더 높은 품질을 달성하면서도 렌더링에 본질적으로 효율적이야. 우리의 표현의 메모리 사용량과 렌더링 비용은 원래 장면의 복잡성과는 무관해.

================================================================================

URL: https://arxiv.org/abs/2409.03763
Title: A Dataset for Mechanical Mechanisms

Original Abstract:
This study introduces a dataset consisting of approximately 9,000 images of mechanical mechanisms and their corresponding descriptions, aimed at supporting research in mechanism design. The dataset consists of a diverse collection of 2D and 3D sketches, meticulously curated to ensure relevance and quality. We demonstrate the application of this dataset by fine-tuning two models: 1) Stable Diffusion (for generating new mechanical designs), and 2) BLIP-2 (for captioning these designs). While the results from Stable Diffusion show promise, particularly in generating coherent 3D sketches, the model struggles with 2D sketches and occasionally produces nonsensical outputs. These limitations underscore the need for further development, particularly in expanding the dataset and refining model architectures. Nonetheless, this work serves as a step towards leveraging generative AI in mechanical design, highlighting both the potential and current limitations of these approaches.

Translated Abstract:
이 연구는 약 9,000장의 기계 메커니즘 이미지와 그에 대한 설명으로 구성된 데이터셋을 소개해. 이 데이터셋은 메커니즘 디자인 연구를 지원하기 위해 만들어졌어. 2D와 3D 스케치가 다양하게 포함되어 있고, 관련성과 품질을 보장하기 위해 신중하게 선별되었어.

우리는 이 데이터셋을 활용해서 두 가지 모델을 세밀하게 조정해봤어: 1) Stable Diffusion (새로운 기계 디자인 생성용), 2) BLIP-2 (이 디자인에 대한 설명 생성용). Stable Diffusion의 결과는 꽤 괜찮은데, 특히 일관된 3D 스케치를 생성하는 데는 좋은 편이야. 하지만 2D 스케치에서는 잘 안되거나 때로는 엉뚱한 결과를 내는 경우도 있어. 이런 한계는 데이터셋을 확장하고 모델 구조를 개선할 필요성을 보여줘.

그럼에도 불구하고, 이 연구는 기계 디자인에 생성적 AI를 활용하기 위한 첫걸음이야. 이 방법들이 가진 잠재력과 현재의 한계를 동시에 강조하고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.03764
Title: Modeling Human Strategy for Flattening Wrinkled Cloth Using Neural Networks

Original Abstract:
This paper explores a novel approach to model strategies for flattening wrinkled cloth learning from humans. A human participant study was conducted where the participants were presented with various wrinkle types and tasked with flattening the cloth using the fewest actions possible. A camera and Aruco marker were used to capture images of the cloth and finger movements, respectively. The human strategies for flattening the cloth were modeled using a supervised regression neural network, where the cloth images served as input and the human actions as output. Before training the neural network, a series of image processing techniques were applied, followed by Principal Component Analysis (PCA) to extract relevant features from each image and reduce the input dimensionality. This reduction decreased the model's complexity and computational cost. The actions predicted by the neural network closely matched the actual human actions on an independent data set, demonstrating the effectiveness of neural networks in modeling human actions for flattening wrinkled cloth.

Translated Abstract:
이 논문은 주름진 천을 펴는 방법을 인간에게서 배우는 새로운 접근 방식을 탐구해. 연구에서는 여러 종류의 주름이 있는 천을 주고, 참가자들이 가능한 최소한의 동작으로 천을 펴는 과제를 수행했어. 카메라와 아루코 마커를 이용해서 천과 손 움직임의 이미지를 찍었지.

인간이 천을 펴는 전략은 감독된 회귀 신경망을 사용해 모델링했어. 천의 이미지를 입력으로, 인간의 동작을 출력으로 사용했지. 신경망을 훈련하기 전에 여러 이미지 처리 기법을 적용하고, 주성분 분석(PCA)을 통해 각 이미지에서 중요한 특징을 추출해서 입력 차원을 줄였어. 이렇게 차원을 줄이면 모델의 복잡성과 계산 비용이 낮아져.

신경망이 예측한 동작이 독립적인 데이터 세트에서 실제 인간 동작과 잘 맞았어. 이로 인해 주름진 천을 펴는 인간 동작 모델링에 신경망이 효과적이라는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.03765
Title: AI and Entrepreneurship: Facial Recognition Technology Detects Entrepreneurs, Outperforming Human Experts

Original Abstract:
Occupational outcomes like entrepreneurship are generally considered personal information that individuals should have the autonomy to disclose. With the advancing capability of artificial intelligence (AI) to infer private details from widely available human-centric data, such as social media, it is crucial to investigate whether AI can accurately extract private occupational information from such data. In this study, we demonstrate that deep neural networks can classify individuals as entrepreneurs based on a single facial image with high accuracy in data sourced from Crunchbase, a premier source for entrepreneurship data. Utilizing a dataset comprising facial images of 40,728 individuals, including both entrepreneurs and non-entrepreneurs, we trained a Convolutional Neural Network (CNN) and evaluated its classification performance. While human experts (n=650) and trained participants (n=133) were unable to classify entrepreneurs with accuracy above chance levels (>50%), the AI model achieved a classification accuracy of 79.51%. Several robustness tests show that this high level of accuracy is maintained under various conditions.

Translated Abstract:
직업적 결과, 예를 들어 창업 같은 것은 일반적으로 개인 정보로 여겨져서 사람들이 자율적으로 공개할 수 있어야 해. 하지만 인공지능(AI)의 기술이 발전하면서, 소셜 미디어 같은 널리 퍼진 데이터로부터 개인의 사적인 정보를 추론할 수 있는 능력이 생겼어. 그래서 AI가 이런 데이터에서 개인의 직업 정보를 정확히 추출할 수 있는지 조사하는 게 중요해.

이번 연구에서는 딥 뉴럴 네트워크를 사용해서, 단 하나의 얼굴 이미지로 개인을 창업자로 분류할 수 있다는 걸 보여줬어. 이 데이터는 창업 관련 정보를 제공하는 대표적인 출처인 Crunchbase에서 가져온 거야. 40,728명의 얼굴 이미지로 구성된 데이터셋을 이용해서, 창업자와 비창업자를 모두 포함시켰고, 합성곱 신경망(CNN)을 훈련시켜서 분류 성능을 평가했어.

인간 전문가(650명)와 훈련받은 참가자(133명)가 창업자를 우연히 분류하는 것 이상의 정확도로 분류하지 못했지만, AI 모델은 79.51%의 정확도로 분류에 성공했어. 여러 강건성 테스트를 통해서 이 높은 정확도가 다양한 조건에서도 유지된다는 걸 확인했어.

================================================================================

URL: https://arxiv.org/abs/2409.03766
Title: OpenCap markerless motion capture estimation of lower extremity kinematics and dynamics in cycling

Original Abstract:
Markerless motion capture offers several benefits over traditional marker-based systems by eliminating the need for physical markers, which are prone to misplacement and artifacts. Utilizing computer vision and deep learning algorithms, markerless systems can directly detect human body landmarks, reducing manual processing and errors associated with marker placement. These systems are adaptable, able to track user-defined features, and practical for real-world applications using consumer-grade devices such as smartphone cameras. This study compares the performance of OpenCap, a markerless motion capture system, with traditional marker-based systems in assessing cycling biomechanics. Ten healthy adults participated in experiments to capture sagittal hip, knee, and ankle kinematics and dynamics using both methods. OpenCap used videos from smartphones and integrated computer vision and musculoskeletal simulations to estimate 3D kinematics. Results showed high agreement between the two systems, with no significant differences in kinematic and kinetic measurements for the hip, knee, and ankle. The correlation coefficients exceeded 0.98, indicating very strong consistency. Errors were minimal, with kinematic errors under 4 degrees and kinetic errors below 5 Nm. This study concludes that OpenCap is a viable alternative to marker-based motion capture, offering comparable precision without extensive setup for hip (flexion/extension), knee (flexion/extension), and ankle (dorsiflexion/plantarflexion) joints. Future work should aim to enhance the accuracy of ankle joint measurements and extend analyses to 3D kinematics and kinetics for comprehensive biomechanical assessments.

Translated Abstract:
마커 없는 모션 캡처는 전통적인 마커 기반 시스템보다 여러 가지 장점이 있어. 물리적인 마커가 필요 없어서 잘못 놓이거나 아티팩트가 생기는 문제를 없애주거든. 컴퓨터 비전과 딥러닝 알고리즘을 사용해서 사람의 몸의 랜드마크를 직접 감지할 수 있어서, 마커를 놓는 과정에서 생기는 수동적인 처리와 오류를 줄일 수 있어. 

이 시스템은 적응력이 뛰어나서 사용자가 정의한 특징을 추적할 수 있고, 스마트폰 카메라 같은 일반 소비자 기기를 활용한 실제 응용에도 적합해. 이 연구에서는 OpenCap이라는 마커 없는 모션 캡처 시스템과 전통적인 마커 기반 시스템을 비교해서 자전거 biomechanics를 평가했어. 

10명의 건강한 성인이 실험에 참여해서 두 가지 방법으로 엉덩이, 무릎, 발목의 운동학과 동역학을 기록했어. OpenCap은 스마트폰에서 촬영한 비디오를 사용하고, 컴퓨터 비전과 근골격 시뮬레이션을 통합해서 3D 운동학을 추정했어. 결과적으로 두 시스템 간의 일치도가 높았고, 엉덩이, 무릎, 발목의 운동학과 동역학 측정에서 유의미한 차이가 없었어. 상관계수는 0.98을 초과해서 매우 강한 일관성을 나타냈고, 오류도 최소화되어 운동학적 오류는 4도 이하, 동역학적 오류는 5Nm 이하였어. 

이 연구는 OpenCap이 마커 기반 모션 캡처의 실용적인 대안이 될 수 있으며, 엉덩이(구부림/펴짐), 무릎(구부림/펴짐), 발목(등굽힘/바닥굽힘) 관절에 대해 유사한 정밀도를 제공한다고 결론지었어. 앞으로는 발목 관절 측정의 정확도를 높이고, 3D 운동학과 동역학 분석을 확장해 종합적인 생체역학 평가를 목표로 해야 할 것 같아.

================================================================================

URL: https://arxiv.org/abs/2409.03767
Title: EMCNet : Graph-Nets for Electron Micrographs Classification

Original Abstract:
Characterization of materials via electron micrographs is an important and challenging task in several materials processing industries. Classification of electron micrographs is complex due to the high intra-class dissimilarity, high inter-class similarity, and multi-spatial scales of patterns. However, existing methods are ineffective in learning complex image patterns. We propose an effective end-to-end electron micrograph representation learning-based framework for nanomaterial identification to overcome the challenges. We demonstrate that our framework outperforms the popular baselines on the open-source datasets in nanomaterials-based identification tasks. The ablation studies are reported in great detail to support the efficacy of our approach.

Translated Abstract:
전자 현미경 이미지를 통해 소재를 분석하는 것은 여러 소재 가공 산업에서 중요한 동시에 어려운 작업이야. 전자 현미경 이미지를 분류하는 게 복잡한 이유는 같은 클래스 내에서도 차이가 크고, 클래스 간 유사성이 높으며, 패턴의 공간적 규모가 다양하기 때문이야. 하지만 기존 방법들은 복잡한 이미지 패턴을 잘 학습하지 못해.

그래서 우리는 나노 소재 식별을 위한 효과적인 엔드 투 엔드 전자 현미경 이미지 표현 학습 기반의 프레임워크를 제안해. 이 프레임워크가 나노 소재 식별 작업에서 공개 데이터셋에서 유명한 기준 모델들보다 성능이 뛰어난 걸 보여줬어. 우리의 접근 방식의 효과를 뒷받침하기 위해 여러 가지 실험 결과도 자세히 보고했어.

================================================================================

URL: https://arxiv.org/abs/2409.03777
Title: A Greedy Hierarchical Approach to Whole-Network Filter- Pruning in CNNs

Original Abstract:
Deep convolutional neural networks (CNNs) have achieved impressive performance in many computer vision tasks. However, their large model sizes require heavy computational resources, making pruning redundant filters from existing pre-trained CNNs an essential task in developing efficient models for resource-constrained devices. Whole-network filter pruning algorithms prune varying fractions of filters from each layer, hence providing greater flexibility. Current whole-network pruning methods are either computationally expensive due to the need to calculate the loss for each pruned filter using a training dataset, or use various heuristic / learned criteria for determining the pruning fractions for each layer. This paper proposes a two-level hierarchical approach for whole-network filter pruning which is efficient and uses the classification loss as the final criterion. The lower-level algorithm (called filter-pruning) uses a sparse-approximation formulation based on linear approximation of filter weights. We explore two algorithms: orthogonal matching pursuit-based greedy selection and a greedy backward pruning approach. The backward pruning algorithm uses a novel closed-form error criterion for efficiently selecting the optimal filter at each stage, thus making the whole algorithm much faster. The higher-level algorithm (called layer-selection) greedily selects the best-pruned layer (pruning using the filter-selection algorithm) using a global pruning criterion. We propose algorithms for two different global-pruning criteria: (1) layer-wise relative error (HBGS), and (2) final classification error (HBGTS). Our suite of algorithms outperforms state-of-the-art pruning methods on ResNet18, ResNet32, ResNet56, VGG16, and ResNext101. Our method reduces the RAM requirement for ResNext101 from 7.6 GB to 1.5 GB and achieves a 94% reduction in FLOPS without losing accuracy on CIFAR-10.

Translated Abstract:
딥 컨볼루션 신경망(CNN)은 많은 컴퓨터 비전 작업에서 놀라운 성능을 보여줬어. 하지만 모델 크기가 크기 때문에 많은 계산 자원이 필요해. 그래서 기존의 사전 훈련된 CNN에서 불필요한 필터를 잘라내는 작업이 리소스가 제한된 장치에서 효율적인 모델을 개발하는 데 꼭 필요해.

전체 네트워크 필터 가지치기 알고리즘은 각 레이어에서 다양한 비율로 필터를 잘라내기 때문에 더 큰 유연성을 제공해. 현재의 전체 네트워크 가지치기 방법은 각각의 잘린 필터에 대해 손실을 계산해야 해서 계산 비용이 많이 들거나, 각 레이어의 가지치기 비율을 결정하기 위해 여러 가지 휴리스틱 또는 학습된 기준을 사용해.

이 논문에서는 전체 네트워크 필터 가지치기를 위한 두 단계 계층적 접근 방식을 제안해. 이 방법은 효율적이고 분류 손실을 최종 기준으로 사용해. 하위 알고리즘(필터 가지치기라고 불림)은 필터 가중치의 선형 근사를 기반으로 한 희소 근사 형식을 사용해. 우리는 두 가지 알고리즘을 탐구해: 직교 매칭 추구 기반의 탐욕적 선택과 탐욕적 역 가지치기 접근 방식. 역 가지치기 알고리즘은 각 단계에서 최적의 필터를 효율적으로 선택하기 위해 새로운 닫힌 형태의 오류 기준을 사용해서 전체 알고리즘을 훨씬 빠르게 만들어.

상위 알고리즘(레이어 선택이라고 불림)은 글로벌 가지치기 기준을 사용하여 가장 잘 잘린 레이어(필터 선택 알고리즘을 사용한 가지치기)를 탐욕적으로 선택해. 우리는 두 가지 글로벌 가지치기 기준에 대한 알고리즘을 제안해: (1) 레이어별 상대 오류(HBGS)와 (2) 최종 분류 오류(HBGTS). 우리의 알고리즘 모음은 ResNet18, ResNet32, ResNet56, VGG16, ResNext101에서 최신 가지치기 방법보다 더 뛰어난 성능을 보여줘. 우리의 방법은 ResNext101의 RAM 요구량을 7.6GB에서 1.5GB로 줄이고, CIFAR-10에서 정확도를 잃지 않으면서 FLOPS를 94% 줄였어.

================================================================================

URL: https://arxiv.org/abs/2409.03782
Title: Assessing the Uncertainty and Robustness of Object Detection Models for Detecting Stickers on Laptops

Original Abstract:
Refurbishing laptops extends their lives while contributing to reducing electronic waste, which promotes building a sustainable future. To this end, the Danish Technological Institute (DTI) focuses on the research and development of several applications, including laptop refurbishing. This has several steps, including cleaning, which involves identifying and removing stickers from laptop surfaces. DTI trained six sticker detection models (SDMs) based on open-source object detection models to identify such stickers precisely so these stickers can be removed automatically. However, given the diversity in types of stickers (e.g., shapes, colors, locations), identification of the stickers is highly uncertain, thereby requiring explicit quantification of uncertainty associated with the identified stickers. Such uncertainty quantification can help reduce risks in removing stickers, which, for example, could otherwise result in damaging laptop surfaces. For uncertainty quantification, we adopted the Monte Carlo Dropout method to evaluate the six SDMs from DTI using three datasets: the original image dataset from DTI and two datasets generated with vision language models, i.e., DALL-E-3 and Stable Diffusion-3. In addition, we presented novel robustness metrics concerning detection accuracy and uncertainty to assess the robustness of the SDMs based on adversarial datasets generated from the three datasets using a dense adversary method. Our evaluation results show that different SDMs perform differently regarding different metrics. Based on the results, we provide SDM selection guidelines and lessons learned from various perspectives.

Translated Abstract:
노트북을 수리하면 수명을 늘릴 수 있고 전자 폐기물을 줄이는 데도 도움이 돼서 지속 가능한 미래를 만드는 데 기여할 수 있어. 그래서 덴마크 기술 연구소(DTI)는 노트북 수리를 포함한 여러 응용 프로그램의 연구와 개발에 집중하고 있어. 이 과정에는 청소가 포함되는데, 여기서 노트북 표면의 스티커를 찾아서 제거하는 작업이 필요해.

DTI는 스티커를 정확하게 인식해서 자동으로 제거할 수 있도록 오픈 소스 객체 탐지 모델을 기반으로 여섯 개의 스티커 탐지 모델(SDMs)을 훈련했어. 하지만 스티커의 종류가 다양해서(예: 형태, 색상, 위치) 스티커를 찾는 게 불확실성이 높아. 그래서 스티커와 관련된 불확실성을 명확히 측정할 필요가 있어. 이 불확실성 측정은 스티커를 제거할 때의 위험을 줄이는 데 도움이 되는데, 예를 들어, 스티커를 잘못 제거하면 노트북 표면이 손상될 수 있어.

우리는 여섯 개 SDMs의 불확실성을 측정하기 위해 몬테 카를로 드롭아웃 방법을 사용했어. DTI의 원본 이미지 데이터셋과 비전 언어 모델인 DALL-E-3와 스테이블 디퓨전-3로 생성한 두 개의 데이터셋을 활용했지. 게다가 우리는 탐지 정확성과 불확실성과 관련된 새로운 강건성 메트릭을 제시했어. 이 메트릭은 세 개의 데이터셋을 바탕으로 생성한 적대적 데이터셋을 사용해 SDMs의 강건성을 평가하는 데 도움을 줘. 

평가 결과는 각 SDMs가 다른 메트릭에 따라 성능이 다르다는 걸 보여줘. 우리는 이 결과를 바탕으로 SDM 선택 가이드라인과 다양한 관점에서 얻은 교훈을 제공할 거야.

================================================================================

URL: https://arxiv.org/abs/2409.03868
Title: Few-shot Adaptation of Medical Vision-Language Models

Original Abstract:
Integrating image and text data through multi-modal learning has emerged as a new approach in medical imaging research, following its successful deployment in computer vision. While considerable efforts have been dedicated to establishing medical foundation models and their zero-shot transfer to downstream tasks, the popular few-shot setting remains relatively unexplored. Following on from the currently strong emergence of this setting in computer vision, we introduce the first structured benchmark for adapting medical vision-language models (VLMs) in a strict few-shot regime and investigate various adaptation strategies commonly used in the context of natural images. Furthermore, we evaluate a simple generalization of the linear-probe adaptation baseline, which seeks an optimal blending of the visual prototypes and text embeddings via learnable class-wise multipliers. Surprisingly, such a text-informed linear probe yields competitive performances in comparison to convoluted prompt-learning and adapter-based strategies, while running considerably faster and accommodating the black-box setting. Our extensive experiments span three different medical modalities and specialized foundation models, nine downstream tasks, and several state-of-the-art few-shot adaptation methods. We made our benchmark and code publicly available to trigger further developments in this emergent subject: \url{this https URL}.

Translated Abstract:
이미지와 텍스트 데이터를 결합하는 다중 모달 학습이 의료 이미징 연구에서 새로운 접근법으로 떠오르고 있어. 이 방법은 컴퓨터 비전에서 성공적으로 사용된 바 있어. 많은 노력이 의료 기초 모델을 구축하고 이들을 다양한 작업에 제로샷으로 전이하는 데 쏟아졌지만, 상대적으로 잘 알려지지 않은 몇 샷 설정에 대한 연구는 부족해. 

이번 연구에서는 현재 컴퓨터 비전에서 강력하게 부상하고 있는 몇 샷 설정을 기반으로, 의료 비전-언어 모델(VLM)을 몇 샷 환경에 적응시키기 위한 첫 번째 구조화된 벤치마크를 소개해. 또한 자연 이미지에서 일반적으로 사용되는 다양한 적응 전략을 조사했어. 우리는 시각적 프로토타입과 텍스트 임베딩을 학습 가능한 클래스별 배수기를 통해 최적 혼합을 찾는 간단한 선형 프로브 적응 방법을 평가했어. 놀랍게도, 이런 텍스트 정보를 활용한 선형 프로브는 복잡한 프롬프트 학습이나 어댑터 기반 전략에 비해 경쟁력 있는 성능을 보여주었고, 훨씬 빠르게 실행되며 블랙박스 설정을 수용할 수 있었어.

우리의 실험은 세 가지 다른 의료 모달리티와 특화된 기초 모델, 아홉 개의 다운스트림 작업, 그리고 여러 최첨단 몇 샷 적응 방법을 포함하고 있어. 이 새로운 주제의 발전을 촉진하기 위해 우리의 벤치마크와 코드를 공개했어: \url{this https URL}.

================================================================================

URL: https://arxiv.org/abs/2409.03878
Title: Ground-roll Separation From Land Seismic Records Based on Convolutional Neural Network

Original Abstract:
Ground-roll wave is a common coherent noise in land field seismic data. This Rayleigh-type surface wave usually has low frequency, low apparent velocity, and high amplitude, therefore obscures the reflection events of seismic shot gathers. Commonly used techniques focus on the differences of ground-roll and reflection in transformed domain such as $f-k$ domain, wavelet domain, or curvelet domain. These approaches use a series of fixed atoms or bases to transform the data in time-space domain into transformed domain to separate different waveforms, thus tend to suffer from the complexity for a delicate design of the parameters of the transform domain filter. To deal with these problems, a novel way is proposed to separate ground-roll from reflections using convolutional neural network (CNN) model based method to learn to extract the features of ground-roll and reflections automatically based on training data. In the proposed method, low-pass filtered seismic data which is contaminated by ground-roll wave is used as input of CNN, and then outputs both ground-roll component and low-frequency part of reflection component simultaneously. Discriminative loss is applied together with similarity loss in the training process to enhance the similarity to their train labels as well as the difference between the two outputs. Experiments are conducted on both synthetic and real data, showing that CNN based method can separate ground roll from reflections effectively, and has generalization ability to a certain extent.

Translated Abstract:
지면 롤 파는 육상의 지진 데이터에서 흔히 발생하는 일관된 소음이야. 이 레일리 타입의 표면 파는 보통 주파수가 낮고, 겉보기 속도가 느리며, 진폭이 커서 지진 촬영에서의 반사 사건들을 가리는 경우가 많아. 일반적으로 사용되는 기술들은 $f-k$ 영역, 웨이브렛 영역, 또는 커브렛 영역 같은 변환 영역에서 지면 롤과 반사를 구분하는 데 집중해. 이런 접근법은 고정된 원자나 기초를 사용해 시간-공간 영역의 데이터를 변환 영역으로 바꿔서 서로 다른 파형을 분리하려고 하는데, 변환 필터의 매개변수를 세밀하게 설계하는 게 복잡해지는 경향이 있어.

이런 문제를 해결하기 위해, CNN(합성곱 신경망) 모델을 이용해 지면 롤과 반사를 자동으로 구분하는 새로운 방법이 제안됐어. 이 방법에서는 지면 롤로 오염된 저역 통과 필터링된 지진 데이터를 CNN의 입력으로 사용하고, 동시에 지면 롤 성분과 반사의 저주파 성분을 출력해. 훈련 과정에서는 판별 손실과 유사성 손실을 함께 적용해서 훈련 라벨과의 유사성을 높이고 두 출력 간의 차이를 강조해. 

실제로 합성 데이터와 실제 데이터를 대상으로 실험을 진행했는데, CNN 기반 방법이 지면 롤을 반사에서 효과적으로 분리할 수 있고 어느 정도 일반화 능력도 있다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.03879
Title: Multi-Camera Industrial Open-Set Person Re-Identification and Tracking

Original Abstract:
In recent years, the development of deep learning approaches for the task of person re-identification led to impressive results. However, this comes with a limitation for industrial and practical real-world applications. Firstly, most of the existing works operate on closed-world scenarios, in which the people to re-identify (probes) are compared to a closed-set (gallery). Real-world scenarios often are open-set problems in which the gallery is not known a priori, but the number of open-set approaches in the literature is significantly lower. Secondly, challenges such as multi-camera setups, occlusions, real-time requirements, etc., further constrain the applicability of off-the-shelf methods. This work presents MICRO-TRACK, a Modular Industrial multi-Camera Re_identification and Open-set Tracking system that is real-time, scalable, and easy to integrate into existing industrial surveillance scenarios. Furthermore, we release a novel Re-ID and tracking dataset acquired in an industrial manufacturing facility, dubbed Facility-ReID, consisting of 18-minute videos captured by 8 surveillance cameras.

Translated Abstract:
최근 몇 년 동안, 딥러닝을 이용한 사람 재식별(person re-identification) 기술이 놀라운 결과를 보여줬어. 하지만 이 기술은 산업이나 실제 환경에서는 한계가 있어. 

첫째로, 기존의 연구들은 대부분 닫힌 세계(closed-world) 상황에서 작동해. 즉, 재식별할 사람(프로브)과 비교할 수 있는 고정된 집합(갤러리)이 있어야 해. 하지만 실제 상황은 보통 열린 세계(open-set) 문제라서 갤러리를 미리 알 수 없는 경우가 많아. 그런데 열린 세계 접근 방식에 대한 연구는 정말 적어. 

둘째로, 여러 카메라를 사용하는 환경, 가려짐(occlusions), 실시간 요구사항 같은 문제들도 기존 방법의 적용을 더 어렵게 해. 

이 연구에서는 MICRO-TRACK이라는 시스템을 소개해. 이 시스템은 모듈형 산업 다중 카메라 재식별 및 열린 세계 추적 시스템으로, 실시간으로 작동하고, 확장 가능하며, 기존 산업 감시 시스템에 쉽게 통합할 수 있어. 

게다가, 우리는 산업 제조 시설에서 수집한 새로운 재식별 및 추적 데이터셋인 Facility-ReID도 공개해. 이 데이터셋은 8개의 감시 카메라로 촬영한 18분짜리 비디오로 구성되어 있어.

================================================================================

URL: https://arxiv.org/abs/2409.03887
Title: The Influence of Faulty Labels in Data Sets on Human Pose Estimation

Original Abstract:
In this study we provide empirical evidence demonstrating that the quality of training data impacts model performance in Human Pose Estimation (HPE). Inaccurate labels in widely used data sets, ranging from minor errors to severe mislabeling, can negatively influence learning and distort performance metrics. We perform an in-depth analysis of popular HPE data sets to show the extent and nature of label inaccuracies. Our findings suggest that accounting for the impact of faulty labels will facilitate the development of more robust and accurate HPE models for a variety of real-world applications. We show improved performance with cleansed data.

Translated Abstract:
이번 연구에서는 훈련 데이터의 품질이 사람 자세 추정(HPE) 모델의 성능에 미치는 영향을 실제 데이터를 통해 보여줍니다. 널리 사용되는 데이터 세트에서 잘못된 레이블이 있을 수 있는데, 이건 사소한 오류부터 심각한 잘못된 레이블까지 다양합니다. 이런 문제는 학습에 부정적인 영향을 주고 성능 지표를 왜곡할 수 있습니다.

우리는 인기 있는 HPE 데이터 세트에 대한 깊이 있는 분석을 진행하여 레이블 부정확성의 정도와 성격을 보여줍니다. 우리의 연구 결과는 잘못된 레이블의 영향을 고려하면 다양한 실제 응용을 위한 더 강력하고 정확한 HPE 모델 개발에 도움이 될 것이라고 제안합니다. 우리는 정제된 데이터를 사용했을 때 성능이 개선되는 것을 보여줍니다.

================================================================================

URL: https://arxiv.org/abs/2409.03890
Title: MVTN: A Multiscale Video Transformer Network for Hand Gesture Recognition

Original Abstract:
In this paper, we introduce a novel Multiscale Video Transformer Network (MVTN) for dynamic hand gesture recognition, since multiscale features can extract features with variable size, pose, and shape of hand which is a challenge in hand gesture recognition. The proposed model incorporates a multiscale feature hierarchy to capture diverse levels of detail and context within hand gestures which enhances the model's ability. This multiscale hierarchy is obtained by extracting different dimensions of attention in different transformer stages with initial stages to model high-resolution features and later stages to model low-resolution features. Our approach also leverages multimodal data, utilizing depth maps, infrared data, and surface normals along with RGB images from NVGesture and Briareo datasets. Experiments show that the proposed MVTN achieves state-of-the-art results with less computational complexity and parameters. The source code is available at this https URL.

Translated Abstract:
이 논문에서는 동적인 손 제스처 인식을 위해 새로운 다중 스케일 비디오 트랜스포머 네트워크(MVTN)를 소개해. 손 제스처는 크기, 자세, 모양이 다양해서 인식하기 어려운 부분이 있는데, 다중 스케일 특징을 활용하면 이 문제를 해결할 수 있어.

제안된 모델은 손 제스처의 다양한 세부 사항과 맥락을 포착하기 위해 다중 스케일 특징 계층을 포함하고 있어. 이 다중 스케일 계층은 서로 다른 트랜스포머 단계에서 다양한 주의 차원을 추출해서 생성되는데, 초기 단계에서는 고해상도 특징을 모델링하고, 나중 단계에서는 저해상도 특징을 모델링해.

우리의 접근 방식은 NVGesture와 Briareo 데이터셋에서 RGB 이미지와 함께 깊이 맵, 적외선 데이터, 표면 법선 같은 다중 모달 데이터를 활용해. 실험 결과, 제안된 MVTN이 계산 복잡성과 파라미터 수를 줄이면서도 최첨단 성능을 달성하는 걸 보여줬어. 소스 코드는 이 https URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.03901
Title: On-board Satellite Image Classification for Earth Observation: A Comparative Study of Pre-Trained Vision Transformer Models

Original Abstract:
Remote sensing image classification is a critical component of Earth observation (EO) systems, traditionally dominated by convolutional neural networks (CNNs) and other deep learning techniques. However, the advent of Transformer-based architectures and large-scale pre-trained models has significantly shifted, offering enhanced performance and efficiency. This study focuses on identifying the most effective pre-trained model for land use classification in onboard satellite processing, emphasizing achieving high accuracy, computational efficiency, and robustness against noisy data conditions commonly encountered during satellite-based inference. Through extensive experimentation, we compared traditional CNN-based models, ResNet-based models, and various pre-trained vision Transformer models. Our findings demonstrate that pre-trained Transformer models, particularly MobileViTV2 and EfficientViT-M2, outperform models trained from scratch in accuracy and efficiency. These models achieve high performance with reduced computational requirements and exhibit greater resilience during inference under noisy conditions. While MobileViTV2 excelled on clean validation data, EfficientViT-M2 proved more robust when handling noise, making it the most suitable model for onboard satellite Earth observation tasks. In conclusion, EfficientViT-M2 is the optimal choice for reliable and efficient remote sensing image classification in satellite operations, achieving 98.76\% accuracy, precision, and recall. Specifically, EfficientViT-M2 delivered the highest performance across all metrics, excelled in training efficiency (1,000s) and inference time (10s), and demonstrated greater robustness (overall robustness score at 0.79).

Translated Abstract:
원격 감지 이미지 분류는 지구 관측 시스템에서 중요한 역할을 해. 전통적으로는 합성곱 신경망(CNN) 같은 딥러닝 기술이 주를 이루었지. 하지만 요즘은 트랜스포머 기반 아키텍처와 대규모 사전 훈련 모델들이 등장하면서 성능과 효율성이 크게 향상됐어.

이 연구는 위성에서의 토지 이용 분류를 위해 가장 효과적인 사전 훈련 모델을 찾는 데 초점을 맞추고 있어. 여기서 우리는 높은 정확도, 계산 효율성, 그리고 일반적으로 위성 기반 추론에서 발생하는 노이즈 데이터에 대한 강건성을 강조했어. 여러 실험을 통해 전통적인 CNN 기반 모델, ResNet 기반 모델, 그리고 다양한 사전 훈련된 비전 트랜스포머 모델을 비교했지.

결과적으로 우리는 사전 훈련된 트랜스포머 모델, 특히 MobileViTV2와 EfficientViT-M2가 처음부터 훈련된 모델보다 더 높은 정확도와 효율성을 보인다는 걸 발견했어. 이 모델들은 계산 요구사항이 적으면서도 노이즈 조건에서도 더 잘 작동했어. MobileViTV2는 깨끗한 검증 데이터에서 뛰어났고, EfficientViT-M2는 노이즈를 처리하는 데 더 강한 모습을 보여서 위성 관측 작업에 가장 적합한 모델이었어.

결론적으로, EfficientViT-M2는 신뢰할 수 있고 효율적인 원격 감지 이미지 분류를 위해 가장 좋은 선택이야. 이 모델은 98.76%의 정확도, 정밀도, 재현율을 달성했어. 특히, EfficientViT-M2는 모든 메트릭에서 최고의 성능을 보였고, 훈련 효율성(1,000s)과 추론 시간(10s)에서도 뛰어난 성과를 냈어. 전체 강건성 점수는 0.79로, 더 강한 강건성을 입증했지.

================================================================================

URL: https://arxiv.org/abs/2409.03911
Title: The Role of Generative Systems in Historical Photography Management: A Case Study on Catalan Archives

Original Abstract:
The use of image analysis in automated photography management is an increasing trend in heritage institutions. Such tools alleviate the human cost associated with the manual and expensive annotation of new data sources while facilitating fast access to the citizenship through online indexes and search engines. However, available tagging and description tools are usually designed around modern photographs in English, neglecting historical corpora in minoritized languages, each of which exhibits intrinsic particularities. The primary objective of this research is to study the quantitative contribution of generative systems in the description of historical sources. This is done by contextualizing the task of captioning historical photographs from the Catalan archives as a case study. Our findings provide practitioners with tools and directions on transfer learning for captioning models based on visual adaptation and linguistic proximity.

Translated Abstract:
자동화된 사진 관리에서 이미지 분석의 사용이 유산 기관에서 점점 더 늘어나고 있어. 이런 도구들은 새로운 데이터 소스를 수작업으로 비싸게 주석을 다는 데 드는 인간의 비용을 줄여주고, 온라인 색인과 검색 엔진을 통해 시민들이 빠르게 접근할 수 있게 도와줘.

하지만 현재 사용 가능한 태깅 및 설명 도구들은 보통 현대 사진에 맞춰 영어로 설계되어 있어서, 소수 언어로 된 역사적 자료들은 무시되고 있어. 각 언어는 고유한 특성을 가지고 있는데 말이야. 

이 연구의 주요 목표는 역사적 자료 설명에서 생성 시스템의 양적 기여를 살펴보는 거야. 이를 위해 카탈루냐 아카이브의 역사적 사진에 대한 캡셔닝 작업을 사례 연구로 삼았어. 우리의 발견은 시각적 적응과 언어적 근접성에 기반한 캡셔닝 모델을 위한 전이 학습에 대해 실무자들에게 도구와 방향을 제공해.

================================================================================

URL: https://arxiv.org/abs/2409.03913
Title: Image Recognition for Garbage Classification Based on Pixel Distribution Learning

Original Abstract:
The exponential growth in waste production due to rapid economic and industrial development necessitates efficient waste management strategies to mitigate environmental pollution and resource depletion. Leveraging advancements in computer vision, this study proposes a novel approach inspired by pixel distribution learning techniques to enhance automated garbage classification. The method aims to address limitations of conventional convolutional neural network (CNN)-based approaches, including computational complexity and vulnerability to image variations. We will conduct experiments using the Kaggle Garbage Classification dataset, comparing our approach with existing models to demonstrate the strength and efficiency of pixel distribution learning in automated garbage classification technologies.

Translated Abstract:
빠른 경제와 산업 발전으로 인해 쓰레기 생산이 급격히 증가하고 있어, 환경 오염과 자원 고갈을 줄이기 위한 효율적인 쓰레기 관리 전략이 필요해. 

이 연구는 컴퓨터 비전의 발전을 활용해서 픽셀 분포 학습 기법에서 영감을 받은 새로운 방법을 제안해. 이 방법은 전통적인 CNN(합성곱 신경망) 방식의 한계를 해결하려고 해. 여기서 한계는 계산 복잡성과 이미지 변화에 대한 취약성이야.

우리는 Kaggle의 쓰레기 분류 데이터셋을 사용해서 실험을 진행할 거고, 기존 모델들과 비교할 거야. 이 과정을 통해 픽셀 분포 학습이 자동 쓰레기 분류 기술에서 얼마나 강력하고 효율적인지를 보여줄 거야.

================================================================================

URL: https://arxiv.org/abs/2409.03929
Title: Data-Efficient Generation for Dataset Distillation

Original Abstract:
While deep learning techniques have proven successful in image-related tasks, the exponentially increased data storage and computation costs become a significant challenge. Dataset distillation addresses these challenges by synthesizing only a few images for each class that encapsulate all essential information. Most current methods focus on matching. The problems lie in the synthetic images not being human-readable and the dataset performance being insufficient for downstream learning tasks. Moreover, the distillation time can quickly get out of bounds when the number of synthetic images per class increases even slightly. To address this, we train a class conditional latent diffusion model capable of generating realistic synthetic images with labels. The sampling time can be reduced to several tens of images per seconds. We demonstrate that models can be effectively trained using only a small set of synthetic images and evaluated on a large real test set. Our approach achieved rank \(1\) in The First Dataset Distillation Challenge at ECCV 2024 on the CIFAR100 and TinyImageNet datasets.

Translated Abstract:
딥러닝 기술이 이미지 관련 작업에서 성공을 거두긴 했지만, 데이터 저장과 계산 비용이 너무 많이 드는 게 큰 문제야. 데이터셋 증류는 각 클래스에 대해 필수 정보만 담고 있는 몇 장의 이미지만을 합성함으로써 이 문제를 해결하려고 해. 대부분의 현재 방법들은 이미지 간의 매칭에 집중하고 있어. 하지만 합성된 이미지가 사람에게 읽히기 어렵고, 데이터셋의 성능이 후속 학습 작업에 충분하지 않다는 문제가 있어. 게다가 클래스당 합성된 이미지 수가 조금만 늘어나도 증류 시간이 급격히 증가할 수 있어.

이런 문제를 해결하기 위해, 우리는 레이블이 있는 사실적인 합성 이미지를 생성할 수 있는 클래스 조건부 잠재 확산 모델을 훈련했어. 샘플링 시간은 초당 수십 장의 이미지로 줄일 수 있어. 우리는 소규모의 합성 이미지 세트만으로도 효과적으로 모델을 훈련하고, 큰 실제 테스트 세트에서 평가할 수 있음을 보여줬어. 우리의 접근 방식은 ECCV 2024의 첫 번째 데이터셋 증류 챌린지에서 CIFAR100과 TinyImageNet 데이터셋에 대해 1위를 차지했어.

================================================================================

URL: https://arxiv.org/abs/2409.03938
Title: Deep Clustering of Remote Sensing Scenes through Heterogeneous Transfer Learning

Original Abstract:
This paper proposes a method for unsupervised whole-image clustering of a target dataset of remote sensing scenes with no labels. The method consists of three main steps: (1) finetuning a pretrained deep neural network (DINOv2) on a labelled source remote sensing imagery dataset and using it to extract a feature vector from each image in the target dataset, (2) reducing the dimension of these deep features via manifold projection into a low-dimensional Euclidean space, and (3) clustering the embedded features using a Bayesian nonparametric technique to infer the number and membership of clusters simultaneously. The method takes advantage of heterogeneous transfer learning to cluster unseen data with different feature and label distributions. We demonstrate the performance of this approach outperforming state-of-the-art zero-shot classification methods on several remote sensing scene classification datasets.

Translated Abstract:
이 논문은 라벨이 없는 원거리 감지 장면 데이터셋에 대해 비지도 전체 이미지 클러스터링 방법을 제안해. 이 방법은 세 가지 주요 단계로 이루어져 있어. 

첫 번째, 라벨이 있는 원거리 감지 이미지 데이터셋에서 사전 훈련된 딥 뉴럴 네트워크(DINOv2)를 파인튜닝해서, 목표 데이터셋의 각 이미지에서 특징 벡터를 추출해. 

두 번째, 이 딥 특징의 차원을 줄여서 저차원 유클리드 공간으로 투영해. 

세 번째, 임베딩된 특징을 베이지안 비모수 기법을 사용해서 클러스터링하고, 클러스터의 개수와 멤버십을 동시에 추론해. 

이 방법은 다양한 특성과 라벨 분포를 가진 보지 못한 데이터를 클러스터링하는 데 이질적인 전이 학습을 활용해. 우리는 여러 원거리 감지 장면 분류 데이터셋에서 이 접근법이 최신 제로샷 분류 방법보다 더 뛰어난 성능을 보여준다는 걸 입증했어.

================================================================================

URL: https://arxiv.org/abs/2409.03944
Title: HUMOS: Human Motion Model Conditioned on Body Shape

Original Abstract:
Generating realistic human motion is essential for many computer vision and graphics applications. The wide variety of human body shapes and sizes greatly impacts how people move. However, most existing motion models ignore these differences, relying on a standardized, average body. This leads to uniform motion across different body types, where movements don't match their physical characteristics, limiting diversity. To solve this, we introduce a new approach to develop a generative motion model based on body shape. We show that it's possible to train this model using unpaired data by applying cycle consistency, intuitive physics, and stability constraints, which capture the relationship between identity and movement. The resulting model generates diverse, physically plausible, and dynamically stable human motions that are both quantitatively and qualitatively more realistic than current state-of-the-art methods. More details are available on our project page this https URL.

Translated Abstract:
사람의 움직임을 현실감 있게 생성하는 건 컴퓨터 비전과 그래픽스 분야에서 정말 중요해. 사람의 몸 모양과 크기가 다양해서 이게 움직임에 큰 영향을 미쳐. 하지만 지금까지의 대부분 모션 모델들은 이런 차이를 무시하고 평균적인 몸체를 기준으로 하고 있어. 그래서 다양한 몸 타입에 따라 움직임이 일치하지 않아서 다양성이 제한되는 문제가 있어.

이 문제를 해결하기 위해, 우리는 몸 모양을 기반으로 한 생성 모션 모델을 개발하는 새로운 접근 방식을 소개해. 우리는 사이클 일관성, 직관적인 물리 법칙, 안정성 제약을 적용해서 비슷하지 않은 데이터를 사용해도 이 모델을 훈련할 수 있다는 걸 보여줬어. 이 방법은 신원과 움직임 간의 관계를 잘 포착해.

결과적으로 이 모델은 다양한, 물리적으로 그럴듯하고 동적으로 안정적인 사람의 움직임을 만들어내. 이 움직임은 현재의 최첨단 방법보다 정량적, 정성적으로 더 현실적이야. 더 많은 정보는 우리의 프로젝트 페이지에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.03945
Title: TropNNC: Structured Neural Network Compression Using Tropical Geometry

Original Abstract:
We present TropNNC, a structured pruning framework for compressing neural networks with linear and convolutional layers and ReLU activations. Our approximation is based on a geometrical approach to machine/deep learning, using tropical geometry and extending the work of Misiakos et al. (2022). We use the Hausdorff distance of zonotopes in its standard continuous form to achieve a tighter approximation bound for tropical polynomials compared to Misiakos et al. (2022). This enhancement allows for superior functional approximations of neural networks, leading to a more effective compression algorithm. Our method is significantly easier to implement compared to other frameworks, and does not depend on the availability of training data samples. We validate our framework through extensive empirical evaluations on the MNIST, CIFAR, and ImageNet datasets. Our results demonstrate that TropNNC achieves performance on par with the state-of-the-art method ThiNet, even surpassing it in compressing linear layers, and to the best of our knowledge, it is the first method that achieves this using tropical geometry.

Translated Abstract:
우리는 TropNNC라는 구조적 프루닝 프레임워크를 제안해. 이건 선형 및 컨볼루션 레이어와 ReLU 활성화가 있는 신경망을 압축하는 데 사용돼. 우리 방식은 기하학적 접근을 기반으로 하고, 열대 기하학을 활용해 Misiakos 외의 연구(2022)를 확장한 거야.

우리는 표준 연속 형태의 존토프의 하우스도르프 거리를 사용해서 열대 다항식에 대한 더 엄격한 근사 경계를 얻었어. 이 개선 덕분에 신경망의 기능적 근사화가 더 뛰어나지고, 더 효과적인 압축 알고리즘이 탄생했어. 우리 방법은 다른 프레임워크들에 비해 구현이 훨씬 쉽고, 훈련 데이터 샘플의 유무에 의존하지 않아.

우리는 MNIST, CIFAR, ImageNet 데이터셋을 사용해서 이 프레임워크를 광범위하게 검증했어. 결과적으로 TropNNC는 최신 기술인 ThiNet과 비슷한 성능을 보여주고, 특히 선형 레이어 압축에서는 그걸 뛰어넘기도 했어. 우리가 아는 한, 이 기술을 열대 기하학을 사용해서 구현한 건 처음이야.

================================================================================

URL: https://arxiv.org/abs/2409.03947
Title: FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes

Original Abstract:
Automatic Medical Imaging Narrative generation aims to alleviate the workload of radiologists by producing accurate clinical descriptions directly from radiological images. However, the subtle visual nuances and domain-specific terminology in medical images pose significant challenges compared to generic image captioning tasks. Existing approaches often neglect the vital distinction between normal and abnormal findings, leading to suboptimal performance. In this work, we propose FODA-PG, a novel Fine-grained Organ-Disease Adaptive Partitioning Graph framework that addresses these limitations through domain-adaptive learning. FODA-PG constructs a granular graphical representation of radiological findings by separating disease-related attributes into distinct "disease-specific" and "disease-free" categories based on their clinical significance and location. This adaptive partitioning enables our model to capture the nuanced differences between normal and pathological states, mitigating the impact of data biases. By integrating this fine-grained semantic knowledge into a powerful transformer-based architecture and providing rigorous mathematical justifications for its effectiveness, FODA-PG generates precise and clinically coherent reports with enhanced generalization capabilities. Extensive experiments on the IU-Xray and MIMIC-CXR benchmarks demonstrate the superiority of our approach over state-of-the-art methods, highlighting the importance of domain adaptation in medical report generation.

Translated Abstract:
자동 의료 이미징 내러티브 생성은 방사선 전문의의 업무 부담을 줄이기 위해 방사선 이미지를 바탕으로 정확한 임상 설명을 만들어내는 걸 목표로 해. 하지만 의료 이미지의 미세한 시각적 차이와 특정 분야의 용어 때문에 일반 이미지 캡션 작업보다 더 많은 어려움이 있어. 기존 방법들은 정상과 비정상 소견의 중요한 차이를 종종 간과해서 성능이 떨어지는 경우가 많아.

이번 연구에서는 FODA-PG라는 새로운 프레임워크를 제안해. 이건 세분화된 장기-질병 적응형 분할 그래프 구조로, 도메인 적응 학습을 통해 이런 한계를 해결해. FODA-PG는 방사선 소견을 세분화된 그래픽 표현으로 구성하는데, 질병 관련 속성들을 임상적 중요성과 위치에 따라 "질병 특정"과 "질병 비특정" 카테고리로 나눠.

이런 적응형 분할 덕분에 모델이 정상 상태와 병리 상태의 미세한 차이를 잘 포착할 수 있어서 데이터 편향의 영향을 줄일 수 있어. 강력한 트랜스포머 기반 구조에 이 세분화된 의미 지식을 통합하고 그 효과에 대한 엄격한 수학적 근거를 제공함으로써, FODA-PG는 정확하고 임상적으로 일관된 보고서를 생성하고 일반화 능력을 높였어.

IU-Xray와 MIMIC-CXR 벤치마크에서의 광범위한 실험 결과, 우리의 방법이 최신 기술보다 우수하다는 걸 보여주었고, 의료 보고서 생성에서 도메인 적응의 중요성을 강조했어.

================================================================================

URL: https://arxiv.org/abs/2409.03961
Title: Generating Faithful and Salient Text from Multimodal Data

Original Abstract:
While large multimodal models (LMMs) have obtained strong performance on many multimodal tasks, they may still hallucinate while generating text. Their performance on detecting salient features from visual data is also unclear. In this paper, we develop a framework to generate faithful and salient text from mixed-modal data, which includes images and structured data ( represented in knowledge graphs or tables). Specifically, we train a small vision critic model to identify hallucinated and non-salient features from the image modality. The critic model also generates a list of salient image features. This information is used in the post editing step to improve the generation quality. Experiments on two datasets show that our framework improves LMMs' generation quality on both faithfulness and saliency, outperforming recent techniques aimed at reducing hallucination.

Translated Abstract:
대형 다중 모달 모델(LMMs)은 여러 다중 모달 작업에서 좋은 성능을 보였지만, 텍스트를 생성할 때 여전히 환각을 일으킬 수 있어. 그리고 시각 데이터에서 중요한 특징을 감지하는 성능도 불확실해.

이 논문에서는 이미지와 구조화된 데이터(지식 그래프나 표로 표현된)를 포함한 혼합 모달 데이터에서 신뢰할 수 있고 중요한 텍스트를 생성하는 프레임워크를 개발했어. 구체적으로, 우리는 작은 비전 비평가 모델을 훈련시켜 이미지 모달리티에서 환각된 특징과 비중요 특징을 구별하도록 했어. 이 비평가 모델은 또한 중요한 이미지 특징 목록도 생성해.

이 정보는 생성 품질을 개선하기 위한 후처리 단계에서 사용돼. 두 개의 데이터셋에서 실험해본 결과, 우리 프레임워크는 LMMs의 생성 품질을 신뢰성과 중요성 모두에서 향상시켰고, 환각을 줄이기 위한 최근 기술들을 능가했어.

================================================================================

URL: https://arxiv.org/abs/2409.03982
Title: Boundary feature fusion network for tooth image segmentation

Original Abstract:
Tooth segmentation is a critical technology in the field of medical image segmentation, with applications ranging from orthodontic treatment to human body identification and dental pathology assessment. Despite the development of numerous tooth image segmentation models by researchers, a common shortcoming is the failure to account for the challenges of blurred tooth boundaries. Dental diagnostics require precise delineation of tooth boundaries. This paper introduces an innovative tooth segmentation network that integrates boundary information to address the issue of indistinct boundaries between teeth and adjacent tissues. This network's core is its boundary feature extraction module, which is designed to extract detailed boundary information from high-level features. Concurrently, the feature cross-fusion module merges detailed boundary and global semantic information in a synergistic way, allowing for stepwise layer transfer of feature information. This method results in precise tooth segmentation. In the most recent STS Data Challenge, our methodology was rigorously tested and received a commendable overall score of 0.91. When compared to other existing approaches, this score demonstrates our method's significant superiority in segmenting tooth boundaries.

Translated Abstract:
치아 분할은 의료 이미지 분할 분야에서 중요한 기술로, 교정 치료, 인체 식별, 치과 병리 평가 등 다양한 분야에 활용돼. 많은 연구자들이 여러 치아 이미지 분할 모델을 개발했지만, 대부분의 모델이 흐릿한 치아 경계를 제대로 처리하지 못하는 문제가 있어. 치과 진단에서는 치아 경계를 정확하게 구분하는 게 필요해.

이 논문에서는 경계 정보를 통합한 혁신적인 치아 분할 네트워크를 소개해. 이 네트워크는 치아와 인접한 조직 사이의 불분명한 경계 문제를 해결하는 데 초점을 맞추고 있어. 이 네트워크의 핵심은 경계 특징 추출 모듈인데, 이 모듈은 고수준 특징에서 세부적인 경계 정보를 추출하도록 설계됐어.

동시에, 특징 교차 융합 모듈은 세부 경계 정보와 전반적인 의미 정보를 협력적으로 합쳐서, 단계별로 특징 정보를 전달할 수 있게 해. 이 방법 덕분에 치아 분할의 정확성이 향상돼. 최근 STS 데이터 챌린지에서 우리의 방법론이 엄격한 테스트를 거쳤고, 전체 점수 0.91을 기록했어. 다른 기존 방법들과 비교했을 때, 이 점수는 우리 방법이 치아 경계를 분할하는 데 있어 상당한 우수성을 보여준다는 걸 의미해.

================================================================================

URL: https://arxiv.org/abs/2409.04003
Title: DreamForge: Motion-Aware Autoregressive Video Generation for Multi-View Driving Scenes

Original Abstract:
Recent advances in diffusion models have significantly enhanced the cotrollable generation of streetscapes for and facilitated downstream perception and planning tasks. However, challenges such as maintaining temporal coherence, generating long videos, and accurately modeling driving scenes persist. Accordingly, we propose DreamForge, an advanced diffusion-based autoregressive video generation model designed for the long-term generation of 3D-controllable and extensible video. In terms of controllability, our DreamForge supports flexible conditions such as text descriptions, camera poses, 3D bounding boxes, and road layouts, while also providing perspective guidance to produce driving scenes that are both geometrically and contextually accurate. For consistency, we ensure inter-view consistency through cross-view attention and temporal coherence via an autoregressive architecture enhanced with motion cues. Codes will be available at this https URL.

Translated Abstract:
최근의 확산 모델 발전 덕분에 거리 풍경을 조절 가능하게 생성하고, 이후의 인식 및 계획 작업을 쉽게 할 수 있게 되었어. 하지만 시간 일관성 유지, 긴 비디오 생성, 그리고 운전 장면의 정확한 모델링 같은 문제는 여전히 남아있어. 그래서 우리는 DreamForge라는 새로운 모델을 제안해. 이건 고급 확산 기반 자회귀 비디오 생성 모델로, 3D 조절 가능하고 확장 가능한 비디오를 장기적으로 생성할 수 있도록 설계됐어.

조절 가능성 면에서 DreamForge는 텍스트 설명, 카메라 자세, 3D 경계 상자, 도로 레이아웃 같은 유연한 조건을 지원해. 또한, 기하학적으로나 맥락적으로 정확한 운전 장면을 만들기 위해 원근감 안내도 제공해. 일관성을 위해서는 교차 시점 주의를 통해 시점 간 일관성을 보장하고, 움직임 신호로 강화된 자회귀 아키텍처를 통해 시간 일관성을 유지해. 코드는 이 https URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04004
Title: One-Shot Diffusion Mimicker for Handwritten Text Generation

Original Abstract:
Existing handwritten text generation methods often require more than ten handwriting samples as style references. However, in practical applications, users tend to prefer a handwriting generation model that operates with just a single reference sample for its convenience and efficiency. This approach, known as "one-shot generation", significantly simplifies the process but poses a significant challenge due to the difficulty of accurately capturing a writer's style from a single sample, especially when extracting fine details from the characters' edges amidst sparse foreground and undesired background noise. To address this problem, we propose a One-shot Diffusion Mimicker (One-DM) to generate handwritten text that can mimic any calligraphic style with only one reference sample. Inspired by the fact that high-frequency information of the individual sample often contains distinct style patterns (e.g., character slant and letter joining), we develop a novel style-enhanced module to improve the style extraction by incorporating high-frequency components from a single sample. We then fuse the style features with the text content as a merged condition for guiding the diffusion model to produce high-quality handwritten text images. Extensive experiments demonstrate that our method can successfully generate handwriting scripts with just one sample reference in multiple languages, even outperforming previous methods using over ten samples. Our source code is available at this https URL.

Translated Abstract:
기존의 손글씨 생성 방법은 스타일 참고 자료로 열 개 이상의 손글씨 샘플이 필요해. 하지만 실제로는 사용자들이 편리하고 효율적인 이유로 단 하나의 샘플로 작동하는 손글씨 생성 모델을 선호해. 이런 접근 방식을 "원샷 생성"이라고 하는데, 이 방식은 과정을 훨씬 간단하게 만들어주지만, 단 하나의 샘플로 작가의 스타일을 정확하게 잡아내는 건 큰 도전이야. 특히, 글자 가장자리의 세밀한 디테일을 희박한 전경과 원하지 않는 배경 소음 속에서 추출하는 게 어려워.

이 문제를 해결하기 위해 우리는 One-shot Diffusion Mimicker (One-DM)을 제안해. 이 모델은 단 하나의 참고 샘플로 어떤 서예 스타일도 모방할 수 있는 손글씨 텍스트를 생성할 수 있어. 개별 샘플의 고주파 정보가 독특한 스타일 패턴(예: 글자의 기울기와 글자 연결)을 포함하고 있다는 사실에서 영감을 받아, 우리는 단일 샘플의 고주파 성분을 활용해 스타일 추출을 개선하는 새로운 스타일 강화 모듈을 개발했어. 그런 다음, 이 스타일 특징을 텍스트 내용과 합쳐서 확산 모델이 고품질 손글씨 텍스트 이미지를 생성하도록 안내하는 조건으로 사용해.

다양한 실험 결과, 우리의 방법이 여러 언어에서 단 하나의 샘플 참고만으로 손글씨를 성공적으로 생성할 수 있음을 보여줬어. 심지어 열 개 이상의 샘플을 사용하는 기존 방법보다 더 나은 성능을 보였어. 우리의 소스 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04005
Title: Qihoo-T2X: An Efficiency-Focused Diffusion Transformer via Proxy Tokens for Text-to-Any-Task

Original Abstract:
The global self-attention mechanism in diffusion transformers involves redundant computation due to the sparse and redundant nature of visual information, and the attention map of tokens within a spatial window shows significant similarity. To address this redundancy, we propose the Proxy Token Diffusion Transformer (PT-DiT), which employs sparse representative token attention (where the number of representative tokens is much smaller than the total number of tokens) to model global visual information efficiently. Specifically, in each transformer block, we randomly sample one token from each spatial-temporal window to serve as a proxy token for that region. The global semantics are captured through the self-attention of these proxy tokens and then injected into all latent tokens via cross-attention. Simultaneously, we introduce window and shift window attention to address the limitations in detail modeling caused by the sparse attention mechanism. Building on the well-designed PT-DiT, we further develop the Qihoo-T2X family, which includes a variety of models for T2I, T2V, and T2MV tasks. Experimental results show that PT-DiT achieves competitive performance while reducing the computational complexity in both image and video generation tasks (e.g., a 48% reduction compared to DiT and a 35% reduction compared to Pixart-alpha). Our source code is available at this https URL.

Translated Abstract:
확산 변환기에서 글로벌 셀프 어텐션 메커니즘은 시각 정보가 희소하고 중복된 특성 때문에 불필요한 계산이 발생해. 그리고 공간 창 내의 토큰들의 어텐션 맵은 상당한 유사성을 보여. 이런 중복 문제를 해결하기 위해 우리는 Proxy Token Diffusion Transformer (PT-DiT)를 제안해. 이건 희소 대표 토큰 어텐션을 사용해서 전체 토큰 수보다 훨씬 적은 수의 대표 토큰으로 글로벌 시각 정보를 효율적으로 모델링해.

구체적으로, 각 변환기 블록에서 우리는 각 공간-시간 창에서 하나의 토큰을 무작위로 샘플링해서 그 지역의 프록시 토큰으로 사용해. 이러한 프록시 토큰의 셀프 어텐션을 통해 글로벌 의미를 포착하고, 이 정보를 모든 잠재 토큰에 크로스 어텐션으로 주입해. 동시에, 우리는 희소 어텐션 메커니즘으로 인한 자세한 모델링 한계를 해결하기 위해 윈도우와 시프트 윈도우 어텐션을 도입해.

PT-DiT를 기반으로 우리는 T2I, T2V, T2MV 작업을 위한 다양한 모델이 포함된 Qihoo-T2X 패밀리도 개발했어. 실험 결과 PT-DiT는 이미지와 비디오 생성 작업에서 계산 복잡성을 줄이면서도 경쟁력 있는 성능을 보여줬어. 예를 들어, DiT에 비해 48% 감소하고, Pixart-alpha에 비해 35% 감소했어. 우리의 소스 코드는 이 https URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04011
Title: Hybrid Mask Generation for Infrared Small Target Detection with Single-Point Supervision

Original Abstract:
Single-frame infrared small target (SIRST) detection poses a significant challenge due to the requirement to discern minute targets amidst complex infrared background clutter. Recently, deep learning approaches have shown promising results in this domain. However, these methods heavily rely on extensive manual annotations, which are particularly cumbersome and resource-intensive for infrared small targets owing to their minute sizes. To address this limitation, we introduce a Hybrid Mask Generation (HMG) approach that recovers high-quality masks for each target from only a single-point label for network training. Specifically, our HMG approach consists of a handcrafted Points-to-Mask Generation strategy coupled with a pseudo mask updating strategy to recover and refine pseudo masks from point labels. The Points-to-Mask Generation strategy divides two distinct stages: Points-to-Box conversion, where individual point labels are transformed into bounding boxes, and subsequently, Box-to-Mask prediction, where these bounding boxes are elaborated into precise masks. The mask updating strategy integrates the complementary strengths of handcrafted and deep-learning algorithms to iteratively refine the initial pseudo masks. Experimental results across three datasets demonstrate that our method outperforms the existing methods for infrared small target detection with single-point supervision.

Translated Abstract:
단일 프레임 적외선 소형 목표(SIRST) 탐지는 복잡한 적외선 배경 속에서 작은 목표를 구별해야 해서 큰 도전 과제가 있어. 최근에는 딥러닝 접근 방식이 이 분야에서 좋은 성과를 보여주고 있어. 하지만 이런 방법들은 많은 수동 주석에 의존하는데, 소형 적외선 목표는 너무 작아서 주석 작업이 특히 힘들고 자원도 많이 들어.

이런 문제를 해결하기 위해 우리는 하이브리드 마스크 생성(HMG) 접근 방식을 도입했어. 이 방법은 네트워크 훈련을 위해 단일 포인트 라벨만으로 각 목표에 대한 고품질 마스크를 복원할 수 있어. HMG는 수공예로 만든 포인트-투-마스크 생성 전략과 가짜 마스크 업데이트 전략으로 구성되어 있어서 포인트 라벨에서 가짜 마스크를 복원하고 다듬는 과정을 거쳐.

포인트-투-마스크 생성 전략은 두 가지 단계로 나뉘어: 첫 번째는 포인트-투-박스 변환으로, 여기서는 개별 포인트 라벨을 바운딩 박스로 변환해. 그 다음에는 박스-투-마스크 예측 단계가 있어서 이 바운딩 박스를 기반으로 정확한 마스크를 만들어. 마스크 업데이트 전략은 수공예와 딥러닝 알고리즘의 장점을 결합해서 초기 가짜 마스크를 점진적으로 다듬어.

세 가지 데이터셋에서 실험 결과, 우리의 방법이 단일 포인트 감독을 사용한 기존 방법들보다 적외선 소형 목표 탐지에서 더 뛰어난 성과를 보였어.

================================================================================

URL: https://arxiv.org/abs/2409.04013
Title: 3D-GP-LMVIC: Learning-based Multi-View Image Coding with 3D Gaussian Geometric Priors

Original Abstract:
Multi-view image compression is vital for 3D-related applications. To effectively model correlations between views, existing methods typically predict disparity between two views on a 2D plane, which works well for small disparities, such as in stereo images, but struggles with larger disparities caused by significant view changes. To address this, we propose a novel approach: learning-based multi-view image coding with 3D Gaussian geometric priors (3D-GP-LMVIC). Our method leverages 3D Gaussian Splatting to derive geometric priors of the 3D scene, enabling more accurate disparity estimation across views within the compression model. Additionally, we introduce a depth map compression model to reduce redundancy in geometric information between views. A multi-view sequence ordering method is also proposed to enhance correlations between adjacent views. Experimental results demonstrate that 3D-GP-LMVIC surpasses both traditional and learning-based methods in performance, while maintaining fast encoding and decoding speed.

Translated Abstract:
다중 뷰 이미지 압축은 3D 관련 애플리케이션에 정말 중요해. 기존 방법들은 두 개의 뷰 간의 차이를 2D 평면에서 예측하는 방식인데, 이건 스테레오 이미지처럼 작은 차이에선 잘 작동해. 하지만 큰 차이, 즉 뷰가 많이 바뀌었을 때는 잘 안 돼.

그래서 우리는 새로운 방법을 제안해: 3D 가우시안 기하학적 사전(3D-GP)을 활용한 학습 기반 다중 뷰 이미지 코딩(3D-GP-LMVIC)이야. 우리 방법은 3D 가우시안 스플래팅을 사용해서 3D 장면의 기하학적 사전을 유도해. 이걸 통해 압축 모델 내에서 뷰 간의 차이를 더 정확하게 추정할 수 있어.

또한, 뷰 간의 기하학적 정보 중복을 줄이기 위해 깊이 맵 압축 모델도 도입했어. 인접한 뷰들 간의 상관관계를 높이기 위한 다중 뷰 시퀀스 정렬 방법도 제안했어.

실험 결과, 3D-GP-LMVIC가 전통적인 방법과 학습 기반 방법 모두보다 성능이 뛰어난 걸 보여줬고, 빠른 인코딩과 디코딩 속도도 유지했어.

================================================================================

URL: https://arxiv.org/abs/2409.04018
Title: Towards Energy-Efficiency by Navigating the Trilemma of Energy, Latency, and Accuracy

Original Abstract:
Extended Reality (XR) enables immersive experiences through untethered headsets but suffers from stringent battery and resource constraints. Energy-efficient design is crucial to ensure both longevity and high performance in XR devices. However, latency and accuracy are often prioritized over energy, leading to a gap in achieving energy efficiency. This paper examines scene reconstruction, a key building block for immersive XR experiences, and demonstrates how energy efficiency can be achieved by navigating the trilemma of energy, latency, and accuracy.
We explore three classes of energy-oriented optimizations, covering the algorithm, execution, and data, that reveal a broad design space through configurable parameters. Our resulting 72 designs expose a wide range of latency and energy trade-offs, with a smaller range of accuracy loss. We identify a Pareto-optimal curve and show that the designs on the curve are achievable only through synergistic co-optimization of all three optimization classes and by considering the latency and accuracy needs of downstream scene reconstruction consumers. Our analysis covering various use cases and measurements on an embedded class system shows that, relative to the baseline, our designs offer energy benefits of up to 60X with potential latency range of 4X slowdown to 2X speedup. Detailed exploration of a use case across representative data sequences from ScanNet showed about 25X energy savings with 1.5X latency reduction and negligible reconstruction quality loss.

Translated Abstract:
확장 현실(XR)은 무선 헤드셋을 통해 몰입감 있는 경험을 제공하지만, 배터리와 자원에 제한이 많아 어려움을 겪고 있어. 에너지 효율적인 설계가 XR 기기의 수명과 성능을 높이는 데 꼭 필요해. 하지만, 보통 에너지보다는 지연 시간과 정확도를 더 중요하게 생각하다 보니 에너지 효율을 달성하기가 힘들어. 

이 논문에서는 XR 경험을 위한 중요한 요소인 장면 재구성을 살펴보고, 에너지, 지연 시간, 정확도의 세 가지 문제를 잘 조율함으로써 에너지 효율성을 어떻게 달성할 수 있는지 보여줘. 

우리는 알고리즘, 실행, 데이터 세 가지 에너지 중심 최적화 방법을 탐구했어. 이 방법들은 조정 가능한 매개변수를 통해 폭넓은 설계 공간을 드러내. 우리 연구 결과로 나온 72개의 설계는 다양한 지연 시간과 에너지 간의 트레이드오프를 보여주는데, 정확도 손실은 비교적 적은 편이야. 우리는 파레토 최적 곡선을 찾아냈고, 이 곡선상의 설계는 모든 세 가지 최적화 방법을 함께 고려해야만 달성 가능하다는 것을 보여줬어. 또한, 장면 재구성을 사용하는 소비자들의 지연 시간과 정확도 요구를 고려해야 해.

다양한 사용 사례와 임베디드 시스템에서 측정한 결과를 분석한 결과, 우리의 설계가 기준 대비 최대 60배의 에너지 이점을 제공하고, 지연 시간은 4배 느려지거나 2배 빨라질 수 있음을 보여줬어. ScanNet의 대표적인 데이터 시퀀스를 통한 구체적인 사용 사례 분석에서는 약 25배의 에너지 절약과 1.5배의 지연 시간 감소가 있었고, 재구성 품질 손실은 거의 없었어.

================================================================================

URL: https://arxiv.org/abs/2409.04025
Title: BFA-YOLO: Balanced multiscale object detection network for multi-view building facade attachments detection

Original Abstract:
Detection of building facade attachments such as doors, windows, balconies, air conditioner units, billboards, and glass curtain walls plays a pivotal role in numerous applications. Building facade attachments detection aids in vbuilding information modeling (BIM) construction and meeting Level of Detail 3 (LOD3) standards. Yet, it faces challenges like uneven object distribution, small object detection difficulty, and background interference. To counter these, we propose BFA-YOLO, a model for detecting facade attachments in multi-view images. BFA-YOLO incorporates three novel innovations: the Feature Balanced Spindle Module (FBSM) for addressing uneven distribution, the Target Dynamic Alignment Task Detection Head (TDATH) aimed at improving small object detection, and the Position Memory Enhanced Self-Attention Mechanism (PMESA) to combat background interference, with each component specifically designed to solve its corresponding challenge. Detection efficacy of deep network models deeply depends on the dataset's characteristics. Existing open source datasets related to building facades are limited by their single perspective, small image pool, and incomplete category coverage. We propose a novel method for building facade attachments detection dataset construction and construct the BFA-3D dataset for facade attachments detection. The BFA-3D dataset features multi-view, accurate labels, diverse categories, and detailed classification. BFA-YOLO surpasses YOLOv8 by 1.8% and 2.9% in mAP@0.5 on the multi-view BFA-3D and street-view Facade-WHU datasets, respectively. These results underscore BFA-YOLO's superior performance in detecting facade attachments.

Translated Abstract:
건물 외관 부착물, 예를 들어 문, 창문, 발코니, 에어컨 유닛, 광고판, 유리 커튼 월 등을 감지하는 것은 여러 응용 프로그램에서 아주 중요한 역할을 해. 이런 부착물 감지는 건축 정보 모델링(BIM)과 레벨 오브 디테일 3(LOD3) 기준을 충족하는 데 도움을 줘. 하지만, 고르지 않은 객체 분포, 작은 객체 감지의 어려움, 배경 간섭 같은 문제들이 있어.

이런 문제들을 해결하기 위해 우리는 BFA-YOLO라는 모델을 제안해. 이 모델은 다중 시점 이미지에서 외관 부착물을 감지하도록 설계됐어. BFA-YOLO는 세 가지 새로운 혁신을 포함하고 있어: 불균형한 분포를 해결하기 위한 특징 균형 스핀들 모듈(FBSM), 작은 객체 감지를 개선하기 위한 타겟 동적 정렬 작업 탐지 헤드(TDATH), 그리고 배경 간섭을 극복하기 위한 위치 메모리 강화 자기 주의 메커니즘(PMESA)이야. 각 구성 요소는 특정 문제를 해결하기 위해 설계됐어.

딥 네트워크 모델의 감지 효율성은 데이터셋의 특성에 크게 의존해. 기존의 건물 외관 관련 공개 데이터셋은 단일 관점, 작은 이미지 풀, 불완전한 카테고리 커버리지 때문에 한계가 있어. 그래서 우리는 건물 외관 부착물 감지를 위한 새로운 데이터셋 구축 방법을 제안하고 BFA-3D 데이터셋을 만들어냈어. BFA-3D 데이터셋은 다중 시점, 정확한 레이블, 다양한 카테고리, 그리고 자세한 분류를 특징으로 해.

BFA-YOLO는 다중 시점 BFA-3D와 스트리트 뷰 Facade-WHU 데이터셋에서 각각 1.8%와 2.9% 더 높은 mAP@0.5를 기록하며 YOLOv8을 초월했어. 이 결과들은 BFA-YOLO가 외관 부착물 감지에서 뛰어난 성능을 보여준다는 것을 강조해.

================================================================================

URL: https://arxiv.org/abs/2409.04033
Title: Dense Hand-Object(HO) GraspNet with Full Grasping Taxonomy and Dynamics

Original Abstract:
Existing datasets for 3D hand-object interaction are limited either in the data cardinality, data variations in interaction scenarios, or the quality of annotations. In this work, we present a comprehensive new training dataset for hand-object interaction called HOGraspNet. It is the only real dataset that captures full grasp taxonomies, providing grasp annotation and wide intraclass variations. Using grasp taxonomies as atomic actions, their space and time combinatorial can represent complex hand activities around objects. We select 22 rigid objects from the YCB dataset and 8 other compound objects using shape and size taxonomies, ensuring coverage of all hand grasp configurations. The dataset includes diverse hand shapes from 99 participants aged 10 to 74, continuous video frames, and a 1.5M RGB-Depth of sparse frames with annotations. It offers labels for 3D hand and object meshes, 3D keypoints, contact maps, and \emph{grasp labels}. Accurate hand and object 3D meshes are obtained by fitting the hand parametric model (MANO) and the hand implicit function (HALO) to multi-view RGBD frames, with the MoCap system only for objects. Note that HALO fitting does not require any parameter tuning, enabling scalability to the dataset's size with comparable accuracy to MANO. We evaluate HOGraspNet on relevant tasks: grasp classification and 3D hand pose estimation. The result shows performance variations based on grasp type and object class, indicating the potential importance of the interaction space captured by our dataset. The provided data aims at learning universal shape priors or foundation models for 3D hand-object interaction. Our dataset and code are available at this https URL.

Translated Abstract:
기존의 3D 손-객체 상호작용 데이터셋은 데이터 양이나 상호작용 시나리오의 다양성, 주석 품질이 부족한 경우가 많아. 이번 연구에서는 HOGraspNet이라는 새로운 손-객체 상호작용 훈련 데이터셋을 소개해. 이 데이터셋은 완전한 그립 분류를 담고 있는 유일한 실제 데이터셋으로, 그립 주석과 다양한 클래스 내 변화를 제공해.

그립 분류를 기본 동작으로 보고, 이들의 공간과 시간 조합을 통해 객체 주변의 복잡한 손 활동을 나타낼 수 있어. YCB 데이터셋에서 22개의 단단한 객체를 선택하고, 형태와 크기 분류를 사용해 8개의 복합 객체를 추가했어. 이로써 모든 손 그립 구성이 포함되도록 했지. 데이터셋에는 10세에서 74세까지 99명의 다양한 손 모양과 연속 비디오 프레임, 주석이 달린 1.5M RGB-Depth 희소 프레임이 포함돼 있어.

3D 손과 객체 메쉬, 3D 키포인트, 접촉 맵, 그리고 '그립 레이블' 같은 라벨을 제공해. 정확한 손과 객체의 3D 메쉬는 다중 뷰 RGBD 프레임에 손 매개변수 모델(MANO)과 손 암시 함수(HALO)를 맞추어 얻어지며, 객체에 대해서는 MoCap 시스템만 사용해. HALO 맞추기는 어떤 매개변수 조정도 필요하지 않아서 데이터셋의 크기에 비례해 확장 가능하고 MANO와 비슷한 정확도를 제공해.

HOGraspNet은 그립 분류와 3D 손 자세 추정 같은 관련 작업에서 평가했어. 결과는 그립 유형과 객체 클래스에 따라 성능 차이가 나타나서, 우리 데이터셋이 포착한 상호작용 공간이 중요할 수 있다는 가능성을 보여줘. 제공하는 데이터는 3D 손-객체 상호작용을 위한 보편적인 형태 사전이나 기초 모델 학습을 목표로 해. 우리의 데이터셋과 코드는 이 https URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04035
Title: MultiCounter: Multiple Action Agnostic Repetition Counting in Untrimmed Videos

Original Abstract:
Multi-instance Repetitive Action Counting (MRAC) aims to estimate the number of repetitive actions performed by multiple instances in untrimmed videos, commonly found in human-centric domains like sports and exercise. In this paper, we propose MultiCounter, a fully end-to-end deep learning framework that enables simultaneous detection, tracking, and counting of repetitive actions of multiple human instances. Specifically, MultiCounter incorporates two novel modules: 1) mixed spatiotemporal interaction for efficient context correlation across consecutive frames, and 2) task-specific heads for accurate perception of periodic boundaries and generalization for action-agnostic human instances. We train MultiCounter on a synthetic dataset called MultiRep generated from annotated real-world videos. Experiments on the MultiRep dataset validate the fundamental challenge of MRAC tasks and showcase the superiority of our proposed model. Compared to ByteTrack+RepNet, a solution that combines an advanced tracker with a single repetition counter, MultiCounter substantially improves Period-mAP by 41.0%, reduces AvgMAE by 58.6%, and increases AvgOBO 1.48 times. This sets a new benchmark in the field of MRAC. Moreover, MultiCounter runs in real-time on a commodity GPU server and is insensitive to the number of human instances in a video.

Translated Abstract:
다중 인스턴스 반복 행동 카운팅(MRAC)은 스포츠나 운동처럼 사람 중심의 도메인에서 자주 발견되는 잘라내지 않은 비디오에서 여러 인스턴스가 수행한 반복 행동의 수를 추정하는 것을 목표로 해. 이 논문에서는 MultiCounter라는 완전 엔드 투 엔드 딥러닝 프레임워크를 제안하는데, 이걸로 여러 사람 인스턴스의 반복 행동을 동시에 감지하고 추적하며 세는 게 가능해.

특히, MultiCounter는 두 가지 새로운 모듈을 포함하고 있어: 1) 연속적인 프레임 간의 효율적인 맥락 상관관계를 위한 혼합 시공간 상호작용, 2) 정기적인 경계 인식을 위한 작업 특화 헤드와 행동에 무관한 사람 인스턴스에 대한 일반화를 위한 헤드. 우리는 실제로 주석이 달린 비디오로부터 생성된 합성 데이터셋인 MultiRep에서 MultiCounter를 훈련했어.

MultiRep 데이터셋에서의 실험은 MRAC 작업의 근본적인 도전 과제를 검증하고, 우리가 제안한 모델의 우수성을 보여줘. ByteTrack+RepNet이라는 고급 추적기와 단일 반복 카운터를 결합한 솔루션과 비교했을 때, MultiCounter는 Period-mAP를 41.0% 개선하고, AvgMAE를 58.6% 줄이며, AvgOBO를 1.48배 증가시켰어. 이로 인해 MRAC 분야에서 새로운 기준을 세웠지. 게다가, MultiCounter는 일반 GPU 서버에서 실시간으로 작동하고, 비디오의 사람 인스턴스 수에 대해 민감하지 않아.

================================================================================

URL: https://arxiv.org/abs/2409.04038
Title: PlantSeg: A Large-Scale In-the-wild Dataset for Plant Disease Segmentation

Original Abstract:
Plant diseases pose significant threats to agriculture. It necessitates proper diagnosis and effective treatment to safeguard crop yields. To automate the diagnosis process, image segmentation is usually adopted for precisely identifying diseased regions, thereby advancing precision agriculture. Developing robust image segmentation models for plant diseases demands high-quality annotations across numerous images. However, existing plant disease datasets typically lack segmentation labels and are often confined to controlled laboratory settings, which do not adequately reflect the complexity of natural environments. Motivated by this fact, we established PlantSeg, a large-scale segmentation dataset for plant diseases. PlantSeg distinguishes itself from existing datasets in three key aspects. (1) Annotation type: Unlike the majority of existing datasets that only contain class labels or bounding boxes, each image in PlantSeg includes detailed and high-quality segmentation masks, associated with plant types and disease names. (2) Image source: Unlike typical datasets that contain images from laboratory settings, PlantSeg primarily comprises in-the-wild plant disease images. This choice enhances the practical applicability, as the trained models can be applied for integrated disease management. (3) Scale: PlantSeg is extensive, featuring 11,400 images with disease segmentation masks and an additional 8,000 healthy plant images categorized by plant type. Extensive technical experiments validate the high quality of PlantSeg's annotations. This dataset not only allows researchers to evaluate their image classification methods but also provides a critical foundation for developing and benchmarking advanced plant disease segmentation algorithms.

Translated Abstract:
식물 질병은 농업에 큰 위협이 돼. 이를 위해서는 적절한 진단과 효과적인 치료가 필요해. 진단 과정을 자동화하기 위해 보통 이미지 분할 기술을 사용해서 병든 부분을 정확히 찾아내고, 이를 통해 정밀 농업을 발전시키고 있어.

식물 질병에 대한 강력한 이미지 분할 모델을 개발하려면 많은 이미지에 대한 고품질 주석이 필요해. 하지만 기존의 식물 질병 데이터셋은 보통 분할 라벨이 부족하고, 실험실 환경에서 찍은 이미지에만 국한되어 있어서 자연 환경의 복잡성을 제대로 반영하지 못해. 이런 문제를 해결하기 위해 우리는 PlantSeg라는 대규모 분할 데이터셋을 만들었어.

PlantSeg는 기존 데이터셋과 세 가지 주요 특징에서 차별화돼. 

첫째, 주석 유형: 대부분의 데이터셋은 클래스 라벨이나 바운딩 박스만 포함하는 반면, PlantSeg의 각 이미지는 식물 종류와 질병 이름과 관련된 상세하고 고품질의 분할 마스크를 포함하고 있어.

둘째, 이미지 출처: 일반적인 데이터셋이 실험실 환경의 이미지를 포함하는 것과 달리, PlantSeg는 주로 자연에서 촬영된 식물 질병 이미지를 포함해. 이런 선택은 실제 적용 가능성을 높여줘. 훈련된 모델은 통합 질병 관리에 사용될 수 있어.

셋째, 규모: PlantSeg는 11,400개의 질병 분할 마스크가 있는 이미지를 포함하고, 추가로 식물 종류별로 분류된 8,000개의 건강한 식물 이미지를 가지고 있어. 많은 기술 실험을 통해 PlantSeg의 주석 품질이 높다는 걸 검증했어. 이 데이터셋은 연구자들이 이미지 분류 방법을 평가하는 데 도움을 줄 뿐만 아니라, 고급 식물 질병 분할 알고리즘을 개발하고 벤치마킹하는 중요한 기초 자료를 제공해.

================================================================================

URL: https://arxiv.org/abs/2409.04041
Title: On Evaluation of Vision Datasets and Models using Human Competency Frameworks

Original Abstract:
Evaluating models and datasets in computer vision remains a challenging task, with most leaderboards relying solely on accuracy. While accuracy is a popular metric for model evaluation, it provides only a coarse assessment by considering a single model's score on all dataset items. This paper explores Item Response Theory (IRT), a framework that infers interpretable latent parameters for an ensemble of models and each dataset item, enabling richer evaluation and analysis beyond the single accuracy number. Leveraging IRT, we assess model calibration, select informative data subsets, and demonstrate the usefulness of its latent parameters for analyzing and comparing models and datasets in computer vision.

Translated Abstract:
컴퓨터 비전에서 모델과 데이터셋을 평가하는 것은 여전히 어려운 과제야. 대부분의 평가 순위는 오직 정확도에만 의존하고 있어. 정확도는 모델 평가에서 흔히 사용하는 지표지만, 모든 데이터셋 항목에 대한 단일 모델의 점수만 고려해서 대충 평가하는 것에 불과해.

이 논문은 아이템 반응 이론(Item Response Theory, IRT)을 다루고 있어. 이 이론은 여러 모델과 각 데이터셋 항목에 대해 해석 가능한 숨겨진 매개변수를 추론할 수 있게 해줘. 이를 통해 단순한 정확도 수치를 넘어 더 풍부한 평가와 분석이 가능해.

IRT를 활용해서 우리는 모델의 보정 상태를 평가하고, 유용한 데이터 하위 집합을 선택해. 또한, 컴퓨터 비전에서 모델과 데이터셋을 분석하고 비교하는 데 있어 숨겨진 매개변수가 얼마나 유용한지 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.04053
Title: COLUMBUS: Evaluating COgnitive Lateral Understanding through Multiple-choice reBUSes

Original Abstract:
While visual question-answering (VQA) benchmarks have catalyzed the development of reasoning techniques, they have focused on vertical thinking. Effective problem-solving also necessitates lateral thinking, which remains understudied in AI and has not been used to test visual perception systems. To bridge this gap, we formulate visual lateral thinking as a multiple-choice question-answering task and describe a three-step taxonomy-driven methodology for instantiating task examples. Then, we develop COLUMBUS, a synthetic benchmark that applies the task pipeline to create QA sets with text and icon rebus puzzles based on publicly available collections of compounds and common phrases. COLUMBUS comprises over 1,000 puzzles, each with four answer candidates. While the SotA vision-language models (VLMs) achieve decent performance, our evaluation demonstrates a substantial gap between humans and models. VLMs benefit from human-curated descriptions but struggle to self-generate such representations at the right level of abstraction.

Translated Abstract:
시각 질문-응답(VQA) 벤치마크는 추론 기술 발전에 큰 도움이 됐지만, 주로 수직적 사고에만 집중했어. 하지만 효과적인 문제 해결을 위해서는 수평적 사고도 필요해. 이 부분은 AI에서 연구가 부족하고 시각 인식 시스템을 테스트하는 데도 사용되지 않았어. 

그래서 우리는 시각 수평적 사고를 다중 선택 질문-응답 과제로 정의하고, 이 과제를 만들기 위한 세 단계의 방법론을 설명해. 그 다음에는 COLUMBUS라는 합성 벤치마크를 개발했어. 이건 공공 데이터에서 수집한 화합물과 일반 구문을 바탕으로 텍스트와 아이콘 리버스 퍼즐을 포함한 QA 세트를 만드는 작업 프로세스를 적용해. COLUMBUS는 1,000개 이상의 퍼즐로 구성되어 있고, 각 퍼즐마다 네 개의 답안 후보가 있어.

현재의 최첨단 비전-언어 모델(VLMs)은 괜찮은 성능을 보이지만, 우리의 평가 결과 인간과 모델 간에 큰 차이가 있다는 걸 보여줘. VLMs는 인간이 만든 설명의 도움을 받지만, 자신이 필요한 수준의 추상화로 그런 표현을 스스로 생성하는 데 어려움을 겪어.

================================================================================

URL: https://arxiv.org/abs/2409.04060
Title: D4: Text-guided diffusion model-based domain adaptive data augmentation for vineyard shoot detection

Original Abstract:
In an agricultural field, plant phenotyping using object detection models is gaining attention. However, collecting the training data necessary to create generic and high-precision models is extremely challenging due to the difficulty of annotation and the diversity of domains. Furthermore, it is difficult to transfer training data across different crops, and although machine learning models effective for specific environments, conditions, or crops have been developed, they cannot be widely applied in actual fields. In this study, we propose a generative data augmentation method (D4) for vineyard shoot detection. D4 uses a pre-trained text-guided diffusion model based on a large number of original images culled from video data collected by unmanned ground vehicles or other means, and a small number of annotated datasets. The proposed method generates new annotated images with background information adapted to the target domain while retaining annotation information necessary for object detection. In addition, D4 overcomes the lack of training data in agriculture, including the difficulty of annotation and diversity of domains. We confirmed that this generative data augmentation method improved the mean average precision by up to 28.65% for the BBox detection task and the average precision by up to 13.73% for the keypoint detection task for vineyard shoot detection. Our generative data augmentation method D4 is expected to simultaneously solve the cost and domain diversity issues of training data generation in agriculture and improve the generalization performance of detection models.

Translated Abstract:
농업 분야에서 식물 표현형 분석을 위해 물체 탐지 모델을 사용하는 것이 주목받고 있어. 하지만 일반적이고 고정밀 모델을 만들기 위해 필요한 훈련 데이터를 수집하는 게 정말 어려워. 주석 작업이 힘들고 다양한 분야가 있다 보니 더더욱 그렇지. 게다가 서로 다른 작물 간에 훈련 데이터를 전이하는 것도 힘들고, 특정 환경이나 조건, 작물에 효과적인 머신러닝 모델이 개발되긴 했지만, 실제 현장에선 널리 적용하기 어려워.

이번 연구에서는 포도밭에서의 새싹 탐지를 위한 생성적 데이터 증강 방법(D4)을 제안해. D4는 무인 지상 차량이나 다른 방법으로 수집한 많은 원본 이미지와 소량의 주석 데이터셋을 바탕으로 한 사전 훈련된 텍스트 유도 확산 모델을 사용해. 이 방법은 대상 분야에 맞춰 배경 정보를 조정하면서도 물체 탐지에 필요한 주석 정보를 유지한 새로운 주석 이미지를 생성해.

또한, D4는 농업에서의 훈련 데이터 부족 문제, 즉 주석 작업의 어려움과 다양한 분야 문제를 해결해. 이 생성적 데이터 증강 방법이 포도밭 새싹 탐지 작업에서 BBox 탐지 과제의 평균 평균 정밀도를 최대 28.65% 향상시키고, 키포인트 탐지 과제에서 평균 정밀도를 최대 13.73% 개선했다는 걸 확인했어. 우리의 D4 방법은 농업에서 훈련 데이터 생성의 비용과 분야 다양성 문제를 동시에 해결하고 탐지 모델의 일반화 성능을 향상시킬 것으로 기대돼.

================================================================================

URL: https://arxiv.org/abs/2409.04068
Title: Site-Specific Color Features of Green Coffee Beans

Original Abstract:
Coffee is one of the most valuable primary commodities. Despite this, the common selection technique of green coffee beans relies on personnel visual inspection, which is labor-intensive and subjective. Therefore, an efficient way to evaluate the quality of beans is needed. In this paper, we demonstrate a site-independent approach to find site-specific color features of the seed coat in qualified green coffee beans. We then propose two evaluation schemes for green coffee beans based on this site-specific color feature of qualified beans. Due to the site-specific properties of these color features, machine learning classifiers indicate that compared with the existing evaluation schemes of beans, our evaluation schemes have the advantages of being simple, having less computational costs, and having universal applicability. Finally, this site-specific color feature can distinguish qualified beans from different growing sites. Moreover, this function can prevent cheating in the coffee business and is unique to our evaluation scheme of beans.

Translated Abstract:
커피는 가장 가치 있는 원자재 중 하나야. 그런데 일반적으로 생두를 고르는 방법은 사람이 직접 눈으로 검사하는 방식이어서, 노동력이 많이 들어가고 주관적이야. 그래서 커피 원두의 품질을 효율적으로 평가할 방법이 필요해.

이 논문에서는 자리가 달라도 사용할 수 있는 방법을 통해, 자격이 있는 생두의 씨껍질에서 특정한 색깔 특징을 찾는 방법을 보여줘. 그리고 이 색깔 특징을 바탕으로 생두를 평가하는 두 가지 방안을 제안해.

이 색깔 특징이 자리마다 다르기 때문에, 머신러닝 분류기를 사용해 보면 기존의 생두 평가 방식보다 우리 평가 방식이 더 간단하고, 계산 비용이 적으며, 보편적으로 적용할 수 있는 장점이 있어. 마지막으로, 이 자리 특정 색깔 특징 덕분에 다양한 재배 지역의 자격이 있는 생두를 구별할 수 있어. 게다가, 이 기능은 커피 사업에서 부정행위를 방지할 수 있고, 우리 생두 평가 방식만의 특징이야.

================================================================================

URL: https://arxiv.org/abs/2409.04082
Title: SDformerFlow: Spatiotemporal swin spikeformer for event-based optical flow estimation

Original Abstract:
Event cameras generate asynchronous and sparse event streams capturing changes in light intensity. They offer significant advantages over conventional frame-based cameras, such as a higher dynamic range and an extremely faster data rate, making them particularly useful in scenarios involving fast motion or challenging lighting conditions. Spiking neural networks (SNNs) share similar asynchronous and sparse characteristics and are well-suited for processing data from event cameras. Inspired by the potential of transformers and spike-driven transformers (spikeformers) in other computer vision tasks, we propose two solutions for fast and robust optical flow estimation for event cameras: STTFlowNet and SDformerFlow. STTFlowNet adopts a U-shaped artificial neural network (ANN) architecture with spatiotemporal shifted window self-attention (swin) transformer encoders, while SDformerFlow presents its fully spiking counterpart, incorporating swin spikeformer encoders. Furthermore, we present two variants of the spiking version with different neuron models. Our work is the first to make use of spikeformers for dense optical flow estimation. We conduct end-to-end training for all models using supervised learning. Our results yield state-of-the-art performance among SNN-based event optical flow methods on both the DSEC and MVSEC datasets, and show significant reduction in power consumption compared to the equivalent ANNs.

Translated Abstract:
이벤트 카메라는 빛의 강도 변화를 포착하는 비동기식이고 희박한 이벤트 스트림을 생성해. 이 카메라는 전통적인 프레임 기반 카메라보다 더 높은 동적 범위와 훨씬 빠른 데이터 전송 속도를 제공해서, 빠른 움직임이나 어려운 조명 조건에서 특히 유용해.

스파이킹 신경망(SNN)은 비슷한 비동기식 및 희박한 특징을 가지고 있어서 이벤트 카메라에서 나오는 데이터를 처리하는 데 잘 맞아. 우리는 다른 컴퓨터 비전 작업에서 트랜스포머와 스파이크 기반 트랜스포머(스파이크포머)의 가능성에 영감을 받아, 이벤트 카메라를 위한 빠르고 강력한 광학 흐름 추정을 위한 두 가지 솔루션인 STTFlowNet과 SDformerFlow를 제안해.

STTFlowNet은 공간-시간 이동 창 셀프 어텐션(스윈) 트랜스포머 인코더가 있는 U자형 인공 신경망(ANN) 구조를 채택하고, SDformerFlow는 스윈 스파이크포머 인코더를 포함하는 완전한 스파이킹 버전을 보여줘. 또한, 서로 다른 뉴런 모델을 가진 스파이킹 버전의 두 가지 변형도 제시해. 우리의 연구는 스파이크포머를 사용해 밀집 광학 흐름 추정을 한 첫 번째 사례야.

모든 모델은 감독 학습을 통해 엔드 투 엔드로 훈련했어. 우리의 결과는 DSEC와 MVSEC 데이터셋에서 SNN 기반 이벤트 광학 흐름 방법 중 최고의 성능을 보여주고, 동등한 ANN에 비해 전력 소비를 크게 줄였어.

================================================================================

URL: https://arxiv.org/abs/2409.04086
Title: Introducing a Class-Aware Metric for Monocular Depth Estimation: An Automotive Perspective

Original Abstract:
The increasing accuracy reports of metric monocular depth estimation models lead to a growing interest from the automotive domain. Current model evaluations do not provide deeper insights into the models' performance, also in relation to safety-critical or unseen classes. Within this paper, we present a novel approach for the evaluation of depth estimation models. Our proposed metric leverages three components, a class-wise component, an edge and corner image feature component, and a global consistency retaining component. Classes are further weighted on their distance in the scene and on criticality for automotive applications. In the evaluation, we present the benefits of our metric through comparison to classical metrics, class-wise analytics, and the retrieval of critical situations. The results show that our metric provides deeper insights into model results while fulfilling safety-critical requirements. We release the code and weights on the following repository: \href{this https URL}

Translated Abstract:
단일 카메라 깊이 추정 모델의 정확도가 높아지면서 자동차 분야에서 관심이 커지고 있어. 하지만 현재 모델 평가 방법은 모델 성능에 대한 깊은 통찰을 제공하지 못하고, 특히 안전과 관련된 클래스나 보지 못한 클래스에 대해서는 더더욱 그래. 

이 논문에서는 깊이 추정 모델을 평가하기 위한 새로운 접근 방식을 제안해. 우리가 제안한 지표는 세 가지 요소를 활용해: 클래스별 요소, 이미지의 엣지와 코너 특징 요소, 그리고 전반적인 일관성을 유지하는 요소야. 클래스는 장면에서의 거리와 자동차 응용에 대한 중요성에 따라 가중치가 부여돼. 

평가에서는 우리의 지표가 전통적인 지표와 비교할 때의 장점, 클래스별 분석, 그리고 중요한 상황을 찾아내는 데 도움이 된다는 걸 보여줘. 결과적으로 우리의 지표가 모델 결과에 대한 더 깊은 통찰을 제공하고, 안전과 관련된 요구사항을 충족시킨다는 걸 알 수 있어. 코드는 다음 레포지토리에서 확인할 수 있어: \href{this https URL}

================================================================================

URL: https://arxiv.org/abs/2409.04095
Title: UNIT: Unifying Image and Text Recognition in One Vision Encoder

Original Abstract:
Currently, vision encoder models like Vision Transformers (ViTs) typically excel at image recognition tasks but cannot simultaneously support text recognition like human visual recognition. To address this limitation, we propose UNIT, a novel training framework aimed at UNifying Image and Text recognition within a single model. Starting with a vision encoder pre-trained with image recognition tasks, UNIT introduces a lightweight language decoder for predicting text outputs and a lightweight vision decoder to prevent catastrophic forgetting of the original image encoding capabilities. The training process comprises two stages: intra-scale pretraining and inter-scale finetuning. During intra-scale pretraining, UNIT learns unified representations from multi-scale inputs, where images and documents are at their commonly used resolution, to enable fundamental recognition capability. In the inter-scale finetuning stage, the model introduces scale-exchanged data, featuring images and documents at resolutions different from the most commonly used ones, to enhance its scale robustness. Notably, UNIT retains the original vision encoder architecture, making it cost-free in terms of inference and deployment. Experiments across multiple benchmarks confirm that our method significantly outperforms existing methods on document-related tasks (e.g., OCR and DocQA) while maintaining the performances on natural images, demonstrating its ability to substantially enhance text recognition without compromising its core image recognition capabilities.

Translated Abstract:
현재 비전 인코더 모델인 비전 트랜스포머(ViTs)는 이미지 인식 작업에서는 잘 작동하지만, 인간처럼 텍스트 인식을 동시에 지원하지는 못해. 이 한계를 극복하기 위해, 우리는 UNIT라는 새로운 훈련 프레임워크를 제안해. 이 모델은 이미지와 텍스트 인식을 하나의 모델로 통합하려는 거야.

UNIT는 이미지 인식 작업으로 사전 훈련된 비전 인코더로 시작해. 여기서 텍스트 출력을 예측하기 위한 가벼운 언어 디코더와 원래 이미지 인코딩 능력을 잃지 않도록 하는 가벼운 비전 디코더를 도입해. 훈련 과정은 두 단계로 구성돼: 내부 스케일 사전 훈련과 외부 스케일 미세 조정이야.

내부 스케일 사전 훈련 동안, UNIT는 여러 스케일 입력으로부터 통합된 표현을 배워. 여기서 이미지는 일반적으로 사용되는 해상도로 문서와 함께 사용돼 기본적인 인식 능력을 키우는 거지. 외부 스케일 미세 조정 단계에서는 가장 일반적으로 사용되는 해상도와 다른 해상도를 가진 이미지와 문서를 포함한 스케일 교환 데이터를 도입해 모델의 스케일 강인성을 높여.

특히 UNIT는 원래 비전 인코더 아키텍처를 그대로 유지하므로, 추론이나 배포 비용이 따르지 않아. 여러 벤치마크 실험 결과, 우리의 방법이 문서 관련 작업(예: OCR, DocQA)에서 기존 방법보다 훨씬 뛰어난 성능을 보이는 동시에 자연 이미지에서도 성능을 유지한다는 걸 확인했어. 이는 핵심 이미지 인식 능력을 손상시키지 않으면서 텍스트 인식을 크게 향상시킬 수 있다는 걸 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.04116
Title: Smooth-edged Perturbations Improve Perturbation-based Image Explanations

Original Abstract:
Perturbation-based post-hoc image explanation methods are commonly used to explain image prediction models by perturbing parts of the input to measure how those parts affect the output. Due to the intractability of perturbing each pixel individually, images are typically attributed to larger segments. The Randomized Input Sampling for Explanations (RISE) method solved this issue by using smooth perturbation masks.
While this method has proven effective and popular, it has not been investigated which parts of the method are responsible for its success. This work tests many combinations of mask sampling, segmentation techniques, smoothing, and attribution calculation. The results show that the RISE-style pixel attribution is beneficial to all evaluated methods. Furthermore, it is shown that attribution calculation is the least impactful parameter.
The implementation of this work is available online: this https URL.

Translated Abstract:
이미지 예측 모델을 설명하기 위해 흔히 사용되는 방법 중 하나가 이미지의 일부를 변형해서 그 부분이 출력에 어떻게 영향을 미치는지 측정하는 방식이야. 하지만 각 픽셀을 개별적으로 변형하는 건 어려워서, 보통은 더 큰 구역으로 나누어서 설명해. RISE라는 방법은 부드러운 변형 마스크를 사용해서 이 문제를 해결했어.

이 방법이 효과적이고 인기가 많긴 하지만, 어떤 부분이 성공에 기여했는지는 잘 조사되지 않았어. 그래서 이 연구에서는 마스크 샘플링, 분할 기법, 부드럽게 하기, 그리고 기여도 계산 같은 여러 조합을 테스트했어. 결과적으로 RISE 스타일의 픽셀 기여도가 평가한 모든 방법에 도움이 된다는 걸 보여줬어. 그리고 기여도 계산이 가장 영향력이 적은 요소라는 것도 확인했어.

이 연구의 구현은 온라인에서 확인할 수 있어: 이 링크.

================================================================================

URL: https://arxiv.org/abs/2409.04117
Title: Confidence-Aware Document OCR Error Detection

Original Abstract:
Optical Character Recognition (OCR) continues to face accuracy challenges that impact subsequent applications. To address these errors, we explore the utility of OCR confidence scores for enhancing post-OCR error detection. Our study involves analyzing the correlation between confidence scores and error rates across different OCR systems. We develop ConfBERT, a BERT-based model that incorporates OCR confidence scores into token embeddings and offers an optional pre-training phase for noise adjustment. Our experimental results demonstrate that integrating OCR confidence scores can enhance error detection capabilities. This work underscores the importance of OCR confidence scores in improving detection accuracy and reveals substantial disparities in performance between commercial and open-source OCR technologies.

Translated Abstract:
광학 문자 인식(OCR)은 여전히 정확성 문제에 직면해 있어서 이후의 응용에 영향을 미치고 있어. 이런 오류를 해결하기 위해 우리는 OCR 신뢰 점수를 활용해서 OCR 후 오류 감지를 개선하는 방법을 연구했어. 

우리 연구는 서로 다른 OCR 시스템에서 신뢰 점수와 오류 비율 간의 상관관계를 분석하는 거야. 그리고 우리는 ConfBERT라는 모델을 개발했는데, 이 모델은 BERT 기반으로 OCR 신뢰 점수를 토큰 임베딩에 포함시키고, 노이즈 조정을 위한 선택적 사전 훈련 단계를 제공해. 

실험 결과에 따르면, OCR 신뢰 점수를 통합하면 오류 감지 능력이 향상된다는 걸 보여줘. 이 연구는 OCR 신뢰 점수가 감지 정확도를 높이는 데 중요하다는 걸 강조하고, 상업용 OCR 기술과 오픈 소스 OCR 기술 간의 성능 차이가 상당하다는 것도 밝혀냈어.

================================================================================

URL: https://arxiv.org/abs/2409.04133
Title: Secure Traffic Sign Recognition: An Attention-Enabled Universal Image Inpainting Mechanism against Light Patch Attacks

Original Abstract:
Traffic sign recognition systems play a crucial role in assisting drivers to make informed decisions while driving. However, due to the heavy reliance on deep learning technologies, particularly for future connected and autonomous driving, these systems are susceptible to adversarial attacks that pose significant safety risks to both personal and public transportation. Notably, researchers recently identified a new attack vector to deceive sign recognition systems: projecting well-designed adversarial light patches onto traffic signs. In comparison with traditional adversarial stickers or graffiti, these emerging light patches exhibit heightened aggression due to their ease of implementation and outstanding stealthiness. To effectively counter this security threat, we propose a universal image inpainting mechanism, namely, SafeSign. It relies on attention-enabled multi-view image fusion to repair traffic signs contaminated by adversarial light patches, thereby ensuring the accurate sign recognition. Here, we initially explore the fundamental impact of malicious light patches on the local and global feature spaces of authentic traffic signs. Then, we design a binary mask-based U-Net image generation pipeline outputting diverse contaminated sign patterns, to provide our image inpainting model with needed training data. Following this, we develop an attention mechanism-enabled neural network to jointly utilize the complementary information from multi-view images to repair contaminated signs. Finally, extensive experiments are conducted to evaluate SafeSign's effectiveness in resisting potential light patch-based attacks, bringing an average accuracy improvement of 54.8% in three widely-used sign recognition models

Translated Abstract:
교통 신호 인식 시스템은 운전자가 안전하게 결정을 내리는 데 중요한 역할을 해. 하지만, 딥러닝 기술에 많이 의존하다 보니, 특히 자율주행과 연결된 미래의 운전에서는 악의적인 공격에 취약해져. 이런 공격은 개인과 대중 교통 모두에게 큰 안전 위험을 초래할 수 있어. 최근 연구자들은 교통 신호 인식 시스템을 속일 수 있는 새로운 공격 방법을 발견했어. 그건 잘 설계된 악의적인 조명 패치를 교통 신호에 비추는 거야. 전통적인 스티커나 그래피티에 비해, 이런 조명 패치는 구현이 쉽고 은밀해서 더 위험해.

이런 보안 위협에 효과적으로 대응하기 위해 우리는 SafeSign이라는 보편적인 이미지 인페인팅 기법을 제안해. 이 방법은 주의(attention)를 이용한 다중 시점 이미지 융합을 통해 악의적인 조명 패치에 오염된 교통 신호를 복구해서 정확한 신호 인식을 보장해. 먼저, 우리는 악의적인 조명 패치가 진짜 교통 신호의 지역적 및 전역적 특징에 미치는 기본적인 영향을 살펴봐. 그 다음, 우리는 다양한 오염된 신호 패턴을 출력하는 이진 마스크 기반 U-Net 이미지 생성 파이프라인을 설계해서 인페인팅 모델이 필요한 훈련 데이터를 제공해.

그리고 나서, 우리는 다중 시점 이미지에서의 보완 정보를 함께 활용할 수 있도록 주의 메커니즘이 적용된 신경망을 개발해. 마지막으로, SafeSign이 조명 패치 기반 공격에 저항하는 효과를 평가하기 위해 많은 실험을 진행했어. 그 결과, 세 가지 널리 사용되는 신호 인식 모델에서 평균적으로 54.8%의 정확도 향상을 가져왔어.

================================================================================

URL: https://arxiv.org/abs/2409.04178
Title: Reprojection Errors as Prompts for Efficient Scene Coordinate Regression

Original Abstract:
Scene coordinate regression (SCR) methods have emerged as a promising area of research due to their potential for accurate visual localization. However, many existing SCR approaches train on samples from all image regions, including dynamic objects and texture-less areas. Utilizing these areas for optimization during training can potentially hamper the overall performance and efficiency of the model. In this study, we first perform an in-depth analysis to validate the adverse impacts of these areas. Drawing inspiration from our analysis, we then introduce an error-guided feature selection (EGFS) mechanism, in tandem with the use of the Segment Anything Model (SAM). This mechanism seeds low reprojection areas as prompts and expands them into error-guided masks, and then utilizes these masks to sample points and filter out problematic areas in an iterative manner. The experiments demonstrate that our method outperforms existing SCR approaches that do not rely on 3D information on the Cambridge Landmarks and Indoor6 datasets.

Translated Abstract:
장면 좌표 회귀(SCR) 방법은 정확한 시각적 위치 추정에서 가능성이 높은 연구 분야로 떠오르고 있어. 하지만 기존의 많은 SCR 접근 방식은 동적 물체나 텍스처가 없는 영역을 포함한 모든 이미지 지역의 샘플로 훈련해. 이런 영역을 훈련 중 최적화에 사용하면 모델의 전체 성능과 효율성을 해칠 수 있어.

이 연구에서는 먼저 이런 영역들이 미치는 부정적인 영향을 확인하기 위해 깊이 있는 분석을 해. 그리고 이 분석에서 영감을 받아 에러 유도 피처 선택(EGFS) 메커니즘을 도입해. 이 메커니즘은 낮은 재투영 영역을 프롬프트로 사용하고, 그걸 에러 유도 마스크로 확장해. 그리고 이 마스크를 활용해서 포인트를 샘플링하고 문제 있는 영역을 반복적으로 필터링해.

실험 결과, 우리의 방법이 3D 정보를 사용하지 않는 기존 SCR 접근 방식보다 Cambridge Landmarks와 Indoor6 데이터셋에서 더 좋은 성과를 낸다는 걸 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.04187
Title: LITE: A Paradigm Shift in Multi-Object Tracking with Efficient ReID Feature Integration

Original Abstract:
The Lightweight Integrated Tracking-Feature Extraction (LITE) paradigm is introduced as a novel multi-object tracking (MOT) approach. It enhances ReID-based trackers by eliminating inference, pre-processing, post-processing, and ReID model training costs. LITE uses real-time appearance features without compromising speed. By integrating appearance feature extraction directly into the tracking pipeline using standard CNN-based detectors such as YOLOv8m, LITE demonstrates significant performance improvements. The simplest implementation of LITE on top of classic DeepSORT achieves a HOTA score of 43.03% at 28.3 FPS on the MOT17 benchmark, making it twice as fast as DeepSORT on MOT17 and four times faster on the more crowded MOT20 dataset, while maintaining similar accuracy. Additionally, a new evaluation framework for tracking-by-detection approaches reveals that conventional trackers like DeepSORT remain competitive with modern state-of-the-art trackers when evaluated under fair conditions. The code will be available post-publication at this https URL.

Translated Abstract:
경량 통합 추적-특징 추출(LITE) 패러다임이 새로운 다중 객체 추적(MOT) 접근 방식으로 소개돼. 이 방법은 ReID 기반 추적기의 추론, 전처리, 후처리, ReID 모델 학습 비용을 없애서 성능을 높여. LITE는 속도를 포기하지 않으면서 실시간 외형 특징을 사용해.

LITE는 YOLOv8m 같은 표준 CNN 기반 탐지기를 사용해서 외형 특징 추출을 추적 파이프라인에 직접 통합해. 이 덕분에 성능이 크게 향상됐어. LITE의 가장 간단한 구현이 고전적인 DeepSORT 위에서 MOT17 벤치마크에서 28.3 FPS에서 43.03%의 HOTA 점수를 달성했어. 이건 MOT17에서 DeepSORT보다 두 배 빠르고, 더 복잡한 MOT20 데이터셋에서는 네 배 빠르면서도 비슷한 정확도를 유지해.

또한, 탐지에 의한 추적 접근 방식에 대한 새로운 평가 프레임워크가 마련돼서, DeepSORT 같은 전통적인 추적기가 현대의 최신 추적기와 공정한 조건에서 평가할 때 경쟁력을 유지한다는 걸 보여줘. 코드 출판 후 이 URL에서 확인할 수 있을 거야.

================================================================================

URL: https://arxiv.org/abs/2409.04196
Title: GST: Precise 3D Human Body from a Single Image with Gaussian Splatting Transformers

Original Abstract:
Reconstructing realistic 3D human models from monocular images has significant applications in creative industries, human-computer interfaces, and healthcare. We base our work on 3D Gaussian Splatting (3DGS), a scene representation composed of a mixture of Gaussians. Predicting such mixtures for a human from a single input image is challenging, as it is a non-uniform density (with a many-to-one relationship with input pixels) with strict physical constraints. At the same time, it needs to be flexible to accommodate a variety of clothes and poses. Our key observation is that the vertices of standardized human meshes (such as SMPL) can provide an adequate density and approximate initial position for Gaussians. We can then train a transformer model to jointly predict comparatively small adjustments to these positions, as well as the other Gaussians' attributes and the SMPL parameters. We show empirically that this combination (using only multi-view supervision) can achieve fast inference of 3D human models from a single image without test-time optimization, expensive diffusion models, or 3D points supervision. We also show that it can improve 3D pose estimation by better fitting human models that account for clothes and other variations. The code is available on the project website this https URL .

Translated Abstract:
단안 이미지를 통해 현실적인 3D 인간 모델을 재구성하는 것은 창의 산업, 인간-컴퓨터 인터페이스, 헬스케어 등에서 중요한 활용이 있어. 우리는 3D 가우시안 스플래팅(3DGS)이라는 장면 표현 방식을 바탕으로 작업을 진행해. 이건 가우시안 혼합체로 구성되어 있어.

단일 입력 이미지로부터 인간에 대한 이런 혼합체를 예측하는 건 어려운 일인데, 그 이유는 비균일 밀도가 존재하고 입력 픽셀과의 관계가 복잡하기 때문이야. 또, 다양한 옷과 자세를 수용할 수 있을 만큼 유연해야 해. 우리가 주목한 점은 표준화된 인간 메쉬의 꼭짓점(SMPL 같은)이 가우시안의 적절한 밀도와 대략적인 초기 위치를 제공할 수 있다는 거야.

그런 다음, 우리는 트랜스포머 모델을 훈련시켜서 이러한 위치에 대한 비교적 작은 조정과 다른 가우시안의 속성, SMPL 파라미터를 동시에 예측할 수 있게 했어. 실험적으로, 이 조합(다중 뷰 감독만 사용하는 것)이 단일 이미지에서 3D 인간 모델을 빠르게 추론할 수 있다는 것을 보여줬어. 이 과정에서 테스트 시간 최적화, 비싼 확산 모델, 3D 포인트 감독이 필요 없었어.

또한, 이 방법이 옷과 다른 변화를 고려한 인간 모델을 더 잘 맞춤으로써 3D 자세 추정도 개선할 수 있다는 걸 보여줬어. 코드도 프로젝트 웹사이트에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04205
Title: Introducing Gating and Context into Temporal Action Detection

Original Abstract:
Temporal Action Detection (TAD), the task of localizing and classifying actions in untrimmed video, remains challenging due to action overlaps and variable action durations. Recent findings suggest that TAD performance is dependent on the structural design of transformers rather than on the self-attention mechanism. Building on this insight, we propose a refined feature extraction process through lightweight, yet effective operations. First, we employ a local branch that employs parallel convolutions with varying window sizes to capture both fine-grained and coarse-grained temporal features. This branch incorporates a gating mechanism to select the most relevant features. Second, we introduce a context branch that uses boundary frames as key-value pairs to analyze their relationship with the central frame through cross-attention. The proposed method captures temporal dependencies and improves contextual understanding. Evaluations of the gating mechanism and context branch on challenging datasets (THUMOS14 and EPIC-KITCHEN 100) show a consistent improvement over the baseline and existing methods.

Translated Abstract:
시간 행동 탐지(TAD)는 잘라지지 않은 비디오에서 행동을 찾아내고 분류하는 작업이야. 행동이 겹치거나 지속 시간이 다 달라서 여전히 어려운 부분이 많아. 최근 연구 결과에 따르면, TAD 성능은 자기 주의 메커니즘보다는 트랜스포머의 구조 디자인에 더 의존하는 것 같아.

이런 통찰을 바탕으로, 우리는 가볍고도 효과적인 작업을 통해 더 나은 특징 추출 과정을 제안해. 먼저, 다양한 창 크기를 가진 병렬 합성을 사용하는 로컬 브랜치를 도입해서 세밀한 시간적 특징과 대략적인 시간적 특징을 모두 잡아내. 이 브랜치는 가장 관련성 높은 특징을 선택할 수 있도록 게이팅 메커니즘을 포함하고 있어.

두 번째로, 중앙 프레임과의 관계를 분석하기 위해 경계 프레임을 키-값 쌍으로 사용하는 컨텍스트 브랜치를 소개해. 이 방법은 시간적 의존성을 잘 포착하고 맥락 이해도를 향상시켜. 게이팅 메커니즘과 컨텍스트 브랜치의 성능을 THUMOS14와 EPIC-KITCHEN 100 같은 어려운 데이터셋에서 평가해봤더니, 기존 방법들보다 일관되게 개선된 결과를 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.04208
Title: Learning to Learn Transferable Generative Attack for Person Re-Identification

Original Abstract:
Deep learning-based person re-identification (re-id) models are widely employed in surveillance systems and inevitably inherit the vulnerability of deep networks to adversarial attacks. Existing attacks merely consider cross-dataset and cross-model transferability, ignoring the cross-test capability to perturb models trained in different domains. To powerfully examine the robustness of real-world re-id models, the Meta Transferable Generative Attack (MTGA) method is proposed, which adopts meta-learning optimization to promote the generative attacker producing highly transferable adversarial examples by learning comprehensively simulated transfer-based cross-model\&dataset\&test black-box meta attack tasks. Specifically, cross-model\&dataset black-box attack tasks are first mimicked by selecting different re-id models and datasets for meta-train and meta-test attack processes. As different models may focus on different feature regions, the Perturbation Random Erasing module is further devised to prevent the attacker from learning to only corrupt model-specific features. To boost the attacker learning to possess cross-test transferability, the Normalization Mix strategy is introduced to imitate diverse feature embedding spaces by mixing multi-domain statistics of target models. Extensive experiments show the superiority of MTGA, especially in cross-model\&dataset and cross-model\&dataset\&test attacks, our MTGA outperforms the SOTA methods by 21.5\% and 11.3\% on mean mAP drop rate, respectively. The code of MTGA will be released after the paper is accepted.

Translated Abstract:
딥러닝 기반의 사람 재식별(re-id) 모델은 감시 시스템에서 널리 사용되고 있는데, 이 모델들은 깊은 네트워크의 적대적 공격에 취약한 특성을 물려받게 돼. 기존의 공격들은 단지 데이터셋 간의 전이성과 모델 간의 전이성만 고려하고, 서로 다른 도메인에서 훈련된 모델을 방해할 수 있는 능력은 무시하고 있어.

현실 세계의 re-id 모델의 강건성을 제대로 검토하기 위해 메타 전이 생성 공격(MTGA) 방법이 제안되었어. 이 방법은 메타 학습 최적화를 통해 생성 공격자가 매우 전이 가능한 적대적 예제를 만들도록 도와줘. 이 과정을 위해 다양한 시뮬레이션된 전이 기반의 크로스 모델, 데이터셋, 테스트 블랙박스 메타 공격 작업을 학습해.

구체적으로, 크로스 모델과 데이터셋 블랙박스 공격 작업은 메타 훈련과 메타 테스트 공격 과정을 위해 다른 re-id 모델과 데이터셋을 선택해서 모방해. 서로 다른 모델은 서로 다른 특징 영역에 집중할 수 있기 때문에, 공격자가 모델 특정 특징만 망가뜨리는 걸 방지하기 위해 Perturbation Random Erasing 모듈도 추가했어.

또한, 공격자가 크로스 테스트 전이성을 갖도록 학습할 수 있게 Normalization Mix 전략을 도입해, 여러 도메인의 통계를 혼합해서 다양한 특징 임베딩 공간을 흉내 내. 실험 결과 MTGA의 우수성이 입증되었고, 특히 크로스 모델과 데이터셋 및 크로스 모델, 데이터셋, 테스트 공격에서 MTGA는 평균 mAP 감소율에서 각각 21.5%와 11.3% 더 나은 성능을 보여줬어. MTGA의 코드는 논문이 수락된 후 공개될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.04214
Title: Diagram Formalization Enhanced Multi-Modal Geometry Problem Solver

Original Abstract:
Mathematical reasoning remains an ongoing challenge for AI models, especially for geometry problems that require both linguistic and visual signals. As the vision encoders of most MLLMs are trained on natural scenes, they often struggle to understand geometric diagrams, performing no better in geometry problem solving than LLMs that only process text. This limitation is amplified by the lack of effective methods for representing geometric relationships. To address these issues, we introduce the Diagram Formalization Enhanced Geometry Problem Solver (DFE-GPS), a new framework that integrates visual features, geometric formal language, and natural language representations. We propose a novel synthetic data approach and create a large-scale geometric dataset, SynthGeo228K, annotated with both formal and natural language captions, designed to enhance the vision encoder for a better understanding of geometric structures. Our framework improves MLLMs' ability to process geometric diagrams and extends their application to open-ended tasks on the formalgeo7k dataset.

Translated Abstract:
수학적 추론은 AI 모델에게 여전히 어려운 문제야, 특히 언어적 신호와 시각적 신호 둘 다 필요한 기하학 문제에서는 더 그렇지. 대부분의 MLLM(다중 모달 언어 모델) 비전 인코더는 자연 장면을 학습하기 때문에 기하학 도형을 이해하는 데 어려움을 겪어. 그래서 텍스트만 처리하는 LLM보다 기하학 문제를 해결하는 데 성과가 나지 않아. 이런 한계는 기하학적 관계를 표현할 효과적인 방법이 부족해서 더 심해져.

이 문제를 해결하기 위해 우리는 DFE-GPS(도형 형식화 강화 기하학 문제 해결기)라는 새로운 프레임워크를 소개해. 이 프레임워크는 시각적 특징, 기하학적 형식 언어, 그리고 자연 언어 표현을 통합해. 우리는 새로운 합성 데이터 접근 방식을 제안하고, 공식 언어와 자연 언어 캡션이 모두 주석 처리된 대규모 기하학 데이터셋인 SynthGeo228K를 만들었어. 이 데이터셋은 비전 인코더가 기하학 구조를 더 잘 이해할 수 있도록 돕기 위해 설계됐어.

우리의 프레임워크는 MLLM이 기하학 도형을 처리하는 능력을 향상시키고, formalgeo7k 데이터셋에서 열린 과제에 대한 적용 범위를 넓혀.

================================================================================

URL: https://arxiv.org/abs/2409.04218
Title: MpoxMamba: A Grouped Mamba-based Lightweight Hybrid Network for Mpox Detection

Original Abstract:
Due to the lack of effective mpox detection tools, the mpox virus continues to spread worldwide and has once again been declared a public health emergency of international concern by the World Health Organization. Deep learning-based mpox detection tools are crucial to alleviate mpox outbreak. However, existing methods have difficulty in achieving a good trade-off between detection performance, parameter size, and model complexity, which is crucial for practical applications and widespread deployment, especially in resource-limited scenarios. Given that the success of Mamba in modeling long-range dependencies and its linear complexity, we proposed a lightweight hybrid architecture called MpoxMamba. MpoxMamba utilizes deep separable convolutions to extract local feature representations in mpox skin lesions, and greatly enhances the model's ability to model the global contextual information by grouped Mamba modules. Experimental results on two widely recognized mpox datasets demonstrate that MpoxMamba outperforms existing mpox detection methods and state-of-the-art lightweight models. We also developed a web-based online application to provide free mpox detection services to the public in the epidemic areas (this http URL). The source codes of MpoxMamba are available at this https URL.

Translated Abstract:
mpox 감지 도구가 효과적으로 부족해서, mpox 바이러스가 전 세계적으로 퍼지고 있어. 세계보건기구는 다시 한 번 이걸 국제적인 공중 보건 비상사태로 선언했어. 딥러닝 기반의 mpox 감지 도구는 mpox 발생을 줄이는 데 정말 중요해. 

하지만 기존 방법들은 감지 성능, 파라미터 크기, 모델 복잡도 사이에서 좋은 균형을 맞추는 데 어려움이 있어. 이건 실제 적용과 널리 퍼지는데 중요한 요소야, 특히 자원이 부족한 상황에서는 더더욱 그렇고. 

Mamba가 긴 거리 의존성을 모델링하는 데 성공적이고 선형 복잡성을 가지고 있다는 점을 고려해서, 우리는 MpoxMamba라는 경량 하이브리드 아키텍처를 제안했어. MpoxMamba는 깊은 분리 가능한 합성곱을 사용해서 mpox 피부 병변에서 지역적 특징을 추출하고, 그룹화된 Mamba 모듈을 통해 모델의 전반적인 맥락 정보를 모델링하는 능력을 크게 향상시켜. 

두 개의 잘 알려진 mpox 데이터셋에 대한 실험 결과, MpoxMamba가 기존의 mpox 감지 방법과 최신 경량 모델보다 더 뛰어난 성능을 보여줬어. 또 우리는 전염 지역의 사람들에게 무료 mpox 감지 서비스를 제공하는 웹 기반 온라인 애플리케이션도 개발했어 (이 링크). MpoxMamba의 소스 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04234
Title: UniDet3D: Multi-dataset Indoor 3D Object Detection

Original Abstract:
Growing customer demand for smart solutions in robotics and augmented reality has attracted considerable attention to 3D object detection from point clouds. Yet, existing indoor datasets taken individually are too small and insufficiently diverse to train a powerful and general 3D object detection model. In the meantime, more general approaches utilizing foundation models are still inferior in quality to those based on supervised training for a specific task. In this work, we propose \ours{}, a simple yet effective 3D object detection model, which is trained on a mixture of indoor datasets and is capable of working in various indoor environments. By unifying different label spaces, \ours{} enables learning a strong representation across multiple datasets through a supervised joint training scheme. The proposed network architecture is built upon a vanilla transformer encoder, making it easy to run, customize and extend the prediction pipeline for practical use. Extensive experiments demonstrate that \ours{} obtains significant gains over existing 3D object detection methods in 6 indoor benchmarks: ScanNet (+1.1 mAP50), ARKitScenes (+19.4 mAP25), S3DIS (+9.1 mAP50), MultiScan (+9.3 mAP50), 3RScan (+3.2 mAP50), and ScanNet++ (+2.7 mAP50). Code is available at this https URL .

Translated Abstract:
고객들이 로봇과 증강 현실에서 스마트 솔루션을 원하면서, 포인트 클라우드에서 3D 객체 탐지에 대한 관심이 커지고 있어. 하지만 기존의 실내 데이터셋은 각각 너무 작고 다양성이 부족해서 강력하고 일반적인 3D 객체 탐지 모델을 훈련하기에는 부족해. 게다가, 기초 모델을 활용한 더 일반적인 접근법은 특정 작업을 위한 감독 학습 기반 방법보다 질이 떨어져.

이번 연구에서는 \ours{}라는 간단하면서도 효과적인 3D 객체 탐지 모델을 제안해. 이 모델은 여러 실내 데이터셋을 혼합해서 훈련되며 다양한 실내 환경에서 작동할 수 있어. 서로 다른 레이블 공간을 통합함으로써, \ours{}는 감독 공동 훈련 방식으로 여러 데이터셋에서 강력한 표현을 학습할 수 있게 해. 제안된 네트워크 구조는 일반적인 트랜스포머 인코더를 기반으로 해서 실행하기 쉽고, 커스터마이즈하거나 확장하는 것도 편리해.

광범위한 실험 결과, \ours{}는 6개의 실내 벤치마크에서 기존 3D 객체 탐지 방법들보다 상당한 성능 향상을 보여줬어: ScanNet (+1.1 mAP50), ARKitScenes (+19.4 mAP25), S3DIS (+9.1 mAP50), MultiScan (+9.3 mAP50), 3RScan (+3.2 mAP50), 그리고 ScanNet++ (+2.7 mAP50). 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04243
Title: Hybrid Cost Volume for Memory-Efficient Optical Flow

Original Abstract:
Current state-of-the-art flow methods are mostly based on dense all-pairs cost volumes. However, as image resolution increases, the computational and spatial complexity of constructing these cost volumes grows at a quartic rate, making these methods impractical for high-resolution images. In this paper, we propose a novel Hybrid Cost Volume for memory-efficient optical flow, named HCV. To construct HCV, we first propose a Top-k strategy to separate the 4D cost volume into two global 3D cost volumes. These volumes significantly reduce memory usage while retaining a substantial amount of matching information. We further introduce a local 4D cost volume with a local search space to supplement the local information for HCV. Based on HCV, we design a memory-efficient optical flow network, named HCVFlow. Compared to the recurrent flow methods based the all-pairs cost volumes, our HCVFlow significantly reduces memory consumption while ensuring high accuracy. We validate the effectiveness and efficiency of our method on the Sintel and KITTI datasets and real-world 4K (2160*3840) resolution images. Extensive experiments show that our HCVFlow has very low memory usage and outperforms other memory-efficient methods in terms of accuracy. The code is publicly available at this https URL.

Translated Abstract:
현재 최첨단 흐름 방법들은 대부분 밀집된 모든 쌍 비용 볼륨에 기반하고 있어. 하지만 이미지 해상도가 높아질수록 이 비용 볼륨을 만드는 데 필요한 계산과 공간 복잡성이 4차적으로 증가해. 그래서 고해상도 이미지에 이 방법들을 적용하기가 힘들어. 

이 논문에서는 메모리 효율적인 광학 흐름을 위한 새로운 하이브리드 비용 볼륨, HCV를 제안해. HCV를 만들기 위해 먼저 Top-k 전략을 제안해서 4D 비용 볼륨을 두 개의 글로벌 3D 비용 볼륨으로 나눠. 이렇게 하면 메모리 사용량이 확 줄어들면서도 상당한 양의 매칭 정보를 유지할 수 있어. 그리고 HCV를 보완하기 위해 지역 정보를 추가할 수 있는 지역 4D 비용 볼륨도 도입했어. 

HCV를 바탕으로 HCVFlow라는 메모리 효율적인 광학 흐름 네트워크를 설계했어. 기존의 모든 쌍 비용 볼륨 기반 순환 흐름 방법들과 비교했을 때, HCVFlow는 메모리 소모를 크게 줄이면서도 높은 정확도를 보장해. 우리는 Sintel과 KITTI 데이터셋, 그리고 실제 4K (2160*3840) 해상도 이미지에서 우리 방법의 효과성과 효율성을 검증했어. 다양한 실험 결과, HCVFlow는 메모리 사용량이 매우 낮고, 정확도 면에서도 다른 메모리 효율적인 방법들을 능가해. 코드도 공개되어 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04272
Title: Cycle Pixel Difference Network for Crisp Edge Detection

Original Abstract:
Edge detection, as a fundamental task in computer vision, has garnered increasing attention. The advent of deep learning has significantly advanced this field. However, recent deep learning-based methods which rely on large-scale pre-trained weights cannot be trained from scratch, with very limited research addressing this issue. This paper proposes a novel cycle pixel difference convolution (CPDC), which effectively integrates image gradient information with modern convolution operations. Based on the CPDC, we develop a U-shape encoder-decoder model named CPD-Net, which is a purely end-to-end network. Additionally, to address the issue of edge thickness produced by most existing methods, we construct a multi-scale information enhancement module (MSEM) to enhance the discriminative ability of the model, thereby generating crisp and clean contour maps. Comprehensive experiments conducted on three standard benchmarks demonstrate that our method achieves competitive performance on the BSDS500 dataset (ODS=0.813), NYUD-V2 (ODS=0.760), and BIPED dataset (ODS=0.898). Our approach provides a novel perspective for addressing these challenges in edge detection.

Translated Abstract:
에지 검출은 컴퓨터 비전에서 기본적인 작업으로, 요즘 점점 더 주목받고 있어. 딥러닝의 발전 덕분에 이 분야가 많이 발전했지. 그런데 최근의 딥러닝 기반 방법들은 대규모로 미리 학습된 가중치에 의존하고 있어서 처음부터 학습하기가 어려워. 이 문제를 다룬 연구는 거의 없어.

이 논문에서는 새로운 '사이클 픽셀 차이 컨볼루션(CPDC)'을 제안해. 이 방법은 이미지의 경계 정보를 현대적인 컨볼루션 작업과 효과적으로 통합해. CPDC를 기반으로 해서 'CPD-Net'이라는 U자형 인코더-디코더 모델을 개발했어. 이건 완전한 엔드 투 엔드 네트워크야.

또한 대부분의 기존 방법들이 만들어내는 에지 두께 문제를 해결하기 위해 '다중 스케일 정보 강화 모듈(MSEM)'을 만들어서 모델의 구분 능력을 높였어. 이 덕분에 선명하고 깔끔한 윤곽 맵을 생성할 수 있게 됐지. 

세 개의 표준 벤치마크에서 진행한 종합 실험 결과, 우리 방법이 BSDS500 데이터셋(ODS=0.813), NYUD-V2(ODS=0.760), BIPED 데이터셋(ODS=0.898)에서 경쟁력 있는 성과를 달성했어. 우리의 접근 방식은 에지 검출의 이런 문제들을 해결하기 위한 새로운 시각을 제공해.

================================================================================

URL: https://arxiv.org/abs/2409.04298
Title: FS-MedSAM2: Exploring the Potential of SAM2 for Few-Shot Medical Image Segmentation without Fine-tuning

Original Abstract:
The Segment Anything Model 2 (SAM2) has recently demonstrated exceptional performance in zero-shot prompt segmentation for natural images and videos. However, it faces significant challenges when applied to medical images. Since its release, many attempts have been made to adapt SAM2's segmentation capabilities to the medical imaging domain. These efforts typically involve using a substantial amount of labeled data to fine-tune the model's weights. In this paper, we explore SAM2 from a different perspective via making the full use of its trained memory attention module and its ability of processing mask prompts. We introduce FS-MedSAM2, a simple yet effective framework that enables SAM2 to achieve superior medical image segmentation in a few-shot setting, without the need for fine-tuning. Our framework outperforms the current state-of-the-arts on two publicly available medical image datasets. The code is available at this https URL.

Translated Abstract:
세그먼트 아무것도 모델 2(SAM2)는 최근 자연 이미지와 비디오에서 제로샷 프롬프트 세그멘테이션에서 뛰어난 성능을 보였어. 하지만 의료 이미지에 적용할 때는 큰 어려움이 있어. 출시 이후, SAM2의 세그멘테이션 기능을 의료 이미지에 맞추기 위해 많은 시도가 있었는데, 보통은 모델의 가중치를 조정하기 위해 많은 양의 라벨 데이터가 필요했어.

이번 논문에서는 SAM2를 다른 시각에서 살펴보려고 해. SAM2의 훈련된 메모리 어텐션 모듈과 마스크 프롬프트 처리 능력을 최대한 활용하는 방법을 연구했어. 우리는 FS-MedSAM2라는 간단하지만 효과적인 프레임워크를 소개해. 이 프레임워크를 통해 SAM2는 몇 번의 샷으로 의료 이미지 세그멘테이션에서 뛰어난 성능을 낼 수 있어, 가중치 조정 없이 말이지. 

우리의 프레임워크는 두 개의 공개 의료 이미지 데이터셋에서 현재의 최고 성능을 초월했어. 코드도 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04310
Title: Advancing SEM Based Nano-Scale Defect Analysis in Semiconductor Manufacturing for Advanced IC Nodes

Original Abstract:
In this research, we introduce a unified end-to-end Automated Defect Classification-Detection-Segmentation (ADCDS) framework for classifying, detecting, and segmenting multiple instances of semiconductor defects for advanced nodes. This framework consists of two modules: (a) a defect detection module, followed by (b) a defect segmentation module. The defect detection module employs Deformable DETR to aid in the classification and detection of nano-scale defects, while the segmentation module utilizes BoxSnake. BoxSnake facilitates box-supervised instance segmentation of nano-scale defects, supported by the former module. This simplifies the process by eliminating the laborious requirement for ground-truth pixel-wise mask annotation by human experts, which is typically associated with training conventional segmentation models. We have evaluated the performance of our ADCDS framework using two distinct process datasets from real wafers, as ADI and AEI, specifically focusing on Line-space patterns. We have demonstrated the applicability and significance of our proposed methodology, particularly in the nano-scale segmentation and generation of binary defect masks, using the challenging ADI SEM dataset where ground-truth pixelwise segmentation annotations were unavailable. Furthermore, we have presented a comparative analysis of our proposed framework against previous approaches to demonstrate its effectiveness. Our proposed framework achieved an overall mAP@IoU0.5 of 72.19 for detection and 78.86 for segmentation on the ADI dataset. Similarly, for the AEI dataset, these metrics were 90.38 for detection and 95.48 for segmentation. Thus, our proposed framework effectively fulfils the requirements of advanced defect analysis while addressing significant constraints.

Translated Abstract:
이 연구에서는 반도체 결함을 분류하고 탐지하며 분할하는 통합 자동 결함 분류-탐지-분할(ADCDS) 프레임워크를 소개해. 이 프레임워크는 두 개의 모듈로 구성되어 있어: (a) 결함 탐지 모듈과 (b) 결함 분할 모듈이야. 결함 탐지 모듈은 Deformable DETR을 사용해서 나노 스케일 결함의 분류와 탐지를 도와주고, 분할 모듈은 BoxSnake를 활용해. BoxSnake는 나노 스케일 결함의 박스 기반 인스턴스 분할을 지원해주는데, 이게 이전 모듈 덕분에 가능해. 이렇게 하면 인간 전문가가 해야 하는 번거로운 픽셀 단위 마스크 주석 작업을 없앨 수 있어서, 전통적인 분할 모델 훈련과 비교했을 때 훨씬 간단해.

우리는 실제 웨이퍼에서 두 개의 다른 프로세스 데이터셋(ADI와 AEI)을 사용해서 ADCDS 프레임워크의 성능을 평가했어. 특히 선-공간 패턴에 초점을 맞췄지. 우리가 제안한 방법론이 나노 스케일 분할과 이진 결함 마스크 생성을 잘 수행한다는 것을 ADI SEM 데이터셋을 통해 보여줬어. 이 데이터셋은 실제 픽셀 단위 분할 주석이 없었거든. 게다가, 우리가 제안한 프레임워크와 이전 접근 방식들을 비교 분석해서 효과성을 입증했어.

우리 프레임워크는 ADI 데이터셋에서 탐지에 대한 전체 mAP@IoU0.5가 72.19, 분할에 대한 mAP가 78.86이었어. AEI 데이터셋에서는 탐지에 대한 mAP가 90.38, 분할에 대한 mAP가 95.48로 나왔고. 그래서, 우리 프레임워크는 고급 결함 분석의 요구를 효과적으로 충족하면서도 중요한 제약 사항을 해결할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04330
Title: How to Identify Good Superpixels for Deforestation Detection on Tropical Rainforests

Original Abstract:
The conservation of tropical forests is a topic of significant social and ecological relevance due to their crucial role in the global ecosystem. Unfortunately, deforestation and degradation impact millions of hectares annually, requiring government or private initiatives for effective forest monitoring. However, identifying deforested regions in satellite images is challenging due to data imbalance, image resolution, low-contrast regions, and occlusion. Superpixel segmentation can overcome these drawbacks, reducing workload and preserving important image boundaries. However, most works for remote sensing images do not exploit recent superpixel methods. In this work, we evaluate 16 superpixel methods in satellite images to support a deforestation detection system in tropical forests. We also assess the performance of superpixel methods for the target task, establishing a relationship with segmentation methodological evaluation. According to our results, ERS, GMMSP, and DISF perform best on UE, BR, and SIRS, respectively, whereas ERS has the best trade-off with CO and Reg. In classification, SH, DISF, and ISF perform best on RGB, UMDA, and PCA compositions, respectively. According to our experiments, superpixel methods with better trade-offs between delineation, homogeneity, compactness, and regularity are more suitable for identifying good superpixels for deforestation detection tasks.

Translated Abstract:
열대 숲 보존은 전 세계 생태계에서 중요한 역할을 하기 때문에 사회적, 생태학적으로 매우 중요한 주제야. 안타깝게도, 매년 수백만 헥타르의 숲이 사라지고 있어. 그래서 정부나 민간의 효과적인 숲 모니터링이 필요해. 하지만 위성 이미지에서 삼림 파괴된 지역을 찾는 건 쉽지 않아. 데이터 불균형, 이미지 해상도, 저명암 영역, 그리고 가림 현상 같은 문제들이 있거든.

슈퍼픽셀 분할은 이런 문제들을 극복할 수 있는 방법으로, 작업 부담을 줄이고 중요한 이미지 경계를 유지할 수 있어. 그런데 대부분의 원격 탐지 이미지 연구는 최신 슈퍼픽셀 방법을 활용하지 않고 있어. 그래서 우리는 열대 숲의 삼림 파괴 탐지 시스템을 지원하기 위해 위성 이미지에서 16개의 슈퍼픽셀 방법을 평가했어. 또한, 특정 작업에 대해 슈퍼픽셀 방법의 성능을 평가하고, 분할 방법론 평가와의 관계를 설정했어.

우리 결과에 따르면, ERS, GMMSP, DISF는 각각 UE, BR, SIRS에서 가장 잘 작동했어. 그리고 ERS는 CO와 Reg 간의 균형이 가장 좋았어. 분류에서는 SH, DISF, ISF가 각각 RGB, UMDA, PCA 조합에서 가장 잘 작동했어. 실험에 따르면, 경계 설정, 동질성, 컴팩트함, 규칙성 사이에서 더 좋은 균형을 가진 슈퍼픽셀 방법이 삼림 파괴 탐지 작업에 적합하다는 걸 알 수 있었어.

================================================================================

URL: https://arxiv.org/abs/2409.04345
Title: Computer-Generated Sand Mixtures and Sand-based Images

Original Abstract:
This paper aims to verify the effectiveness of the software implementation of the proposed algorithm in creating computer-generated images of sand mixtures using a photograph of sand as an input and its effectiveness in converting digital pictures into sand-based images out of the mixtures it generated. The method of this paper is to visually compare the photographed image of the actual mixtures to its computer-generated counterpart to verify if the mixture generation produces results as expected and compare the computer-generated sand-based images with its source to verify image reproduction maintains same image content. The results of the mixture comparison shows that the actual and the computer-generated ones have similar overall shade and color. Still, the generated one has a rougher texture and higher contrast due to the method of inheriting visual features by pixel, not by individual sand particles. The comparison of the sand-based image and its source has demonstrated the software's ability to maintain the essence of its contents during conversion while replacing its texture with the visual properties of the generated sand mixture. The result have shown that the software implementation of the proposed algorithm can effectively use the images of sand to generate images of its mixtures and use those mixture images to convert a digital picture into a computer-generated sand-based image.

Translated Abstract:
이 논문은 제안된 알고리즘의 소프트웨어 구현이 어떻게 모래 사진을 입력으로 해서 모래 혼합물의 컴퓨터 생성 이미지를 만드는 데 효과적인지 확인하는 걸 목표로 해. 그리고 생성한 혼합물에서 디지털 사진을 모래 기반 이미지로 변환하는 것도 검증하려고 해.

이 논문의 방법은 실제 혼합물 사진과 컴퓨터에서 생성한 이미지를 시각적으로 비교하는 거야. 이렇게 해서 혼합물 생성 결과가 예상대로 나왔는지 확인하고, 컴퓨터 생성된 모래 기반 이미지를 그 원본과 비교해서 이미지 내용이 유지되는지 확인하는 거지.

비교 결과, 실제 이미지와 컴퓨터 생성된 이미지의 전체적인 색조와 색깔은 비슷해. 하지만 생성된 이미지는 질감이 더 거칠고 대비가 더 강해. 그 이유는 시각적 특징을 개별 모래 입자가 아니라 픽셀 단위로 상속받기 때문이야.

모래 기반 이미지와 원본의 비교를 통해 소프트웨어가 변환 과정에서 내용의 본질을 유지하면서 질감은 생성된 모래 혼합물의 시각적 특성으로 대체할 수 있는 능력을 보여줬어. 결과적으로, 제안된 알고리즘의 소프트웨어 구현은 모래 이미지를 사용해 혼합물 이미지를 효과적으로 생성하고, 그 혼합물 이미지를 이용해 디지털 사진을 컴퓨터 생성된 모래 기반 이미지로 변환할 수 있다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.04356
Title: Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with Selective State-Space Model

Original Abstract:
Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) images capture high-resolution views of the retina with typically 200 spanning degrees. Accurate segmentation of vessels in UWF-SLO images is essential for detecting and diagnosing fundus disease. Recent studies have revealed that the selective State Space Model (SSM) in Mamba performs well in modeling long-range dependencies, which is crucial for capturing the continuity of elongated vessel structures. Inspired by this, we propose the first Serpentine Mamba (Serp-Mamba) network to address this challenging task. Specifically, we recognize the intricate, varied, and delicate nature of the tubular structure of vessels. Furthermore, the high-resolution of UWF-SLO images exacerbates the imbalance between the vessel and background categories. Based on the above observations, we first devise a Serpentine Interwoven Adaptive (SIA) scan mechanism, which scans UWF-SLO images along curved vessel structures in a snake-like crawling manner. This approach, consistent with vascular texture transformations, ensures the effective and continuous capture of curved vascular structure features. Second, we propose an Ambiguity-Driven Dual Recalibration (ADDR) module to address the category imbalance problem intensified by high-resolution images. Our ADDR module delineates pixels by two learnable thresholds and refines ambiguous pixels through a dual-driven strategy, thereby accurately distinguishing vessels and background regions. Experiment results on three datasets demonstrate the superior performance of our Serp-Mamba on high-resolution vessel segmentation. We also conduct a series of ablation studies to verify the impact of our designs. Our code shall be released upon publication of this work.

Translated Abstract:
초광대역 스캐닝 레이저 안저촬영(UWF-SLO) 이미지는 보통 200도 정도의 고해상도 망막 이미지를 포착해. 이 이미지에서 혈관을 정확하게 구분하는 건 안저 질환을 발견하고 진단하는 데 꼭 필요해. 최근 연구에서는 Mamba에서 선택적 상태 공간 모델(SSM)이 긴 거리 의존성을 잘 모델링한다는 사실이 밝혀졌어. 이건 길게 늘어진 혈관 구조의 연속성을 포착하는 데 중요해. 

이런 점에서 영감을 받아, 우리는 이 어려운 작업을 해결하기 위해 '첫 번째 세르펜타인 맘바(Serp-Mamba)' 네트워크를 제안해. 특히, 우리는 혈관의 복잡하고 다양하며 섬세한 관 구조의 특성을 인식해. 게다가 UWF-SLO 이미지의 고해상도는 혈관과 배경 카테고리 간의 불균형을 더 심화시켜. 

이런 관찰을 바탕으로, 먼저 '세르펜타인 상호 엮인 적응(SIA)' 스캔 메커니즘을 고안했어. 이 메커니즘은 UWF-SLO 이미지를 뱀처럼 구부러진 혈관 구조를 따라 스캔해. 이 방법은 혈관 질감 변화를 일관되게 반영하면서 구부러진 혈관 구조의 특징을 효과적이고 연속적으로 포착할 수 있어. 

두 번째로, 우리는 고해상도 이미지로 인해 심화된 카테고리 불균형 문제를 해결하기 위해 '모호성 기반 이중 재보정(ADDR)' 모듈을 제안해. 이 ADDR 모듈은 두 개의 학습 가능한 임계값으로 픽셀을 구분하고, 이중 구동 전략을 통해 모호한 픽셀을 정제해. 그래서 혈관과 배경 영역을 정확하게 구별할 수 있어. 

세 가지 데이터셋에서 실험 결과, 우리의 세르-Mamba가 고해상도 혈관 분할에서 우수한 성능을 보였어. 또한, 우리의 디자인이 미치는 영향을 검증하기 위해 일련의 소거 연구도 진행했어. 우리의 코드는 이 연구가 발표되면 공개될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.04360
Title: Connectivity-Inspired Network for Context-Aware Recognition

Original Abstract:
The aim of this paper is threefold. We inform the AI practitioner about the human visual system with an extensive literature review; we propose a novel biologically motivated neural network for image classification; and, finally, we present a new plug-and-play module to model context awareness. We focus on the effect of incorporating circuit motifs found in biological brains to address visual recognition. Our convolutional architecture is inspired by the connectivity of human cortical and subcortical streams, and we implement bottom-up and top-down modulations that mimic the extensive afferent and efferent connections between visual and cognitive areas. Our Contextual Attention Block is simple and effective and can be integrated with any feed-forward neural network. It infers weights that multiply the feature maps according to their causal influence on the scene, modeling the co-occurrence of different objects in the image. We place our module at different bottlenecks to infuse a hierarchical context awareness into the model. We validated our proposals through image classification experiments on benchmark data and found a consistent improvement in performance and the robustness of the produced explanations via class activation. Our code is available at this https URL.

Translated Abstract:
이 논문의 목표는 세 가지에요. 첫째, AI 실무자들에게 인간의 시각 시스템에 대한 폭넓은 문헌 리뷰를 제공하고, 둘째, 이미지 분류를 위한 새로운 생물학적으로 영감을 받은 신경망을 제안하고, 마지막으로 새로운 플러그 앤 플레이 모듈을 통해 상황 인식을 모델링하는 거예요.

우리는 생물학적 뇌에서 발견되는 회로 모티프를 포함하는 것이 시각 인식에 미치는 영향을 중점적으로 다뤘어요. 우리의 컨볼루션 구조는 인간의 피질과 피질 하부의 연결성을 바탕으로 영감을 받았고, 시각 영역과 인지 영역 간의 광범위한 신경 연결을 모방하는 하향식 및 상향식 조절을 구현했어요.

우리의 Contextual Attention Block은 간단하면서도 효과적이며, 어떤 피드 포워드 신경망과도 통합할 수 있어요. 이 블록은 장면에 대한 원인적 영향을 바탕으로 특징 맵에 곱해지는 가중치를 추론해서 이미지 내 다양한 객체의 동시 발생을 모델링해요. 우리는 이 모듈을 다양한 병목 지점에 배치해 모델에 계층적 상황 인식을 주입했어요.

우리는 기준 데이터에 대한 이미지 분류 실험을 통해 제안된 방법을 검증했고, 성능과 생성된 설명의 견고성에서 일관된 개선을 발견했어요. 우리의 코드는 이 URL에서 확인할 수 있어요.

================================================================================

URL: https://arxiv.org/abs/2409.04363
Title: RCNet: Deep Recurrent Collaborative Network for Multi-View Low-Light Image Enhancement

Original Abstract:
Scene observation from multiple perspectives would bring a more comprehensive visual experience. However, in the context of acquiring multiple views in the dark, the highly correlated views are seriously alienated, making it challenging to improve scene understanding with auxiliary views. Recent single image-based enhancement methods may not be able to provide consistently desirable restoration performance for all views due to the ignorance of potential feature correspondence among different views. To alleviate this issue, we make the first attempt to investigate multi-view low-light image enhancement. First, we construct a new dataset called Multi-View Low-light Triplets (MVLT), including 1,860 pairs of triple images with large illumination ranges and wide noise distribution. Each triplet is equipped with three different viewpoints towards the same scene. Second, we propose a deep multi-view enhancement framework based on the Recurrent Collaborative Network (RCNet). Specifically, in order to benefit from similar texture correspondence across different views, we design the recurrent feature enhancement, alignment and fusion (ReEAF) module, in which intra-view feature enhancement (Intra-view EN) followed by inter-view feature alignment and fusion (Inter-view AF) is performed to model the intra-view and inter-view feature propagation sequentially via multi-view collaboration. In addition, two different modules from enhancement to alignment (E2A) and from alignment to enhancement (A2E) are developed to enable the interactions between Intra-view EN and Inter-view AF, which explicitly utilize attentive feature weighting and sampling for enhancement and alignment, respectively. Experimental results demonstrate that our RCNet significantly outperforms other state-of-the-art methods. All of our dataset, code, and model will be available at this https URL.

Translated Abstract:
여러 관점에서 장면을 관찰하면 더 풍부한 시각적 경험을 얻을 수 있어. 하지만 어두운 환경에서 여러 뷰를 얻으려고 하면, 서로 강하게 연관된 뷰들이 크게 달라져서 보조 뷰를 통해 장면 이해를 개선하는 게 어려워져. 최근의 단일 이미지 기반 향상 방법들은 서로 다른 뷰 간의 잠재적인 특징 대응을 무시하기 때문에 모든 뷰에 대해 일관되게 좋은 복원 성능을 제공하지 못할 수도 있어.

이 문제를 해결하기 위해, 우리는 다중 뷰 저조도 이미지 향상을 조사하는 첫 번째 시도를 해봤어. 먼저, 조명 범위가 넓고 잡음 분포가 다양한 1,860 쌍의 트리플 이미지를 포함한 새로운 데이터셋인 Multi-View Low-light Triplets (MVLT)를 만들었어. 각 트리플은 같은 장면을 향한 세 가지 다른 시점으로 구성되어 있어.

두 번째로, 우리는 Recurrent Collaborative Network (RCNet) 기반의 심층 다중 뷰 향상 프레임워크를 제안했어. 특히, 서로 다른 뷰에서 비슷한 텍스처 대응의 이점을 얻기 위해, 우리는 반복적인 특징 향상, 정렬 및 융합(ReEAF) 모듈을 설계했어. 이 모듈에서는 먼저 내부 뷰 특징 향상(Intra-view EN)을 하고, 그 다음에 외부 뷰 특징 정렬 및 융합(Inter-view AF)을 수행해서 다중 뷰 협력을 통해 내부 및 외부 뷰 특징 전파를 순차적으로 모델링해.

또한, 향상에서 정렬로(E2A)와 정렬에서 향상으로(A2E) 가는 두 가지 모듈을 개발해, Intra-view EN과 Inter-view AF 간의 상호작용을 가능하게 했어. 이 모듈들은 각각 향상과 정렬을 위해 주의 깊은 특징 가중치와 샘플링을 활용해.

실험 결과는 우리의 RCNet이 다른 최신 방법들보다 훨씬 뛰어나다는 걸 보여줘. 우리의 데이터셋, 코드, 모델은 이 URL에서 이용할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04381
Title: Enhancing Skin Lesion Diagnosis with Ensemble Learning

Original Abstract:
Skin lesions are an increasingly significant medical concern, varying widely in severity from benign to cancerous. Accurate diagnosis is essential for ensuring timely and appropriate treatment. This study examines the implementation of deep learning methods to assist in the diagnosis of skin lesions using the HAM10000 dataset, which contains seven distinct types of lesions. First, we evaluated three pre-trained models: MobileNetV2, ResNet18, and VGG11, achieving accuracies of 0.798, 0.802, and 0.805, respectively. To further enhance classification accuracy, we developed ensemble models employing max voting, average voting, and stacking, resulting in accuracies of 0.803, 0.82, and 0.83. Building on the best-performing ensemble learning model, stacking, we developed our proposed model, SkinNet, which incorporates a customized architecture and fine-tuning, achieving an accuracy of 0.867 and an AUC of 0.96. This substantial improvement over individual models demonstrates the effectiveness of ensemble learning in improving skin lesion classification.

Translated Abstract:
피부 병변은 점점 더 큰 의료 문제로 떠오르고 있어. 그 심각성은 양성부터 암까지 다양해. 정확한 진단이 중요하고, 그래야 적절한 치료를 제때 받을 수 있어. 

이 연구는 HAM10000 데이터셋을 사용해서 피부 병변 진단을 도와주는 딥러닝 방법을 살펴봤어. 이 데이터셋에는 일곱 가지 다른 유형의 병변이 포함되어 있어. 먼저, 세 가지 사전 훈련된 모델인 MobileNetV2, ResNet18, VGG11을 평가했더니 각각 0.798, 0.802, 0.805의 정확도를 기록했어. 

더 나은 분류 정확도를 위해서, 우리는 최대 투표, 평균 투표, 스태킹을 사용하는 앙상블 모델을 개발했어. 이 모델들의 정확도는 각각 0.803, 0.82, 0.83이었어. 가장 성능이 좋은 앙상블 학습 모델인 스태킹을 기반으로 해서, 우리는 SkinNet이라는 제안 모델을 만들었어. 이 모델은 맞춤형 아키텍처와 세부 조정을 포함해서 0.867의 정확도와 0.96의 AUC를 달성했어. 

이렇게 개별 모델들보다 훨씬 나은 성능을 보여준 건 앙상블 학습이 피부 병변 분류에 효과적이라는 걸 보여주는 거야.

================================================================================

URL: https://arxiv.org/abs/2409.04384
Title: Empirical Bayesian image restoration by Langevin sampling with a denoising diffusion implicit prior

Original Abstract:
Score-based diffusion methods provide a powerful strategy to solve image restoration tasks by flexibly combining a pre-trained foundational prior model with a likelihood function specified during test time. Such methods are predominantly derived from two stochastic processes: reversing Ornstein-Uhlenbeck, which underpins the celebrated denoising diffusion probabilistic models (DDPM) and denoising diffusion implicit models (DDIM), and the Langevin diffusion process. The solutions delivered by DDPM and DDIM are often remarkably realistic, but they are not always consistent with measurements because of likelihood intractability issues and the associated required approximations. Alternatively, using a Langevin process circumvents the intractable likelihood issue, but usually leads to restoration results of inferior quality and longer computing times. This paper presents a novel and highly computationally efficient image restoration method that carefully embeds a foundational DDPM denoiser within an empirical Bayesian Langevin algorithm, which jointly calibrates key model hyper-parameters as it estimates the model's posterior mean. Extensive experimental results on three canonical tasks (image deblurring, super-resolution, and inpainting) demonstrate that the proposed approach improves on state-of-the-art strategies both in image estimation accuracy and computing time.

Translated Abstract:
점수 기반 확산 방법은 이미지 복원 작업을 해결하는 강력한 전략을 제공해. 이 방법은 미리 훈련된 기초 모델과 테스트 시 지정된 가능성 함수를 유연하게 결합하는 방식이야.

이러한 방법은 주로 두 가지 확률 과정에서 파생돼: 유명한 노이즈 제거 확산 확률 모델(DDPM)과 노이즈 제거 확산 암묵적 모델(DDIM)의 기초가 되는 오른스타인-울렌벡 과정과 랑주뱅 확산 과정이야. DDPM과 DDIM이 제공하는 솔루션은 꽤 현실적이지만, 가능성의 비가시성 문제와 관련된 근사치 때문에 측정값과 항상 일치하지는 않아.

반대로, 랑주뱅 과정을 사용하면 비가시성 가능성 문제를 피할 수 있지만 보통 품질이 떨어지고 계산 시간이 길어지는 결과를 가져와. 이 논문에서는 기초 DDPM 노이저를 경험적 베이지안 랑주뱅 알고리즘에 정교하게 포함시킨 새로운 이미지 복원 방법을 제안해. 이 방법은 모델의 후방 평균을 추정하면서 주요 모델 하이퍼 파라미터를 함께 조정해.

세 가지 대표적인 작업(이미지 블러 제거, 초해상도, 인페인팅)에 대한 광범위한 실험 결과는 제안된 접근 방식이 이미지 추정 정확도와 계산 시간 모두에서 최신 전략보다 개선된다는 것을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.04388
Title: Question-Answering Dense Video Events

Original Abstract:
Multimodal Large Language Models (MLLMs) have shown excellent performance in question-answering of single-event videos. In this paper, we present question-answering dense video events, a novel task that requires answering and grounding the dense-event questions in long videos, thus challenging MLLMs to faithfully comprehend and reason about multiple events occurring over extended time periods. To facilitate the study, we construct DeVE-QA - a dataset featuring 78K questions about 26K events on 10.6K long videos. We then benchmark and show that existing MLLMs excelling at single-event QA struggle to perform well in DeVE-QA. For improvement, we propose DeVi, a novel training-free MLLM approach that highlights a hierarchical captioning module, a temporal event memory module, and a self-consistency checking module to respectively detect, contextualize and memorize, and ground dense-events in long videos for question answering. Extensive experiments show that DeVi is superior at answering dense-event questions and grounding relevant video moments. Compared with existing MLLMs, it achieves a remarkable increase of 4.1 percent and 3.7 percent for G(round)QA accuracy on DeVE-QA and NExT-GQA respectively.

Translated Abstract:
다중 모달 대형 언어 모델(MLLMs)은 단일 이벤트 비디오의 질문-답변에서 뛰어난 성능을 보여줬어. 이 논문에서는 긴 비디오에서 밀집 이벤트 질문에 답하고 그 질문을 토대로 이해하는 새로운 작업인 질문-답변 밀집 비디오 이벤트를 소개해. 이 작업은 MLLMs가 오랜 시간 동안 발생하는 여러 사건을 잘 이해하고 추론해야 하니까 도전적이야.

이 연구를 위해 우리는 DeVE-QA라는 데이터셋을 만들었어. 이 데이터셋은 10.6K개의 긴 비디오에서 26K 이벤트에 대한 78K 질문을 포함하고 있어. 그리고 기존의 MLLMs는 단일 이벤트 QA에서는 잘하지만, DeVE-QA에서는 제대로 성능을 내지 못하는 걸 보여줬어.

개선을 위해 우리는 DeVi라는 새로운 훈련 없는 MLLM 접근법을 제안해. DeVi는 계층적 캡셔닝 모듈, 시간 이벤트 메모리 모듈, 자기 일관성 확인 모듈을 강조해. 이 모듈들은 각각 밀집 이벤트를 감지하고, 맥락을 제공하고, 기억하고, 긴 비디오에서 질문에 답하기 위해 밀집 이벤트를 기반으로 하는 역할을 해.

많은 실험을 통해 DeVi가 밀집 이벤트 질문에 답하고 관련 비디오 순간을 잘 찾는 데 뛰어나다는 걸 보여줬어. 기존 MLLMs와 비교했을 때, DeVE-QA와 NExT-GQA에서 각각 4.1%와 3.7%의 G(round)QA 정확도를 크게 향상시켰어.

================================================================================

URL: https://arxiv.org/abs/2409.04390
Title: Future Does Matter: Boosting 3D Object Detection with Temporal Motion Estimation in Point Cloud Sequences

Original Abstract:
Accurate and robust LiDAR 3D object detection is essential for comprehensive scene understanding in autonomous driving. Despite its importance, LiDAR detection performance is limited by inherent constraints of point cloud data, particularly under conditions of extended distances and occlusions. Recently, temporal aggregation has been proven to significantly enhance detection accuracy by fusing multi-frame viewpoint information and enriching the spatial representation of objects. In this work, we introduce a novel LiDAR 3D object detection framework, namely LiSTM, to facilitate spatial-temporal feature learning with cross-frame motion forecasting information. We aim to improve the spatial-temporal interpretation capabilities of the LiDAR detector by incorporating a dynamic prior, generated from a non-learnable motion estimation model. Specifically, Motion-Guided Feature Aggregation (MGFA) is proposed to utilize the object trajectory from previous and future motion states to model spatial-temporal correlations into gaussian heatmap over a driving sequence. This motion-based heatmap then guides the temporal feature fusion, enriching the proposed object features. Moreover, we design a Dual Correlation Weighting Module (DCWM) that effectively facilitates the interaction between past and prospective frames through scene- and channel-wise feature abstraction. In the end, a cascade cross-attention-based decoder is employed to refine the 3D prediction. We have conducted experiments on the Waymo and nuScenes datasets to demonstrate that the proposed framework achieves superior 3D detection performance with effective spatial-temporal feature learning.

Translated Abstract:
정확하고 강력한 LiDAR 3D 객체 감지는 자율주행에서 장면을 잘 이해하는 데 꼭 필요해. 그런데 LiDAR 감지 성능은 포인트 클라우드 데이터의 본질적인 제약 때문에 제한돼. 특히, 먼 거리나 가려진 상황에서 더 그렇지. 최근에는 여러 프레임의 시점 정보를 합쳐서 공간 정보를 풍부하게 만들면 감지 정확도가 크게 향상된다는 게 입증됐어.

이번 연구에서는 LiSTM이라는 새로운 LiDAR 3D 객체 감지 프레임워크를 소개할 거야. 이 프레임워크는 프레임 간의 움직임 예측 정보를 활용해 공간-시간 특성 학습을 도와줘. 우리는 비학습 가능한 움직임 추정 모델에서 생성된 동적 사전 정보를 포함시켜 LiDAR 감지기의 공간-시간 해석 능력을 개선하려고 해.

특히, Motion-Guided Feature Aggregation (MGFA)을 제안해서 이전과 미래의 움직임 상태에서 객체의 궤적을 활용해 공간-시간 상관관계를 가우시안 히트맵으로 모델링해. 이 움직임 기반 히트맵은 시간적 특성 융합을 안내하고, 제안된 객체 특성을 풍부하게 만들어 줘. 게다가 Dual Correlation Weighting Module (DCWM)을 설계해서 과거와 미래의 프레임 간 상호작용을 장면과 채널 단위로 효과적으로 추상화해.

마지막으로, 3D 예측을 다듬기 위해 cascade cross-attention 기반 디코더를 사용해. Waymo와 nuScenes 데이터셋에서 실험을 진행해본 결과, 제안된 프레임워크가 효과적인 공간-시간 특성 학습 덕분에 3D 감지 성능이 뛰어나다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.04398
Title: HiSC4D: Human-centered interaction and 4D Scene Capture in Large-scale Space Using Wearable IMUs and LiDAR

Original Abstract:
We introduce HiSC4D, a novel Human-centered interaction and 4D Scene Capture method, aimed at accurately and efficiently creating a dynamic digital world, containing large-scale indoor-outdoor scenes, diverse human motions, rich human-human interactions, and human-environment interactions. By utilizing body-mounted IMUs and a head-mounted LiDAR, HiSC4D can capture egocentric human motions in unconstrained space without the need for external devices and pre-built maps. This affords great flexibility and accessibility for human-centered interaction and 4D scene capturing in various environments. Taking into account that IMUs can capture human spatially unrestricted poses but are prone to drifting for long-period using, and while LiDAR is stable for global localization but rough for local positions and orientations, HiSC4D employs a joint optimization method, harmonizing all sensors and utilizing environment cues, yielding promising results for long-term capture in large scenes. To promote research of egocentric human interaction in large scenes and facilitate downstream tasks, we also present a dataset, containing 8 sequences in 4 large scenes (200 to 5,000 $m^2$), providing 36k frames of accurate 4D human motions with SMPL annotations and dynamic scenes, 31k frames of cropped human point clouds, and scene mesh of the environment. A variety of scenarios, such as the basketball gym and commercial street, alongside challenging human motions, such as daily greeting, one-on-one basketball playing, and tour guiding, demonstrate the effectiveness and the generalization ability of HiSC4D. The dataset and code will be publicated on this http URL available for research purposes.

Translated Abstract:
우리는 HiSC4D라는 새로운 방법을 소개해. 이 방법은 사람 중심의 상호작용과 4D 장면 캡처를 목표로 해서, 동적인 디지털 세계를 정확하고 효율적으로 만드는 데 도움을 줘. 여기에는 대규모 실내외 장면, 다양한 인간 동작, 풍부한 사람 간 상호작용, 그리고 사람과 환경 간의 상호작용이 포함돼.

HiSC4D는 몸에 장착한 IMU와 머리에 장착한 LiDAR를 사용해서, 외부 장치나 미리 만들어진 지도가 없어도 자유로운 공간에서 사람의 동작을 캡처할 수 있어. 이 덕분에 다양한 환경에서 사람 중심의 상호작용과 4D 장면 캡처를 더 유연하고 접근하기 쉽게 할 수 있어.

IMU는 사람의 제약 없는 자세를 캡처할 수 있지만, 오랫동안 사용하면 드리프트가 발생할 수 있어. 반면 LiDAR는 전역 위치 파악은 안정적이지만, 지역 위치와 방향 측면에서는 다소 거칠어. 그래서 HiSC4D는 모든 센서를 조화롭게 사용하고 환경의 단서를 활용하는 공동 최적화 방법을 적용해, 대규모 장면에서 장기간 캡처를 위한 좋은 결과를 얻고 있어.

우리는 또한 대규모 장면에서의 에고 중심 인간 상호작용 연구를 촉진하고 후속 작업을 쉽게 하기 위해 데이터셋도 공개할 거야. 이 데이터셋은 4개의 큰 장면(200에서 5,000 $m^2$)에서 8개의 시퀀스를 포함하고, 36,000개의 정확한 4D 인간 동작과 SMPL 주석, 동적 장면을 제공해. 또 31,000개의 잘린 인간 포인트 클라우드와 환경의 장면 메쉬도 포함돼. 농구 체육관이나 상업 거리 같은 다양한 상황과 일상적인 인사, 일대일 농구, 투어 가이드 같은 도전적인 인간 동작들이 HiSC4D의 효과와 일반화 능력을 보여줘. 데이터셋과 코드는 연구 목적으로 사용할 수 있도록 공개될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.04409
Title: Train Till You Drop: Towards Stable and Robust Source-free Unsupervised 3D Domain Adaptation

Original Abstract:
We tackle the challenging problem of source-free unsupervised domain adaptation (SFUDA) for 3D semantic segmentation. It amounts to performing domain adaptation on an unlabeled target domain without any access to source data; the available information is a model trained to achieve good performance on the source domain. A common issue with existing SFUDA approaches is that performance degrades after some training time, which is a by product of an under-constrained and ill-posed problem. We discuss two strategies to alleviate this issue. First, we propose a sensible way to regularize the learning problem. Second, we introduce a novel criterion based on agreement with a reference model. It is used (1) to stop the training when appropriate and (2) as validator to select hyperparameters without any knowledge on the target domain. Our contributions are easy to implement and readily amenable for all SFUDA methods, ensuring stable improvements over all baselines. We validate our findings on various 3D lidar settings, achieving state-of-the-art performance. The project repository (with code) is: this http URL.

Translated Abstract:
우리는 3D 의미 분할을 위한 소스 없는 비지도 도메인 적응(SFUDA) 문제를 다뤄. 소스 데이터에 접근하지 않고 레이블이 없는 타겟 도메인에서 도메인 적응을 수행하는 거야. 사용할 수 있는 정보는 소스 도메인에서 좋은 성능을 내도록 훈련된 모델이야.

기존 SFUDA 방법들의 공통적인 문제는 훈련 시간이 지나면 성능이 떨어진다는 거야. 이건 제약이 부족하고 잘못 설정된 문제에서 오는 결과야. 그래서 우리는 이 문제를 해결하기 위한 두 가지 전략을 제안해.

첫 번째는 학습 문제를 규제하는 합리적인 방법을 제안해. 두 번째는 참조 모델과의 일치를 기반으로 한 새로운 기준을 도입해. 이 기준은 (1) 적절할 때 훈련을 멈추고, (2) 타겟 도메인에 대한 지식 없이 하이퍼파라미터를 선택하는 검증자로 사용돼.

우리의 기여는 구현하기 쉽고 모든 SFUDA 방법에 적용 가능해. 이로 인해 모든 기준선에서 안정적인 개선을 보장해. 우리는 다양한 3D 라이다 환경에서 우리의 발견을 검증했고, 최첨단 성능을 달성했어. 프로젝트 저장소(코드 포함)는 이 URL에 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04410
Title: Open-MAGVIT2: An Open-Source Project Toward Democratizing Auto-regressive Visual Generation

Original Abstract:
We present Open-MAGVIT2, a family of auto-regressive image generation models ranging from 300M to 1.5B. The Open-MAGVIT2 project produces an open-source replication of Google's MAGVIT-v2 tokenizer, a tokenizer with a super-large codebook (i.e., $2^{18}$ codes), and achieves the state-of-the-art reconstruction performance (1.17 rFID) on ImageNet $256 \times 256$. Furthermore, we explore its application in plain auto-regressive models and validate scalability properties. To assist auto-regressive models in predicting with a super-large vocabulary, we factorize it into two sub-vocabulary of different sizes by asymmetric token factorization, and further introduce "next sub-token prediction" to enhance sub-token interaction for better generation quality. We release all models and codes to foster innovation and creativity in the field of auto-regressive visual generation.

Translated Abstract:
우리는 Open-MAGVIT2라는 새로운 이미지 생성 모델 시리즈를 소개해. 이 모델들은 300M에서 1.5B까지의 크기를 가지고 있어. Open-MAGVIT2 프로젝트는 구글의 MAGVIT-v2 토크나이저를 오픈소스로 복제한 거고, 이 토크나이저는 엄청 큰 코드북(즉, $2^{18}$ 코드)을 가지고 있어. 이 모델은 ImageNet $256 \times 256$에서 최신 복원 성능인 1.17 rFID를 달성했어.

그리고 우리는 이 모델을 일반적인 자기 회귀 모델에 적용해 보고, 확장성도 검증했어. 자기 회귀 모델이 엄청 큰 어휘로 예측할 수 있도록 돕기 위해, 비대칭 토큰 분해를 사용해 두 개의 크기가 다른 서브 어휘로 나눴고, 더 나은 생성 품질을 위해 "다음 서브 토큰 예측"이라는 방법도 도입했어. 

우리는 모든 모델과 코드를 공개해서 자기 회귀 기반 비주얼 생성 분야에서 혁신과 창의성을 촉진하고자 해.

================================================================================

URL: https://arxiv.org/abs/2409.04429
Title: VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation

Original Abstract:
VILA-U is a Unified foundation model that integrates Video, Image, Language understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content, which can lead to misalignment and increased complexity. In contrast, VILA-U employs a single autoregressive next-token prediction framework for both tasks, eliminating the need for additional components like diffusion models. This approach not only simplifies the model but also achieves near state-of-the-art performance in visual language understanding and generation. The success of VILA-U is attributed to two main factors: the unified vision tower that aligns discrete visual tokens with textual inputs during pretraining, which enhances visual perception, and autoregressive image generation can achieve similar quality as diffusion models with high-quality dataset. This allows VILA-U to perform comparably to more complex models using a fully token-based autoregressive framework.

Translated Abstract:
VILA-U는 비디오, 이미지, 언어 이해 및 생성을 통합한 통합 기반 모델이야. 전통적인 시각 언어 모델(VLM)은 시각 콘텐츠를 이해하고 생성하기 위해 별도의 모듈을 사용하는데, 이러면 정렬이 잘 안 되거나 복잡해질 수 있어. 

반면에 VILA-U는 두 가지 작업을 위해 단일 자동 회귀 다음 토큰 예측 프레임워크를 사용해. 그래서 확산 모델 같은 추가 구성 요소가 필요 없어. 이 방식은 모델을 간소화할 뿐만 아니라 시각 언어 이해와 생성에서 거의 최첨단 성능을 달성해. 

VILA-U의 성공은 두 가지 주요 요소 덕분이야. 첫 번째는 사전 훈련 중에 분리된 시각 토큰과 텍스트 입력을 정렬하는 통합 비전 타워로, 이게 시각 인식을 향상시켜. 두 번째는 자동 회귀 이미지 생성이 고품질 데이터셋을 사용하면 확산 모델과 비슷한 품질을 낼 수 있다는 거야. 이 덕분에 VILA-U는 완전히 토큰 기반 자동 회귀 프레임워크를 사용해 더 복잡한 모델들과 비슷한 성능을 낼 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04440
Title: Synergy and Synchrony in Couple Dances

Original Abstract:
This paper asks to what extent social interaction influences one's behavior. We study this in the setting of two dancers dancing as a couple. We first consider a baseline in which we predict a dancer's future moves conditioned only on their past motion without regard to their partner. We then investigate the advantage of taking social information into account by conditioning also on the motion of their dancing partner. We focus our analysis on Swing, a dance genre with tight physical coupling for which we present an in-the-wild video dataset. We demonstrate that single-person future motion prediction in this context is challenging. Instead, we observe that prediction greatly benefits from considering the interaction partners' behavior, resulting in surprisingly compelling couple dance synthesis results (see supp. video). Our contributions are a demonstration of the advantages of socially conditioned future motion prediction and an in-the-wild, couple dance video dataset to enable future research in this direction. Video results are available on the project website: this https URL

Translated Abstract:
이 논문은 사회적 상호작용이 개인 행동에 얼마나 영향을 미치는지를 다뤄. 우리는 두 댄서가 커플로 춤추는 상황에서 이걸 연구해. 

먼저, 파트너는 고려하지 않고 오직 자신만의 과거 동작만으로 미래 동작을 예측하는 기본적인 상황을 생각해봐. 그다음엔, 춤추는 파트너의 동작도 고려할 때의 장점을 조사해. 우리는 스윙이라는 댄스 장르에 집중하고, 이 장르의 물리적 결합이 강한 상황에서 자연스럽게 촬영한 비디오 데이터셋을 보여줘. 

이런 맥락에서 한 사람의 미래 동작 예측이 어렵다는 걸 보여줘. 대신, 파트너의 행동을 고려할 때 예측이 크게 향상된다는 걸 관찰했어. 그래서 놀라운 커플 댄스 합성 결과를 얻었지 (보조 비디오 참조). 

우리의 기여는 사회적으로 조건화된 미래 동작 예측의 장점을 보여주는 것과, 앞으로 이 방향으로 연구를 지원할 수 있는 커플 댄스 비디오 데이터셋을 만드는 거야. 비디오 결과는 프로젝트 웹사이트에서 확인할 수 있어: 이 URL.

================================================================================

URL: https://arxiv.org/abs/2409.03772
Title: Exploiting XAI maps to improve MS lesion segmentation and detection in MRI

Original Abstract:
To date, several methods have been developed to explain deep learning algorithms for classification tasks. Recently, an adaptation of two of such methods has been proposed to generate instance-level explainable maps in a semantic segmentation scenario, such as multiple sclerosis (MS) lesion segmentation. In the mentioned work, a 3D U-Net was trained and tested for MS lesion segmentation, yielding an F1 score of 0.7006, and a positive predictive value (PPV) of 0.6265. The distribution of values in explainable maps exposed some differences between maps of true and false positive (TP/FP) examples. Inspired by those results, we explore in this paper the use of characteristics of lesion-specific saliency maps to refine segmentation and detection scores. We generate around 21000 maps from as many TP/FP lesions in a batch of 72 patients (training set) and 4868 from the 37 patients in the test set. 93 radiomic features extracted from the first set of maps were used to train a logistic regression model and classify TP versus FP. On the test set, F1 score and PPV were improved by a large margin when compared to the initial model, reaching 0.7450 and 0.7817, with 95% confidence intervals of [0.7358, 0.7547] and [0.7679, 0.7962], respectively. These results suggest that saliency maps can be used to refine prediction scores, boosting a model's performances.

Translated Abstract:
지금까지 여러 가지 방법이 분류 작업을 위한 딥러닝 알고리즘을 설명하는 데 개발됐어. 최근에는 이러한 방법 두 가지를 수정해서 다발성 경화증(MS) 병변 분할 같은 의미론적 분할 시나리오에서 인스턴스 수준의 설명 가능한 맵을 생성하는 방법이 제안됐어.

이 연구에서는 3D U-Net을 훈련하고 테스트해서 MS 병변 분할을 했고, F1 점수는 0.7006, 긍정 예측 값(PPV)은 0.6265가 나왔어. 설명 가능한 맵의 값 분포를 보면 진짜 양성(TP)과 가짜 양성(FP) 예시의 맵 사이에 차이가 있었어. 이 결과에 영감을 받아서, 우리는 병변 특화된 주목 맵의 특성을 활용해 분할과 탐지 점수를 개선하는 방법을 탐구했어.

72명의 환자에서 약 21,000개의 TP/FP 병변 맵을 만들었고, 37명의 환자 테스트 세트에서 4,868개의 맵을 생성했어. 첫 번째 맵 세트에서 추출한 93개의 방사형 특징을 사용해서 로지스틱 회귀 모델을 훈련하고 TP와 FP를 분류했어. 테스트 세트에서는 초기 모델과 비교했을 때 F1 점수와 PPV가 크게 개선돼서 각각 0.7450과 0.7817에 도달했어. 신뢰 구간은 각각 [0.7358, 0.7547]과 [0.7679, 0.7962]였어. 

이 결과는 주목 맵이 예측 점수를 개선하는 데 사용될 수 있다는 걸 보여줘, 모델의 성능을 높이는 데 도움이 된다는 거지.

================================================================================

URL: https://arxiv.org/abs/2409.03794
Title: Evaluating Machine Learning-based Skin Cancer Diagnosis

Original Abstract:
This study evaluates the reliability of two deep learning models for skin cancer detection, focusing on their explainability and fairness. Using the HAM10000 dataset of dermatoscopic images, the research assesses two convolutional neural network architectures: a MobileNet-based model and a custom CNN model. Both models are evaluated for their ability to classify skin lesions into seven categories and to distinguish between dangerous and benign lesions. Explainability is assessed using Saliency Maps and Integrated Gradients, with results interpreted by a dermatologist. The study finds that both models generally highlight relevant features for most lesion types, although they struggle with certain classes like seborrheic keratoses and vascular lesions. Fairness is evaluated using the Equalized Odds metric across sex and skin tone groups. While both models demonstrate fairness across sex groups, they show significant disparities in false positive and false negative rates between light and dark skin tones. A Calibrated Equalized Odds postprocessing strategy is applied to mitigate these disparities, resulting in improved fairness, particularly in reducing false negative rate differences. The study concludes that while the models show promise in explainability, further development is needed to ensure fairness across different skin tones. These findings underscore the importance of rigorous evaluation of AI models in medical applications, particularly in diverse population groups.

Translated Abstract:
이 연구는 피부암 탐지를 위한 두 개의 딥러닝 모델의 신뢰성을 평가하고, 그 설명 가능성과 공정성에 초점을 맞추고 있어. HAM10000 데이터셋의 피부경 검사 이미지를 사용해서, 두 가지 컨볼루션 신경망 구조인 MobileNet 기반 모델과 맞춤형 CNN 모델을 평가했어. 두 모델이 피부 병변을 일곱 가지 카테고리로 분류하고, 위험한 병변과 양성 병변을 구분하는 능력을 평가했지.

설명 가능성은 Saliency Maps와 Integrated Gradients를 사용해서 평가했고, 결과는 피부과 의사가 해석했어. 연구 결과, 두 모델 모두 대부분의 병변 유형에 대해 관련된 특징을 강조하지만, 지루성 각화증(seborrheic keratoses)과 혈관 병변 같은 특정 클래스에서는 어려움을 겪는다고 해.

공정성은 성별과 피부 톤 그룹에 대한 Equalized Odds 메트릭을 사용해서 평가했어. 두 모델 모두 성별 그룹에서는 공정성을 보여줬지만, 밝은 피부 톤과 어두운 피부 톤 사이의 거짓 긍정률과 거짓 부정률에서 큰 차이가 나타났어. 이런 차이를 완화하기 위해 Calibrated Equalized Odds 후처리 전략을 적용했더니, 특히 거짓 부정률 차이를 줄이면서 공정성이 개선됐어.

연구는 모델들이 설명 가능성에서는 가능성을 보이지만, 다양한 피부 톤 간의 공정성을 보장하기 위해서는 추가 개발이 필요하다고 결론짓고 있어. 이 연구 결과는 다양한 인구 집단에서 의료 응용 분야의 AI 모델을 철저히 평가하는 것이 중요하다는 점을 강조해.

================================================================================

URL: https://arxiv.org/abs/2409.03806
Title: Mpox Screen Lite: AI-Driven On-Device Offline Mpox Screening for Low-Resource African Mpox Emergency Response

Original Abstract:
Background: The 2024 Mpox outbreak, particularly severe in Africa with clade 1b emergence, has highlighted critical gaps in diagnostic capabilities in resource-limited settings. This study aimed to develop and validate an artificial intelligence (AI)-driven, on-device screening tool for Mpox, designed to function offline in low-resource environments.
Methods: We developed a YOLOv8n-based deep learning model trained on 2,700 images (900 each of Mpox, other skin conditions, and normal skin), including synthetic data. The model was validated on 360 images and tested on 540 images. A larger external validation was conducted using 1,500 independent images. Performance metrics included accuracy, precision, recall, F1-score, sensitivity, and specificity.
Findings: The model demonstrated high accuracy (96%) in the final test set. For Mpox detection, it achieved 93% precision, 97% recall, and an F1-score of 95%. Sensitivity and specificity for Mpox detection were 97% and 96%, respectively. Performance remained consistent in the larger external validation, confirming the model's robustness and generalizability.
Interpretation: This AI-driven screening tool offers a rapid, accurate, and scalable solution for Mpox detection in resource-constrained settings. Its offline functionality and high performance across diverse datasets suggest significant potential for improving Mpox surveillance and management, particularly in areas lacking traditional diagnostic infrastructure.

Translated Abstract:
배경: 2024년 Mpox 발생이 특히 아프리카에서 심각하게 나타났고, 1b 클레이드가 등장하면서 자원이 부족한 곳에서 진단 능력의 큰 격차가 드러났어. 이 연구는 저자원 환경에서 오프라인으로 작동할 수 있는 인공지능(AI) 기반 Mpox 선별 도구를 개발하고 검증하는 것이 목표였어.

방법: 우리는 2,700장의 이미지(각각 Mpox, 다른 피부 질환, 정상 피부 이미지 900장씩)를 사용해 YOLOv8n 기반의 딥러닝 모델을 개발했어. 여기에는 합성 데이터도 포함됐고, 모델은 360장의 이미지로 검증하고 540장의 이미지로 테스트했어. 또 1,500장의 독립적인 이미지를 사용해 더 큰 외부 검증도 진행했어. 성능 지표로는 정확도, 정밀도, 재현율, F1 점수, 민감도, 특이도를 사용했어.

결과: 모델은 최종 테스트 세트에서 높은 정확도(96%)를 보여줬어. Mpox 탐지에서는 93%의 정밀도, 97%의 재현율, 95%의 F1 점수를 기록했어. Mpox 탐지에 대한 민감도와 특이도는 각각 97%와 96%였고, 더 큰 외부 검증에서도 성능이 일관되게 유지되면서 모델의 강건성과 일반화 가능성을 확인했어.

해석: 이 AI 기반 선별 도구는 자원이 제한된 환경에서 Mpox 탐지를 위한 빠르고 정확하며 확장 가능한 해결책을 제공해. 오프라인 기능과 다양한 데이터셋에서의 높은 성능 덕분에, 전통적인 진단 인프라가 부족한 지역에서 Mpox 감시와 관리 개선에 큰 잠재력이 있을 것으로 보여.

================================================================================

URL: https://arxiv.org/abs/2409.03889
Title: Recon-all-clinical: Cortical surface reconstruction and analysis of heterogeneous clinical brain MRI

Original Abstract:
Surface-based analysis of the cerebral cortex is ubiquitous in human neuroimaging with MRI. It is crucial for cortical registration, parcellation, and thickness estimation. Traditionally, these analyses require high-resolution, isotropic scans with good gray-white matter contrast, typically a 1mm T1-weighted scan. This excludes most clinical MRI scans, which are often anisotropic and lack the necessary T1 contrast. To enable large-scale neuroimaging studies using vast clinical data, we introduce recon-all-clinical, a novel method for cortical reconstruction, registration, parcellation, and thickness estimation in brain MRI scans of any resolution and contrast. Our approach employs a hybrid analysis method that combines a convolutional neural network (CNN) trained with domain randomization to predict signed distance functions (SDFs) and classical geometry processing for accurate surface placement while maintaining topological and geometric constraints. The method does not require retraining for different acquisitions, thus simplifying the analysis of heterogeneous clinical datasets. We tested recon-all-clinical on multiple datasets, including over 19,000 clinical scans. The method consistently produced precise cortical reconstructions and high parcellation accuracy across varied MRI contrasts and resolutions. Cortical thickness estimates are precise enough to capture aging effects independently of MRI contrast, although accuracy varies with slice thickness. Our method is publicly available at this https URL, enabling researchers to perform detailed cortical analysis on the huge amounts of already existing clinical MRI scans. This advancement may be particularly valuable for studying rare diseases and underrepresented populations where research-grade MRI data is scarce.

Translated Abstract:
뇌의 피질을 분석하는 건 MRI로 인간 뇌 이미징에서 흔히 사용돼. 이 분석은 피질 등록, 분할, 두께 추정에 중요해. 전통적으로 이런 분석은 고해상도, 등방성 스캔과 좋은 회색-백색 물질 대비가 필요해. 보통 1mm T1 가중치 스캔이 필요하거든. 하지만 대부분의 임상 MRI 스캔은 종종 비등방성이어서 필요한 T1 대비가 부족해.

우리는 대규모 신경 이미징 연구를 위해 "recon-all-clinical"이라는 새로운 방법을 소개해. 이 방법은 어떤 해상도와 대비의 뇌 MRI 스캔에서도 피질 재구성, 등록, 분할, 두께 추정을 가능하게 해. 우리의 접근 방식은 도메인 무작위화로 훈련된 합성곱 신경망(CNN)과 고전 기하학 처리를 결합한 하이브리드 분석 방법을 사용해. 이렇게 해서 정확한 표면 배치를 유지하면서도 위상적 및 기하학적 제약을 지킬 수 있어.

이 방법은 서로 다른 스캔에 대해 재훈련이 필요 없어, 그래서 다양한 임상 데이터셋을 쉽게 분석할 수 있어. 우리는 19,000개 이상의 임상 스캔이 포함된 여러 데이터셋에서 recon-all-clinical을 테스트했어. 이 방법은 다양한 MRI 대비와 해상도에서도 정확한 피질 재구성과 높은 분할 정확도를 계속해서 보여줬어. 피질 두께 추정도 MRI 대비와 관계없이 노화 효과를 독립적으로 포착할 만큼 정확하지만, slice 두께에 따라 정확도가 다를 수 있어.

우리 방법은 공개되어 있어서, 연구자들이 이미 존재하는 많은 임상 MRI 스캔에 대해 상세한 피질 분석을 할 수 있게 해. 이 발전은 연구용 MRI 데이터가 부족한 희귀 질병이나 소외된 집단 연구에 특히 유용할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.03977
Title: Bi-modality Images Transfer with a Discrete Process Matching Method

Original Abstract:
Recently, medical image synthesis gains more and more popularity, along with the rapid development of generative models. Medical image synthesis aims to generate an unacquired image modality, often from other observed data modalities. Synthesized images can be used for clinical diagnostic assistance, data augmentation for model training and validation or image quality improving. In the meanwhile, the flow-based models are among the successful generative models for the ability of generating realistic and high-quality synthetic images. However, most flow-based models require to calculate flow ordinary different equation (ODE) evolution steps in transfer process, for which the performances are significantly limited by heavy computation time due to a large number of time iterations. In this paper, we propose a novel flow-based model, namely Discrete Process Matching (DPM) to accomplish the bi-modality image transfer tasks. Different to other flow matching based models, we propose to utilize both forward and backward ODE flow and enhance the consistency on the intermediate images of few discrete time steps, resulting in a transfer process with much less iteration steps while maintaining high-quality generations for both modalities. Our experiments on three datasets of MRI T1/T2 and CT/MRI demonstrate that DPM outperforms other state-of-the-art flow-based methods for bi-modality image synthesis, achieving higher image quality with less computation time cost.

Translated Abstract:
최근 의료 이미지 합성이 생성 모델의 빠른 발전과 함께 점점 더 인기를 끌고 있어. 의료 이미지 합성은 다른 관측 데이터 모달리티에서 얻지 못한 이미지 모달리티를 생성하는 걸 목표로 해. 합성된 이미지는 임상 진단 지원, 모델 훈련과 검증을 위한 데이터 증강, 또는 이미지 품질 향상에 사용될 수 있어.

한편, 흐름 기반 모델은 사실적이고 고품질의 합성 이미지를 생성할 수 있는 성공적인 생성 모델 중 하나야. 하지만 대부분의 흐름 기반 모델은 전달 과정에서 흐름 보통 미분 방정식(ODE) 발전 단계를 계산해야 해서, 많은 시간 반복 때문에 성능이 크게 제한돼. 

이번 논문에서는 Discrete Process Matching(DPM)이라는 새로운 흐름 기반 모델을 제안해. 다른 흐름 매칭 기반 모델과는 다르게, 우리는 앞쪽과 뒤쪽 ODE 흐름을 모두 활용하고 몇 개의 이산 시간 단계에서 중간 이미지의 일관성을 높이는 방식을 도입했어. 이렇게 해서 더 적은 반복 단계로도 두 가지 모달리티 모두에서 고품질 생성을 유지하면서 전달 과정을 수행할 수 있게 됐지.

MRI T1/T2와 CT/MRI의 세 가지 데이터 세트에서 실험한 결과, DPM이 이중 모달리티 이미지 합성에서 다른 최신 흐름 기반 방법들보다 더 나은 성능을 보였고, 적은 계산 시간으로 더 높은 이미지 품질을 달성했어.

================================================================================

URL: https://arxiv.org/abs/2409.04050
Title: EigenSR: Eigenimage-Bridged Pre-Trained RGB Learners for Single Hyperspectral Image Super-Resolution

Original Abstract:
Single hyperspectral image super-resolution (single-HSI-SR) aims to improve the resolution of a single input low-resolution HSI. Due to the bottleneck of data scarcity, the development of single-HSI-SR lags far behind that of RGB natural images. In recent years, research on RGB SR has shown that models pre-trained on large-scale benchmark datasets can greatly improve performance on unseen data, which may stand as a remedy for HSI. But how can we transfer the pre-trained RGB model to HSI, to overcome the data-scarcity bottleneck? Because of the significant difference in the channels between the pre-trained RGB model and the HSI, the model cannot focus on the correlation along the spectral dimension, thus limiting its ability to utilize on HSI. Inspired by the HSI spatial-spectral decoupling, we propose a new framework that first fine-tunes the pre-trained model with the spatial components (known as eigenimages), and then infers on unseen HSI using an iterative spectral regularization (ISR) to maintain the spectral correlation. The advantages of our method lie in: 1) we effectively inject the spatial texture processing capabilities of the pre-trained RGB model into HSI while keeping spectral fidelity, 2) learning in the spectral-decorrelated domain can improve the generalizability to spectral-agnostic data, and 3) our inference in the eigenimage domain naturally exploits the spectral low-rank property of HSI, thereby reducing the complexity. This work bridges the gap between pre-trained RGB models and HSI via eigenimages, addressing the issue of limited HSI training data, hence the name EigenSR. Extensive experiments show that EigenSR outperforms the state-of-the-art (SOTA) methods in both spatial and spectral metrics. Our code will be released.

Translated Abstract:
단일 하이퍼스펙트럴 이미지 초해상도(single-HSI-SR)는 입력으로 주어진 저해상도 하이퍼스펙트럴 이미지의 해상도를 높이는 걸 목표로 해. 데이터가 부족해서 단일 HSI-SR의 발전이 RGB 자연 이미지에 비해 많이 뒤쳐져 있어. 최근 RGB 초해상도 연구에서는 대규모 벤치마크 데이터셋에서 미리 학습된 모델이 보지 못한 데이터에서 성능을 크게 향상시킬 수 있다는 걸 보여줬어. 이게 HSI에도 도움이 될 수 있을 것 같아.

그런데 미리 학습된 RGB 모델을 HSI로 어떻게 옮길 수 있을까? RGB 모델과 HSI 간의 채널 차이가 크기 때문에, 모델이 스펙트럴 차원에서의 상관관계에 집중할 수 없어. 그래서 HSI에서의 활용 능력이 제한돼. 우리는 HSI의 공간-스펙트럴 분리를 영감을 받아서, 새로운 프레임워크를 제안해. 이 방법은 먼저 미리 학습된 모델을 공간 성분(고유 이미지라고 불림)으로 파인튜닝하고, 그 다음에 반복적인 스펙트럴 정규화(ISR)를 사용해서 보지 못한 HSI를 추론해. 

우리 방법의 장점은: 1) 미리 학습된 RGB 모델의 공간 텍스처 처리 능력을 HSI에 효과적으로 주입하면서 스펙트럴 충실도를 유지해, 2) 스펙트럴 비연관 도메인에서 학습하면 스펙트럴에 무관한 데이터에 대한 일반화 능력이 향상돼, 3) 고유 이미지 도메인에서의 추론은 HSI의 스펙트럴 저랭크 특성을 자연스럽게 활용해 복잡성을 줄여줘. 

이 연구는 고유 이미지를 통해 미리 학습된 RGB 모델과 HSI 간의 간극을 메우고, 제한된 HSI 훈련 데이터 문제를 다뤄. 그래서 이름을 EigenSR로 지었어. 다양한 실험 결과, EigenSR이 공간 및 스펙트럴 메트릭에서 최신 기술(SOTA) 방법들을 능가했어. 우리 코드는 곧 공개할 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.04104
Title: MixNet: Joining Force of Classical and Modern Approaches Toward the Comprehensive Pipeline in Motor Imagery EEG Classification

Original Abstract:
Recent advances in deep learning (DL) have significantly impacted motor imagery (MI)-based brain-computer interface (BCI) systems, enhancing the decoding of electroencephalography (EEG) signals. However, most studies struggle to identify discriminative patterns across subjects during MI tasks, limiting MI classification performance. In this article, we propose MixNet, a novel classification framework designed to overcome this limitation by utilizing spectral-spatial signals from MI data, along with a multitask learning architecture named MIN2Net, for classification. Here, the spectral-spatial signals are generated using the filter-bank common spatial patterns (FBCSPs) method on MI data. Since the multitask learning architecture is used for the classification task, the learning in each task may exhibit different generalization rates and potential overfitting across tasks. To address this issue, we implement adaptive gradient blending, simultaneously regulating multiple loss weights and adjusting the learning pace for each task based on its generalization/overfitting tendencies. Experimental results on six benchmark data sets of different data sizes demonstrate that MixNet consistently outperforms all state-of-the-art algorithms in subject-dependent and -independent settings. Finally, the low-density EEG MI classification results show that MixNet outperforms all state-of-the-art algorithms, offering promising implications for Internet of Thing (IoT) applications, such as lightweight and portable EEG wearable devices based on low-density montages.

Translated Abstract:
최근 딥러닝(DL)의 발전은 운동 이미지(MI) 기반 뇌-컴퓨터 인터페이스(BCI) 시스템에 큰 영향을 미쳤고, 뇌파(EEG) 신호 해독을 개선했어. 하지만 대부분의 연구는 MI 작업 중에 참가자 간의 구분 가능한 패턴을 찾는 데 어려움이 있어, 그래서 MI 분류 성능이 제한돼. 

이 논문에서는 MixNet이라는 새로운 분류 프레임워크를 제안해. 이 프레임워크는 MI 데이터에서 스펙트럴-스페이셜 신호를 사용하고, MIN2Net이라는 멀티태스크 학습 구조를 통해 분류 작업을 수행해. 여기서 스펙트럴-스페이셜 신호는 MI 데이터에 필터 뱅크 공통 공간 패턴(FBCSP) 방법을 적용해서 생성돼. 

멀티태스크 학습 구조를 사용하기 때문에, 각 작업의 학습 속도와 일반화 성능이 다를 수 있어. 이 문제를 해결하기 위해, 우리는 적응형 그래디언트 블렌딩을 구현했어. 이 방법은 여러 손실 가중치를 동시에 조정하고, 각 작업의 일반화/오버피팅 경향에 따라 학습 속도를 조절해. 

여섯 개의 다양한 데이터 크기의 벤치마크 데이터 세트에서 실험한 결과, MixNet은 항상 최신 알고리즘보다 더 뛰어난 성능을 보였어. 마지막으로, 저밀도 EEG MI 분류 결과에서도 MixNet이 모든 최신 알고리즘을 초월하면서, 저밀도 배열을 기반으로 한 가벼운 휴대용 EEG 웨어러블 기기와 같은 IoT 응용 프로그램에 대한 유망한 가능성을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.04137
Title: Optical Coherence Tomography Angiography-OCTA dataset for the study of Diabetic Retinopathy

Original Abstract:
This study presents a dataset consisting of 268 retinal images from 179 individuals, including 133 left-eye and 135 right-eye images, collected from Natasha Eye Care and Research Institute in Pune, Maharashtra, India. The images were captured using a nonmydriatic Optical Coherence Tomography Angiography (OCTA) device, specifically the Optovue Avanti Edition machine as per the protocol mentioned in this paper. Two ophthalmologists then annotated the images. This dataset can be used by researchers and doctors to develop automated diagnostic tools for early detection of diabetic retinopathy (DR).

Translated Abstract:
이 연구는 179명의 사람들로부터 수집한 268개의 망막 이미지로 구성된 데이터셋을 소개해. 여기에는 133개의 왼쪽 눈 이미지와 135개의 오른쪽 눈 이미지가 포함되어 있어. 이 이미지는 인도 마하라슈트라주 푸네에 있는 나타샤 안과 및 연구소에서 촬영됐어. 

이미지는 비산동 광간섭단층촬영 혈관조영술(OCTA) 장비인 Optovue Avanti Edition 기계를 사용해서 찍었고, 논문에 언급된 프로토콜에 따라 진행됐어. 두 명의 안과 의사가 이 이미지를 주석 처리했어. 

이 데이터셋은 연구자와 의사들이 당뇨병성 망막병증(DR)을 조기 발견하기 위한 자동 진단 도구를 개발하는 데 사용할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04175
Title: CISCA and CytoDArk0: a Cell Instance Segmentation and Classification method for histo(patho)logical image Analyses and a new, open, Nissl-stained dataset for brain cytoarchitecture studies

Original Abstract:
Delineating and classifying individual cells in microscopy tissue images is a complex task, yet it is a pivotal endeavor in various medical and biological investigations. We propose a new deep learning framework (CISCA) for automatic cell instance segmentation and classification in histological slices to support detailed morphological and structural analysis or straightforward cell counting in digital pathology workflows and brain cytoarchitecture studies. At the core of CISCA lies a network architecture featuring a lightweight U-Net with three heads in the decoder. The first head classifies pixels into boundaries between neighboring cells, cell bodies, and background, while the second head regresses four distance maps along four directions. The network outputs from the first and second heads are integrated through a tailored post-processing step, which ultimately yields the segmentation of individual cells. A third head enables simultaneous classification of cells into relevant classes, if required. We showcase the effectiveness of our method using four datasets, including CoNIC, PanNuke, and MoNuSeg, which are publicly available H\&E datasets. Additionally, we introduce CytoDArk0, a novel dataset consisting of Nissl-stained images of the cortex, cerebellum, and hippocampus from mammals belonging to the orders Cetartiodactyla and Primates. We evaluate CISCA in comparison to other state-of-the-art methods, demonstrating CISCA's robustness and accuracy in segmenting and classifying cells across diverse tissue types, magnifications, and staining techniques.

Translated Abstract:
세포를 구분하고 분류하는 건 현미경 조직 이미지에서 복잡한 작업이야. 하지만 이건 여러 의학적 및 생물학적 연구에서 정말 중요한 일이기도 해. 우리는 조직 슬라이스에서 세포 인스턴스 분할과 분류를 자동으로 할 수 있는 새로운 딥러닝 프레임워크인 CISCA를 제안해. 이건 디지털 병리학 작업 흐름이나 뇌 세포 구조 연구에서 자세한 형태학적 분석이나 간단한 세포 수 세기에 도움을 줄 수 있어.

CISCA의 핵심은 세 개의 헤드를 가진 가벼운 U-Net 네트워크 구조야. 첫 번째 헤드는 이웃 세포와 세포 몸체, 배경 사이의 경계를 픽셀 단위로 분류해. 두 번째 헤드는 네 방향에 따라 네 개의 거리 맵을 회귀해. 첫 번째와 두 번째 헤드의 네트워크 출력은 맞춤형 후처리 단계를 통해 통합되고, 결국 개별 세포의 분할 결과를 만들어 내. 만약 필요하다면, 세 번째 헤드를 통해 세포를 관련된 클래스에 동시에 분류할 수도 있어.

우리는 CoNIC, PanNuke, MoNuSeg 같은 네 가지 공개된 H&E 데이터셋을 사용해서 우리 방법의 효과를 보여줘. 또, 우리는 CytoDArk0라는 새로운 데이터셋도 소개해. 이 데이터셋은 Cetartiodactyla와 Primates에 속하는 포유류의 피질, 소뇌, 해마에서 Nissl 염색된 이미지를 포함하고 있어. CISCA를 다른 최신 방법들과 비교해서 평가해봤는데, 다양한 조직 유형, 배율, 염색 기법에서도 세포를 구분하고 분류하는 데 CISCA의 강력함과 정확성이 입증됐어.

================================================================================

URL: https://arxiv.org/abs/2409.04241
Title: Calibration of Network Confidence for Unsupervised Domain Adaptation Using Estimated Accuracy

Original Abstract:
This study addresses the problem of calibrating network confidence while adapting a model that was originally trained on a source domain to a target domain using unlabeled samples from the target domain. The absence of labels from the target domain makes it impossible to directly calibrate the adapted network on the target domain. To tackle this challenge, we introduce a calibration procedure that relies on estimating the network's accuracy on the target domain. The network accuracy is first computed on the labeled source data and then is modified to represent the actual accuracy of the model on the target domain. The proposed algorithm calibrates the prediction confidence directly in the target domain by minimizing the disparity between the estimated accuracy and the computed confidence. The experimental results show that our method significantly outperforms existing methods, which rely on importance weighting, across several standard datasets.

Translated Abstract:
이 연구는 원래 소스 도메인에서 훈련된 모델을 라벨이 없는 타겟 도메인 샘플을 사용해 타겟 도메인에 맞게 조정할 때, 네트워크 신뢰도를 조정하는 문제를 다루고 있어.

타겟 도메인에서 라벨이 없기 때문에, 조정된 네트워크를 직접적으로 타겟 도메인에서 조정하는 건 불가능해. 이 문제를 해결하기 위해, 우리는 네트워크의 정확도를 타겟 도메인에서 추정하는 조정 절차를 도입했어.

먼저, 네트워크의 정확도를 라벨이 있는 소스 데이터에서 계산하고, 그 다음에 이 값을 수정해서 모델의 실제 타겟 도메인에서의 정확도를 나타내도록 해. 제안한 알고리즘은 추정된 정확도와 계산된 신뢰도 간의 차이를 최소화함으로써 타겟 도메인에서 직접적으로 예측 신뢰도를 조정해.

실험 결과, 우리의 방법이 여러 표준 데이터셋에서 중요도 가중치에 의존하는 기존 방법들보다 훨씬 더 뛰어난 성능을 보였어.

================================================================================

URL: https://arxiv.org/abs/2409.04368
Title: The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study

Original Abstract:
Purpose: Medical images acquired using different scanners and protocols can differ substantially in their appearance. This phenomenon, scanner domain shift, can result in a drop in the performance of deep neural networks which are trained on data acquired by one scanner and tested on another. This significant practical issue is well-acknowledged, however, no systematic study of the issue is available across different modalities and diagnostic tasks. Materials and Methods: In this paper, we present a broad experimental study evaluating the impact of scanner domain shift on convolutional neural network performance for different automated diagnostic tasks. We evaluate this phenomenon in common radiological modalities, including X-ray, CT, and MRI. Results: We find that network performance on data from a different scanner is almost always worse than on same-scanner data, and we quantify the degree of performance drop across different datasets. Notably, we find that this drop is most severe for MRI, moderate for X-ray, and quite small for CT, on average, which we attribute to the standardized nature of CT acquisition systems which is not present in MRI or X-ray. We also study how injecting varying amounts of target domain data into the training set, as well as adding noise to the training data, helps with generalization. Conclusion: Our results provide extensive experimental evidence and quantification of the extent of performance drop caused by scanner domain shift in deep learning across different modalities, with the goal of guiding the future development of robust deep learning models for medical image analysis.

Translated Abstract:
목적: 서로 다른 스캐너와 프로토콜로 얻은 의료 이미지는 외관에서 큰 차이를 보일 수 있어. 이런 현상을 '스캐너 도메인 변화'라고 하는데, 이는 한 스캐너로 훈련된 딥 뉴럴 네트워크가 다른 스캐너에서 테스트될 때 성능이 떨어지는 원인이 돼. 이 문제는 꽤 중요한데, 다양한 모달리티와 진단 작업에 대한 체계적인 연구는 아직 없어.

자료 및 방법: 이 논문에서는 서로 다른 자동 진단 작업에 대한 스캐너 도메인 변화가 합성곱 신경망 성능에 미치는 영향을 평가하는 광범위한 실험 연구를 소개해. 우리는 X-ray, CT, MRI 같은 일반적인 방사선 모달리티에서 이 현상을 평가했어.

결과: 다른 스캐너에서 얻은 데이터에 대한 네트워크 성능이 같은 스캐너에서 얻은 데이터보다 거의 항상 나쁘다는 것을 발견했어. 그리고 다양한 데이터셋에서 성능 저하의 정도를 수치로 나타냈어. 특히 MRI의 경우 성능 저하가 가장 심하고, X-ray는 중간 정도, CT는 평균적으로 꽤 작다는 걸 알아냈어. CT는 표준화된 취득 시스템 덕분이고, MRI나 X-ray에는 이런 게 없어서 그런 것 같아. 또한, 훈련 세트에 다양한 양의 목표 도메인 데이터를 주입하거나 훈련 데이터에 노이즈를 추가하면 일반화에 도움이 된다는 것도 연구했어.

결론: 우리의 결과는 스캐너 도메인 변화가 딥러닝에서 성능 저하를 초래하는 정도에 대한 광범위한 실험적 증거와 수치를 제공해. 이는 의료 이미지 분석을 위한 강력한 딥러닝 모델 개발에 도움이 될 거야.

================================================================================

URL: https://arxiv.org/abs/2409.04424
Title: Exploring Foundation Models for Synthetic Medical Imaging: A Study on Chest X-Rays and Fine-Tuning Techniques

Original Abstract:
Machine learning has significantly advanced healthcare by aiding in disease prevention and treatment identification. However, accessing patient data can be challenging due to privacy concerns and strict regulations. Generating synthetic, realistic data offers a potential solution for overcoming these limitations, and recent studies suggest that fine-tuning foundation models can produce such data effectively. In this study, we explore the potential of foundation models for generating realistic medical images, particularly chest x-rays, and assess how their performance improves with fine-tuning. We propose using a Latent Diffusion Model, starting with a pre-trained foundation model and refining it through various configurations. Additionally, we performed experiments with input from a medical professional to assess the realism of the images produced by each trained model.

Translated Abstract:
머신러닝은 질병 예방과 치료 방법 찾기에 큰 도움이 되면서 의료 분야에서 많이 발전했어. 하지만 환자 데이터에 접근하는 건 개인정보 보호 문제와 엄격한 규제 때문에 쉽지 않아. 그래서 합성된 현실적인 데이터를 만드는 방법이 이러한 한계를 극복할 수 있는 잠재적인 해결책이 될 수 있어. 최근 연구들은 기초 모델을 미세 조정하면 이런 데이터를 효과적으로 생성할 수 있다고 제안하고 있어.

이번 연구에서는 기초 모델이 현실적인 의료 이미지를 생성할 수 있는 가능성을 살펴봤어. 특히 흉부 X선 이미지를 중심으로 다뤄보고, 미세 조정을 통해 성능이 어떻게 개선되는지 평가했어. 우리는 Latent Diffusion Model을 사용하기로 했고, 미리 훈련된 기초 모델을 시작으로 다양한 설정을 통해 조정했어. 또한, 각 훈련된 모델이 만든 이미지의 현실성을 평가하기 위해 의료 전문가의 입력을 받아 실험도 진행했어.

================================================================================

URL: https://arxiv.org/abs/2212.11192
Title: Continual Learning Approaches for Anomaly Detection

Original Abstract:
Anomaly Detection is a relevant problem that arises in numerous real-world applications, especially when dealing with images. However, there has been little research for this task in the Continual Learning setting. In this work, we introduce a novel approach called SCALE (SCALing is Enough) to perform Compressed Replay in a framework for Anomaly Detection in Continual Learning setting. The proposed technique scales and compresses the original images using a Super Resolution model which, to the best of our knowledge, is studied for the first time in the Continual Learning setting. SCALE can achieve a high level of compression while maintaining a high level of image reconstruction quality. In conjunction with other Anomaly Detection approaches, it can achieve optimal results. To validate the proposed approach, we use a real-world dataset of images with pixel-based anomalies, with the scope to provide a reliable benchmark for Anomaly Detection in the context of Continual Learning, serving as a foundation for further advancements in the field.

Translated Abstract:
이상 탐지는 많은 실제 응용 프로그램에서 중요한 문제야, 특히 이미지 처리할 때. 그런데 지속적 학습(Continual Learning) 환경에서 이 작업에 대한 연구는 거의 없었어. 

이번 연구에서는 SCALE(스케일링이 충분하다)라는 새로운 접근 방식을 소개해. 이 방법은 지속적 학습 환경에서 이상 탐지를 위한 프레임워크에서 압축 재생(Compressed Replay)을 수행해. 제안된 기술은 슈퍼 해상도 모델을 사용해서 원본 이미지를 스케일링하고 압축해. 우리가 아는 한, 이게 지속적 학습 환경에서 처음으로 연구된 거야. 

SCALE은 높은 압축 비율을 달성하면서도 이미지 재구성 품질을 잘 유지할 수 있어. 다른 이상 탐지 방법들과 함께 사용하면 최적의 결과를 얻을 수 있어. 

제안된 방법을 검증하기 위해, 우리는 픽셀 기반 이상이 있는 이미지의 실제 데이터셋을 사용했어. 이 데이터셋은 지속적 학습 맥락에서 이상 탐지를 위한 신뢰할 수 있는 기준을 제공하는 게 목적이야. 이 연구는 이 분야의 추가 발전을 위한 기반이 될 거야.

================================================================================

URL: https://arxiv.org/abs/2212.12741
Title: LMFLOSS: A Hybrid Loss For Imbalanced Medical Image Classification

Original Abstract:
With advances in digital technology, the classification of medical images has become a crucial step for image-based clinical decision support systems. Automatic medical image classification represents a pivotal domain where the use of AI holds the potential to create a significant social impact. However, several challenges act as obstacles to the development of practical and effective solutions. One of these challenges is the prevalent class imbalance problem in most medical imaging datasets. As a result, existing AI techniques, particularly deep-learning-based methodologies, often underperform in such scenarios. In this study, we propose a novel framework called Large Margin aware Focal (LMF) loss to mitigate the class imbalance problem in medical imaging. The LMF loss represents a linear combination of two loss functions optimized by two hyperparameters. This framework harnesses the distinct characteristics of both loss functions by enforcing wider margins for minority classes while simultaneously emphasizing challenging samples found in the datasets. We perform rigorous experiments on three neural network architectures and with four medical imaging datasets. We provide empirical evidence that our proposed framework consistently outperforms other baseline methods, showing an improvement of 2%-9% in macro-f1 scores. Through class-wise analysis of f1 scores, we also demonstrate how the proposed framework can significantly improve performance for minority classes. The results of our experiments show that our proposed framework can perform consistently well across different architectures and datasets. Overall, our study demonstrates a simple and effective approach to addressing the class imbalance problem in medical imaging datasets. We hope our work will inspire new research toward a more generalized approach to medical image classification.

Translated Abstract:
디지털 기술의 발전 덕분에 의료 이미지를 분류하는 것이 이미지 기반 임상 의사결정 지원 시스템에서 중요한 단계가 됐어. 자동 의료 이미지 분류는 AI를 활용할 수 있는 중요한 분야인데, 사회에 큰 영향을 줄 가능성이 있어. 하지만 몇 가지 도전 과제가 있어 실제적이고 효과적인 해결책을 개발하는 데 방해가 되고 있어. 그 중 하나가 대부분의 의료 이미지 데이터셋에서 나타나는 클래스 불균형 문제야. 그래서 기존의 AI 기술, 특히 딥러닝 기반 방법들은 이런 상황에서 제대로 성과를 내지 못해.

이번 연구에서는 의료 이미지에서 클래스 불균형 문제를 해결하기 위해 "Large Margin aware Focal (LMF) loss"라는 새로운 프레임워크를 제안해. LMF loss는 두 개의 손실 함수의 선형 결합으로, 두 개의 하이퍼파라미터로 최적화돼. 이 프레임워크는 소수 클래스에 대해 더 넓은 마진을 부여하면서 데이터셋에서 어려운 샘플들을 강조하는 방식으로 두 손실 함수의 특징을 잘 활용해.

우리는 세 가지 신경망 구조와 네 가지 의료 이미지 데이터셋을 가지고 엄격한 실험을 진행했어. 우리의 제안된 프레임워크가 다른 기준 방법들보다 일관되게 성능이 뛰어난다는 것을 보여주었고, macro-f1 점수에서 2%-9%의 향상을 나타냈어. 클래스별 f1 점수 분석을 통해 제안한 프레임워크가 소수 클래스의 성능을 크게 개선할 수 있다는 것도 증명했어. 실험 결과는 제안된 프레임워크가 다양한 구조와 데이터셋에서 일관되게 잘 작동한다는 걸 보여줘. 

결론적으로, 우리의 연구는 의료 이미지 데이터셋의 클래스 불균형 문제를 해결하는 간단하고 효과적인 접근 방식을 보여줘. 이 연구가 의료 이미지 분류에 대해 더 일반화된 접근법을 위한 새로운 연구에 영감을 주기를 바래.

================================================================================

URL: https://arxiv.org/abs/2301.06084
Title: Scattering-induced entropy boost for highly-compressed optical sensing and encryption

Original Abstract:
Image sensing often relies on a high-quality machine vision system with a large field of view and high resolution. It requires fine imaging optics, has high computational costs, and requires a large communication bandwidth between image sensors and computing units. In this paper, we propose a novel image-free sensing framework for resource-efficient image classification, where the required number of measurements can be reduced by up to two orders of magnitude. In the proposed framework for single-pixel detection, the optical field for a target is first scattered by an optical diffuser and then two-dimensionally modulated by a spatial light modulator. The optical diffuser simultaneously serves as a compressor and an encryptor for the target information, effectively narrowing the field of view and improving the system's security. The one-dimensional sequence of intensity values, which is measured with time-varying patterns on the spatial light modulator, is then used to extract semantic information based on end-to-end deep learning. The proposed sensing framework is shown to obtain over a 95\% accuracy at sampling rates of 1% and 5% for classification on the MNIST dataset and the recognition of Chinese license plates, respectively, and the framework is up to 24% more efficient than the approach without an optical diffuser. The proposed framework represents a significant breakthrough in high-throughput machine intelligence for scene analysis with low bandwidth, low costs, and strong encryption.

Translated Abstract:
이미지 센싱은 보통 큰 시야와 높은 해상도를 가진 고품질 머신 비전 시스템에 의존해. 이 시스템은 정밀한 이미징 광학이 필요하고, 계산 비용이 높고, 이미지 센서와 컴퓨팅 유닛 사이에 큰 통신 대역폭이 필요해. 

이 논문에서는 자원 효율적인 이미지 분류를 위한 새로운 이미지 없는 센싱 프레임워크를 제안해. 이 프레임워크는 필요한 측정 수를 최대 100배까지 줄일 수 있어. 제안하는 단일 픽셀 탐지 프레임워크에서는 먼저 타겟의 광학 필드가 광학 확산기에 의해 산란되고, 그 다음 공간 광 변조기에 의해 2차원으로 변조돼. 광학 확산기는 동시에 타겟 정보를 압축하고 암호화하는 역할을 해서, 시야를 좁히고 시스템의 보안을 개선해. 

시간에 따라 변하는 패턴으로 측정한 1차원 강도 값 시퀀스는 엔드 투 엔드 딥러닝을 기반으로 의미 정보를 추출하는 데 사용돼. 제안한 센싱 프레임워크는 MNIST 데이터셋에서 1%와 5% 샘플링 속도로 분류할 때 각각 95% 이상의 정확도를 얻는 것으로 나타났고, 광학 확산기가 없는 접근 방식보다 최대 24% 더 효율적이야. 

이 프레임워크는 낮은 대역폭, 낮은 비용, 강력한 암호화로 장면 분석을 위한 고처리량 머신 인텔리전스에서 중요한 돌파구를 나타내.

================================================================================

URL: https://arxiv.org/abs/2304.06841
Title: Video alignment using unsupervised learning of local and global features

Original Abstract:
In this paper, we tackle the problem of video alignment, the process of matching the frames of a pair of videos containing similar actions. The main challenge in video alignment is that accurate correspondence should be established despite the differences in the execution processes and appearances between the two videos. We introduce an unsupervised method for alignment that uses global and local features of the frames. In particular, we introduce effective features for each video frame by means of three machine vision tools: person detection, pose estimation, and VGG network. Then the features are processed and combined to construct a multidimensional time series that represent the video. The resulting time series are used to align videos of the same actions using a novel version of dynamic time warping named Diagonalized Dynamic Time Warping(DDTW). The main advantage of our approach is that no training is required, which makes it applicable for any new type of action without any need to collect training samples for it. Additionally, our approach can be used for framewise labeling of action phases in a dataset with only a few labeled videos. For evaluation, we considered video synchronization and phase classification tasks on the Penn action and subset of UCF101 datasets. Also, for an effective evaluation of the video synchronization task, we present a new metric called Enclosed Area Error(EAE). The results show that our method outperforms previous state-of-the-art methods, such as TCC, and other self-supervised and weakly supervised methods.

Translated Abstract:
이 논문에서는 비디오 정렬 문제를 다룹니다. 비디오 정렬은 비슷한 행동을 포함한 비디오 쌍의 프레임을 맞추는 과정이에요. 비디오 정렬의 주요 도전 과제는 두 비디오 간의 실행 과정과 외관 차이에 관계없이 정확한 대응 관계를 설정해야 한다는 점입니다.

우리는 프레임의 전역적 및 지역적 특징을 사용하는 비지도 방식의 정렬 방법을 소개합니다. 특히, 사람 감지, 자세 추정, VGG 네트워크라는 세 가지 머신 비전 도구를 통해 각 비디오 프레임에 대한 효과적인 특징을 만들어냅니다. 그런 다음 이 특징들을 처리하고 결합해서 비디오를 나타내는 다차원 시간 시퀀스를 구성합니다. 이렇게 생성된 시간 시퀀스는 Diagonalized Dynamic Time Warping(DDTW)이라는 새로운 동적 시간 왜곡 버전을 사용해 같은 행동의 비디오를 정렬하는 데 사용됩니다.

우리 방법의 가장 큰 장점은 훈련이 필요 없다는 점이에요. 그래서 새로운 행동 유형에 대해서도 훈련 샘플을 수집할 필요 없이 적용할 수 있습니다. 게다가, 우리의 접근 방식은 레이블이 몇 개만 있는 비디오로 구성된 데이터셋에서 행동 단계에 대한 프레임별 레이블링에도 사용할 수 있습니다.

평가를 위해 Penn action과 UCF101 데이터셋의 비디오 동기화 및 단계 분류 작업을 고려했습니다. 또한 비디오 동기화 작업을 효과적으로 평가하기 위해 Enclosed Area Error(EAE)라는 새로운 지표를 제시합니다. 결과적으로 우리의 방법이 TCC와 같은 이전의 최첨단 방법들과 다른 자가 지도 및 약한 지도 방법들보다 우수하다는 것을 보여줍니다.

================================================================================

URL: https://arxiv.org/abs/2308.06493
Title: EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse and Intermittent Observations Everywhere

Original Abstract:
Full-body egocentric pose estimation from head and hand poses alone has become an active area of research to power articulate avatar representations on headset-based platforms. However, existing methods over-rely on the indoor motion-capture spaces in which datasets were recorded, while simultaneously assuming continuous joint motion capture and uniform body dimensions. We propose EgoPoser to overcome these limitations with four main contributions. 1) EgoPoser robustly models body pose from intermittent hand position and orientation tracking only when inside a headset's field of view. 2) We rethink input representations for headset-based ego-pose estimation and introduce a novel global motion decomposition method that predicts full-body pose independent of global positions. 3) We enhance pose estimation by capturing longer motion time series through an efficient SlowFast module design that maintains computational efficiency. 4) EgoPoser generalizes across various body shapes for different users. We experimentally evaluate our method and show that it outperforms state-of-the-art methods both qualitatively and quantitatively while maintaining a high inference speed of over 600fps. EgoPoser establishes a robust baseline for future work where full-body pose estimation no longer needs to rely on outside-in capture and can scale to large-scale and unseen environments.

Translated Abstract:
머리와 손 포즈만으로 전체 몸의 자세를 추정하는 연구가 헤드셋 기반 플랫폼에서 아바타 표현을 가능하게 하기 위해 활발히 진행되고 있어. 하지만 기존 방법들은 데이터셋이 기록된 실내 모션 캡처 공간에 지나치게 의존하고, 연속적인 관절 모션 캡처와 균일한 신체 치수를 가정하고 있어. 우리는 이러한 한계를 극복하기 위해 EgoPoser를 제안해.

첫째, EgoPoser는 헤드셋의 시야 안에서만 간헐적인 손 위치와 방향 추적으로 몸의 자세를 안정적으로 모델링해. 둘째, 헤드셋 기반의 에고 자세 추정을 위해 입력 표현을 다시 생각하고, 전역 위치와 관계 없이 전체 몸의 자세를 예측하는 새로운 글로벌 모션 분해 방법을 도입해. 셋째, 효율적인 SlowFast 모듈 디자인을 통해 긴 모션 시간 시퀀스를 캡처하면서 자세 추정을 개선해. 넷째, EgoPoser는 다양한 사용자 신체 형태에 걸쳐 일반화돼.

우리는 실험적으로 이 방법을 평가해봤고, 정성적 및 정량적으로 최신 기술들을 능가하면서도 600fps 이상의 빠른 추론 속도를 유지하는 걸 보여줬어. EgoPoser는 앞으로의 연구를 위한 강력한 기준을 마련해주며, 전체 몸 자세 추정이 더 이상 외부 캡처에 의존하지 않고 대규모 및 보지 못한 환경으로 확장될 수 있도록 해.

================================================================================

URL: https://arxiv.org/abs/2310.12092
Title: HSTR-Net: Reference Based Video Super-resolution with Dual Cameras

Original Abstract:
High-spatio-temporal resolution (HSTR) video recording plays a crucial role in enhancing various imagery tasks that require fine-detailed information. State-of-the-art cameras provide this required high frame-rate and high spatial resolution together, albeit at a high cost. To alleviate this issue, this paper proposes a dual camera system for the generation of HSTR video using reference-based super-resolution (RefSR). One camera captures high spatial resolution low frame rate (HSLF) video while the other captures low spatial resolution high frame rate (LSHF) video simultaneously for the same scene. A novel deep learning architecture is proposed to fuse HSLF and LSHF video feeds and synthesize HSTR video frames. The proposed model combines optical flow estimation and (channel-wise and spatial) attention mechanisms to capture the fine motion and complex dependencies between frames of the two video feeds. Simulations show that the proposed model provides significant improvement over existing reference-based SR techniques in terms of PSNR and SSIM metrics. The method also exhibits sufficient frames per second (FPS) for aerial monitoring when deployed on a power-constrained drone equipped with dual cameras.

Translated Abstract:
고해상도 시간-공간 비디오 기록(HSTR)은 세밀한 정보를 요구하는 다양한 이미지 작업에서 중요한 역할을 해. 최신 카메라들이 이런 고속 프레임과 고해상도를 동시에 제공하지만, 가격이 비싸다는 문제가 있어. 

이 논문에서는 참조 기반 초해상도(RefSR)를 이용해 HSTR 비디오를 생성하는 이중 카메라 시스템을 제안해. 한 카메라는 고해상도 저프레임(HSLF) 비디오를 캡처하고, 다른 카메라는 저해상도 고프레임(LSHF) 비디오를 동시에 같은 장면에서 캡처해. 

새로운 딥러닝 구조가 HSLF와 LSHF 비디오를 결합하고 HSTR 비디오 프레임을 합성하는 데 사용돼. 이 모델은 광학 흐름 추정과 (채널별 및 공간적) 주의 메커니즘을 결합해서 두 비디오 간의 미세한 움직임과 복잡한 의존성을 포착해. 

시뮬레이션 결과, 제안된 모델이 기존의 참조 기반 SR 기법보다 PSNR과 SSIM 지표에서 상당한 개선을 보여줘. 이 방법은 이중 카메라가 장착된 전력 제한 드론에서 사용할 때 충분한 초당 프레임(FPS)도 제공해.

================================================================================

URL: https://arxiv.org/abs/2312.00826
Title: DEVIAS: Learning Disentangled Video Representations of Action and Scene

Original Abstract:
Video recognition models often learn scene-biased action representation due to the spurious correlation between actions and scenes in the training data. Such models show poor performance when the test data consists of videos with unseen action-scene combinations. Although scene-debiased action recognition models might address the issue, they often overlook valuable scene information in the data. To address this challenge, we propose to learn DisEntangled VIdeo representations of Action and Scene (DEVIAS), for more holistic video understanding. We propose an encoder-decoder architecture to learn disentangled action and scene representations with a single model. The architecture consists of a disentangling encoder (DE), an action mask decoder (AMD), and a prediction head. The key to achieving the disentanglement is employing both DE and AMD during training time. The DE uses the slot attention mechanism to learn disentangled action and scene representations. For further disentanglement, an AMD learns to predict action masks, given an action slot. With the resulting disentangled representations, we can achieve robust performance across diverse scenarios, including both seen and unseen action-scene combinations. We rigorously validate the proposed method on the UCF-101, Kinetics-400, and HVU datasets for the seen, and the SCUBA, HAT, and HVU datasets for unseen action-scene combination scenarios. Furthermore, DEVIAS provides flexibility to adjust the emphasis on action or scene information depending on dataset characteristics for downstream tasks. DEVIAS shows favorable performance in various downstream tasks: Diving48, Something-Something-V2, UCF-101, and ActivityNet. The code is available at this https URL.

Translated Abstract:
비디오 인식 모델은 종종 훈련 데이터에서 행동과 장면 사이의 잘못된 상관관계 때문에 장면에 편향된 행동 표현을 배우게 돼. 이런 모델은 테스트 데이터가 보지 못한 행동-장면 조합으로 이루어져 있을 때 성능이 떨어져. 장면 편향을 없애는 행동 인식 모델이 이 문제를 해결할 수 있지만, 데이터에서 중요한 장면 정보를 간과하는 경우가 많아. 

이런 문제를 해결하기 위해 우리는 행동과 장면의 비얽힌 비디오 표현(DisEntangled VIdeo representations of Action and Scene, DEVIAS)을 배우는 방법을 제안해. 이 방법은 비디오를 더 잘 이해할 수 있도록 도와줘. 우리는 하나의 모델로 비얽힌 행동과 장면 표현을 배우기 위해 인코더-디코더 아키텍처를 제안해. 이 아키텍처는 비얽힘 인코더(DE), 행동 마스크 디코더(AMD), 그리고 예측 헤드로 구성돼.

비얽힘을 달성하는 핵심은 훈련할 때 DE와 AMD를 모두 사용하는 거야. DE는 슬롯 주의 메커니즘을 사용해서 비얽힌 행동과 장면 표현을 배우고, AMD는 주어진 행동 슬롯에 따라 행동 마스크를 예측하는 방법을 배워. 이렇게 얻어진 비얽힌 표현 덕분에 우리는 다양한 시나리오에서 강력한 성능을 낼 수 있어, 보거나 보지 못한 행동-장면 조합 모두 포함해서.

우리는 UCF-101, Kinetics-400, HVU 데이터셋에서 보이는 행동-장면 조합에 대해, 그리고 SCUBA, HAT, HVU 데이터셋에서 보지 못한 조합 시나리오에 대해 제안한 방법을 엄격하게 검증했어. 게다가 DEVIAS는 다운스트림 작업의 데이터셋 특성에 따라 행동이나 장면 정보의 강조점을 조정할 수 있는 유연성을 제공해. DEVIAS는 다양한 다운스트림 작업에서도 좋은 성능을 보여: Diving48, Something-Something-V2, UCF-101, ActivityNet. 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2312.01397
Title: Visual Prompting Upgrades Neural Network Sparsification: A Data-Model Perspective

Original Abstract:
The rapid development of large-scale deep learning models questions the affordability of hardware platforms, which necessitates the pruning to reduce their computational and memory footprints. Sparse neural networks as the product, have demonstrated numerous favorable benefits like low complexity, undamaged generalization, etc. Most of the prominent pruning strategies are invented from a model-centric perspective, focusing on searching and preserving crucial weights by analyzing network topologies. However, the role of data and its interplay with model-centric pruning has remained relatively unexplored. In this research, we introduce a novel data-model co-design perspective: to promote superior weight sparsity by learning important model topology and adequate input data in a synergetic manner. Specifically, customized Visual Prompts are mounted to upgrade neural Network sparsification in our proposed VPNs framework. As a pioneering effort, this paper conducts systematic investigations about the impact of different visual prompts on model pruning and suggests an effective joint optimization approach. Extensive experiments with 3 network architectures and 8 datasets evidence the substantial performance improvements from VPNs over existing start-of-the-art pruning algorithms. Furthermore, we find that subnetworks discovered by VPNs from pre-trained models enjoy better transferability across diverse downstream scenarios. These insights shed light on new promising possibilities of data-model co-designs for vision model sparsification.

Translated Abstract:
대규모 딥러닝 모델이 빠르게 발전하면서 하드웨어 플랫폼의 비용 문제에 대한 의문이 생겼어. 그래서 계산량과 메모리 사용량을 줄이기 위해 모델을 가지치기(pruning)해야 해. 희소 신경망은 이런 가지치기의 결과로 저렴한 복잡도와 손상되지 않은 일반화 같은 여러 가지 좋은 장점을 보여주고 있어. 

대부분의 유명한 가지치기 전략은 모델 중심의 관점에서 개발되었고, 네트워크 구조를 분석해서 중요한 가중치를 찾고 보존하는 데 집중하고 있어. 하지만 데이터의 역할과 모델 중심 가지치기와의 상호작용은 상대적으로 많이 연구되지 않았어. 

이 연구에서는 새로운 데이터-모델 공동 설계 관점을 소개해. 중요한 모델 구조와 적절한 입력 데이터를 서로 협력하는 방식으로 학습해서 더 나은 가중치 희소성을 촉진하려고 해. 구체적으로, 맞춤형 비주얼 프롬프트를 사용해 우리 제안하는 VPNs 프레임워크에서 신경망 희소화를 개선하고 있어. 

이 논문은 선구적인 노력으로, 다양한 비주얼 프롬프트가 모델 가지치기에 미치는 영향을 체계적으로 조사하고 효과적인 공동 최적화 접근법을 제안해. 3개의 네트워크 아키텍처와 8개의 데이터셋을 이용한 광범위한 실험 결과, VPNs가 기존의 최신 가지치기 알고리즘보다 성능이 크게 개선된다는 것을 보여줘. 게다가, 사전 훈련된 모델에서 VPNs가 발견한 서브네트워크는 다양한 하류 시나리오에서 더 나은 전이 가능성을 가지고 있다는 것을 발견했어. 이런 통찰력은 비전 모델 희소화를 위한 데이터-모델 공동 설계의 새로운 가능성을 제시해.

================================================================================

URL: https://arxiv.org/abs/2312.02432
Title: Orthogonal Adaptation for Modular Customization of Diffusion Models

Original Abstract:
Customization techniques for text-to-image models have paved the way for a wide range of previously unattainable applications, enabling the generation of specific concepts across diverse contexts and styles. While existing methods facilitate high-fidelity customization for individual concepts or a limited, pre-defined set of them, they fall short of achieving scalability, where a single model can seamlessly render countless concepts. In this paper, we address a new problem called Modular Customization, with the goal of efficiently merging customized models that were fine-tuned independently for individual concepts. This allows the merged model to jointly synthesize concepts in one image without compromising fidelity or incurring any additional computational costs. To address this problem, we introduce Orthogonal Adaptation, a method designed to encourage the customized models, which do not have access to each other during fine-tuning, to have orthogonal residual weights. This ensures that during inference time, the customized models can be summed with minimal interference. Our proposed method is both simple and versatile, applicable to nearly all optimizable weights in the model architecture. Through an extensive set of quantitative and qualitative evaluations, our method consistently outperforms relevant baselines in terms of efficiency and identity preservation, demonstrating a significant leap toward scalable customization of diffusion models.

Translated Abstract:
텍스트-이미지 모델의 커스터마이징 기술 덕분에 이전에는 불가능했던 다양한 응용이 가능해졌어. 특정 개념을 여러 상황과 스타일로 생성할 수 있게 된 거지. 기존 방법들은 개별 개념이나 미리 정의된 제한된 개념 집합에 대해 높은 품질의 커스터마이징을 가능하게 하지만, 수많은 개념을 한 모델에서 원활하게 구현하는 확장성에는 한계가 있어.

이 논문에서는 '모듈형 커스터마이징'이라는 새로운 문제를 다루고, 독립적으로 조정된 커스터마이즈된 모델들을 효율적으로 합치는 방법을 제안해. 이렇게 합쳐진 모델은 한 이미지에서 여러 개념을 함께 합성할 수 있는데, 품질이나 추가적인 계산 비용에 영향을 주지 않아. 이 문제를 해결하기 위해 '직교 적응'이라는 방법을 소개하는데, 이 방법은 커스터마이즈된 모델들이 서로의 정보에 접근하지 않고 조정될 때 직교적인 잔여 가중치를 갖도록 유도해. 이렇게 하면 추론할 때, 커스터마이즈된 모델들이 최소한의 간섭으로 합쳐질 수 있어.

우리가 제안한 방법은 간단하면서도 다양하게 적용할 수 있어서 모델 아키텍처의 거의 모든 최적화 가능한 가중치에 쓸 수 있어. 많은 양의 정량적 및 정성적 평가를 통해, 우리의 방법은 효율성과 정체성 유지 측면에서 관련 기준선을 지속적으로 초월했어. 이로써 확장 가능한 확산 모델의 커스터마이징에 큰 진전을 이룬 거지.

================================================================================

URL: https://arxiv.org/abs/2312.12908
Title: A Unified Representation Framework for the Evaluation of Optical Music Recognition Systems

Original Abstract:
Modern-day Optical Music Recognition (OMR) is a fairly fragmented field. Most OMR approaches use datasets that are independent and incompatible between each other, making it difficult to both combine them and compare recognition systems built upon them. In this paper we identify the need of a common music representation language and propose the Music Tree Notation (MTN) format, with the idea to construct a common endpoint for OMR research that allows coordination, reuse of technology and fair evaluation of community efforts. This format represents music as a set of primitives that group together into higher-abstraction nodes, a compromise between the expression of fully graph-based and sequential notation formats. We have also developed a specific set of OMR metrics and a typeset score dataset as a proof of concept of this idea.

Translated Abstract:
현대의 광학 음악 인식(OMR) 분야는 꽤 분산되어 있어. 대부분의 OMR 방식은 서로 독립적이고 호환되지 않는 데이터셋을 사용하기 때문에, 이 데이터셋을 결합하거나 이를 바탕으로 만든 인식 시스템을 비교하기가 어려워. 

이 논문에서는 공통 음악 표현 언어의 필요성을 강조하고, 음악 트리 표기법(MTN) 형식을 제안해. 이 형식은 OMR 연구를 위한 공통의 기준점을 만들자는 아이디어로, 기술의 재사용과 공정한 평가를 가능하게 해. MTN 형식은 음악을 기본적인 요소들의 집합으로 표현하고, 이 요소들이 더 높은 추상화 수준의 노드로 그룹화돼. 완전히 그래프 기반이거나 순차적인 표기법 형식의 표현 사이에서 절충안이 되는 거야.

또한, 이 아이디어의 개념 증명을 위해 OMR 메트릭스의 특정 집합과 타입셋 점수를 포함한 데이터셋도 개발했어.

================================================================================

URL: https://arxiv.org/abs/2312.16410
Title: Segment Change Model (SCM) for Unsupervised Change detection in VHR Remote Sensing Images: a Case Study of Buildings

Original Abstract:
The field of Remote Sensing (RS) widely employs Change Detection (CD) on very-high-resolution (VHR) images. A majority of extant deep-learning-based methods hinge on annotated samples to complete the CD process. Recently, the emergence of Vision Foundation Model (VFM) enables zero-shot predictions in particular vision tasks. In this work, we propose an unsupervised CD method named Segment Change Model (SCM), built upon the Segment Anything Model (SAM) and Contrastive Language-Image Pre-training (CLIP). Our method recalibrates features extracted at different scales and integrates them in a top-down manner to enhance discriminative change edges. We further design an innovative Piecewise Semantic Attention (PSA) scheme, which can offer semantic representation without training, thereby minimize pseudo change phenomenon. Through conducting experiments on two public datasets, the proposed SCM increases the mIoU from 46.09% to 53.67% on the LEVIR-CD dataset, and from 47.56% to 52.14% on the WHU-CD dataset. Our codes are available at this https URL.

Translated Abstract:
원격 감지(Remote Sensing, RS) 분야에서는 매우 고해상도(VHR) 이미지에서 변화 감지(Change Detection, CD)를 많이 사용해. 현재의 딥러닝 기반 방법들은 대부분 주석이 달린 샘플에 의존해서 CD 과정을 수행하지. 최근에 비전 기초 모델(Vision Foundation Model, VFM)의 등장으로 특정 비전 작업에서 제로샷 예측이 가능하게 되었어.

이 연구에서는 Segment Anything Model(SAM)과 Contrastive Language-Image Pre-training(CLIP)을 기반으로 한 비지도 변화 감지 방법인 Segment Change Model(SCM)을 제안해. 우리 방법은 서로 다른 스케일에서 추출한 특징을 다시 조정하고, 이를 상향식(top-down)으로 통합해서 변화의 경계를 더 잘 구분할 수 있게 해.

또한, 우리는 훈련 없이도 의미 표현을 제공할 수 있는 혁신적인 조각별 의미 주의(Piecewise Semantic Attention, PSA) 방식을 설계했어. 이 방법은 가짜 변화 현상을 최소화하는 데 도움을 줘. 두 개의 공공 데이터셋에서 실험을 진행한 결과, 제안한 SCM은 LEVIR-CD 데이터셋에서 mIoU가 46.09%에서 53.67%로, WHU-CD 데이터셋에서는 47.56%에서 52.14%로 증가했어. 우리의 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2402.03666
Title: QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning

Original Abstract:
The practical deployment of diffusion models still suffers from the high memory and time overhead. While quantization paves a way for compression and acceleration, existing methods unfortunately fail when the models are quantized to low-bits. In this paper, we empirically unravel three properties in quantized diffusion models that compromise the efficacy of current methods: imbalanced activation distributions, imprecise temporal information, and vulnerability to perturbations of specific modules. To alleviate the intensified low-bit quantization difficulty stemming from the distribution imbalance, we propose finetuning the quantized model to better adapt to the activation distribution. Building on this idea, we identify two critical types of quantized layers: those holding vital temporal information and those sensitive to reduced bit-width, and finetune them to mitigate performance degradation with efficiency. We empirically verify that our approach modifies the activation distribution and provides meaningful temporal information, facilitating easier and more accurate quantization. Our method is evaluated over three high-resolution image generation tasks and achieves state-of-the-art performance under various bit-width settings, as well as being the first method to generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion. Code is available \href{this https URL}{here}.

Translated Abstract:
확산 모델을 실제로 사용하려면 메모리와 시간이 많이 드는 문제가 여전히 있어. 양자화는 압축과 가속의 길을 열어주지만, 기존 방법들은 안타깝게도 모델을 저비트로 양자화할 때 잘 작동하지 않아. 

이 논문에서는 저비트 양자화 모델에서 현재 방법의 효과를 떨어뜨리는 세 가지 특성을 살펴봤어: 불균형한 활성화 분포, 부정확한 시간 정보, 특정 모듈의 방해에 대한 취약성. 분포 불균형에서 오는 저비트 양자화의 어려움을 덜기 위해, 우리는 양자화된 모델을 조정해서 활성화 분포에 더 잘 맞도록 하는 방법을 제안했어.

이 아이디어를 바탕으로, 우리는 두 가지 중요한 양자화 레이어를 찾아냈어: 중요한 시간 정보를 담고 있는 레이어와 비트 폭이 줄어드는 것에 민감한 레이어. 이 두 가지를 조정해서 성능 저하를 효율적으로 줄였어. 

우리 방법은 활성화 분포를 수정하고 의미 있는 시간 정보를 제공해서 양자화 과정을 더 쉽게 하고 정확하게 만들어. 이 방법은 세 가지 고해상도 이미지 생성 작업에서 평가되었고, 다양한 비트 폭 설정에서 최첨단 성능을 달성했어. 또한, 최초로 4비트(W4A4) 안정적인 확산에서 읽을 수 있는 이미지를 생성하는 방법이기도 해. 코드도 여기에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2402.09164
Title: Less is More: Fewer Interpretable Region via Submodular Subset Selection

Original Abstract:
Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions. To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate small interpretation regions. To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, effectiveness, consistency, and collaboration scores, to assess the importance of various subsets. Moreover, our theoretical analysis substantiates that the proposed function is in fact submodular. Extensive experiments show that the proposed method outperforms SOTA methods on two face datasets (Celeb-A and VGG-Face2) and one fine-grained dataset (CUB-200-2011). For correctly predicted samples, the proposed method improves the Deletion and Insertion scores with an average of 4.9% and 2.5% gain relative to HSIC-Attribution. For incorrectly predicted samples, our method achieves gains of 81.0% and 18.4% compared to the HSIC-Attribution algorithm in the average highest confidence and Insertion score respectively. The code is released at this https URL.

Translated Abstract:
이미지 기여 알고리즘은 모델의 결정에 중요한 지역을 찾는 것을 목표로 해. 기존의 기여 방법들은 타겟 요소에 중요도를 잘 부여할 수 있지만, 여전히 몇 가지 문제점이 있어. 첫째, 기존 방법들은 부정확한 작은 영역을 생성해서 올바른 기여 방향을 잘못 이끌어. 둘째, 모델은 잘못 예측된 샘플에 대해서는 좋은 기여 결과를 낼 수 없어.

이 논문은 위의 문제를 해결하기 위해 이미지 기여 문제를 부분 모듈 선택 문제로 재구성했어. 이렇게 해서 더 적은 영역을 사용해 모델의 해석 가능성을 높이는 게 목적이야. 지역에 대한 주목이 부족한 문제를 해결하기 위해, 우리는 더 정확한 작은 해석 영역을 찾기 위해 새로운 부분 모듈 함수를 만들었어. 모든 샘플에 대한 기여 효과를 높이기 위해, 우리는 네 가지 다른 제약 조건(신뢰도, 효과성, 일관성, 협력 점수)을 설정해서 여러 하위 집합의 중요성을 평가해.

더 나아가, 우리의 이론 분석을 통해 제안된 함수가 실제로 부분 모듈이라는 걸 입증했어. 광범위한 실험 결과, 이 방법이 두 개의 얼굴 데이터셋(Celeb-A와 VGG-Face2)과 하나의 세분화된 데이터셋(CUB-200-2011)에서 최신 기술보다 더 나은 성능을 보여줬어. 올바르게 예측된 샘플에 대해, 이 방법은 HSIC-Attribution과 비교했을 때 평균 4.9%와 2.5%의 Deletion과 Insertion 점수를 개선했어. 잘못 예측된 샘플에 대해서는, 평균적인 최고 신뢰도와 Insertion 점수에서 HSIC-Attribution 알고리즘보다 각각 81.0%와 18.4%의 향상을 이뤘어. 코드도 이 URL에서 공개했어.

================================================================================

URL: https://arxiv.org/abs/2402.09225
Title: Is my Data in your AI Model? Membership Inference Test with Application to Face Images

Original Abstract:
This article introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if given data was used during the training of AI/ML models. Specifically, we propose two MINT architectures designed to learn the distinct activation patterns that emerge when an Audited Model is exposed to data used during its training process. These architectures are based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs). The experimental framework focuses on the challenging task of Face Recognition, considering three state-of-the-art Face Recognition systems. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Different experimental scenarios are considered depending on the context of the AI model to test. Our proposed MINT approach achieves promising results, with up to 90% accuracy, indicating the potential to recognize if an AI model has been trained with specific data. The proposed MINT approach can serve to enforce privacy and fairness in several AI applications, e.g., revealing if sensitive or private data was used for training or tuning Large Language Models (LLMs).

Translated Abstract:
이 논문에서는 Membership Inference Test (MINT)라는 새로운 방법을 소개해. 이 방법은 주어진 데이터가 AI/ML 모델 훈련에 사용됐는지 실제로 평가하는 걸 목표로 해.

특히, 우리는 Audited Model이 훈련 과정에서 사용된 데이터에 노출될 때 나타나는 독특한 활성화 패턴을 학습하도록 설계된 두 가지 MINT 아키텍처를 제안해. 이 아키텍처는 Multilayer Perceptrons (MLPs)와 Convolutional Neural Networks (CNNs)를 기반으로 하고 있어.

실험은 Face Recognition이라는 어려운 작업에 집중하고, 세 가지 최신 Face Recognition 시스템을 고려해. 우리는 총 2200만 개 이상의 얼굴 이미지로 구성된 여섯 개의 공개 데이터베이스를 사용해서 실험을 진행했어. AI 모델을 테스트하는 맥락에 따라 다양한 실험 시나리오를 고려했어.

우리의 MINT 접근 방식은 최대 90%의 정확도로 유망한 결과를 보여줘. 이는 특정 데이터로 AI 모델이 훈련됐는지를 인식할 가능성이 있다는 걸 의미해. 제안된 MINT 방법은 여러 AI 애플리케이션에서 프라이버시와 공정성을 강화하는 데 도움이 될 수 있어. 예를 들어, 민감하거나 개인적인 데이터가 대규모 언어 모델(LLMs)의 훈련이나 조정에 사용됐는지를 밝혀낼 수 있어.

================================================================================

URL: https://arxiv.org/abs/2402.15761
Title: Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective State Space Models with Deep Residual Learning

Original Abstract:
Food classification is the foundation for developing food vision tasks and plays a key role in the burgeoning field of computational nutrition. Due to the complexity of food requiring fine-grained classification, recent academic research mainly modifies Convolutional Neural Networks (CNNs) and/or Vision Transformers (ViTs) to perform food category classification. However, to learn fine-grained features, the CNN backbone needs additional structural design, whereas ViT, containing the self-attention module, has increased computational complexity. In recent months, a new Sequence State Space (S4) model, through a Selection mechanism and computation with a Scan (S6), colloquially termed Mamba, has demonstrated superior performance and computation efficiency compared to the Transformer architecture. The VMamba model, which incorporates the Mamba mechanism into image tasks (such as classification), currently establishes the state-of-the-art (SOTA) on the ImageNet dataset. In this research, we introduce an academically underestimated food dataset CNFOOD-241, and pioneer the integration of a residual learning framework within the VMamba model to concurrently harness both global and local state features inherent in the original VMamba architectural design. The research results show that VMamba surpasses current SOTA models in fine-grained and food classification. The proposed Res-VMamba further improves the classification accuracy to 79.54\% without pretrained weight. Our findings elucidate that our proposed methodology establishes a new benchmark for SOTA performance in food recognition on the CNFOOD-241 dataset. The code can be obtained on GitHub: this https URL.

Translated Abstract:
음식 분류는 음식 비전 작업을 개발하는 데 기본이 되며, 계산 영양학 분야에서 중요한 역할을 해. 음식이 복잡해서 세밀한 분류가 필요하기 때문에, 최근의 연구들은 주로 합성곱 신경망(CNN)이나 비전 트랜스포머(ViT)를 수정해서 음식 카테고리 분류를 하고 있어.

하지만 세밀한 특징을 학습하려면 CNN의 구조를 추가로 디자인해야 하고, ViT는 자기 주의 모듈 때문에 계산 복잡성이 늘어나. 최근에 새로운 시퀀스 상태 공간(S4) 모델이 선택 메커니즘과 스캔(S6) 계산을 통해, 비공식적으로 '맘바'라고 불리며 트랜스포머 아키텍처보다 더 나은 성능과 계산 효율성을 보여줬어.

VMamba 모델은 이 맘바 메커니즘을 이미지 작업(예: 분류)에 통합해서 현재 ImageNet 데이터셋에서 최첨단(SOTA)을 달성하고 있어. 이 연구에서는 학문적으로 과소평가된 음식 데이터셋인 CNFOOD-241을 소개하고, VMamba 모델에 잔여 학습 프레임워크를 통합해서 원래 VMamba 설계에서의 글로벌 및 로컬 상태 특징을 동시에 활용했어.

연구 결과에 따르면, VMamba는 세밀한 음식 분류에서 현재 SOTA 모델들을 초월했어. 제안된 Res-VMamba는 사전 학습된 가중치 없이 분류 정확도를 79.54%로 더 높였어. 우리의 연구 결과는 제안된 방법론이 CNFOOD-241 데이터셋에서 음식 인식에 대한 새로운 SOTA 성능 기준을 마련했다는 것을 보여줘. 코드도 GitHub에서 받을 수 있어: 이 https URL.

================================================================================

URL: https://arxiv.org/abs/2403.08108
Title: TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection

Original Abstract:
Task-oriented object detection aims to find objects suitable for accomplishing specific tasks. As a challenging task, it requires simultaneous visual data processing and reasoning under ambiguous semantics. Recent solutions are mainly all-in-one models. However, the object detection backbones are pre-trained without text supervision. Thus, to incorporate task requirements, their intricate models undergo extensive learning on a highly imbalanced and scarce dataset, resulting in capped performance, laborious training, and poor generalizability. In contrast, we propose TaskCLIP, a more natural two-stage design composed of general object detection and task-guided object selection. Particularly for the latter, we resort to the recently successful large Vision-Language Models (VLMs) as our backbone, which provides rich semantic knowledge and a uniform embedding space for images and texts. Nevertheless, the naive application of VLMs leads to sub-optimal quality, due to the misalignment between embeddings of object images and their visual attributes, which are mainly adjective phrases. To this end, we design a transformer-based aligner after the pre-trained VLMs to re-calibrate both embeddings. Finally, we employ a trainable score function to post-process the VLM matching results for object selection. Experimental results demonstrate that our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by 3.5% and only requires a single NVIDIA RTX 4090 for both training and inference.

Translated Abstract:
작업 지향 객체 탐지는 특정 작업을 수행하기 위한 적합한 객체를 찾는 거야. 이건 도전적인 일이어서, 애매한 의미 아래에서 시각적 데이터 처리를 동시에 하고 추론해야 해. 최근의 해결책들은 주로 올인원 모델들이야. 하지만 객체 탐지의 백본들은 텍스트 감독 없이 미리 훈련되었어. 그래서 작업 요구 사항을 통합하려고 하면, 복잡한 모델들이 불균형하고 부족한 데이터셋에서 광범위하게 학습해야 해서 성능이 제한되고, 훈련이 힘들고 일반화 능력이 떨어져.

우리의 제안은 TaskCLIP이야. 이건 일반적인 객체 탐지와 작업 유도 객체 선택으로 구성된 더 자연스러운 두 단계 디자인이야. 특히 후자에서는 최근에 성공적인 대형 비전-언어 모델(VLM)을 백본으로 사용해. 이 모델은 풍부한 의미 지식과 이미지와 텍스트를 위한 통일된 임베딩 공간을 제공해. 하지만 VLM을 단순히 적용하면, 객체 이미지와 그 시각적 속성(주로 형용사 구문) 간의 잘못된 정렬 때문에 품질이 떨어져.

그래서 우리는 미리 훈련된 VLM 뒤에 변환기 기반의 정렬기를 설계해서 두 임베딩을 다시 조정해. 마지막으로, 객체 선택을 위해 VLM 매칭 결과를 후처리하는 훈련 가능한 점수 함수를 사용해. 실험 결과, 우리의 TaskCLIP이 최신 DETR 기반 모델 TOIST보다 3.5% 더 나은 성능을 보였고, 훈련과 추론 모두에 단일 NVIDIA RTX 4090만 필요하다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2403.08498
Title: Gaussian Splatting in Style

Original Abstract:
3D scene stylization extends the work of neural style transfer to 3D. A vital challenge in this problem is to maintain the uniformity of the stylized appearance across multiple views. A vast majority of the previous works achieve this by training a 3D model for every stylized image and a set of multi-view images. In contrast, we propose a novel architecture trained on a collection of style images that, at test time, produces real time high-quality stylized novel views. We choose the underlying 3D scene representation for our model as 3D Gaussian splatting. We take the 3D Gaussians and process them using a multi-resolution hash grid and a tiny MLP to obtain stylized views. The MLP is conditioned on different style codes for generalization to different styles during test time. The explicit nature of 3D Gaussians gives us inherent advantages over NeRF-based methods, including geometric consistency and a fast training and rendering regime. This enables our method to be useful for various practical use cases, such as augmented or virtual reality. We demonstrate that our method achieves state-of-the-art performance with superior visual quality on various indoor and outdoor real-world data.

Translated Abstract:
3D 장면 스타일화는 신경 스타일 전이 기술을 3D로 확장하는 거야. 이 문제에서 중요한 도전 과제는 여러 시점에서 스타일화된 외관의 일관성을 유지하는 거야. 이전 연구의 대부분은 스타일화된 이미지와 여러 시점 이미지를 위해 각각 3D 모델을 훈련하는 방식으로 이걸 해결했어.

반면에, 우리는 스타일 이미지 모음으로 훈련된 새로운 아키텍처를 제안해. 이 방법은 테스트할 때 실시간으로 고품질의 스타일화된 새로운 뷰를 만들어내. 우리 모델의 기본 3D 장면 표현으로는 3D 가우시안 스플래팅을 선택했어. 3D 가우시안을 가져와서 멀티 해상도 해시 그리드와 작은 MLP를 사용해 스타일화된 뷰를 얻어. MLP는 다양한 스타일 코드에 따라 조정되어서 테스트할 때 다양한 스타일로 일반화할 수 있어.

3D 가우시안의 명확한 특성 덕분에 NeRF 기반 방법에 비해 기하학적 일관성, 빠른 훈련과 렌더링 등의 장점을 가져. 이 덕분에 우리의 방법은 증강 현실이나 가상 현실 같은 다양한 실용적인 용도에 유용해. 우리는 이 방법이 여러 실내외 실제 데이터에서 뛰어난 시각적 품질로 최첨단 성능을 달성한다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2403.09948
Title: RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training

Original Abstract:
The integration of artificial intelligence (AI) with radiology marks a transformative era in medicine. Vision foundation models have been adopted to enhance radiologic imaging analysis. However, the distinct complexities of radiologic 2D and 3D radiologic data pose unique challenges that existing models, pre-trained on general non-medical images, fail to address adequately. To bridge this gap and capitalize on the diagnostic precision required in radiologic imaging, we introduce Radiologic Contrastive Language-Image Pre-training (RadCLIP): a cross-modal vision-language foundational model that harnesses Vision Language Pre-training (VLP) framework to improve radiologic image analysis. Building upon Contrastive Language-Image Pre-training (CLIP), RadCLIP incorporates a slice pooling mechanism tailored for volumetric image analysis and is pre-trained using a large and diverse dataset of radiologic image-text pairs. The RadCLIP was pre-trained to effectively align radiologic images with their corresponding text annotations, creating a robust vision backbone for radiologic images. Extensive experiments demonstrate RadCLIP's superior performance in both uni-modal radiologic image classification and cross-modal image-text matching, highlighting its significant promise for improving diagnostic accuracy and efficiency in clinical settings. Our Key contributions include curating a large dataset with diverse radiologic 2D/3D radiologic image-text pairs, a slice pooling adapter using an attention mechanism for integrating 2D images, and comprehensive evaluations of RadCLIP on various radiologic downstream tasks.

Translated Abstract:
인공지능(AI)과 방사선학의 통합은 의학에서 큰 변화를 가져오고 있어. 최근 방사선 이미지를 분석하는 데 비전 기초 모델들이 사용되고 있는데, 2D와 3D 방사선 데이터의 복잡성 때문에 기존 모델들이 잘 대응하지 못하고 있어. 이 모델들은 일반적인 비의료 이미지를 기반으로 사전 훈련되었거든.

이런 문제를 해결하고 방사선 이미지를 분석하는 데 필요한 정확성을 높이기 위해 우리는 방사선 대비 언어-이미지 사전 훈련(RadCLIP)을 소개해. 이건 비전 언어 사전 훈련(VLP) 프레임워크를 활용한 크로스 모달 비전-언어 기초 모델이야. RadCLIP은 볼륨 이미지 분석을 위해 맞춤형 슬라이스 풀링 메커니즘을 가지고 있고, 다양한 방사선 이미지-텍스트 쌍으로 구성된 대규모 데이터셋으로 사전 훈련되었어.

RadCLIP은 방사선 이미지와 그에 해당하는 텍스트 주석을 효과적으로 맞추도록 사전 훈련되었고, 방사선 이미지를 위한 강력한 비전 기반을 만들어냈어. 여러 실험 결과 RadCLIP이 단일 모달 방사선 이미지 분류와 크로스 모달 이미지-텍스트 매칭에서 뛰어난 성능을 보였고, 임상 환경에서 진단 정확성과 효율성을 높일 수 있는 가능성이 큰 것으로 나타났어.

우리의 주요 기여는 다양한 방사선 2D/3D 이미지-텍스트 쌍으로 구성된 대규모 데이터셋을 만드는 것, 2D 이미지를 통합하기 위한 주의 메커니즘을 사용하는 슬라이스 풀링 어댑터 개발, 그리고 다양한 방사선 하위 작업에서 RadCLIP을 종합적으로 평가한 거야.

================================================================================

URL: https://arxiv.org/abs/2403.17712
Title: Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A New Benchmark

Original Abstract:
The widespread use of various chemical gases in industrial processes necessitates effective measures to prevent their leakage during transportation and storage, given their high toxicity. Thermal infrared-based computer vision detection techniques provide a straightforward approach to identify gas leakage areas. However, the development of high-quality algorithms has been challenging due to the low texture in thermal images and the lack of open-source datasets. In this paper, we present the RGB-Thermal Cross Attention Network (RT-CAN), which employs an RGB-assisted two-stream network architecture to integrate texture information from RGB images and gas area information from thermal images. Additionally, to facilitate the research of invisible gas detection, we introduce Gas-DB, an extensive open-source gas detection database including about 1.3K well-annotated RGB-thermal images with eight variant collection scenes. Experimental results demonstrate that our method successfully leverages the advantages of both modalities, achieving state-of-the-art (SOTA) performance among RGB-thermal methods, surpassing single-stream SOTA models in terms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%, and 4.88%, respectively. The code and data can be found at this https URL.

Translated Abstract:
산업 과정에서 다양한 화학 가스를 많이 사용하다 보니, 이 가스들이 운송이나 저장 중에 새는 걸 막는 게 중요해. 특히 이 가스들은 독성이 강하니까. 열 적외선 기반의 컴퓨터 비전 기술이 가스 누출 지역을 쉽게 찾아내는 데 도움을 줄 수 있어. 하지만 열화상 이미지의 질감이 낮고 공개 데이터셋이 부족해서 고품질 알고리즘을 개발하는 건 어려웠어.

이 논문에서는 RGB-열 교차 주의 네트워크(RT-CAN)를 소개해. 이건 RGB 이미지의 질감 정보와 열 이미지의 가스 지역 정보를 통합하기 위해 RGB 보조 이중 스트림 네트워크 구조를 사용해. 또, 보이지 않는 가스 감지를 연구하기 쉽게 하기 위해, 약 1,300개의 잘 주석이 달린 RGB-열 이미지와 여덟 가지 다양한 수집 장면을 포함한 대규모 오픈소스 가스 감지 데이터베이스인 Gas-DB도 소개해.

실험 결과, 우리의 방법이 두 가지 모달리티의 장점을 잘 활용해서 RGB-열 방법 중에서 최신 기술(SOTA) 성능을 달성했어. 정확도, 교차 비율(IoU), F2 지표에서 각각 4.86%, 5.65%, 4.88% 더 나은 성과를 냈어. 코드와 데이터는 이 URL에서 찾아볼 수 있어.

================================================================================

URL: https://arxiv.org/abs/2404.17400
Title: Spatial-frequency Dual-Domain Feature Fusion Network for Low-Light Remote Sensing Image Enhancement

Original Abstract:
Low-light remote sensing images generally feature high resolution and high spatial complexity, with continuously distributed surface features in space. This continuity in scenes leads to extensive long-range correlations in spatial domains within remote sensing images. Convolutional Neural Networks, which rely on local correlations for long-distance modeling, struggle to establish long-range correlations in such images. On the other hand, transformer-based methods that focus on global information face high computational complexities when processing high-resolution remote sensing images. From another perspective, Fourier transform can compute global information without introducing a large number of parameters, enabling the network to more efficiently capture the overall image structure and establish long-range correlations. Therefore, we propose a Dual-Domain Feature Fusion Network (DFFN) for low-light remote sensing image enhancement. Specifically, this challenging task of low-light enhancement is divided into two more manageable sub-tasks: the first phase learns amplitude information to restore image brightness, and the second phase learns phase information to refine details. To facilitate information exchange between the two phases, we designed an information fusion affine block that combines data from different phases and scales. Additionally, we have constructed two dark light remote sensing datasets to address the current lack of datasets in dark light remote sensing image enhancement. Extensive evaluations show that our method outperforms existing state-of-the-art methods. The code is available at this https URL.

Translated Abstract:
저조도 원격 감지 이미지는 일반적으로 고해상도와 높은 공간 복잡성을 가지고 있어, 공간에서 연속적으로 분포된 표면 특징을 가지고 있어. 이런 장면의 연속성 때문에 원격 감지 이미지 내에서 광범위한 장거리 상관관계가 생겨. 하지만, 지역 상관관계에 의존하는 컨볼루션 신경망은 이런 이미지에서 장거리 상관관계를 잘 잡지 못해. 반면에, 전 세계 정보를 중점적으로 다루는 트랜스포머 기반 방법은 고해상도 원격 감지 이미지를 처리할 때 계산 복잡성이 너무 높아.

또 다른 관점에서, 푸리에 변환은 많은 매개변수를 추가하지 않고도 전 세계 정보를 계산할 수 있어서 네트워크가 전체 이미지 구조를 더 효율적으로 포착하고 장거리 상관관계를 확립할 수 있게 해. 그래서 우리는 저조도 원격 감지 이미지 향상을 위한 이중 영역 특징 융합 네트워크(DFFN)를 제안해. 구체적으로, 저조도 향상의 어려운 작업을 두 개의 더 관리하기 쉬운 하위 작업으로 나누었어: 첫 번째 단계에서는 진폭 정보를 학습해서 이미지 밝기를 복원하고, 두 번째 단계에서는 위상 정보를 학습해서 세부 사항을 다듬어.

두 단계 간 정보 교환을 돕기 위해서, 우리는 서로 다른 단계와 스케일의 데이터를 결합하는 정보 융합 아핀 블록을 설계했어. 게다가, 저조도 원격 감지 이미지 향상을 위한 데이터셋이 부족한 현재 상황을 해결하기 위해 두 개의 어두운 조명 원격 감지 데이터셋을 구축했어. 여러 평가 결과, 우리의 방법이 기존의 최신 방법들을 능가한다는 것이 입증되었어. 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2405.05953
Title: Frame Interpolation with Consecutive Brownian Bridge Diffusion

Original Abstract:
Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a diffusion-based conditional image generation problem, synthesizing the intermediate frame given a random noise and neighboring frames. Due to the relatively high resolution of videos, Latent Diffusion Models (LDMs) are employed as the conditional generation model, where the autoencoder compresses images into latent representations for diffusion and then reconstructs images from these latent representations. Such a formulation poses a crucial challenge: VFI expects that the output is deterministically equal to the ground truth intermediate frame, but LDMs randomly generate a diverse set of different images when the model runs multiple times. The reason for the diverse generation is that the cumulative variance (variance accumulated at each step of generation) of generated latent representations in LDMs is large. This makes the sampling trajectory random, resulting in diverse rather than deterministic generations. To address this problem, we propose our unique solution: Frame Interpolation with Consecutive Brownian Bridge Diffusion. Specifically, we propose consecutive Brownian Bridge diffusion that takes a deterministic initial value as input, resulting in a much smaller cumulative variance of generated latent representations. Our experiments suggest that our method can improve together with the improvement of the autoencoder and achieve state-of-the-art performance in VFI, leaving strong potential for further enhancement.

Translated Abstract:
최근 비디오 프레임 보간(Video Frame Interpolation, VFI) 관련 연구는 VFI를 확산 기반의 조건부 이미지 생성 문제로 정의하고, 랜덤 노이즈와 이웃 프레임을 이용해 중간 프레임을 합성하는 방법을 시도하고 있어. 비디오의 해상도가 상대적으로 높기 때문에, 조건부 생성 모델로 잠재 확산 모델(Latent Diffusion Models, LDMs)을 사용해. 여기서 오토인코더는 이미지를 잠재 표현으로 압축한 후, 이 잠재 표현으로부터 이미지를 재구성해.

하지만 이런 방식에는 중요한 문제가 있어. VFI는 출력 결과가 중간 프레임의 실제 값과 정확히 같아야 하지만, LDMs는 모델이 여러 번 실행될 때마다 다양한 이미지를 무작위로 생성해. 이런 다양한 생성의 이유는 LDMs에서 생성된 잠재 표현들의 누적 분산이 크기 때문이야. 이 때문에 샘플링 경로가 무작위로 변하고, 결과적으로 다채로운 생성이 이루어져.

이 문제를 해결하기 위해 우리는 독창적인 방법을 제안해: 연속 브라운 다리 확산(Consecutive Brownian Bridge Diffusion)을 이용한 프레임 보간. 구체적으로, 우리가 제안하는 연속 브라운 다리 확산은 결정적인 초기 값을 입력으로 받아서, 생성된 잠재 표현의 누적 분산을 훨씬 작게 만들어. 우리의 실험 결과, 이 방법이 오토인코더의 개선과 함께 성능을 향상시킬 수 있고, VFI에서 최첨단 성과를 달성할 수 있는 가능성이 크다는 것을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2405.14874
Title: Open-Vocabulary Object Detectors: Robustness Challenges under Distribution Shifts

Original Abstract:
The challenge of Out-Of-Distribution (OOD) robustness remains a critical hurdle towards deploying deep vision models. Vision-Language Models (VLMs) have recently achieved groundbreaking results. VLM-based open-vocabulary object detection extends the capabilities of traditional object detection frameworks, enabling the recognition and classification of objects beyond predefined categories. Investigating OOD robustness in recent open-vocabulary object detection is essential to increase the trustworthiness of these models. This study presents a comprehensive robustness evaluation of the zero-shot capabilities of three recent open-vocabulary (OV) foundation object detection models: OWL-ViT, YOLO World, and Grounding DINO. Experiments carried out on the robustness benchmarks COCO-O, COCO-DC, and COCO-C encompassing distribution shifts due to information loss, corruption, adversarial attacks, and geometrical deformation, highlighting the challenges of the model's robustness to foster the research for achieving robustness. Project page: this https URL

Translated Abstract:
Out-Of-Distribution (OOD) 강인성 문제는 딥 비전 모델을 실제로 사용하는 데 큰 장애물로 남아 있어. 최근에 비전-언어 모델(VLMs)이 혁신적인 결과를 보여줬어. VLM을 기반으로 한 열린 어휘 객체 탐지는 전통적인 객체 탐지 프레임워크의 능력을 확장해서 미리 정해진 카테고리를 넘어 다양한 객체를 인식하고 분류할 수 있게 해.

최근의 열린 어휘 객체 탐지에서 OOD 강인성을 조사하는 건 이 모델들의 신뢰성을 높이는 데 중요해. 이 연구는 OWL-ViT, YOLO World, Grounding DINO라는 세 가지 최신 열린 어휘(OV) 기반 객체 탐지 모델의 제로샷 능력에 대한 종합적인 강인성 평가를 제시해. 

COCO-O, COCO-DC, COCO-C 같은 강인성 벤치마크에서 실험을 진행했어. 이 벤치마크는 정보 손실, 손상, 적대적 공격, 기하학적 변형 같은 분포 변화를 포함하고 있어. 이 연구는 모델의 강인성 문제를 돋보이게 하면서 더 강한 모델을 만들기 위한 연구를 촉진하고자 해.

================================================================================

URL: https://arxiv.org/abs/2406.04888
Title: Zero-Shot Video Editing through Adaptive Sliding Score Distillation

Original Abstract:
The rapidly evolving field of Text-to-Video generation (T2V) has catalyzed renewed interest in controllable video editing research. While the application of editing prompts to guide diffusion model denoising has gained prominence, mirroring advancements in image editing, this noise-based inference process inherently compromises the original video's integrity, resulting in unintended over-editing and temporal discontinuities. To address these challenges, this study proposes a novel paradigm of video-based score distillation, facilitating direct manipulation of original video content. Specifically, distinguishing it from image-based score distillation, we propose an Adaptive Sliding Score Distillation strategy, which incorporates both global and local video guidance to reduce the impact of editing errors. Combined with our proposed Image-based Joint Guidance mechanism, it has the ability to mitigate the inherent instability of the T2V model and single-step sampling. Additionally, we design a Weighted Attention Fusion module to further preserve the key features of the original video and avoid over-editing. Extensive experiments demonstrate that these strategies effectively address existing challenges, achieving superior performance compared to current state-of-the-art methods.

Translated Abstract:
텍스트에서 비디오로 변환하는 기술(T2V)이 빠르게 발전하면서, 조절 가능한 비디오 편집 연구에 대한 관심이 다시 높아지고 있어. 편집 프롬프트를 사용해서 확산 모델의 노이즈 제거를 유도하는 방법이 유명해졌는데, 이건 이미지 편집의 발전과 비슷해. 하지만 이 노이즈 기반 추론 과정은 원래 비디오의 품질을 해치기 때문에, 원치 않는 과도한 편집이나 시간적 단절이 발생할 수 있어.

이런 문제를 해결하기 위해, 이 연구에서는 비디오 기반 점수 증류라는 새로운 방식을 제안해. 이 방식은 원본 비디오 내용을 직접 다룰 수 있게 해줘. 특히 이미지 기반 점수 증류와는 다르게, 우리는 글로벌 및 로컬 비디오 가이드를 포함한 적응형 슬라이딩 점수 증류 전략을 제안해. 이 방법은 편집 오류의 영향을 줄여줘.

또한, 우리는 원본 비디오의 핵심 특징을 더 잘 보호하고 과도한 편집을 피하기 위해 가중치 주의 융합 모듈도 설계했어. 여러 실험을 통해 이 전략들이 기존의 문제를 효과적으로 해결할 수 있음을 입증했으며, 현재의 최첨단 방법들보다 더 우수한 성능을 달성했어.

================================================================================

URL: https://arxiv.org/abs/2406.10126
Title: Training-free Camera Control for Video Generation

Original Abstract:
We propose a training-free and robust solution to offer camera movement control for off-the-shelf video diffusion models. Unlike previous work, our method does not require any supervised finetuning on camera-annotated datasets or self-supervised training via data augmentation. Instead, it can be plugged and played with most pretrained video diffusion models and generate camera controllable videos with a single image or text prompt as input. The inspiration of our work comes from the layout prior that intermediate latents hold towards generated results, thus rearranging noisy pixels in them will make output content reallocated as well. As camera move could also be seen as a kind of pixel rearrangement caused by perspective change, videos could be reorganized following specific camera motion if their noisy latents change accordingly. Established on this, we propose our method CamTrol, which enables robust camera control for video diffusion models. It is achieved by a two-stage process. First, we model image layout rearrangement through explicit camera movement in 3D point cloud space. Second, we generate videos with camera motion using layout prior of noisy latents formed by a series of rearranged images. Extensive experiments have demonstrated the robustness our method holds in controlling camera motion of generated videos. Furthermore, we show that our method can produce impressive results in generating 3D rotation videos with dynamic content. Project page at this https URL.

Translated Abstract:
우리는 카메라 움직임 제어를 위한 훈련이 필요 없는 강력한 솔루션을 제안해. 기존 연구와는 달리, 우리의 방법은 카메라 주석이 달린 데이터셋에서 감독 학습을 하거나 데이터 증강을 통한 자기 지도 학습이 필요 없어. 대신, 대부분의 사전 훈련된 비디오 확산 모델에 바로 연결할 수 있고, 단 한 장의 이미지나 텍스트 프롬프트를 입력으로 받아 카메라 제어가 가능한 비디오를 생성할 수 있어.

우리 연구의 영감은 생성된 결과에 대한 중간 잠재 변수의 레이아웃 우선순리에서 왔어. 그래서 이 중간 잠재 변수의 노이즈 픽셀을 재배열하면 출력 내용도 재배치될 수 있어. 카메라 이동도 관점 변화로 인해 발생하는 픽셀 재배열의 일종으로 볼 수 있어서, 만약 노이즈 잠재 변수가 적절히 변하면 특정 카메라 움직임에 따라 비디오를 재구성할 수 있어. 이를 바탕으로, 우리는 CamTrol이라는 방법을 제안해. 이 방법은 비디오 확산 모델에서 강력한 카메라 제어를 가능하게 해.

이건 두 단계로 이루어져 있어. 첫 번째로, 3D 포인트 클라우드 공간에서 명시적인 카메라 움직임을 통해 이미지 레이아웃 재배열을 모델링해. 두 번째로, 재배열된 이미지 시리즈로 형성된 노이즈 잠재 변수의 레이아웃 우선순리를 사용해 카메라 움직임이 있는 비디오를 생성해. 많은 실험을 통해 우리의 방법이 생성된 비디오의 카메라 움직임을 제어하는 데 강력하다는 것을 보여줬어. 게다가, 우리의 방법이 동적인 내용을 가진 3D 회전 비디오를 생성하는 데도 인상적인 결과를 낼 수 있음을 보여줘. 프로젝트 페이지는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2407.05278
Title: HyperKAN: Kolmogorov-Arnold Networks make Hyperspectral Image Classificators Smarter

Original Abstract:
In traditional neural network architectures, a multilayer perceptron (MLP) is typically employed as a classification block following the feature extraction stage. However, the Kolmogorov-Arnold Network (KAN) presents a promising alternative to MLP, offering the potential to enhance prediction accuracy. In this paper, we propose the replacement of linear and convolutional layers of traditional networks with KAN-based counterparts. These modifications allowed us to significantly increase the per-pixel classification accuracy for hyperspectral remote-sensing images. We modified seven different neural network architectures for hyperspectral image classification and observed a substantial improvement in the classification accuracy across all the networks. The architectures considered in the paper include baseline MLP, state-of-the-art 1D (1DCNN) and 3D convolutional (two different 3DCNN, NM3DCNN), and transformer (SSFTT) architectures, as well as newly proposed M1DCNN. The greatest effect was achieved for convolutional networks working exclusively on spectral data, and the best classification quality was achieved using a KAN-based transformer architecture. All the experiments were conducted using seven openly available hyperspectral datasets. Our code is available at this https URL.

Translated Abstract:
전통적인 신경망 구조에서는 다층 퍼셉트론(MLP)이 특징 추출 단계 다음에 분류 블록으로 주로 사용돼. 그런데 콜모고로프-아놀드 네트워크(KAN)는 MLP에 대한 좋은 대안으로, 예측 정확도를 높일 수 있는 가능성을 보여줘. 

이 논문에서는 전통적인 네트워크의 선형 및 컨볼루션 레이어를 KAN 기반으로 교체할 것을 제안해. 이런 수정 덕분에 하이퍼스펙트럴 원거리 감지 이미지에 대해 픽셀별 분류 정확도를 크게 높일 수 있었어. 우리는 하이퍼스펙트럴 이미지 분류를 위해 일곱 가지 다른 신경망 구조를 수정했는데, 모든 네트워크에서 분류 정확도가 상당히 개선되는 걸 확인했어. 

논문에서 고려한 구조는 기본 MLP, 최신 1D(1DCNN) 및 3D 컨볼루션(두 가지 다른 3DCNN, NM3DCNN), 트랜스포머(SSFTT) 구조와 새로 제안한 M1DCNN이 포함돼. 컨볼루션 네트워크가 스펙트럼 데이터에만 작업할 때 가장 큰 효과를 봤고, KAN 기반 트랜스포머 구조를 사용했을 때 최고의 분류 품질을 달성했어. 모든 실험은 일곱 개의 공개된 하이퍼스펙트럴 데이터셋을 사용해 진행했어. 우리의 코드는 이 https URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2407.08061
Title: Geospecific View Generation -- Geometry-Context Aware High-resolution Ground View Inference from Satellite Views

Original Abstract:
Predicting realistic ground views from satellite imagery in urban scenes is a challenging task due to the significant view gaps between satellite and ground-view images. We propose a novel pipeline to tackle this challenge, by generating geospecifc views that maximally respect the weak geometry and texture from multi-view satellite images. Different from existing approaches that hallucinate images from cues such as partial semantics or geometry from overhead satellite images, our method directly predicts ground-view images at geolocation by using a comprehensive set of information from the satellite image, resulting in ground-level images with a resolution boost at a factor of ten or more. We leverage a novel building refinement method to reduce geometric distortions in satellite data at ground level, which ensures the creation of accurate conditions for view synthesis using diffusion networks. Moreover, we proposed a novel geospecific prior, which prompts distribution learning of diffusion models to respect image samples that are closer to the geolocation of the predicted images. We demonstrate our pipeline is the first to generate close-to-real and geospecific ground views merely based on satellite images.

Translated Abstract:
위성 이미지에서 도시 장면의 실제 지면 뷰를 예측하는 것은 어려운 과제야. 왜냐하면 위성 이미지와 지면 이미지 사이에 큰 시각적 차이가 있기 때문이지. 우리는 이 문제를 해결하기 위해 새로운 방법을 제안해. 이 방법은 여러 위성 이미지에서 얻은 약한 기하학적 정보와 질감을 최대한 존중하면서 지리적으로 특정한 뷰를 생성해.

기존의 접근 방식들과는 다르게, 우리는 위성 이미지에서 얻은 다양한 정보를 사용해서 지리적 위치에서 직접적으로 지면 이미지를 예측해. 이로 인해 지면 이미지의 해상도가 10배 이상 향상돼. 우리는 새로운 건물 정제 방법을 활용해서 지면 수준에서 위성 데이터의 기하학적 왜곡을 줄이는데, 이게 확산 네트워크를 사용한 뷰 합성을 위한 정확한 조건을 만들어줘.

또한, 우리는 새로운 지리적 사전 지식을 제안했어. 이건 확산 모델이 예측된 이미지의 지리적 위치에 더 가까운 이미지 샘플을 존중하도록 분포 학습을 촉진해. 우리는 우리의 방법이 단지 위성 이미지만으로도 현실에 가까운 지리적 특정 지면 뷰를 생성하는 첫 번째 방법임을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2407.11625
Title: Beware of Validation by Eye: Visual Validation of Linear Trends in Scatterplots

Original Abstract:
Visual validation of regression models in scatterplots is a common practice for assessing model quality, yet its efficacy remains unquantified. We conducted two empirical experiments to investigate individuals' ability to visually validate linear regression models (linear trends) and to examine the impact of common visualization designs on validation quality. The first experiment showed that the level of accuracy for visual estimation of slope (i.e., fitting a line to data) is higher than for visual validation of slope (i.e., accepting a shown line). Notably, we found bias toward slopes that are "too steep" in both cases. This lead to novel insights that participants naturally assessed regression with orthogonal distances between the points and the line (i.e., ODR regression) rather than the common vertical distances (OLS regression). In the second experiment, we investigated whether incorporating common designs for regression visualization (error lines, bounding boxes, and confidence intervals) would improve visual validation. Even though error lines reduced validation bias, results failed to show the desired improvements in accuracy for any design. Overall, our findings suggest caution in using visual model validation for linear trends in scatterplots.

Translated Abstract:
산점도에서 회귀 모델의 시각적 검증은 모델 품질을 평가하는 일반적인 방법이지만, 그 효과는 아직 정량화되지 않았습니다. 우리는 사람들이 선형 회귀 모델(선형 경향)을 시각적으로 검증할 수 있는 능력을 조사하기 위해 두 가지 실험을 실시했습니다. 또한, 시각화 디자인이 검증 품질에 미치는 영향을 살펴봤습니다.

첫 번째 실험에서는 기울기를 시각적으로 추정하는 정확도가 기울기를 시각적으로 검증하는 것보다 더 높다는 걸 발견했습니다. 특히, 두 경우 모두 "너무 가파른" 기울기에 대한 편향이 있다는 점이 흥미로웠습니다. 참가자들은 일반적으로 수직 거리(OLS 회귀)가 아니라 점과 선 사이의 직교 거리(ODR 회귀)를 사용해 회귀를 평가한다는 새로운 통찰을 얻었습니다.

두 번째 실험에서는 회귀 시각화를 위한 일반적인 디자인(오차선, 경계 상자, 신뢰 구간)을 포함하면 시각적 검증이 개선될지 조사했습니다. 오차선이 검증 편향을 줄였음에도 불구하고, 어떤 디자인에서도 정확도가 개선되지 않았습니다. 전반적으로, 우리의 발견은 산점도에서 선형 경향에 대한 시각적 모델 검증 사용에 신중할 필요가 있다는 것을 시사합니다.

================================================================================

URL: https://arxiv.org/abs/2407.11633
Title: Scaling Diffusion Transformers to 16 Billion Parameters

Original Abstract:
In this paper, we present DiT-MoE, a sparse version of the diffusion Transformer, that is scalable and competitive with dense networks while exhibiting highly optimized inference. The DiT-MoE includes two simple designs: shared expert routing and expert-level balance loss, thereby capturing common knowledge and reducing redundancy among the different routed experts. When applied to conditional image generation, a deep analysis of experts specialization gains some interesting observations: (i) Expert selection shows preference with spatial position and denoising time step, while insensitive with different class-conditional information; (ii) As the MoE layers go deeper, the selection of experts gradually shifts from specific spacial position to dispersion and balance. (iii) Expert specialization tends to be more concentrated at the early time step and then gradually uniform after half. We attribute it to the diffusion process that first models the low-frequency spatial information and then high-frequency complex information. Based on the above guidance, a series of DiT-MoE experimentally achieves performance on par with dense networks yet requires much less computational load during inference. More encouragingly, we demonstrate the potential of DiT-MoE with synthesized image data, scaling diffusion model at a 16.5B parameter that attains a new SoTA FID-50K score of 1.80 in 512$\times$512 resolution settings. The project page: this https URL.

Translated Abstract:
이 논문에서는 DiT-MoE라는 희소한 형태의 확산 변환기(diffusion Transformer)를 소개해. 이 모델은 확장 가능하고, 밀집 네트워크(dense networks)와 경쟁할 수 있으며, 추론(inference)에서도 매우 최적화되어 있어.

DiT-MoE는 두 가지 간단한 설계를 포함해: 공유 전문가 라우팅(shared expert routing)과 전문가 수준의 균형 손실(expert-level balance loss). 이 덕분에 여러 전문가들 사이에서 공통 지식을 포착하고 중복을 줄일 수 있어. 조건부 이미지 생성에 적용했을 때, 전문가의 전문화에 대한 깊은 분석에서 몇 가지 흥미로운 관찰 결과가 나왔어: 

(i) 전문가 선택은 공간 위치와 노이즈 제거 단계(denoising time step)에 따라 선호도가 나타나고, 다른 클래스 조건 정보에는 둔감해.  
(ii) MoE 레이어가 깊어질수록 전문가 선택이 특정 공간 위치에서 분산 및 균형으로 점차 이동해.  
(iii) 전문가 전문화는 초기 시간 단계에서 더 집중되고, 이후 절반이 지나면서 점차 균일해지는 경향이 있어. 이건 확산 과정이 먼저 저주파 공간 정보를 모델링하고, 그 다음 고주파 복잡한 정보를 모델링하기 때문이라고 생각해.

위의 내용에 기반해, DiT-MoE는 밀집 네트워크와 비슷한 성능을 실험적으로 달성하면서도 추론 시 훨씬 적은 계산 부하가 필요해. 더 고무적인 건, DiT-MoE가 합성 이미지 데이터를 통해 16.5B 파라미터 규모의 확산 모델을 확장할 수 있는 잠재력을 보여준다는 거야. 이 모델은 512$\times$512 해상도 설정에서 새로운 SoTA FID-50K 점수인 1.80을 달성했어. 프로젝트 페이지는 여기 있어: 이 URL.

================================================================================

URL: https://arxiv.org/abs/2408.02049
Title: 3D Single-object Tracking in Point Clouds with High Temporal Variation

Original Abstract:
The high temporal variation of the point clouds is the key challenge of 3D single-object tracking (3D SOT). Existing approaches rely on the assumption that the shape variation of the point clouds and the motion of the objects across neighboring frames are smooth, failing to cope with high temporal variation data. In this paper, we present a novel framework for 3D SOT in point clouds with high temporal variation, called HVTrack. HVTrack proposes three novel components to tackle the challenges in the high temporal variation scenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud shape variations; 2) a Base-Expansion Feature Cross-Attention module to deal with similar object distractions in expanded search areas; 3) a Contextual Point Guided Self-Attention module for suppressing heavy background noise. We construct a dataset with high temporal variation (KITTI-HV) by setting different frame intervals for sampling in the KITTI dataset. On the KITTI-HV with 5 frame intervals, our HVTrack surpasses the state-of-the-art tracker CXTracker by 11.3%/15.7% in Success/Precision.

Translated Abstract:
3D 단일 물체 추적(3D SOT)에서 가장 큰 문제는 포인트 클라우드의 시간적 변화가 크다는 거야. 기존 방법들은 포인트 클라우드의 형태 변화와 인접한 프레임에서 물체의 움직임이 부드럽다고 가정하는데, 그래서 시간적 변화가 큰 데이터에는 잘 대응하지 못해.

이 논문에서는 시간적 변화가 큰 포인트 클라우드에서 3D SOT를 위한 새로운 프레임워크인 HVTrack을 소개해. HVTrack은 높은 시간적 변화 상황에서의 문제를 해결하기 위해 세 가지 새로운 요소를 제안해: 

1) 상대 자세 인식 메모리 모듈은 시간에 따른 포인트 클라우드 형태 변화를 처리해.
2) 기본 확장 특징 교차 주의 모듈은 확장된 탐색 영역에서 비슷한 물체의 방해를 다뤄.
3) 맥락 포인트 유도 자기 주의 모듈은 많은 배경 잡음을 억제해.

우리는 KITTI 데이터셋에서 샘플링할 때 다른 프레임 간격을 설정하여 높은 시간적 변화를 가진 데이터셋(KITTI-HV)을 만들었어. 5 프레임 간격의 KITTI-HV에서, 우리 HVTrack은 최신 추적기인 CXTracker보다 성공률 11.3%와 정밀도 15.7%를 초과했어.

================================================================================

URL: https://arxiv.org/abs/2408.05939
Title: UniPortrait: A Unified Framework for Identity-Preserving Single- and Multi-Human Image Personalization

Original Abstract:
This paper presents UniPortrait, an innovative human image personalization framework that unifies single- and multi-ID customization with high face fidelity, extensive facial editability, free-form input description, and diverse layout generation. UniPortrait consists of only two plug-and-play modules: an ID embedding module and an ID routing module. The ID embedding module extracts versatile editable facial features with a decoupling strategy for each ID and embeds them into the context space of diffusion models. The ID routing module then combines and distributes these embeddings adaptively to their respective regions within the synthesized image, achieving the customization of single and multiple IDs. With a carefully designed two-stage training scheme, UniPortrait achieves superior performance in both single- and multi-ID customization. Quantitative and qualitative experiments demonstrate the advantages of our method over existing approaches as well as its good scalability, e.g., the universal compatibility with existing generative control tools. The project page is at this https URL .

Translated Abstract:
이 논문에서는 UniPortrait라는 혁신적인 인물 이미지 개인화 프레임워크를 소개해. 이 시스템은 단일 및 다중 ID 커스터마이징을 통합하고, 얼굴의 세밀함을 유지하면서도, 다양한 얼굴 편집과 자유로운 입력 설명, 다양한 레이아웃 생성을 지원해.

UniPortrait는 두 개의 플러그 앤 플레이 모듈로 구성돼: ID 임베딩 모듈과 ID 라우팅 모듈. ID 임베딩 모듈은 각 ID에 대해 다양한 편집 가능한 얼굴 특징을 추출하고, 이 특징들을 확산 모델의 맥락 공간에 임베드해. 그 다음 ID 라우팅 모듈이 이 임베딩을 조합하고, 합성된 이미지의 해당 영역으로 적응적으로 분배해. 이렇게 해서 단일 및 다중 ID의 커스터마이징이 가능해.

신중하게 설계된 2단계 훈련 방식 덕분에 UniPortrait는 단일 및 다중 ID 커스터마이징 모두에서 뛰어난 성능을 보여줘. 정량적 및 정성적 실험 결과, 우리 방법이 기존 방법들보다 더 나은 점을 보여주고, 확장성도 좋다는 걸 입증했어. 예를 들어, 기존의 생성 제어 도구와의 보편적인 호환성이 있어. 프로젝트 페이지는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2408.09126
Title: Barbie: Text to Barbie-Style 3D Avatars

Original Abstract:
Recent advances in text-guided 3D avatar generation have made substantial progress by distilling knowledge from diffusion models. Despite the plausible generated appearance, existing methods cannot achieve fine-grained disentanglement or high-fidelity modeling between inner body and outfit. In this paper, we propose Barbie, a novel framework for generating 3D avatars that can be dressed in diverse and high-quality Barbie-like garments and accessories. Instead of relying on a holistic model, Barbie achieves fine-grained disentanglement on avatars by semantic-aligned separated models for human body and outfits. These disentangled 3D representations are then optimized by different expert models to guarantee the domain-specific fidelity. To balance geometry diversity and reasonableness, we propose a series of losses for template-preserving and human-prior evolving. The final avatar is enhanced by unified texture refinement for superior texture consistency. Extensive experiments demonstrate that Barbie outperforms existing methods in both dressed human and outfit generation, supporting flexible apparel combination and animation. The code will be released for research purposes. Our project page is: this https URL.

Translated Abstract:
최근 텍스트 기반 3D 아바타 생성 기술이 발전하면서, 확산 모델의 지식을 활용해 많은 진전을 이뤘어. 하지만 기존 방법들은 내부 몸체와 의상 사이의 세밀한 분리나 높은 품질의 모델링을 제대로 수행하지 못해. 

이 논문에서는 다양한 고품질의 바비 스타일 의상과 액세서리로 입힐 수 있는 3D 아바타를 생성하는 새로운 프레임워크인 바비(Barbie)를 제안해. 기존의 통합 모델에 의존하는 대신, 바비는 인체와 의상을 위한 의미적으로 정렬된 분리 모델을 통해 아바타의 세밀한 분리를 이뤄내. 이렇게 분리된 3D 표현은 각기 다른 전문 모델에 의해 최적화되어 특정 도메인에 맞는 품질을 보장해.

기하학적인 다양성과 합리성을 균형 있게 맞추기 위해, 우리는 템플릿 보존과 인간 우선 진화를 위한 일련의 손실 함수를 제안해. 최종 아바타는 통합 텍스처 정제를 통해 우수한 텍스처 일관성을 갖게 돼. 

다양한 실험을 통해 바비가 기존 방법들보다 의상 착용된 인간과 의상 생성에서 더 뛰어난 성능을 보인다는 걸 입증했어. 이로 인해 유연한 의류 조합과 애니메이션이 가능해. 코드는 연구 목적으로 공개될 예정이야. 프로젝트 페이지는 이 URL이야.

================================================================================

URL: https://arxiv.org/abs/2408.09928
Title: DiscoNeRF: Class-Agnostic Object Field for 3D Object Discovery

Original Abstract:
Neural Radiance Fields (NeRFs) have become a powerful tool for modeling 3D scenes from multiple images. However, NeRFs remain difficult to segment into semantically meaningful regions. Previous approaches to 3D segmentation of NeRFs either require user interaction to isolate a single object, or they rely on 2D semantic masks with a limited number of classes for supervision. As a consequence, they generalize poorly to class-agnostic masks automatically generated in real scenes. This is attributable to the ambiguity arising from zero-shot segmentation, yielding inconsistent masks across views. In contrast, we propose a method that is robust to inconsistent segmentations and successfully decomposes the scene into a set of objects of any class. By introducing a limited number of competing object slots against which masks are matched, a meaningful object representation emerges that best explains the 2D supervision and minimizes an additional regularization term. Our experiments demonstrate the ability of our method to generate 3D panoptic segmentations on complex scenes, and extract high-quality 3D assets from NeRFs that can then be used in virtual 3D environments.

Translated Abstract:
신경 방사장(NeRFs)은 여러 이미지에서 3D 장면을 모델링하는 데 강력한 도구가 되었어. 하지만 NeRFs를 의미 있는 영역으로 나누는 건 여전히 어려워. 이전의 3D 분할 방법은 사용자가 직접 개체를 구분해야 하거나, 제한된 수의 클래스만 있는 2D 의미 마스크에 의존했어. 그래서 실제 장면에서 자동으로 생성된 클래스 무관 마스크에는 잘 적용되지 않아. 이건 제로샷 분할에서 오는 모호성 때문인데, 이로 인해 서로 다른 뷰에서 일관성 없는 마스크가 생성돼.

반면, 우리는 일관성 없는 분할에 강한 방법을 제안해. 이 방법은 장면을 어떤 클래스의 개체 집합으로 성공적으로 분해할 수 있어. 우리는 마스크가 맞춰지는 제한된 수의 경쟁 개체 슬롯을 도입해, 2D 감독을 가장 잘 설명하고 추가적인 정규화 항을 최소화하는 의미 있는 개체 표현을 만들어. 

우리 실험에서는 복잡한 장면에서 3D 팬옵틱 분할을 생성할 수 있는 능력을 보여주고, NeRFs에서 고품질 3D 자산을 추출할 수 있음을 입증했어. 이런 자산은 가상 3D 환경에서 사용할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2408.16005
Title: Many-Worlds Inverse Rendering

Original Abstract:
Discontinuous visibility changes remain a major bottleneck when optimizing surfaces within a physically-based inverse renderer. Many previous works have proposed sophisticated algorithms and data structures to sample visibility silhouettes more efficiently.
Our work presents another solution: instead of differentiating a tentative surface locally, we differentiate a volumetric perturbation of a surface. We refer this as a many-worlds representation because it models a non-interacting superposition of conflicting explanations (worlds) of the input dataset. Each world is optically isolated from others, leading to a new transport law that distinguishes our method from prior work based on exponential random media.
The resulting Monte Carlo algorithm is simpler and more efficient than prior methods. We demonstrate that our method promotes rapid convergence, both in terms of the total iteration count and the cost per iteration.

Translated Abstract:
불연속적인 가시성 변화는 물리 기반의 역 렌더러에서 표면을 최적화할 때 큰 장애물로 남아 있어. 이전 연구들에서는 가시성 실루엣을 더 효율적으로 샘플링하기 위해 복잡한 알고리즘과 데이터 구조를 제안했어.

우리 연구는 다른 해결책을 제시해: 임시 표면을 지역적으로 미분하는 대신, 표면의 볼륨 변화를 미분하는 거야. 우리는 이걸 '다중 세계 표현'이라고 부르는데, 입력 데이터셋에 대한 상충하는 설명(세계)의 비상호작용 중첩을 모델링하기 때문이야. 각 세계는 서로 광학적으로 격리되어 있어서, 우리의 방법이 지수 확률 매체에 기반한 이전 연구와 구별되는 새로운 전송 법칙을 만들어.

결과적으로, 우리의 몬테 카를로 알고리즘은 이전 방법들보다 더 간단하고 효율적이야. 우리는 이 방법이 총 반복 횟수와 반복당 비용 모두에서 빠른 수렴을 촉진한다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2408.16879
Title: MSLIQA: Enhancing Learning Representations for Image Quality Assessment through Multi-Scale Learning

Original Abstract:
No-Reference Image Quality Assessment (NR-IQA) remains a challenging task due to the diversity of distortions and the lack of large annotated datasets. Many studies have attempted to tackle these challenges by developing more accurate NR-IQA models, often employing complex and computationally expensive networks, or by bridging the domain gap between various distortions to enhance performance on test datasets. In our work, we improve the performance of a generic lightweight NR-IQA model by introducing a novel augmentation strategy that boosts its performance by almost 28\%. This augmentation strategy enables the network to better discriminate between different distortions in various parts of the image by zooming in and out. Additionally, the inclusion of test-time augmentation further enhances performance, making our lightweight network's results comparable to the current state-of-the-art models, simply through the use of augmentations.

Translated Abstract:
비참조 이미지 품질 평가(No-Reference Image Quality Assessment, NR-IQA)는 왜곡의 다양성과 큰 주석 데이터셋의 부족 때문에 여전히 어려운 과제야. 많은 연구들이 이런 문제를 해결하기 위해 더 정확한 NR-IQA 모델을 개발하려고 했는데, 보통 복잡하고 계산 비용이 많이 드는 네트워크를 사용하거나 다양한 왜곡 간의 도메인 간극을 줄이는 방법을 사용해서 테스트 데이터셋에서 성능을 높이려고 했어.

우리 연구에서는 일반적인 경량 NR-IQA 모델의 성능을 개선하기 위해 새로운 증강 전략을 도입했어. 이 전략 덕분에 성능이 거의 28%나 향상됐어. 이 증강 전략은 네트워크가 이미지의 다양한 부분에서 서로 다른 왜곡을 더 잘 구별할 수 있도록 해주는데, 확대와 축소를 통해 가능해.

또한, 테스트 시간 증강(test-time augmentation)을 추가함으로써 성능이 더 향상되어, 경량 네트워크의 결과가 현재 최첨단 모델과 비슷하게 나오게 되었어. 이 모든 게 단순히 증강을 사용한 덕분이야.

================================================================================

URL: https://arxiv.org/abs/2408.17057
Title: LAR-IQA: A Lightweight, Accurate, and Robust No-Reference Image Quality Assessment Model

Original Abstract:
Recent advancements in the field of No-Reference Image Quality Assessment (NR-IQA) using deep learning techniques demonstrate high performance across multiple open-source datasets. However, such models are typically very large and complex making them not so suitable for real-world deployment, especially on resource- and battery-constrained mobile devices. To address this limitation, we propose a compact, lightweight NR-IQA model that achieves state-of-the-art (SOTA) performance on ECCV AIM UHD-IQA challenge validation and test datasets while being also nearly 5.7 times faster than the fastest SOTA model. Our model features a dual-branch architecture, with each branch separately trained on synthetically and authentically distorted images which enhances the model's generalizability across different distortion types. To improve robustness under diverse real-world visual conditions, we additionally incorporate multiple color spaces during the training process. We also demonstrate the higher accuracy of recently proposed Kolmogorov-Arnold Networks (KANs) for final quality regression as compared to the conventional Multi-Layer Perceptrons (MLPs). Our evaluation considering various open-source datasets highlights the practical, high-accuracy, and robust performance of our proposed lightweight model. Code: this https URL.

Translated Abstract:
최근 딥러닝 기술을 활용한 비참조 이미지 품질 평가(No-Reference Image Quality Assessment, NR-IQA) 분야에서 많은 발전이 있었고, 여러 오픈소스 데이터셋에서 높은 성과를 보여주고 있어. 하지만 이런 모델들은 보통 매우 크고 복잡해서 실제 환경에서 사용하기에는 적합하지 않아, 특히 자원과 배터리 사용이 제한된 모바일 기기에서는 더더욱 그래.

이런 문제를 해결하기 위해서, 우리는 뛰어난 성능을 자랑하면서도 가볍고 컴팩트한 NR-IQA 모델을 제안해. 이 모델은 ECCV AIM UHD-IQA 챌린지의 검증 및 테스트 데이터셋에서 최신 기술(SOTA) 성능을 내면서, 가장 빠른 SOTA 모델보다 거의 5.7배나 빠른 속도를 자랑해. 

우리 모델은 두 개의 브랜치 구조를 가지고 있어, 각 브랜치는 합성 왜곡 이미지와 실제 왜곡 이미지를 따로 훈련해. 이렇게 하면 다양한 왜곡 유형에 대한 모델의 일반화 능력이 향상돼. 또, 다양한 실제 시각 조건에서의 강건성을 높이기 위해 여러 색 공간을 훈련 과정에 추가했어. 

마지막으로, 최근에 제안된 콜모고로프-아놀드 네트워크(KANs)가 기존의 다층 퍼셉트론(MLPs)보다 최종 품질 회귀에서 더 높은 정확도를 보여준다는 것도 밝혀냈어. 다양한 오픈소스 데이터셋을 고려한 우리의 평가 결과는, 우리가 제안한 경량 모델이 실용적이고 높은 정확도와 강건한 성능을 가지고 있음을 강조해. 

코드: 이 링크.

================================================================================

URL: https://arxiv.org/abs/2409.00342
Title: AdaNAT: Exploring Adaptive Policy for Token-Based Image Generation

Original Abstract:
Recent studies have demonstrated the effectiveness of token-based methods for visual content generation. As a representative work, non-autoregressive Transformers (NATs) are able to synthesize images with decent quality in a small number of steps. However, NATs usually necessitate configuring a complicated generation policy comprising multiple manually-designed scheduling rules. These heuristic-driven rules are prone to sub-optimality and come with the requirements of expert knowledge and labor-intensive efforts. Moreover, their one-size-fits-all nature cannot flexibly adapt to the diverse characteristics of each individual sample. To address these issues, we propose AdaNAT, a learnable approach that automatically configures a suitable policy tailored for every sample to be generated. In specific, we formulate the determination of generation policies as a Markov decision process. Under this framework, a lightweight policy network for generation can be learned via reinforcement learning. Importantly, we demonstrate that simple reward designs such as FID or pre-trained reward models, may not reliably guarantee the desired quality or diversity of generated samples. Therefore, we propose an adversarial reward design to guide the training of policy networks effectively. Comprehensive experiments on four benchmark datasets, i.e., ImageNet-256 & 512, MS-COCO, and CC3M, validate the effectiveness of AdaNAT. Code and pre-trained models will be released at this https URL.

Translated Abstract:
최근 연구에서는 토큰 기반 방법이 시각 콘텐츠 생성에 효과적이라는 걸 보여줬어. 대표적인 예로 비자기회귀 트랜스포머(NATs)가 있는데, 이 모델은 적은 단계로도 괜찮은 품질의 이미지를 합성할 수 있어. 

하지만 NATs는 복잡한 생성 정책을 설정해야 하고, 이 정책은 여러 개의 수동으로 디자인된 스케줄링 규칙으로 구성돼. 이런 규칙들은 최적이 아닐 수 있고, 전문가의 지식과 많은 노력이 필요해. 게다가, 모든 샘플에 똑같이 적용되는 방식이라 각 샘플의 다양한 특성에 유연하게 대응하기 어려워.

이런 문제를 해결하기 위해 우리는 AdaNAT라는 학습 가능한 접근 방식을 제안해. 이 방법은 생성할 샘플마다 적합한 정책을 자동으로 설정해. 구체적으로는 생성 정책 결정을 마르코프 결정 과정으로 정의해. 이 틀 아래에서 경량 정책 네트워크를 강화 학습을 통해 배울 수 있어.

중요한 점은, FID 같은 간단한 보상 설계나 사전 훈련된 보상 모델이 원하는 품질이나 다양성을 보장하지 않을 수 있다는 거야. 그래서 우리는 정책 네트워크의 훈련을 효과적으로 안내하기 위해 적대적 보상 설계를 제안해. 

네 가지 기준 데이터셋인 ImageNet-256 & 512, MS-COCO, CC3M에서의 종합 실험 결과, AdaNAT의 효과가 입증됐어. 코드와 사전 훈련된 모델은 이 링크에서 공개될 거야.

================================================================================

URL: https://arxiv.org/abs/2409.00381
Title: 3D Gaussian Splatting for Large-scale 3D Surface Reconstruction from Aerial Images

Original Abstract:
Recently, 3D Gaussian Splatting (3DGS) has garnered significant attention. However, the unstructured nature of 3DGS poses challenges for large-scale surface reconstruction from aerial images. To address this gap, we propose the first large-scale surface reconstruction method for multi-view stereo (MVS) aerial images based on 3DGS, named Aerial Gaussian Splatting (AGS). Initially, we introduce a data chunking method tailored for large-scale aerial imagery, making the modern 3DGS technology feasible for surface reconstruction over extensive scenes. Additionally, we integrate the Ray-Gaussian Intersection method to obtain normal and depth information, facilitating geometric constraints. Finally, we introduce a multi-view geometric consistency constraint to enhance global geometric consistency and improve reconstruction accuracy. Our experiments on multiple datasets demonstrate for the first time that the GS-based technique can match traditional aerial MVS methods on geometric accuracy, and beat state-of-the-art GS-based methods on geometry and rendering quality.

Translated Abstract:
최근 3D 가우시안 스플래팅(3DGS)이 많은 주목을 받고 있어. 하지만 3DGS의 비구조적인 특성 때문에 대규모 항공 이미지에서 표면 재구성이 어려운 문제가 있어. 이 문제를 해결하기 위해, 우리는 3DGS를 기반으로 한 첫 번째 대규모 표면 재구성 방법인 항공 가우시안 스플래팅(AGS)을 제안해.

우선, 대규모 항공 이미지를 위해 특별히 설계된 데이터 청킹 방법을 소개해. 이 방법 덕분에 최신 3DGS 기술이 넓은 장면에서 표면 재구성을 가능하게 해. 또, Ray-Gaussian Intersection 방법을 통합해서 법선과 깊이 정보를 얻고, 기하학적 제약을 도와줘.

마지막으로, 다중 뷰 기하학적 일관성 제약을 도입해서 전반적인 기하학적 일관성을 높이고 재구성 정확도를 개선해. 여러 데이터셋에서 실험을 해본 결과, GS 기반 기술이 전통적인 항공 MVS 방법과 기하학적 정확도에서 동등하게 맞먹는다는 걸 처음으로 보여줬고, 최신 GS 기반 방법들보다 기하학과 렌더링 품질에서 더 우수하다는 걸 입증했어.

================================================================================

URL: https://arxiv.org/abs/2409.02919
Title: HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts

Original Abstract:
The potential for higher-resolution image generation using pretrained diffusion models is immense, yet these models often struggle with issues of object repetition and structural artifacts especially when scaling to 4K resolution and higher. We figure out that the problem is caused by that, a single prompt for the generation of multiple scales provides insufficient efficacy. In response, we propose HiPrompt, a new tuning-free solution that tackles the above problems by introducing hierarchical prompts. The hierarchical prompts offer both global and local guidance. Specifically, the global guidance comes from the user input that describes the overall content, while the local guidance utilizes patch-wise descriptions from MLLMs to elaborately guide the regional structure and texture generation. Furthermore, during the inverse denoising process, the generated noise is decomposed into low- and high-frequency spatial components. These components are conditioned on multiple prompt levels, including detailed patch-wise descriptions and broader image-level prompts, facilitating prompt-guided denoising under hierarchical semantic guidance. It further allows the generation to focus more on local spatial regions and ensures the generated images maintain coherent local and global semantics, structures, and textures with high definition. Extensive experiments demonstrate that HiPrompt outperforms state-of-the-art works in higher-resolution image generation, significantly reducing object repetition and enhancing structural quality.

Translated Abstract:
사전 학습된 확산 모델을 사용한 고해상도 이미지 생성의 가능성은 엄청나지만, 이 모델들은 4K 해상도 이상으로 확대할 때 물체 반복과 구조적 아티팩트 문제를 잘 해결하지 못해. 우리는 여러 스케일을 생성할 때 하나의 프롬프트만으로는 효과가 부족하다는 걸 알아냈어. 그래서 우리는 HiPrompt라는 새로운 조정이 필요 없는 솔루션을 제안해. 

HiPrompt는 계층적 프롬프트를 도입해서 위의 문제들을 해결해. 이 계층적 프롬프트는 전반적인 내용에 대한 사용자 입력에서 오는 글로벌 가이드와, MLLMs를 활용한 패치 단위 설명에서 오는 로컬 가이드를 제공해. 구체적으로, 글로벌 가이드는 전체 내용을 설명하고, 로컬 가이드는 지역 구조와 텍스처 생성을 세밀하게 안내해.

게다가, 역 노이즈 제거 과정에서 생성된 노이즈는 저주파와 고주파 공간 성분으로 분해돼. 이 성분들은 자세한 패치 단위 설명과 더 넓은 이미지 수준 프롬프트 같은 여러 프롬프트 레벨에 따라 조정돼, 계층적 의미 안내에 따라 프롬프트 기반 노이즈 제거를 돕는 거야. 이렇게 하면 생성 과정이 지역 공간에 더 집중할 수 있게 되고, 생성된 이미지가 높은 해상도에서도 일관된 로컬 및 글로벌 의미, 구조, 텍스처를 유지할 수 있어.

광범위한 실험 결과 HiPrompt가 고해상도 이미지 생성에서 최신 기술보다 우수한 성능을 보이며, 물체 반복을 크게 줄이고 구조적 품질을 향상시킨다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.03209
Title: iSeg: An Iterative Refinement-based Framework for Training-free Segmentation

Original Abstract:
Stable diffusion has demonstrated strong image synthesis ability to given text descriptions, suggesting it to contain strong semantic clue for grouping objects. Inspired by this, researchers have explored employing stable diffusion for trainingfree segmentation. Most existing approaches either simply employ cross-attention map or refine it by self-attention map, to generate segmentation masks. We believe that iterative refinement with self-attention map would lead to better results. However, we mpirically demonstrate that such a refinement is sub-optimal likely due to the self-attention map containing irrelevant global information which hampers accurately refining cross-attention map with multiple iterations. To address this, we propose an iterative refinement framework for training-free segmentation, named iSeg, having an entropy-reduced self-attention module which utilizes a gradient descent scheme to reduce the entropy of self-attention map, thereby suppressing the weak responses corresponding to irrelevant global information. Leveraging the entropy-reduced self-attention module, our iSeg stably improves refined crossattention map with iterative refinement. Further, we design a category-enhanced cross-attention module to generate accurate cross-attention map, providing a better initial input for iterative refinement. Extensive experiments across different datasets and diverse segmentation tasks reveal the merits of proposed contributions, leading to promising performance on diverse segmentation tasks. For unsupervised semantic segmentation on Cityscapes, our iSeg achieves an absolute gain of 3.8% in terms of mIoU compared to the best existing training-free approach in literature. Moreover, our proposed iSeg can support segmentation with different kind of images and interactions.

Translated Abstract:
안정적인 확산(Stable diffusion)은 주어진 텍스트 설명에 대해 강력한 이미지 합성 능력을 보여줬고, 이는 객체를 그룹화하는 데 강한 의미적 단서를 포함하고 있음을 시사해. 이런 점에 영감을 받아 연구자들은 훈련 없이 세분화(segmentation)를 수행하기 위해 안정적인 확산을 활용하는 방법을 탐구했어. 

대부분의 기존 방법들은 단순히 교차 주의 맵(cross-attention map)을 사용하거나 자기 주의 맵(self-attention map)으로 이를 보완해 세분화 마스크를 생성해. 우리는 자기 주의 맵으로 반복적으로 보정하는 것이 더 좋은 결과를 낼 거라고 믿었어. 하지만 실험적으로 보여준 바에 따르면, 이런 보정은 최적이 아니었고, 자기 주의 맵이 관련 없는 전역 정보(global information)를 포함하고 있어 교차 주의 맵을 정확하게 보정하는 데 방해가 되는 것 같아.

이 문제를 해결하기 위해 우리는 iSeg라는 훈련 없는 세분화 프레임워크를 제안해. iSeg는 자기 주의 맵의 엔트로피를 줄이는 모듈을 사용하여, 관련 없는 전역 정보에 해당하는 약한 응답을 억제해. 엔트로피가 감소된 자기 주의 모듈을 활용하여, iSeg는 반복적인 보정을 통해 교차 주의 맵을 안정적으로 개선해. 

또한, 우리는 정확한 교차 주의 맵을 생성하기 위해 카테고리 강화 교차 주의 모듈(category-enhanced cross-attention module)을 설계했어. 이렇게 하면 반복 보정을 위한 더 나은 초기 입력을 제공할 수 있어. 다양한 데이터셋과 여러 세분화 작업에 대한 광범위한 실험을 통해 제안된 기여의 장점을 보여주며, 다양한 세분화 작업에서 유망한 성능을 낼 수 있었어. 

Cityscapes에서 비지도 세분화(unsupervised semantic segmentation)를 수행했을 때, 우리 iSeg는 기존의 훈련 없는 방법과 비교해 mIoU 기준으로 3.8%의 절대 이득을 달성했어. 게다가, 우리의 iSeg는 다양한 종류의 이미지와 상호작용에 대한 세분화를 지원할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.03431
Title: UV-Mamba: A DCN-Enhanced State Space Model for Urban Village Boundary Identification in High-Resolution Remote Sensing Images

Original Abstract:
Owing to the diverse geographical environments, intricate landscapes, and high-density settlements, the automatic identification of urban village boundaries using remote sensing images is a highly challenging task. This paper proposes a novel and efficient neural network model called UV-Mamba for accurate boundary detection in high-resolution remote sensing images. UV-Mamba mitigates the memory loss problem in long sequence modeling, which arises in state space model (SSM) with increasing image size, by incorporating deformable convolutions (DCN). Its architecture utilizes an encoder-decoder framework, includes an encoder with four deformable state space augmentation (DSSA) blocks for efficient multi-level semantic extraction and a decoder to integrate the extracted semantic information. We conducted experiments on the Beijing and Xi'an datasets, and the results show that UV-Mamba achieves state-of-the-art performance. Specifically, our model achieves 73.3% and 78.1% IoU on the Beijing and Xi'an datasets, respectively, representing improvements of 1.2% and 3.4% IoU over the previous best model, while also being 6x faster in inference speed and 40x smaller in parameter count. Source code and pre-trained models are available in the supplementary material.

Translated Abstract:
지리 환경이 다양하고 복잡한 경관, 고밀도 정주지 때문에 원격 탐지 이미지를 사용해서 도시 마을 경계를 자동으로 식별하는 건 정말 어려운 일이야. 이 논문에서는 UV-Mamba라는 새로운 신경망 모델을 제안하는데, 이 모델은 고해상도 원격 탐지 이미지에서 경계를 정확하게 탐지하는 데 도움을 줘.

UV-Mamba는 이미지 크기가 커질 때 발생하는 메모리 손실 문제를 해결하기 위해 변형 가능한 합성곱(DCN)을 사용해. 이 모델 구조는 인코더-디코더 프레임워크를 기반으로 하고, 효율적인 다층 의미 추출을 위해 4개의 변형 가능한 상태 공간 증강(DSSA) 블록을 가진 인코더와 추출된 의미 정보를 통합하는 디코더를 포함해.

우리는 베이징과 시안 데이터셋에서 실험을 했고, 결과는 UV-Mamba가 최신 기술 수준의 성능을 보여준다는 걸 나타냈어. 구체적으로, 우리 모델은 베이징 데이터셋에서 73.3%, 시안 데이터셋에서 78.1%의 IoU를 달성했어. 이는 이전의 최고 모델보다 각각 1.2%와 3.4% 향상된 수치고, 추론 속도는 6배 빠르며, 파라미터 수는 40배 적어. 소스 코드와 사전 훈련된 모델은 보충 자료에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2309.15638
Title: RSF-Conv: Rotation-and-Scale Equivariant Fourier Parameterized Convolution for Retinal Vessel Segmentation

Original Abstract:
Retinal vessel segmentation is of great clinical significance for the diagnosis of many eye-related diseases, but it is still a formidable challenge due to the intricate vascular morphology. With the skillful characterization of the translation symmetry existing in retinal vessels, convolutional neural networks (CNNs) have achieved great success in retinal vessel segmentation. However, the rotation-and-scale symmetry, as a more widespread image prior in retinal vessels, fails to be characterized by CNNs. Therefore, we propose a rotation-and-scale equivariant Fourier parameterized convolution (RSF-Conv) specifically for retinal vessel segmentation, and provide the corresponding equivariance analysis. As a general module, RSF-Conv can be integrated into existing networks in a plug-and-play manner while significantly reducing the number of parameters. For instance, we replace the traditional convolution filters in U-Net and Iter-Net with RSF-Convs, and faithfully conduct comprehensive experiments. RSF-Conv+U-Net and RSF-Conv+Iter-Net not only have slight advantages under in-domain evaluation, but more importantly, outperform all comparison methods by a significant margin under out-of-domain evaluation. It indicates the remarkable generalization of RSF-Conv, which holds greater practical clinical significance for the prevalent cross-device and cross-hospital challenges in clinical practice. To comprehensively demonstrate the effectiveness of RSF-Conv, we also apply RSF-Conv+U-Net and RSF-Conv+Iter-Net to retinal artery/vein classification and achieve promising performance as well, indicating its clinical application potential.

Translated Abstract:
망막 혈관 세분화는 많은 눈 관련 질병 진단에 매우 중요하지만, 복잡한 혈관 형태 때문에 여전히 큰 도전 과제야. 망막 혈관에 존재하는 변환 대칭을 잘 활용한 덕분에, 합성곱 신경망(CNN)은 망막 혈관 세분화에서 큰 성공을 거두었어. 하지만 회전 및 스케일 대칭은 망막 혈관에서 더 일반적인 이미지 특성인데, 이건 CNN으로 잘 표현되지 않아. 

그래서 우리는 망막 혈관 세분화를 위해 회전 및 스케일 불변 푸리에 매개변수화 합성곱(RSF-Conv)을 제안하고, 이에 대한 불변성 분석도 제공해. RSF-Conv는 일반적인 모듈로서 기존 네트워크에 플러그 앤 플레이 방식으로 통합할 수 있으면서, 매개변수 수도 크게 줄일 수 있어. 예를 들어, U-Net과 Iter-Net의 전통적인 합성곱 필터를 RSF-Conv로 교체하고, 다양한 실험을 통해 성능을 검증했어. 

RSF-Conv+U-Net과 RSF-Conv+Iter-Net은 도메인 내 평가에서는 약간의 이점을 보였고, 더 중요한 건 도메인 외 평가에서는 모든 비교 방법보다 크게 뛰어난 성능을 나타냈어. 이건 RSF-Conv의 뛰어난 일반화 능력을 보여주고, 임상 실습에서 흔히 발생하는 기기 간 및 병원 간 문제에 대해 더 큰 실용적 의미를 가지게 해. 

RSF-Conv의 효과를 종합적으로 보여주기 위해, 우리는 RSF-Conv+U-Net과 RSF-Conv+Iter-Net을 망막 동맥/정맥 분류에 적용해봤고, 괜찮은 성과를 얻었어. 이건 RSF-Conv의 임상 적용 가능성을 나타내는 거야.

================================================================================

URL: https://arxiv.org/abs/2311.13110
Title: White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?

Original Abstract:
In this paper, we contend that a natural objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a low-dimensional Gaussian mixture supported on incoherent subspaces. The goodness of such a representation can be evaluated by a principled measure, called sparse rate reduction, that simultaneously maximizes the intrinsic information gain and extrinsic sparsity of the learned representation. From this perspective, popular deep network architectures, including transformers, can be viewed as realizing iterative schemes to optimize this measure. Particularly, we derive a transformer block from alternating optimization on parts of this objective: the multi-head self-attention operator compresses the representation by implementing an approximate gradient descent step on the coding rate of the features, and the subsequent multi-layer perceptron sparsifies the features. This leads to a family of white-box transformer-like deep network architectures, named CRATE, which are mathematically fully interpretable. We show, by way of a novel connection between denoising and compression, that the inverse to the aforementioned compressive encoding can be realized by the same class of CRATE architectures. Thus, the so-derived white-box architectures are universal to both encoders and decoders. Experiments show that these networks, despite their simplicity, indeed learn to compress and sparsify representations of large-scale real-world image and text datasets, and achieve performance very close to highly engineered transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the proposed computational framework demonstrates great potential in bridging the gap between theory and practice of deep learning, from a unified perspective of data compression. Code is available at: this https URL .

Translated Abstract:
이 논문에서는 표현 학습의 자연스러운 목표가 데이터의 분포, 예를 들어 토큰의 집합을 압축하고 변환해서 저차원 가우시안 혼합으로 만드는 것이라고 주장해. 이런 표현의 좋음은 "희소성 비율 감소"라는 원칙적인 측정 방법으로 평가할 수 있어. 이 방법은 학습된 표현의 내재적 정보 이득과 외재적 희소성을 동시에 극대화해.

이 관점에서, 인기 있는 딥 네트워크 구조들, 특히 트랜스포머는 이 측정을 최적화하는 반복적인 방식을 실현하는 것으로 볼 수 있어. 특히, 우리는 이 목표의 일부에 대한 교대 최적화를 통해 트랜스포머 블록을 유도했어. 멀티 헤드 자기 주의 연산자는 특성의 코딩 비율에 대해 근사적인 경량 하강 단계를 구현해서 표현을 압축하고, 이어지는 다층 퍼셉트론은 특성을 희소화해.

이렇게 해서 CRATE라는 수학적으로 완전히 해석 가능한 화이트 박스 트랜스포머 같은 딥 네트워크 구조가 생겼어. 우리는 노이즈 제거와 압축 간의 새로운 연결을 통해, 앞서 말한 압축 인코딩의 역을 같은 CRATE 구조로 실현할 수 있다는 것을 보여줘. 따라서 이렇게 유도된 화이트 박스 구조는 인코더와 디코더 모두에 보편적이야.

실험 결과, 이 네트워크들은 단순함에도 불구하고 대규모 실제 이미지와 텍스트 데이터셋의 표현을 압축하고 희소화하는 것을 배우며, ViT, MAE, DINO, BERT, GPT2 같은 고도로 설계된 트랜스포머 기반 모델들과 매우 비슷한 성능을 보여줘. 우리는 제안된 계산 프레임워크가 데이터 압축의 통합 관점에서 딥 러닝 이론과 실제 간의 간극을 메우는 큰 잠재력을 보여준다고 믿어. 코드도 제공돼.

================================================================================

URL: https://arxiv.org/abs/2401.08281
Title: The Faiss library

Original Abstract:
Vector databases typically manage large collections of embedding vectors. Currently, AI applications are growing rapidly, and so is the number of embeddings that need to be stored and indexed. The Faiss library is dedicated to vector similarity search, a core functionality of vector databases. Faiss is a toolkit of indexing methods and related primitives used to search, cluster, compress and transform vectors. This paper describes the trade-off space of vector search and the design principles of Faiss in terms of structure, approach to optimization and interfacing. We benchmark key features of the library and discuss a few selected applications to highlight its broad applicability.

Translated Abstract:
벡터 데이터베이스는 일반적으로 큰 양의 임베딩 벡터 컬렉션을 관리해. 요즘 AI 어플리케이션이 빠르게 성장하고 있어서 저장하고 인덱싱해야 할 임베딩의 수도 많아지고 있어. 

Faiss 라이브러리는 벡터 유사성 검색에 특화돼 있는데, 이건 벡터 데이터베이스의 핵심 기능이야. Faiss는 벡터를 검색하고, 클러스터링하고, 압축하고, 변환하는 데 사용되는 인덱싱 방법과 관련된 도구 모음이야. 

이 논문에서는 벡터 검색의 트레이드오프 공간과 Faiss의 설계 원칙에 대해 설명해. 구조, 최적화 접근 방식, 인터페이스 측면에서 말이지. 우리는 라이브러리의 주요 기능을 벤치마킹하고 몇 가지 선택된 어플리케이션에 대해 이야기하면서 Faiss의 폭넓은 활용 가능성을 강조할 거야.

================================================================================

URL: https://arxiv.org/abs/2405.03486
Title: UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images

Original Abstract:
With the advent of text-to-image models and concerns about their misuse, developers are increasingly relying on image safety classifiers to moderate their generated unsafe images. Yet, the performance of current image safety classifiers remains unknown for both real-world and AI-generated images. In this work, we propose UnsafeBench, a benchmarking framework that evaluates the effectiveness and robustness of image safety classifiers, with a particular focus on the impact of AI-generated images on their performance. First, we curate a large dataset of 10K real-world and AI-generated images that are annotated as safe or unsafe based on a set of 11 unsafe categories of images (sexual, violent, hateful, etc.). Then, we evaluate the effectiveness and robustness of five popular image safety classifiers, as well as three classifiers that are powered by general-purpose visual language models. Our assessment indicates that existing image safety classifiers are not comprehensive and effective enough to mitigate the multifaceted problem of unsafe images. Also, there exists a distribution shift between real-world and AI-generated images in image qualities, styles, and layouts, leading to degraded effectiveness and robustness. Motivated by these findings, we build a comprehensive image moderation tool called PerspectiveVision, which addresses the main drawbacks of existing classifiers with improved effectiveness and robustness, especially on AI-generated images. UnsafeBench and PerspectiveVision can aid the research community in better understanding the landscape of image safety classification in the era of generative AI.

Translated Abstract:
텍스트-이미지 모델이 등장하면서 그 남용에 대한 우려도 커지고 있어, 개발자들은 생성된 위험한 이미지를 조절하기 위해 이미지 안전 분류기에 점점 더 의존하고 있어. 하지만 현재의 이미지 안전 분류기가 실제 이미지와 AI가 생성한 이미지에 대해 얼마나 잘 작동하는지는 잘 알려져 있지 않아.

이 연구에서는 UnsafeBench라는 벤치마킹 프레임워크를 제안해. 이 프레임워크는 이미지 안전 분류기의 효과성과 강인성을 평가하는 데 초점을 맞추고 있어, 특히 AI가 생성한 이미지가 성능에 미치는 영향을 살펴봐. 먼저, 1만 개의 실제 이미지와 AI 생성 이미지를 모아서 안전하거나 위험하다고 라벨링했어. 이 라벨은 11개의 위험한 이미지 카테고리(성적, 폭력적, 증오 등)를 기준으로 정해졌지.

그 다음으로, 다섯 개의 인기 있는 이미지 안전 분류기와 일반적인 비주얼 언어 모델로 구동되는 세 개의 분류기의 효과성과 강인성을 평가했어. 평가 결과, 기존의 이미지 안전 분류기는 안전하지 않은 이미지의 복잡한 문제를 해결하기에는 충분히 포괄적이지도 않고 효과적이지도 않다는 걸 알게 됐어. 그리고 실제 이미지와 AI 생성 이미지 간의 품질, 스타일, 레이아웃에서 분포의 변화가 있어, 이로 인해 효과성과 강인성이 저하된다는 것도 발견했어.

이런 발견에 힘입어, 우리는 PerspectiveVision이라는 포괄적인 이미지 조절 도구를 만들었어. 이 도구는 기존 분류기의 주요 단점을 해결하고, 특히 AI 생성 이미지에 대해 효과성과 강인성을 개선했어. UnsafeBench와 PerspectiveVision은 생성 AI 시대에서 이미지 안전 분류의 맥락을 더 잘 이해하는 데 연구자들에게 도움이 될 거야.

================================================================================

URL: https://arxiv.org/abs/2405.08621
Title: RMT-BVQA: Recurrent Memory Transformer-based Blind Video Quality Assessment for Enhanced Video Content

Original Abstract:
With recent advances in deep learning, numerous algorithms have been developed to enhance video quality, reduce visual artifacts, and improve perceptual quality. However, little research has been reported on the quality assessment of enhanced content - the evaluation of enhancement methods is often based on quality metrics that were designed for compression applications. In this paper, we propose a novel blind deep video quality assessment (VQA) method specifically for enhanced video content. It employs a new Recurrent Memory Transformer (RMT) based network architecture to obtain video quality representations, which is optimized through a novel content-quality-aware contrastive learning strategy based on a new database containing 13K training patches with enhanced content. The extracted quality representations are then combined through linear regression to generate video-level quality indices. The proposed method, RMT-BVQA, has been evaluated on the VDPVE (VQA Dataset for Perceptual Video Enhancement) database through a five-fold cross validation. The results show its superior correlation performance when compared to ten existing no-reference quality metrics.

Translated Abstract:
최근 딥러닝 기술이 발전하면서, 여러 알고리즘이 비디오 품질을 향상시키고 시각적 아티팩트를 줄이며 지각 품질을 개선하는 데 개발되었어. 하지만 향상된 콘텐츠의 품질 평가에 대한 연구는 거의 없었어. 보통 향상 방법의 평가는 압축 응용 프로그램을 위해 설계된 품질 지표를 기반으로 이루어지거든.

이 논문에서는 향상된 비디오 콘텐츠를 위한 새로운 블라인드 딥 비디오 품질 평가(VQA) 방법을 제안해. 이 방법은 새로운 Recurrent Memory Transformer(RMT) 기반의 네트워크 구조를 사용해 비디오 품질 표현을 얻고, 13K개의 향상된 콘텐츠 훈련 패치를 포함한 새로운 데이터베이스를 바탕으로 한 콘텐츠 품질 인식 대조 학습 전략을 통해 최적화돼.

추출된 품질 표현은 선형 회귀를 통해 결합되어 비디오 수준의 품질 지수를 생성해. 제안된 방법인 RMT-BVQA는 VDPVE(지각 비디오 향상을 위한 VQA 데이터셋) 데이터베이스에서 5배 교차 검증을 통해 평가되었고, 기존의 10개 비참조 품질 지표와 비교했을 때 더 우수한 상관 성능을 보였어.

================================================================================

URL: https://arxiv.org/abs/2407.09050
Title: Refusing Safe Prompts for Multi-modal Large Language Models

Original Abstract:
Multimodal large language models (MLLMs) have become the cornerstone of today's generative AI ecosystem, sparking intense competition among tech giants and startups. In particular, an MLLM generates a text response given a prompt consisting of an image and a question. While state-of-the-art MLLMs use safety filters and alignment techniques to refuse unsafe prompts, in this work, we introduce MLLM-Refusal, the first method that induces refusals for safe prompts. In particular, our MLLM-Refusal optimizes a nearly-imperceptible refusal perturbation and adds it to an image, causing target MLLMs to likely refuse a safe prompt containing the perturbed image and a safe question. Specifically, we formulate MLLM-Refusal as a constrained optimization problem and propose an algorithm to solve it. Our method offers competitive advantages for MLLM model providers by potentially disrupting user experiences of competing MLLMs, since competing MLLM's users will receive unexpected refusals when they unwittingly use these perturbed images in their prompts. We evaluate MLLM-Refusal on four MLLMs across four datasets, demonstrating its effectiveness in causing competing MLLMs to refuse safe prompts while not affecting non-competing MLLMs. Furthermore, we explore three potential countermeasures-adding Gaussian noise, DiffPure, and adversarial training. Our results show that though they can mitigate MLLM-Refusal's effectiveness, they also sacrifice the accuracy and/or efficiency of the competing MLLM. The code is available at this https URL.

Translated Abstract:
다중모달 대형 언어 모델(MLLMs)은 현재 생성 AI 생태계의 핵심이 되었고, 기술 대기업과 스타트업 간의 치열한 경쟁을 불러일으켰어. 특히 MLLM은 이미지와 질문으로 구성된 프롬프트를 주면 텍스트 응답을 생성해. 최신 MLLM들은 안전하지 않은 프롬프트를 거부하기 위해 안전 필터와 정렬 기술을 사용하지만, 이번 연구에서는 안전한 프롬프트에 대해 거부를 유도하는 MLLM-거부(MLLM-Refusal)라는 첫 번째 방법을 소개해.

MLLM-거부는 거의 감지할 수 없는 거부 방해 요소를 최적화해서 이미지를 추가해. 이렇게 하면 대상 MLLM이 방해 요소가 포함된 이미지와 안전한 질문이 있는 안전한 프롬프트에 대해 거부할 가능성이 커져. 구체적으로, MLLM-거부를 제약 최적화 문제로 정의하고 이를 해결하기 위한 알고리즘을 제안했어.

이 방법은 MLLM 모델 제공자에게 경쟁 우위를 제공해. 왜냐하면 경쟁 MLLM의 사용자들이 의도치 않게 이런 방해 요소가 포함된 이미지를 프롬프트에 사용할 경우 예상치 못한 거부를 받을 수 있기 때문이야. 우리는 네 개의 데이터셋에서 네 개의 MLLM에 대해 MLLM-거부를 평가했고, 경쟁 MLLM이 안전한 프롬프트를 거부하도록 하는 데 효과적임을 입증했어. 또한 비경쟁 MLLM에는 영향을 미치지 않음을 보여줬어.

마지막으로, 우리는 세 가지 가능한 대응책인 가우시안 노이즈 추가, DiffPure, 적대적 훈련을 탐색했어. 결과적으로 이들이 MLLM-거부의 효과를 줄일 수는 있지만, 경쟁 MLLM의 정확도와 효율성을 희생해야 한다는 것을 보여줬어. 코드는 이 https URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2407.13567
Title: Hyp2Nav: Hyperbolic Planning and Curiosity for Crowd Navigation

Original Abstract:
Autonomous robots are increasingly becoming a strong fixture in social environments. Effective crowd navigation requires not only safe yet fast planning, but should also enable interpretability and computational efficiency for working in real-time on embedded devices. In this work, we advocate for hyperbolic learning to enable crowd navigation and we introduce Hyp2Nav. Different from conventional reinforcement learning-based crowd navigation methods, Hyp2Nav leverages the intrinsic properties of hyperbolic geometry to better encode the hierarchical nature of decision-making processes in navigation tasks. We propose a hyperbolic policy model and a hyperbolic curiosity module that results in effective social navigation, best success rates, and returns across multiple simulation settings, using up to 6 times fewer parameters than competitor state-of-the-art models. With our approach, it becomes even possible to obtain policies that work in 2-dimensional embedding spaces, opening up new possibilities for low-resource crowd navigation and model interpretability. Insightfully, the internal hyperbolic representation of Hyp2Nav correlates with how much attention the robot pays to the surrounding crowds, e.g. due to multiple people occluding its pathway or to a few of them showing colliding plans, rather than to its own planned route. The code is available at this https URL.

Translated Abstract:
자율 로봇이 사회적 환경에서 점점 더 중요한 역할을 하고 있어. 사람들 사이를 안전하고 빠르게 내비게이션하려면, 실시간으로 작동하는 임베디드 장치에서 해석 가능성과 계산 효율성도 필요해. 

이 연구에서는 군중 내비게이션을 위한 하이퍼볼릭 학습을 제안하고, Hyp2Nav라는 시스템을 소개할게. 기존의 강화 학습 기반 군중 내비게이션 방법과는 다르게, Hyp2Nav는 하이퍼볼릭 기하학의 고유한 특성을 활용해서 내비게이션 작업에서 의사 결정 과정의 계층적 특성을 더 잘 인코딩해. 

우리는 하이퍼볼릭 정책 모델과 하이퍼볼릭 호기심 모듈을 제안했는데, 이 덕분에 효과적인 사회적 내비게이션이 가능해지고, 여러 시뮬레이션 환경에서 최고의 성공률과 수익률을 기록했어. 게다가 경쟁하는 최신 모델들보다 최대 6배 적은 매개변수를 사용했어. 

우리의 접근 방식 덕분에 2차원 임베딩 공간에서도 작동하는 정책을 얻을 수 있게 되어, 자원이 적은 군중 내비게이션과 모델 해석 가능성에 대한 새로운 가능성이 열렸어. 흥미롭게도, Hyp2Nav의 내부 하이퍼볼릭 표현은 로봇이 주변 군중에게 얼마나 주의를 기울이는지를 나타내는데, 예를 들어 여러 사람이 로봇의 경로를 가리거나 몇몇이 충돌하는 계획을 보여줄 때 말이야. 코드도 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2408.03035
Title: Training-Free Condition Video Diffusion Models for single frame Spatial-Semantic Echocardiogram Synthesis

Original Abstract:
Conditional video diffusion models (CDM) have shown promising results for video synthesis, potentially enabling the generation of realistic echocardiograms to address the problem of data scarcity. However, current CDMs require a paired segmentation map and echocardiogram dataset. We present a new method called Free-Echo for generating realistic echocardiograms from a single end-diastolic segmentation map without additional training data. Our method is based on the 3D-Unet with Temporal Attention Layers model and is conditioned on the segmentation map using a training-free conditioning method based on SDEdit. We evaluate our model on two public echocardiogram datasets, CAMUS and EchoNet-Dynamic. We show that our model can generate plausible echocardiograms that are spatially aligned with the input segmentation map, achieving performance comparable to training-based CDMs. Our work opens up new possibilities for generating echocardiograms from a single segmentation map, which can be used for data augmentation, domain adaptation, and other applications in medical imaging. Our code is available at \url{this https URL}

Translated Abstract:
조건부 비디오 확산 모델(Conditional Video Diffusion Models, CDM)은 비디오 생성에서 좋은 결과를 보여줬고, 데이터 부족 문제를 해결하기 위해 현실적인 심장 초음파 영상을 생성할 수 있는 가능성을 열어줬어. 하지만 현재의 CDM은 쌍으로 된 분할 맵(segmentation map)과 심장 초음파 데이터셋이 필요해.

우리는 Free-Echo라는 새로운 방법을 제안해. 이 방법은 추가적인 학습 데이터 없이 단일 이완기 분할 맵(end-diastolic segmentation map)만으로 현실적인 심장 초음파 영상을 생성할 수 있어. 우리의 방법은 3D-Unet 모델에 시간적 주의 레이어(Temporal Attention Layers)를 추가한 것이고, SDEdit 기반의 학습 없는 조건화 방법으로 분할 맵에 맞춰져 있어.

우리는 CAMUS와 EchoNet-Dynamic이라는 두 개의 공개 심장 초음파 데이터셋에서 우리의 모델을 평가했어. 그 결과, 모델이 입력된 분할 맵과 공간적으로 정렬된 그럴듯한 심장 초음파 영상을 생성할 수 있다는 것을 보여줬어. 성능도 학습 기반 CDM과 비슷했어. 

우리의 연구는 단일 분할 맵으로 심장 초음파 영상을 생성할 수 있는 새로운 가능성을 열어줬고, 이건 데이터 증강(data augmentation), 도메인 적응(domain adaptation) 및 기타 의료 영상 응용 프로그램에 활용될 수 있어. 우리의 코드는 \url{this https URL}에서 사용할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2408.08632
Title: A Survey on Benchmarks of Multimodal Large Language Models

Original Abstract:
Multimodal Large Language Models (MLLMs) are gaining increasing popularity in both academia and industry due to their remarkable performance in various applications such as visual question answering, visual perception, understanding, and reasoning. Over the past few years, significant efforts have been made to examine MLLMs from multiple perspectives. This paper presents a comprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on (1)perception and understanding, (2)cognition and reasoning, (3)specific domains, (4)key capabilities, and (5)other modalities. Finally, we discuss the limitations of the current evaluation methods for MLLMs and explore promising future directions. Our key argument is that evaluation should be regarded as a crucial discipline to support the development of MLLMs better. For more details, please visit our GitHub repository: this https URL.

Translated Abstract:
다중 모달 대형 언어 모델(MLLMs)이 다양한 응용 분야에서 뛰어난 성능 덕분에 학계와 산업에서 점점 더 인기를 얻고 있어. 예를 들어, 시각 질문 응답, 시각 인식, 이해, 추론 같은 분야에서 그렇지.

최근 몇 년 동안 MLLMs를 여러 관점에서 살펴보는 데 많은 노력이 있었어. 이 논문에서는 MLLMs에 대한 200개의 벤치마크와 평가를 종합적으로 정리했어. 주로 다룬 내용은 (1) 인식 및 이해, (2) 인지 및 추론, (3) 특정 도메인, (4) 주요 기능, (5) 기타 모달리티야.

마지막으로, 현재 MLLMs 평가 방법의 한계에 대해 이야기하고, 앞으로의 유망한 방향도 탐구해 봤어. 우리의 핵심 주장은 평가가 MLLMs 발전을 지원하는 중요한 분야로 여겨져야 한다는 거야. 자세한 내용은 우리의 GitHub 저장소를 확인해 봐.

================================================================================

