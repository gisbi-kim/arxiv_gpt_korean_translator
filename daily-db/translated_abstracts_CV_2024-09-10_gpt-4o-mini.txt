URL: https://arxiv.org/abs/2409.04482
Title: SCARF: Scalable Continual Learning Framework for Memory-efficient Multiple Neural Radiance Fields

Original Abstract:
This paper introduces a novel continual learning framework for synthesising novel views of multiple scenes, learning multiple 3D scenes incrementally, and updating the network parameters only with the training data of the upcoming new scene. We build on Neural Radiance Fields (NeRF), which uses multi-layer perceptron to model the density and radiance field of a scene as the implicit function. While NeRF and its extensions have shown a powerful capability of rendering photo-realistic novel views in a single 3D scene, managing these growing 3D NeRF assets efficiently is a new scientific problem. Very few works focus on the efficient representation or continuous learning capability of multiple scenes, which is crucial for the practical applications of NeRF. To achieve these goals, our key idea is to represent multiple scenes as the linear combination of a cross-scene weight matrix and a set of scene-specific weight matrices generated from a global parameter generator. Furthermore, we propose an uncertain surface knowledge distillation strategy to transfer the radiance field knowledge of previous scenes to the new model. Representing multiple 3D scenes with such weight matrices significantly reduces memory requirements. At the same time, the uncertain surface distillation strategy greatly overcomes the catastrophic forgetting problem and maintains the photo-realistic rendering quality of previous scenes. Experiments show that the proposed approach achieves state-of-the-art rendering quality of continual learning NeRF on NeRF-Synthetic, LLFF, and TanksAndTemples datasets while preserving extra low storage cost.

Translated Abstract:
이 논문에서는 여러 장면의 새로운 시각을 합성하고, 여러 3D 장면을 점진적으로 학습하며, 다가오는 새로운 장면의 훈련 데이터만으로 네트워크 파라미터를 업데이트하는 새로운 지속적 학습 프레임워크를 소개해. 우리는 Neural Radiance Fields (NeRF)를 기반으로 하는데, NeRF는 다층 퍼셉트론을 사용해서 장면의 밀도와 방사 필드를 암묵적인 함수로 모델링해.

NeRF와 그 확장판이 단일 3D 장면에서 포토 리얼리스틱한 새로운 시각을 렌더링하는 데 강력한 능력을 보여줬지만, 이렇게 증가하는 3D NeRF 자산을 효율적으로 관리하는 건 새로운 과학적 문제야. 여러 장면의 효율적인 표현이나 지속적 학습 능력에 초점을 맞춘 연구는 매우 적어, 이게 NeRF의 실용적인 응용에선 정말 중요해.

이 목표를 달성하기 위해 우리의 핵심 아이디어는 여러 장면을 교차 장면 가중치 행렬과 전역 파라미터 생성기로부터 생성된 장면별 가중치 행렬의 선형 조합으로 표현하는 거야. 더불어, 우리는 불확실한 표면 지식을 증류하는 전략을 제안해, 이전 장면의 방사 필드 지식을 새로운 모델로 전이해.

이런 가중치 행렬로 여러 3D 장면을 표현하면 메모리 요구량이 크게 줄어들어. 동시에, 이 불확실한 표면 증류 전략은 치명적인 망각 문제를 크게 극복하고 이전 장면의 포토 리얼리스틱 렌더링 품질을 유지해. 실험 결과는 제안한 접근 방식이 NeRF-Synthetic, LLFF, TanksAndTemples 데이터셋에서 지속적 학습 NeRF의 최첨단 렌더링 품질을 달성하면서도 매우 낮은 저장 비용을 유지한다는 걸 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.04559
Title: Thinking Outside the BBox: Unconstrained Generative Object Compositing

Original Abstract:
Compositing an object into an image involves multiple non-trivial sub-tasks such as object placement and scaling, color/lighting harmonization, viewpoint/geometry adjustment, and shadow/reflection generation. Recent generative image compositing methods leverage diffusion models to handle multiple sub-tasks at once. However, existing models face limitations due to their reliance on masking the original object during training, which constrains their generation to the input mask. Furthermore, obtaining an accurate input mask specifying the location and scale of the object in a new image can be highly challenging. To overcome such limitations, we define a novel problem of unconstrained generative object compositing, i.e., the generation is not bounded by the mask, and train a diffusion-based model on a synthesized paired dataset. Our first-of-its-kind model is able to generate object effects such as shadows and reflections that go beyond the mask, enhancing image realism. Additionally, if an empty mask is provided, our model automatically places the object in diverse natural locations and scales, accelerating the compositing workflow. Our model outperforms existing object placement and compositing models in various quality metrics and user studies.

Translated Abstract:
이미지에 객체를 합성하는 건 여러 가지 복잡한 작업을 포함해. 여기에는 객체의 위치와 크기 조정, 색상/조명 조화, 시점/형태 조정, 그리고 그림자/반사 생성 같은 것들이 있어. 최근의 생성적 이미지 합성 방법들은 확산 모델을 활용해서 여러 작업을 동시에 처리해.

하지만 기존 모델들은 훈련할 때 원래 객체를 마스킹하는 데 의존하기 때문에 한계가 있어. 이러면 생성 결과가 입력 마스크에 제한되거든. 게다가 새로운 이미지에서 객체의 위치와 크기를 정확하게 지정하는 마스크를 얻는 건 정말 어려워. 

이런 한계를 극복하기 위해, 우리는 마스크에 구애받지 않는 비제한 생성 객체 합성이라는 새로운 문제를 정의했어. 이건 생성이 마스크에 제한되지 않는다는 거야. 그리고 우리는 합성된 쌍 데이터셋을 기반으로 확산 모델을 훈련시켰어. 

우리의 모델은 마스크를 넘어서는 그림자와 반사 같은 객체 효과를 생성할 수 있어서 이미지의 현실감을 높여줘. 게다가 빈 마스크가 주어지면, 우리 모델은 객체를 다양한 자연적인 위치와 크기로 자동 배치해주기 때문에 합성 작업이 빨라져. 

우리 모델은 다양한 품질 지표와 사용자 연구에서 기존의 객체 배치 및 합성 모델보다 성능이 뛰어나.

================================================================================

URL: https://arxiv.org/abs/2409.04560
Title: Multi-Modal Diffusion for Hand-Object Grasp Generation

Original Abstract:
In this work, we focus on generating hand grasp over objects. Compared to previous works of generating hand poses with a given object, we aim to allow the generalization of both hand and object shapes by a single model. Our proposed method Multi-modal Grasp Diffusion (MGD) learns the prior and conditional posterior distribution of both modalities from heterogeneous data sources. Therefore it relieves the limitation of hand-object grasp datasets by leveraging the large-scale 3D object datasets. According to both qualitative and quantitative experiments, both conditional and unconditional generation of hand grasp achieve good visual plausibility and diversity. The proposed method also generalizes well to unseen object shapes. The code and weights will be available at \url{this https URL}.

Translated Abstract:
이번 연구에서는 물체에 대한 손의 그립(grasp)을 생성하는 데 집중했어. 기존의 손 포즈를 특정 물체와 함께 생성하는 방법들과 비교해서, 우리는 하나의 모델로 손과 물체의 형태를 모두 일반화할 수 있도록 하는 게 목표야.

우리가 제안한 방법인 다중 모달 그립 확산(MGD)은 다양한 데이터 소스에서 손과 물체의 사전 분포와 조건부 후분포를 학습해. 그래서 대규모 3D 물체 데이터셋을 활용함으로써 손-물체 그립 데이터셋의 한계를 해소할 수 있어.

질적 및 양적 실험 결과에 따르면, 조건부 생성과 비조건부 생성 모두 손의 그립을 시각적으로 그럴듯하고 다양하게 만들어. 제안한 방법은 보지 못한 물체 형태에도 잘 일반화돼. 코드와 가중치는 이 링크에서 받을 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04561
Title: Dual-Level Cross-Modal Contrastive Clustering

Original Abstract:
Image clustering, which involves grouping images into different clusters without labels, is a key task in unsupervised learning. Although previous deep clustering methods have achieved remarkable results, they only explore the intrinsic information of the image itself but overlook external supervision knowledge to improve the semantic understanding of images. Recently, visual-language pre-trained model on large-scale datasets have been used in various downstream tasks and have achieved great results. However, there is a gap between visual representation learning and textual semantic learning, and how to properly utilize the representation of two different modalities for clustering is still a big challenge. To tackle the challenges, we propose a novel image clustering framwork, named Dual-level Cross-Modal Contrastive Clustering (DXMC). Firstly, external textual information is introduced for constructing a semantic space which is adopted to generate image-text pairs. Secondly, the image-text pairs are respectively sent to pre-trained image and text encoder to obtain image and text embeddings which subsquently are fed into four well-designed networks. Thirdly, dual-level cross-modal contrastive learning is conducted between discriminative representations of different modalities and distinct level. Extensive experimental results on five benchmark datasets demonstrate the superiority of our proposed method.

Translated Abstract:
이미지 클러스터링은 라벨 없이 이미지를 서로 다른 그룹으로 나누는 작업으로, 비지도 학습에서 중요한 역할을 해. 이전의 딥 클러스터링 방법들은 좋은 성과를 냈지만, 이미지 자체의 정보만 살펴보고 외부의 지도 지식을 활용하지 않아서 이미지의 의미를 잘 이해하지 못했어. 

최근에는 대규모 데이터셋에서 시각-언어 사전 학습 모델이 여러 하위 작업에 사용돼서 좋은 결과를 내고 있어. 하지만 시각적 표현 학습과 텍스트 의미 학습 사이에는 여전히 간극이 있고, 서로 다른 두 가지 모달리티의 표현을 클러스터링에 잘 활용하는 게 큰 도전이야. 

이 문제를 해결하기 위해, 우리는 DXMC(이중 수준 교차 모달 대조 클러스터링)라는 새로운 이미지 클러스터링 프레임워크를 제안해. 첫째로, 외부 텍스트 정보를 도입해서 의미 공간을 만들고, 이를 통해 이미지-텍스트 쌍을 생성해. 둘째로, 이 이미지-텍스트 쌍은 각각 사전 학습된 이미지와 텍스트 인코더에 보내져서 이미지와 텍스트 임베딩을 얻고, 이후 네 개의 잘 설계된 네트워크에 입력돼. 셋째로, 서로 다른 모달리티와 수준의 구별된 표현 사이에서 이중 수준 교차 모달 대조 학습을 수행해. 

다섯 개의 벤치마크 데이터셋에서의 광범위한 실험 결과는 우리가 제안한 방법의 우수성을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.04563
Title: Influence of Early through Late Fusion on Pancreas Segmentation from Imperfectly Registered Multimodal MRI

Original Abstract:
Multimodal fusion promises better pancreas segmentation. However, where to perform fusion in models is still an open question. It is unclear if there is a best location to fuse information when analyzing pairs of imperfectly aligned images. Two main alignment challenges in this pancreas segmentation study are 1) the pancreas is deformable and 2) breathing deforms the abdomen. Even after image registration, relevant deformations are often not corrected. We examine how early through late fusion impacts pancreas segmentation. We used 353 pairs of T2-weighted (T2w) and T1-weighted (T1w) abdominal MR images from 163 subjects with accompanying pancreas labels. We used image registration (deeds) to align the image pairs. We trained a collection of basic UNets with different fusion points, spanning from early to late, to assess how early through late fusion influenced segmentation performance on imperfectly aligned images. We assessed generalization of fusion points on nnUNet. The single-modality T2w baseline using a basic UNet model had a Dice score of 0.73, while the same baseline on the nnUNet model achieved 0.80. For the basic UNet, the best fusion approach occurred in the middle of the encoder (early/mid fusion), which led to a statistically significant improvement of 0.0125 on Dice score compared to the baseline. For the nnUNet, the best fusion approach was naïve image concatenation before the model (early fusion), which resulted in a statistically significant Dice score increase of 0.0021 compared to baseline. Fusion in specific blocks can improve performance, but the best blocks for fusion are model specific, and the gains are small. In imperfectly registered datasets, fusion is a nuanced problem, with the art of design remaining vital for uncovering potential insights. Future innovation is needed to better address fusion in cases of imperfect alignment of abdominal image pairs.

Translated Abstract:
다중 모달 융합은 췌장 분할을 더 잘 할 수 있게 해줘. 하지만 모델에서 융합을 어디서 해야 할지는 아직 확실하지 않아. 불완전하게 정렬된 이미지 쌍을 분석할 때 정보를 융합할 최적의 위치가 있는지 불분명해. 이번 췌장 분할 연구에서 두 가지 주요 정렬 문제는 1) 췌장이 변형 가능하고 2) 호흡이 복부를 변형시킨다는 거야. 이미지 등록을 해도 관련된 변형이 종종 수정되지 않아.

우리는 초기 융합부터 후반 융합까지 췌장 분할에 미치는 영향을 살펴봤어. 163명의 피험자에서 353 쌍의 T2 가중치(T2w)와 T1 가중치(T1w) 복부 MR 이미지를 사용했고, 각 췌장 레이블도 포함했어. 이미지 쌍을 정렬하기 위해 이미지 등록(deeds)을 사용했어. 우리는 서로 다른 융합 지점을 가진 기본 U-Net 모델을 훈련시켜서 불완전하게 정렬된 이미지에서 초기와 후반 융합이 분할 성능에 어떻게 영향을 미치는지 평가했어. nnUNet의 융합 지점 일반화도 검토했어.

기본 U-Net 모델을 사용한 단일 모달 T2w 기준에서는 Dice 점수가 0.73이었고, nnUNet 모델에서는 0.80이었어. 기본 U-Net에서 가장 효과적인 융합 방법은 인코더 중간에서 발생했어(초기/중간 융합) 이로 인해 기준 대비 Dice 점수가 0.0125만큼 통계적으로 유의미하게 개선됐어. nnUNet에서는 모델 이전에 단순 이미지 연결(초기 융합)이 가장 좋은 융합 방법이었고, 이로 인해 기준 대비 Dice 점수가 0.0021만큼 통계적으로 유의미하게 증가했어.

특정 블록에서의 융합이 성능을 개선할 수 있지만, 융합에 가장 좋은 블록은 모델에 따라 다르고, 향상 효과는 작아. 불완전하게 등록된 데이터셋에서는 융합이 복잡한 문제고, 디자인의 기술이 잠재적인 통찰을 발견하는 데 여전히 중요해. 앞으로 불완전하게 정렬된 복부 이미지 쌍의 융합 문제를 더 잘 해결하기 위한 혁신이 필요해.

================================================================================

URL: https://arxiv.org/abs/2409.04598
Title: A Novel Dataset for Video-Based Autism Classification Leveraging Extra-Stimulatory Behavior

Original Abstract:
Autism Spectrum Disorder (ASD) can affect individuals at varying degrees of intensity, from challenges in overall health, communication, and sensory processing, and this often begins at a young age. Thus, it is critical for medical professionals to be able to accurately diagnose ASD in young children, but doing so is difficult. Deep learning can be responsibly leveraged to improve productivity in addressing this task. The availability of data, however, remains a considerable obstacle. Hence, in this work, we introduce the Video ASD dataset--a dataset that contains video frame convolutional and attention map feature data--to foster further progress in the task of ASD classification. The original videos showcase children reacting to chemo-sensory stimuli, among auditory, touch, and vision This dataset contains the features of the frames spanning 2,467 videos, for a total of approximately 1.4 million frames. Additionally, head pose angles are included to account for head movement noise, as well as full-sentence text labels for the taste and smell videos that describe how the facial expression changes before, immediately after, and long after interaction with the stimuli. In addition to providing features, we also test foundation models on this data to showcase how movement noise affects performance and the need for more data and more complex labels.

Translated Abstract:
자폐 스펙트럼 장애(ASD)는 개인마다 심각도가 다르게 나타날 수 있어. 건강, 의사소통, 감각 처리에서 어려움을 겪는 경우가 많고, 보통 어린 나이에 시작해. 그래서 의료 전문가들이 어린 아이들에게 ASD를 정확히 진단하는 게 정말 중요하지만, 이게 쉽지가 않아. 딥러닝을 활용하면 이 작업의 생산성을 높일 수 있어. 하지만 데이터가 부족한 게 큰 장애물로 남아있어.

그래서 이번 연구에서는 Video ASD 데이터셋을 소개할 거야. 이 데이터셋은 비디오 프레임의 컨볼루션과 주의 맵 특징 데이터를 포함하고 있어. 원래 비디오는 아이들이 청각, 촉각, 시각 자극에 반응하는 모습을 보여줘. 이 데이터셋은 2,467개의 비디오에서 약 140만 개의 프레임을 포함하고 있어. 또한, 머리 움직임 소음을 고려하기 위해 머리 방향 각도도 포함하고, 자극과의 상호작용 전후에 얼굴 표정이 어떻게 변하는지 설명하는 전체 문장 텍스트 레이블도 제공해.

우리는 이 데이터를 사용해 기본 모델을 테스트해보면서 움직임 소음이 성능에 미치는 영향을 보여주고, 더 많은 데이터와 복잡한 레이블이 필요하다는 것도 강조할 거야.

================================================================================

URL: https://arxiv.org/abs/2409.04601
Title: Multi-scale Feature Fusion with Point Pyramid for 3D Object Detection

Original Abstract:
Effective point cloud processing is crucial to LiDARbased autonomous driving systems. The capability to understand features at multiple scales is required for object detection of intelligent vehicles, where road users may appear in different sizes. Recent methods focus on the design of the feature aggregation operators, which collect features at different scales from the encoder backbone and assign them to the points of interest. While efforts are made into the aggregation modules, the importance of how to fuse these multi-scale features has been overlooked. This leads to insufficient feature communication across scales. To address this issue, this paper proposes the Point Pyramid RCNN (POP-RCNN), a feature pyramid-based framework for 3D object detection on point clouds. POP-RCNN consists of a Point Pyramid Feature Enhancement (PPFE) module to establish connections across spatial scales and semantic depths for information exchange. The PPFE module effectively fuses multi-scale features for rich information without the increased complexity in feature aggregation. To remedy the impact of inconsistent point densities, a point density confidence module is deployed. This design integration enables the use of a lightweight feature aggregator, and the emphasis on both shallow and deep semantics, realising a detection framework for 3D object detection. With great adaptability, the proposed method can be applied to a variety of existing frameworks to increase feature richness, especially for long-distance detection. By adopting the PPFE in the voxel-based and point-voxel-based baselines, experimental results on KITTI and Waymo Open Dataset show that the proposed method achieves remarkable performance even with limited computational headroom.

Translated Abstract:
LiDAR 기반 자율주행 시스템에서는 효과적인 포인트 클라우드 처리가 정말 중요해. 다양한 크기의 도로 사용자가 나타날 수 있기 때문에, 여러 스케일에서 특징을 이해하는 능력이 필요해. 최근에는 특징 집합 연산자를 설계하는 데 초점을 맞추고 있는데, 이 연산자들은 인코더 백본에서 다양한 스케일의 특징을 모아서 관심 있는 포인트에 할당해. 

하지만 집합 모듈에 많은 노력을 기울이는 반면, 이 여러 스케일의 특징을 어떻게 융합할지가 간과되고 있어. 이로 인해 스케일 간의 특징 소통이 부족해지고 있어. 이 문제를 해결하기 위해, 이 논문에서는 Point Pyramid RCNN(POP-RCNN)이라는 3D 물체 탐지를 위한 특징 피라미드 기반 프레임워크를 제안해. POP-RCNN은 정보 교환을 위해 공간적 스케일과 의미적 깊이 간의 연결을 설정하는 Point Pyramid Feature Enhancement (PPFE) 모듈로 구성돼.

PPFE 모듈은 집합의 복잡성을 증가시키지 않으면서도 풍부한 정보를 위해 여러 스케일의 특징을 효과적으로 융합해. 또한, 불균일한 포인트 밀도의 영향을 줄이기 위해 포인트 밀도 신뢰도 모듈도 배치했어. 이런 설계 통합 덕분에 가벼운 특징 집합기를 사용할 수 있게 되고, 얕은 의미와 깊은 의미 모두에 중점을 두게 돼서 3D 물체 탐지 프레임워크가 실현되는 거야. 

제안된 방법은 적응력이 뛰어나서 다양한 기존 프레임워크에 적용할 수 있고, 특히 장거리 탐지에 대해 특징의 풍부함을 증가시킬 수 있어. PPFE를 voxel 기반과 point-voxel 기반의 기준에 적용했을 때, KITTI와 Waymo Open Dataset에서 실험 결과가 놀라운 성과를 보여줬어. 제한된 계산 자원으로도 성능을 잘 발휘하는 거지.

================================================================================

URL: https://arxiv.org/abs/2409.04607
Title: Self-Supervised Contrastive Learning for Videos using Differentiable Local Alignment

Original Abstract:
Robust frame-wise embeddings are essential to perform video analysis and understanding tasks. We present a self-supervised method for representation learning based on aligning temporal video sequences. Our framework uses a transformer-based encoder to extract frame-level features and leverages them to find the optimal alignment path between video sequences. We introduce the novel Local-Alignment Contrastive (LAC) loss, which combines a differentiable local alignment loss to capture local temporal dependencies with a contrastive loss to enhance discriminative learning. Prior works on video alignment have focused on using global temporal ordering across sequence pairs, whereas our loss encourages identifying the best-scoring subsequence alignment. LAC uses the differentiable Smith-Waterman (SW) affine method, which features a flexible parameterization learned through the training phase, enabling the model to adjust the temporal gap penalty length dynamically. Evaluations show that our learned representations outperform existing state-of-the-art approaches on action recognition tasks.

Translated Abstract:
비디오 분석과 이해를 위해서는 강력한 프레임 단위 임베딩이 꼭 필요해. 우리는 시간적 비디오 시퀀스를 정렬하는 자기 감독 방식의 표현 학습 방법을 제안해. 이 프레임워크는 트랜스포머 기반 인코더를 사용해서 프레임 수준의 특징을 추출하고, 이를 사용해 비디오 시퀀스 간의 최적 정렬 경로를 찾는 거야.

우리는 새로운 Local-Alignment Contrastive (LAC) 손실 함수를 도입했어. 이 손실 함수는 지역적인 정렬 손실을 결합해서 지역 시간적 의존성을 포착하고, 대조 손실을 통해 구별 학습을 강화해. 이전 연구들은 시퀀스 쌍 사이의 전역 시간 순서에 집중했지만, 우리의 손실 함수는 가장 높은 점수를 받는 부분 시퀀스 정렬을 찾도록 유도해.

LAC는 미분 가능한 Smith-Waterman (SW) 방법을 사용해. 이 방법은 훈련 과정에서 학습된 유연한 파라미터화를 특징으로 해서, 모델이 시간 간격 페널티 길이를 동적으로 조정할 수 있도록 해. 평가 결과, 우리가 학습한 표현이 기존의 최첨단 방법들보다 행동 인식 작업에서 더 뛰어난 성능을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.04670
Title: Multi-Conditioned Denoising Diffusion Probabilistic Model (mDDPM) for Medical Image Synthesis

Original Abstract:
Medical imaging applications are highly specialized in terms of human anatomy, pathology, and imaging domains. Therefore, annotated training datasets for training deep learning applications in medical imaging not only need to be highly accurate but also diverse and large enough to encompass almost all plausible examples with respect to those specifications. We argue that achieving this goal can be facilitated through a controlled generation framework for synthetic images with annotations, requiring multiple conditional specifications as input to provide control. We employ a Denoising Diffusion Probabilistic Model (DDPM) to train a large-scale generative model in the lung CT domain and expand upon a classifier-free sampling strategy to showcase one such generation framework. We show that our approach can produce annotated lung CT images that can faithfully represent anatomy, convincingly fooling experts into perceiving them as real. Our experiments demonstrate that controlled generative frameworks of this nature can surpass nearly every state-of-the-art image generative model in achieving anatomical consistency in generated medical images when trained on comparable large medical datasets.

Translated Abstract:
의료 이미징 응용 프로그램은 인체 해부학, 병리학, 그리고 이미징 분야에서 매우 전문화되어 있어. 그래서 의료 이미징에 딥러닝을 적용하기 위해서는 주석이 붙은 훈련 데이터셋이 정확해야 할 뿐만 아니라, 다양한 예제를 포함할 수 있을 만큼 충분히 커야 해.

우리는 이 목표를 달성하기 위해 주석이 붙은 합성 이미지를 통제된 방식으로 생성하는 프레임워크가 필요하다고 주장해. 이 프레임워크는 여러 조건 사양을 입력으로 받아야 제어가 가능해. 우리는 폐 CT 분야에서 대규모 생성 모델을 훈련하기 위해 Denoising Diffusion Probabilistic Model (DDPM)을 사용하고, 분류기 없는 샘플링 전략을 확장해서 이런 생성 프레임워크를 보여줄 거야.

우리의 접근 방식은 해부학을 충실히 나타내는 주석이 붙은 폐 CT 이미지를 만들어낼 수 있다는 걸 보여줘. 전문가들이 이 이미지를 실제로 인식하게 속일 정도야. 우리의 실험 결과는 이런 통제된 생성 프레임워크가 비교 가능한 대규모 의료 데이터셋으로 훈련했을 때, 생성된 의료 이미지에서 해부학적 일관성을 달성하는 데 있어 거의 모든 최신 이미지 생성 모델을 초월할 수 있음을 입증해.

================================================================================

URL: https://arxiv.org/abs/2409.04679
Title: Neural Augmentation Based Panoramic High Dynamic Range Stitching

Original Abstract:
Due to saturated regions of inputting low dynamic range (LDR) images and large intensity changes among the LDR images caused by different exposures, it is challenging to produce an information enriched panoramic LDR image without visual artifacts for a high dynamic range (HDR) scene through stitching multiple geometrically synchronized LDR images with different exposures and pairwise overlapping fields of views (OFOVs). Fortunately, the stitching of such images is innately a perfect scenario for the fusion of a physics-driven approach and a data-driven approach due to their OFOVs. Based on this new insight, a novel neural augmentation based panoramic HDR stitching algorithm is proposed in this paper. The physics-driven approach is built up using the OFOVs. Different exposed images of each view are initially generated by using the physics-driven approach, are then refined by a data-driven approach, and are finally used to produce panoramic LDR images with different exposures. All the panoramic LDR images with different exposures are combined together via a multi-scale exposure fusion algorithm to produce the final panoramic LDR image. Experimental results demonstrate the proposed algorithm outperforms existing panoramic stitching algorithms.

Translated Abstract:
저조도(LDR) 이미지에서 포화된 영역과 서로 다른 노출로 인해 생기는 큰 밝기 변화 때문에, 여러 개의 기하학적으로 동기화된 LDR 이미지를 이어 붙여서 시각적 아티팩트 없이 정보가 풍부한 파노라마 LDR 이미지를 만드는 게 어려워. 특히 고동적 범위(HDR) 장면을 다룰 때 더욱 그렇지. 

다행히도 이런 이미지를 이어 붙이는 작업은 물리 기반 접근 방식과 데이터 기반 접근 방식을 융합하기에 아주 좋은 상황이야. 이 논문에서는 이러한 새로운 통찰을 바탕으로 한 신경망 증강 기반의 파노라마 HDR 스티칭 알고리즘을 제안해. 

물리 기반 접근 방식은 OFOV를 사용해 구성돼. 각 뷰의 서로 다른 노출 이미지는 처음에 물리 기반 접근 방식으로 생성되고, 그 다음에 데이터 기반 접근 방식으로 다듬어져. 마지막으로 이 이미지들을 조합해서 다양한 노출을 가진 파노라마 LDR 이미지를 만들어. 

모든 다양한 노출의 파노라마 LDR 이미지는 다중 스케일 노출 융합 알고리즘을 통해 합쳐져서 최종 파노라마 LDR 이미지를 생성해. 실험 결과를 보면, 제안한 알고리즘이 기존의 파노라마 스티칭 알고리즘보다 성능이 뛰어난 걸 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.04683
Title: C2F-CHART: A Curriculum Learning Approach to Chart Classification

Original Abstract:
In scientific research, charts are usually the primary method for visually representing data. However, the accessibility of charts remains a significant concern. In an effort to improve chart understanding pipelines, we focus on optimizing the chart classification component. We leverage curriculum learning, which is inspired by the human learning process. In this paper, we introduce a novel training approach for chart classification that utilizes coarse-to-fine curriculum learning. Our approach, which we name C2F-CHART (for coarse-to-fine) exploits inter-class similarities to create learning tasks of varying difficulty levels. We benchmark our method on the ICPR 2022 CHART-Infographics UB UNITEC PMC dataset, outperforming the state-of-the-art results.

Translated Abstract:
과학 연구에서 차트는 데이터를 시각적으로 나타내는 주된 방법이야. 하지만 차트의 접근성은 여전히 큰 문제야. 차트를 더 잘 이해할 수 있도록 돕기 위해, 우리는 차트 분류 부분을 최적화하는 데 집중했어. 

우리는 사람의 학습 과정에서 영감을 받은 커리큘럼 학습을 활용했어. 이 논문에서는 거친 것에서 세밀한 것까지의 커리큘럼 학습을 사용하는 새로운 차트 분류 훈련 방법을 소개해. 우리가 만든 방법은 C2F-CHART(거친 것에서 세밀한 것까지의 약자)라고 해. 이 방법은 클래스 간의 유사성을 이용해서 난이도가 다른 학습 과제를 만들어. 

우리는 ICPR 2022 CHART-Infographics UB UNITEC PMC 데이터셋에서 이 방법을 평가했는데, 기존의 최고 성능을 초과했어.

================================================================================

URL: https://arxiv.org/abs/2409.04699
Title: Dual-stream Feature Augmentation for Domain Generalization

Original Abstract:
Domain generalization (DG) task aims to learn a robust model from source domains that could handle the out-of-distribution (OOD) issue. In order to improve the generalization ability of the model in unseen domains, increasing the diversity of training samples is an effective solution. However, existing augmentation approaches always have some limitations. On the one hand, the augmentation manner in most DG methods is not enough as the model may not see the perturbed features in approximate the worst case due to the randomness, thus the transferability in features could not be fully explored. On the other hand, the causality in discriminative features is not involved in these methods, which harms the generalization ability of model due to the spurious correlations. To address these issues, we propose a Dual-stream Feature Augmentation~(DFA) method by constructing some hard features from two perspectives. Firstly, to improve the transferability, we construct some targeted features with domain related augmentation manner. Through the guidance of uncertainty, some hard cross-domain fictitious features are generated to simulate domain shift. Secondly, to take the causality into consideration, the spurious correlated non-causal information is disentangled by an adversarial mask, then the more discriminative features can be extracted through these hard causal related information. Different from previous fixed synthesizing strategy, the two augmentations are integrated into a unified learnable feature disentangle model. Based on these hard features, contrastive learning is employed to keep the semantic consistency and improve the robustness of the model. Extensive experiments on several datasets demonstrated that our approach could achieve state-of-the-art performance for domain generalization. Our code is available at: this https URL.

Translated Abstract:
도메인 일반화(DG) 작업은 소스 도메인에서 강력한 모델을 학습해 분포 밖(out-of-distribution, OOD) 문제를 해결하는 걸 목표로 해. 모델이 보지 못한 도메인에서도 잘 작동하도록 하려면 훈련 샘플의 다양성을 늘리는 게 효과적인 방법이야. 하지만 기존의 데이터 증강 방법들은 몇 가지 한계가 있어.

하나는, 대부분의 DG 방법에서 사용하는 증강 방식이 충분하지 않다는 거야. 모델이 랜덤성 때문에 최악의 경우에서 변형된 특징을 보지 못할 수 있어서, 특징의 전이 가능성을 충분히 활용하지 못해. 또 다른 문제는, 이런 방법들이 판별적 특징에서의 인과성을 고려하지 않아서, 잘못된 상관관계로 인해 모델의 일반화 능력이 떨어진다는 거야.

이 문제를 해결하기 위해 우리는 두 가지 관점에서 강한 특징을 만드는 이중 스트림 특징 증강(Dual-stream Feature Augmentation, DFA) 방법을 제안해. 첫째, 전이 가능성을 높이기 위해 도메인 관련 증강 방법으로 목표 특징을 만들고, 불확실성의 지침을 통해 도메인 변화에 맞춰 하드 크로스 도메인 가상의 특징을 생성해. 둘째, 인과성을 고려하기 위해, 잘못된 상관관계의 비인과 정보를 적대적 마스크로 분리하고, 이 하드 인과 관련 정보를 통해 더 판별적인 특징을 추출해.

이전의 고정된 합성 전략과는 다르게, 두 가지 증강 방법을 통합하여 하나의 학습 가능한 특징 분리 모델로 만들었어. 이 하드 특징들을 바탕으로 대조 학습을 사용해 의미의 일관성을 유지하고 모델의 강인성을 향상시켜. 여러 데이터셋에서 진행한 실험 결과, 우리의 방법이 도메인 일반화에서 최첨단 성능을 달성할 수 있음을 보여줬어. 코드도 제공하고 있으니, 이 링크에서 확인해봐.

================================================================================

URL: https://arxiv.org/abs/2409.04714
Title: Unleashing the Power of Generic Segmentation Models: A Simple Baseline for Infrared Small Target Detection

Original Abstract:
Recent advancements in deep learning have greatly advanced the field of infrared small object detection (IRSTD). Despite their remarkable success, a notable gap persists between these IRSTD methods and generic segmentation approaches in natural image domains. This gap primarily arises from the significant modality differences and the limited availability of infrared data. In this study, we aim to bridge this divergence by investigating the adaptation of generic segmentation models, such as the Segment Anything Model (SAM), to IRSTD tasks. Our investigation reveals that many generic segmentation models can achieve comparable performance to state-of-the-art IRSTD methods. However, their full potential in IRSTD remains untapped. To address this, we propose a simple, lightweight, yet effective baseline model for segmenting small infrared objects. Through appropriate distillation strategies, we empower smaller student models to outperform state-of-the-art methods, even surpassing fine-tuned teacher results. Furthermore, we enhance the model's performance by introducing a novel query design comprising dense and sparse queries to effectively encode multi-scale features. Through extensive experimentation across four popular IRSTD datasets, our model demonstrates significantly improved performance in both accuracy and throughput compared to existing approaches, surpassing SAM and Semantic-SAM by over 14 IoU on NUDT and 4 IoU on IRSTD1k. The source code and models will be released at this https URL.

Translated Abstract:
최근 딥러닝의 발전 덕분에 적외선 소형 물체 탐지(IRSTD) 분야가 많이 발전했어. 하지만 이 IRSTD 방법들이 일반적인 세분화 접근법과 비교했을 때 아직 괴리가 있어. 이 괴리는 주로 적외선 데이터의 부족과 서로 다른 모달리티 차이 때문이야.

이번 연구에서는 Segment Anything Model (SAM) 같은 일반 세분화 모델을 IRSTD 작업에 적응시켜서 이 괴리를 줄여보려고 해. 조사해보니 많은 일반 세분화 모델들이 최신 IRSTD 방법들과 비슷한 성능을 낼 수 있다는 걸 발견했어. 하지만 IRSTD에서 그들의 잠재력을 모두 활용하지 못하고 있어.

그래서 우리는 작고 가벼우면서도 효과적인 소형 적외선 물체 분할을 위한 기본 모델을 제안해. 적절한 증류 전략을 통해 작은 학생 모델이 최신 방법보다 더 나은 성과를 낼 수 있도록 했고, 심지어 튜닝된 선생님 모델의 결과도 초과했어. 게다가 밀집 쿼리와 희소 쿼리를 포함한 새로운 쿼리 디자인을 도입해서 다중 스케일 특징을 효과적으로 인코딩하여 모델의 성능을 향상시켰어.

네 개의 인기 있는 IRSTD 데이터셋에서 광범위한 실험을 진행한 결과, 우리의 모델이 기존 접근법보다 정확도와 처리량 모두에서 크게 개선된 성능을 보였어. NUDT에서는 SAM과 Semantic-SAM보다 14 IoU 이상, IRSTD1k에서는 4 IoU 이상 초과했어. 소스 코드와 모델은 이 https URL에서 공개될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.04718
Title: Cross-Organ Domain Adaptive Neural Network for Pancreatic Endoscopic Ultrasound Image Segmentation

Original Abstract:
Accurate segmentation of lesions in pancreatic endoscopic ultrasound (EUS) images is crucial for effective diagnosis and treatment. However, the collection of enough crisp EUS images for effective diagnosis is arduous. Recently, domain adaptation (DA) has been employed to address these challenges by leveraging related knowledge from other domains. Most DA methods only focus on multi-view representations of the same organ, which makes it still tough to clearly depict the tumor lesion area with limited semantic information. Although transferring homogeneous similarity from different organs could benefit the issue, there is a lack of relevant work due to the enormous domain gap between them. To address these challenges, we propose the Cross-Organ Tumor Segmentation Networks (COTS-Nets), consisting of a universal network and an auxiliary network. The universal network utilizes boundary loss to learn common boundary information of different tumors, enabling accurate delineation of tumors in EUS despite limited and low-quality data. Simultaneously, we incorporate consistency loss in the universal network to align the prediction of pancreatic EUS with tumor boundaries from other organs to mitigate the domain gap. To further reduce the cross-organ domain gap, the auxiliary network integrates multi-scale features from different organs, aiding the universal network in acquiring domain-invariant knowledge. Systematic experiments demonstrate that COTS-Nets significantly improves the accuracy of pancreatic cancer diagnosis. Additionally, we developed the Pancreatic Cancer Endoscopic Ultrasound (PCEUS) dataset, comprising 501 pathologically confirmed pancreatic EUS images, to facilitate model development.

Translated Abstract:
췌장 내시경 초음파(EUS) 이미지에서 병변을 정확하게 분할하는 것은 효과적인 진단과 치료를 위해 정말 중요해. 하지만 효과적인 진단을 위해 충분히 선명한 EUS 이미지를 모으는 것은 쉽지 않아. 최근에는 도메인 적응(DA) 기법이 이 문제를 해결하기 위해 다른 도메인에서 관련된 지식을 활용하는 데 사용되고 있어. 

대부분의 DA 방법은 같은 장기의 다중 뷰 표현에만 집중하는데, 이러면 제한된 의미 정보로는 종양 병변을 명확하게 묘사하기 어려워. 다른 장기에서 동질적인 유사성을 전이하는 것이 도움이 될 수 있지만, 이들 간의 도메인 차이가 커서 관련된 연구가 부족해. 

이런 문제를 해결하기 위해 우리는 Cross-Organ Tumor Segmentation Networks (COTS-Nets)를 제안해. 이 모델은 보편 네트워크와 보조 네트워크로 구성되어 있어. 보편 네트워크는 경계 손실을 이용해 서로 다른 종양의 공통 경계 정보를 학습하고, 제한적이고 저품질의 데이터로도 EUS에서 종양을 정확하게 구분할 수 있도록 해. 동시에, 우리는 보편 네트워크에 일관성 손실을 적용해 췌장 EUS의 예측을 다른 장기에서의 종양 경계와 맞추어 도메인 차이를 줄여. 

교차 장기 도메인 차이를 더 줄이기 위해 보조 네트워크는 서로 다른 장기에서의 다중 스케일 특징을 통합해, 보편 네트워크가 도메인 불변 지식을 얻는 데 도움을 줘. 체계적인 실험 결과, COTS-Nets가 췌장암 진단의 정확성을 크게 향상시킨다는 것을 보여줬어. 또한, 모델 개발을 돕기 위해 501개의 병리학적으로 확인된 췌장 EUS 이미지를 포함한 췌장암 내시경 초음파(PCEUS) 데이터셋도 만들었어.

================================================================================

URL: https://arxiv.org/abs/2409.04732
Title: VidLPRO: A $\underline{Vid}$eo-$\underline{L}$anguage $\underline{P}$re-training Framework for $\underline{Ro}$botic and Laparoscopic Surgery

Original Abstract:
We introduce VidLPRO, a novel video-language (VL) pre-training framework designed specifically for robotic and laparoscopic surgery. While existing surgical VL models primarily rely on contrastive learning, we propose a more comprehensive approach to capture the intricate temporal dynamics and align video with language. VidLPRO integrates video-text contrastive learning, video-text matching, and masked language modeling objectives to learn rich VL representations. To support this framework, we present GenSurg+, a carefully curated dataset derived from GenSurgery, comprising 17k surgical video clips paired with captions generated by GPT-4 using transcripts extracted by the Whisper model. This dataset addresses the need for large-scale, high-quality VL data in the surgical domain. Extensive experiments on benchmark datasets, including Cholec80 and AutoLaparo, demonstrate the efficacy of our approach. VidLPRO achieves state-of-the-art performance in zero-shot surgical phase recognition, significantly outperforming existing surgical VL models such as SurgVLP and HecVL. Our model demonstrates improvements of up to 21.5\% in accuracy and 15.7% in F1 score, setting a new benchmark in the field. Notably, VidLPRO exhibits robust performance even with single-frame inference, while effectively scaling with increased temporal context. Ablation studies reveal the impact of frame sampling strategies on model performance and computational efficiency. These results underscore VidLPRO's potential as a foundation model for surgical video understanding.

Translated Abstract:
우리는 로봇 수술과 복강경 수술을 위해 특별히 설계된 새로운 비디오-언어(VL) 사전 훈련 프레임워크인 VidLPRO를 소개해. 기존의 수술 VL 모델들은 주로 대비 학습에 의존하는데, 우리는 비디오와 언어의 복잡한 시간적 동력을 포착하고 정렬할 수 있는 더 포괄적인 접근 방식을 제안해.

VidLPRO는 비디오-텍스트 대비 학습, 비디오-텍스트 매칭, 마스크 언어 모델링 목표를 통합해서 풍부한 VL 표현을 학습해. 이 프레임워크를 지원하기 위해, 우리는 GenSurg+라는 데이터셋을 소개하는데, 이 데이터셋은 GenSurgery에서 파생된 17,000개의 수술 비디오 클립과 Whisper 모델로 추출된 대본을 사용해 GPT-4가 생성한 캡션으로 구성되어 있어. 이 데이터셋은 수술 분야에서 대규모, 고품질 VL 데이터의 필요성을 해결해.

Cholec80와 AutoLaparo 같은 벤치마크 데이터셋에서의 광범위한 실험 결과, 우리의 접근 방식이 효과적임을 보여줘. VidLPRO는 제로샷 수술 단계 인식에서 최첨단 성능을 달성하고, SurgVLP와 HecVL 같은 기존 수술 VL 모델들보다 훨씬 더 우수한 성과를 내. 우리 모델은 정확도에서 최대 21.5% 향상되었고, F1 점수에서 15.7% 향상되었어, 이는 이 분야에서 새로운 기준을 세운 거야.

특히 VidLPRO는 단일 프레임 추론에서도 강력한 성능을 보이며, 시간적 맥락이 증가할수록 효과적으로 확장돼. 제거 연구 결과는 프레임 샘플링 전략이 모델 성능과 계산 효율성에 미치는 영향을 보여줘. 이러한 결과는 VidLPRO가 수술 비디오 이해를 위한 기초 모델로서의 잠재력을 강조해.

================================================================================

URL: https://arxiv.org/abs/2409.04734
Title: Swin Transformer for Robust Differentiation of Real and Synthetic Images: Intra- and Inter-Dataset Analysis

Original Abstract:
\textbf{Purpose} This study aims to address the growing challenge of distinguishing computer-generated imagery (CGI) from authentic digital images in the RGB color space. Given the limitations of existing classification methods in handling the complexity and variability of CGI, this research proposes a Swin Transformer-based model for accurate differentiation between natural and synthetic images.
\textbf{Methods} The proposed model leverages the Swin Transformer's hierarchical architecture to capture local and global features crucial for distinguishing CGI from natural images. The model's performance was evaluated through intra-dataset and inter-dataset testing across three distinct datasets: CiFAKE, JSSSTU, and Columbia. The datasets were tested individually (D1, D2, D3) and in combination (D1+D2+D3) to assess the model's robustness and domain generalization capabilities.
\textbf{Results} The Swin Transformer-based model demonstrated high accuracy, consistently achieving a range of 97-99\% across all datasets and testing scenarios. These results confirm the model's effectiveness in detecting CGI, showcasing its robustness and reliability in both intra-dataset and inter-dataset evaluations.
\textbf{Conclusion} The findings of this study highlight the Swin Transformer model's potential as an advanced tool for digital image forensics, particularly in distinguishing CGI from natural images. The model's strong performance across multiple datasets indicates its capability for domain generalization, making it a valuable asset in scenarios requiring precise and reliable image classification.

Translated Abstract:
**목적** 이 연구는 RGB 색 공간에서 컴퓨터 생성 이미지(CGI)와 진짜 디지털 이미지를 구분하는 어려움을 해결하는 것을 목표로 해. 기존의 분류 방법들이 CGI의 복잡성과 다양성을 처리하는 데 한계가 있어서, 우리는 자연 이미지와 합성 이미지를 정확하게 구분할 수 있는 스윈 트랜스포머 기반 모델을 제안해.

**방법** 제안한 모델은 스윈 트랜스포머의 계층적 구조를 활용해서 CGI와 자연 이미지를 구별하는 데 중요한 지역적 및 전역적 특징을 포착해. 모델의 성능은 세 가지 서로 다른 데이터셋인 CiFAKE, JSSSTU, Columbia를 사용해서 평가했어. 데이터셋을 개별적으로(D1, D2, D3) 그리고 조합해서(D1+D2+D3) 테스트해서 모델의 강건성과 도메인 일반화 능력을 확인했어.

**결과** 스윈 트랜스포머 기반 모델은 모든 데이터셋과 테스트 시나리오에서 97-99%의 높은 정확도를 보여줬어. 이 결과는 모델이 CGI를 감지하는 데 효과적이라는 것을 확인해 주고, 데이터셋 내외부 평가에서 강건성과 신뢰성을 보여줬어.

**결론** 이 연구의 결과는 스윈 트랜스포머 모델이 디지털 이미지 포렌식에서 CGI와 자연 이미지를 구분하는 고급 도구로서의 가능성을 강조해. 여러 데이터셋에서 강력한 성능을 보이는 이 모델은 정확하고 신뢰할 수 있는 이미지 분류가 필요한 상황에서 유용한 자산이 될 거야.

================================================================================

URL: https://arxiv.org/abs/2409.04742
Title: Enhancing Image Authenticity Detection: Swin Transformers and Color Frame Analysis for CGI vs. Real Images

Original Abstract:
The rapid advancements in computer graphics have greatly enhanced the quality of computer-generated images (CGI), making them increasingly indistinguishable from authentic images captured by digital cameras (ADI). This indistinguishability poses significant challenges, especially in an era of widespread misinformation and digitally fabricated content. This research proposes a novel approach to classify CGI and ADI using Swin Transformers and preprocessing techniques involving RGB and CbCrY color frame analysis. By harnessing the capabilities of Swin Transformers, our method foregoes handcrafted features instead of relying on raw pixel data for model training. This approach achieves state-of-the-art accuracy while offering substantial improvements in processing speed and robustness against joint image manipulations such as noise addition, blurring, and JPEG compression. Our findings highlight the potential of Swin Transformers combined with advanced color frame analysis for effective and efficient image authenticity detection.

Translated Abstract:
컴퓨터 그래픽스의 빠른 발전 덕분에 컴퓨터 생성 이미지(CGI)의 품질이 많이 향상되었고, 이제는 디지털 카메라로 촬영한 진짜 이미지(ADI)와 구분하기 어려운 수준이 되었어. 이런 구분이 어려운 상황은 허위 정보가 만연한 요즘, 큰 문제를 일으킬 수 있어.

이 연구에서는 CGI와 ADI를 분류하기 위한 새로운 접근 방식을 제안해. 이 방법은 Swin Transformers와 RGB 및 CbCrY 색상 프레임 분석을 활용한 전처리 기술을 사용해. Swin Transformers의 장점을 살려서, 수작업으로 만든 특징 대신에 원래의 픽셀 데이터를 모델 학습에 사용해. 

이 접근 방식은 최신 기술 수준의 정확도를 달성하면서도, 처리 속도와 노이즈 추가, 블러링, JPEG 압축 같은 이미지 변형에 대한 강인성을 크게 개선했어. 우리의 연구 결과는 고급 색상 프레임 분석과 결합된 Swin Transformers가 효과적이고 효율적인 이미지 진위 감지에 큰 잠재력을 가지고 있음을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.04747
Title: Explicit Mutual Information Maximization for Self-Supervised Learning

Original Abstract:
Recently, self-supervised learning (SSL) has been extensively studied. Theoretically, mutual information maximization (MIM) is an optimal criterion for SSL, with a strong theoretical foundation in information theory. However, it is difficult to directly apply MIM in SSL since the data distribution is not analytically available in applications. In practice, many existing methods can be viewed as approximate implementations of the MIM criterion. This work shows that, based on the invariance property of MI, explicit MI maximization can be applied to SSL under a generic distribution assumption, i.e., a relaxed condition of the data distribution. We further illustrate this by analyzing the generalized Gaussian distribution. Based on this result, we derive a loss function based on the MIM criterion using only second-order statistics. We implement the new loss for SSL and demonstrate its effectiveness via extensive experiments.

Translated Abstract:
최근에 자기 지도 학습(SSL)에 대한 연구가 많이 진행되고 있어. 이론적으로, 상호 정보 최대화(MIM)는 SSL에 대한 최적 기준으로, 정보 이론에서 강력한 이론적 기반을 가지고 있어. 하지만 실제로는 데이터 분포를 직접적으로 사용할 수 없기 때문에 MIM을 SSL에 적용하는 게 어려워.

실제로 많은 기존 방법들은 MIM 기준을 근사적으로 구현한 것으로 볼 수 있어. 이 연구에서는 MI의 불변성 속성을 바탕으로, 일반적인 분포 가정 하에 SSL에 명시적인 MI 최대화를 적용할 수 있음을 보여줘. 즉, 데이터 분포의 완화된 조건을 사용하는 거야.

우리는 이 내용을 일반화된 가우시안 분포를 분석하여 더 자세히 설명해. 이 결과를 바탕으로, 2차 통계량만을 사용해서 MIM 기준에 기반한 손실 함수를 도출했어. 그리고 이 새로운 손실 함수를 SSL에 적용해보고, 여러 실험을 통해 그 효과를 입증했어.

================================================================================

URL: https://arxiv.org/abs/2409.04750
Title: Training-Free Style Consistent Image Synthesis with Condition and Mask Guidance in E-Commerce

Original Abstract:
Generating style-consistent images is a common task in the e-commerce field, and current methods are largely based on diffusion models, which have achieved excellent results. This paper introduces the concept of the QKV (query/key/value) level, referring to modifications in the attention maps (self-attention and cross-attention) when integrating UNet with image conditions. Without disrupting the product's main composition in e-commerce images, we aim to use a train-free method guided by pre-set conditions. This involves using shared KV to enhance similarity in cross-attention and generating mask guidance from the attention map to cleverly direct the generation of style-consistent images. Our method has shown promising results in practical applications.

Translated Abstract:
스타일이 일관된 이미지를 생성하는 건 전자상거래 분야에서 흔한 작업이야. 현재 방법들은 대부분 확산 모델에 기반하고 있는데, 이 방법들이 정말 좋은 결과를 내고 있어. 

이번 논문에서는 QKV(쿼리/키/값) 레벨 개념을 소개해. 이건 UNet과 이미지 조건을 통합할 때 자기 주의(attention) 맵(셀프 어텐션과 크로스 어텐션)을 수정하는 걸 말해. 

전자상거래 이미지에서 제품의 주요 구성을 방해하지 않으면서, 우리는 사전 설정된 조건에 의해 안내되는 트레인 프리(train-free) 방법을 사용하려고 해. 여기에는 공유된 KV를 사용해서 크로스 어텐션에서 유사성을 높이고, 주의 맵에서 마스크 가이드를 생성해서 스타일이 일관된 이미지를 똑똑하게 생성하는 방향으로 안내하는 게 포함돼. 

우리의 방법은 실제 응용에서 유망한 결과를 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.04751
Title: Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for Fisheye Cameras

Original Abstract:
Recently, 3D Gaussian Splatting (3DGS) has garnered attention for its high fidelity and real-time rendering. However, adapting 3DGS to different camera models, particularly fisheye lenses, poses challenges due to the unique 3D to 2D projection calculation. Additionally, there are inefficiencies in the tile-based splatting, especially for the extreme curvature and wide field of view of fisheye lenses, which are crucial for its broader real-life applications. To tackle these challenges, we introduce Fisheye-GS.This innovative method recalculates the projection transformation and its gradients for fisheye cameras. Our approach can be seamlessly integrated as a module into other efficient 3D rendering methods, emphasizing its extensibility, lightweight nature, and modular design. Since we only modified the projection component, it can also be easily adapted for use with different camera models. Compared to methods that train after undistortion, our approach demonstrates a clear improvement in visual quality.

Translated Abstract:
최근 3D 가우시안 스플래팅(3DGS)이 높은 품질과 실시간 렌더링으로 주목받고 있어. 그런데 3DGS를 다양한 카메라 모델에 맞추는 건 쉽지 않은데, 특히 어안 렌즈의 경우 3D를 2D로 변환하는 계산이 독특해서 더 그래. 또, 타일 기반 스플래팅에서는 비효율적이어서 어안 렌즈의 극단적인 곡률과 넓은 시야각을 제대로 처리하기 어려워. 이런 문제를 해결하기 위해 Fisheye-GS라는 새로운 방법을 소개해.

이 방법은 어안 카메라를 위한 변환과 그 기울기를 다시 계산해. 우리의 접근 방식은 다른 효율적인 3D 렌더링 방법에 모듈로 쉽게 통합할 수 있어서 확장성, 가벼운 특성, 그리고 모듈 설계를 강조해. 변환 부분만 수정했기 때문에 다른 카메라 모델에도 쉽게 적용할 수 있어. 왜냐하면 왜곡을 제거한 후에 훈련하는 방법들에 비해, 우리의 접근 방식은 시각적 품질에서 확실한 개선을 보여주거든.

================================================================================

URL: https://arxiv.org/abs/2409.04758
Title: SGSeg: Enabling Text-free Inference in Language-guided Segmentation of Chest X-rays via Self-guidance

Original Abstract:
Segmentation of infected areas in chest X-rays is pivotal for facilitating the accurate delineation of pulmonary structures and pathological anomalies. Recently, multi-modal language-guided image segmentation methods have emerged as a promising solution for chest X-rays where the clinical text reports, depicting the assessment of the images, are used as guidance. Nevertheless, existing language-guided methods require clinical reports alongside the images, and hence, they are not applicable for use in image segmentation in a decision support context, but rather limited to retrospective image analysis after clinical reporting has been completed. In this study, we propose a self-guided segmentation framework (SGSeg) that leverages language guidance for training (multi-modal) while enabling text-free inference (uni-modal), which is the first that enables text-free inference in language-guided segmentation. We exploit the critical location information of both pulmonary and pathological structures depicted in the text reports and introduce a novel localization-enhanced report generation (LERG) module to generate clinical reports for self-guidance. Our LERG integrates an object detector and a location-based attention aggregator, weakly-supervised by a location-aware pseudo-label extraction module. Extensive experiments on a well-benchmarked QaTa-COV19 dataset demonstrate that our SGSeg achieved superior performance than existing uni-modal segmentation methods and closely matched the state-of-the-art performance of multi-modal language-guided segmentation methods.

Translated Abstract:
흉부 X선에서 감염된 영역을 분리하는 것은 폐 구조와 병리학적 이상을 정확하게 구별하는 데 매우 중요해. 최근에는 임상 텍스트 보고서를 이용한 다중 모드 언어 안내 이미지 분할 방법이 등장했어. 이 방법은 이미지 평가를 설명하는 임상 보고서를 가이드로 사용해. 

하지만 기존의 언어 안내 방법은 이미지와 함께 임상 보고서를 필요로 해서, 의사 결정 지원 맥락에서 이미지 분할에 사용하기는 어려워. 오히려 임상 보고가 완료된 후에 이미지를 분석하는 데만 제한적이야. 

이 연구에서는 언어 안내를 활용해 훈련하는 동시에 텍스트 없이 추론할 수 있는 자기 안내 분할 프레임워크(SGSeg)를 제안해. 이건 언어 안내 분할에서 텍스트 없이 추론할 수 있는 첫 번째 방법이야. 우리는 임상 보고서에 나타난 폐와 병리학적 구조의 중요한 위치 정보를 활용하고, 자기 안내를 위해 임상 보고서를 생성하는 새로운 모듈인 위치 강화 보고서 생성기(LERG)를 도입했어. 

우리 LERG는 객체 탐지기와 위치 기반 주의 집합기를 통합하고, 위치 인식 의사 레이블 추출 모듈로 약한 감독을 받아. 잘 알려진 QaTa-COV19 데이터셋에 대한 광범위한 실험 결과, 우리의 SGSeg는 기존의 단일 모드 분할 방법보다 우수한 성능을 보여줬고, 다중 모드 언어 안내 분할 방법의 최첨단 성능과도 밀접하게 일치했어.

================================================================================

URL: https://arxiv.org/abs/2409.04759
Title: Adaptative Context Normalization: A Boost for Deep Learning in Image Processing

Original Abstract:
Deep Neural network learning for image processing faces major challenges related to changes in distribution across layers, which disrupt model convergence and performance. Activation normalization methods, such as Batch Normalization (BN), have revolutionized this field, but they rely on the simplified assumption that data distribution can be modelled by a single Gaussian distribution. To overcome these limitations, Mixture Normalization (MN) introduced an approach based on a Gaussian Mixture Model (GMM), assuming multiple components to model the data. However, this method entails substantial computational requirements associated with the use of Expectation-Maximization algorithm to estimate parameters of each Gaussian components. To address this issue, we introduce Adaptative Context Normalization (ACN), a novel supervised approach that introduces the concept of "context", which groups together a set of data with similar characteristics. Data belonging to the same context are normalized using the same parameters, enabling local representation based on contexts. For each context, the normalized parameters, as the model weights are learned during the backpropagation phase. ACN not only ensures speed, convergence, and superior performance compared to BN and MN but also presents a fresh perspective that underscores its particular efficacy in the field of image processing.

Translated Abstract:
딥 뉴럴 네트워크를 이미지 처리에 활용할 때, 레이어 간 데이터 분포 변화 때문에 모델이 수렴하지 않거나 성능이 떨어지는 문제가 있어. 배치 정규화(BN) 같은 활성화 정규화 방법들이 이 문제를 해결하는 데 큰 도움을 줬지만, 이들은 데이터 분포를 단일 가우시안 분포로 단순화해서 가정해. 

이런 한계를 극복하기 위해서 혼합 정규화(MN)가 도입됐는데, 이 방법은 가우시안 혼합 모델(GMM)을 기반으로 여러 요소를 가정해서 데이터를 모델링해. 하지만 이 방법은 각 가우시안 요소의 파라미터를 추정하는 데 기대 최대화 알고리즘을 사용해야 해서 계산량이 많아져. 

이 문제를 해결하기 위해 우리는 적응형 컨텍스트 정규화(ACN)를 소개해. 이 새로운 방법은 "컨텍스트"라는 개념을 도입해서 비슷한 특성을 가진 데이터들을 그룹으로 묶어. 같은 컨텍스트에 속하는 데이터는 동일한 파라미터로 정규화돼, 그래서 컨텍스트에 기반한 로컬 표현이 가능해. 각 컨텍스트에 대해 정규화된 파라미터는 모델 가중치로, 역전파 단계에서 학습돼. 

ACN은 속도와 수렴성을 보장할 뿐만 아니라 BN이나 MN보다 뛰어난 성능을 보여줘. 게다가 이미지 처리 분야에서 특히 효과적이라는 새로운 관점을 제공해.

================================================================================

URL: https://arxiv.org/abs/2409.04760
Title: Training-Free Point Cloud Recognition Based on Geometric and Semantic Information Fusion

Original Abstract:
The trend of employing training-free methods for point cloud recognition is becoming increasingly popular due to its significant reduction in computational resources and time costs. However, existing approaches are limited as they typically extract either geometric or semantic features. To address this limitation, we propose a novel method that integrates both geometric and semantic features, thereby enhancing the comprehensive understanding of point clouds. For the geometric branch, we adopt a non-parametric strategy to extract geometric features. In the semantic branch, we leverage a model pre-trained through contrastive learning and aligned with text features to obtain semantic features. Experimental results demonstrate that our method outperforms existing state-of-the-art training-free approaches on several popular benchmark datasets, including ModelNet and ScanObiectNN.

Translated Abstract:
점 구름 인식을 위해 훈련이 필요 없는 방법을 사용하는 것이 점점 더 인기를 끌고 있어. 이 방법은 계산 자원과 시간 비용을 크게 줄여주거든. 하지만 기존의 방법들은 대개 기하학적 특성이나 의미적 특성 중 하나만 추출하는 한계가 있어.

그래서 우리는 기하학적 특성과 의미적 특성을 모두 통합하는 새로운 방법을 제안해. 이 방법은 점 구름을 더 잘 이해할 수 있게 도와줘. 기하학적 부분에서는 비모수적 전략을 사용해 기하학적 특성을 추출하고, 의미적 부분에서는 대비 학습을 통해 미리 훈련된 모델을 활용해서 의미적 특성을 얻어.

실험 결과, 우리의 방법이 ModelNet과 ScanObjectNN 같은 여러 인기 있는 벤치마크 데이터셋에서 기존의 최첨단 훈련 없는 접근법보다 더 뛰어난 성능을 보였어.

================================================================================

URL: https://arxiv.org/abs/2409.04766
Title: Cross-Dataset Gaze Estimation by Evidential Inter-intra Fusion

Original Abstract:
Achieving accurate and reliable gaze predictions in complex and diverse environments remains challenging. Fortunately, it is straightforward to access diverse gaze datasets in real-world applications. We discover that training these datasets jointly can significantly improve the generalization of gaze estimation, which is overlooked in previous works. However, due to the inherent distribution shift across different datasets, simply mixing multiple dataset decreases the performance in the original domain despite gaining better generalization abilities. To address the problem of ``cross-dataset gaze estimation'', we propose a novel Evidential Inter-intra Fusion EIF framework, for training a cross-dataset model that performs well across all source and unseen domains. Specifically, we build independent single-dataset branches for various datasets where the data space is partitioned into overlapping subspaces within each dataset for local regression, and further create a cross-dataset branch to integrate the generalizable features from single-dataset branches. Furthermore, evidential regressors based on the Normal and Inverse-Gamma (NIG) distribution are designed to additionally provide uncertainty estimation apart from predicting gaze. Building upon this foundation, our proposed framework achieves both intra-evidential fusion among multiple local regressors within each dataset and inter-evidential fusion among multiple branches by Mixture \textbfof Normal Inverse-Gamma (MoNIG distribution. Experiments demonstrate that our method consistently achieves notable improvements in both source domains and unseen domains.

Translated Abstract:
복잡하고 다양한 환경에서 정확하고 신뢰할 수 있는 시선 예측을 하는 게 여전히 어려워. 하지만, 실제 응용 프로그램에서는 다양한 시선 데이터셋에 쉽게 접근할 수 있어. 우리가 발견한 건, 이런 데이터셋을 함께 훈련시키면 시선 추정의 일반화가 크게 개선된다는 거야. 이전 연구에서는 이 점을 간과했어. 

그런데, 서로 다른 데이터셋 간의 분포 차이 때문에 여러 데이터셋을 단순히 섞으면 원래 도메인에서 성능이 떨어지는 문제가 생겨. 그래서 '교차 데이터셋 시선 추정' 문제를 해결하기 위해, 우리는 새로운 Evidential Inter-intra Fusion (EIF) 프레임워크를 제안해. 이 프레임워크는 모든 소스와 보지 못한 도메인에서 잘 작동하는 교차 데이터셋 모델을 훈련시키는 데 사용돼.

구체적으로 말하면, 우리는 다양한 데이터셋을 위해 독립적인 단일 데이터셋 브랜치를 만들고, 각 데이터셋 내에서 데이터 공간을 겹치는 서브스페이스로 나누어서 로컬 회귀를 해. 그리고는 단일 데이터셋 브랜치에서 일반화 가능한 특징을 통합하기 위해 교차 데이터셋 브랜치를 추가로 만들어. 더 나아가, 시선을 예측하는 것 외에 불확실성을 추정하기 위해 Normal과 Inverse-Gamma (NIG) 분포를 기반으로 한 증거 회귀기를 설계했어.

이 기반 위에서, 우리의 제안된 프레임워크는 각 데이터셋 내 여러 로컬 회귀기 간의 내부 증거 융합과 여러 브랜치 간의 외부 증거 융합을 Mixture of Normal Inverse-Gamma (MoNIG) 분포를 통해 수행해. 실험 결과, 우리의 방법이 소스 도메인과 보지 못한 도메인 모두에서 꾸준히 눈에 띄는 개선을 이룬다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.04768
Title: Medical Image Segmentation via Single-Source Domain Generalization with Random Amplitude Spectrum Synthesis

Original Abstract:
The field of medical image segmentation is challenged by domain generalization (DG) due to domain shifts in clinical datasets. The DG challenge is exacerbated by the scarcity of medical data and privacy concerns. Traditional single-source domain generalization (SSDG) methods primarily rely on stacking data augmentation techniques to minimize domain discrepancies. In this paper, we propose Random Amplitude Spectrum Synthesis (RASS) as a training augmentation for medical images. RASS enhances model generalization by simulating distribution changes from a frequency perspective. This strategy introduces variability by applying amplitude-dependent perturbations to ensure broad coverage of potential domain variations. Furthermore, we propose random mask shuffle and reconstruction components, which can enhance the ability of the backbone to process structural information and increase resilience intra- and cross-domain changes. The proposed Random Amplitude Spectrum Synthesis for Single-Source Domain Generalization (RAS^4DG) is validated on 3D fetal brain images and 2D fundus photography, and achieves an improved DG segmentation performance compared to other SSDG models.

Translated Abstract:
의료 이미지 분할 분야는 임상 데이터셋에서의 도메인 변화 때문에 도메인 일반화(DG)라는 문제에 직면해 있어. 의료 데이터는 부족하고 개인 정보 보호 문제도 있어서 DG 도전이 더 심각해져. 전통적인 단일 출처 도메인 일반화(SSDG) 방법은 주로 데이터 증강 기법을 쌓아서 도메인 차이를 최소화하는 데 의존해.

이 논문에서는 의료 이미지를 위한 훈련 증강 기법으로 랜덤 진폭 스펙트럼 합성(Random Amplitude Spectrum Synthesis, RASS)을 제안해. RASS는 주파수 관점에서 분포 변화를 시뮬레이션함으로써 모델의 일반화를 향상시켜. 이 전략은 진폭에 의존하는 변동을 적용해 다양한 도메인 변화를 폭넓게 커버할 수 있도록 해.

또한, 랜덤 마스크 셔플과 재구성 요소를 제안하는데, 이건 백본 모델이 구조적 정보를 처리하는 능력을 향상시키고 도메인 내외의 변화에 더 잘 견딜 수 있도록 해. 제안된 단일 출처 도메인 일반화를 위한 랜덤 진폭 스펙트럼 합성(RAS^4DG)은 3D 태아 뇌 이미지와 2D 망막 사진에서 검증되었고, 다른 SSDG 모델에 비해 DG 분할 성능이 개선되었어.

================================================================================

URL: https://arxiv.org/abs/2409.04796
Title: Enhancing Outlier Knowledge for Few-Shot Out-of-Distribution Detection with Extensible Local Prompts

Original Abstract:
Out-of-Distribution (OOD) detection, aiming to distinguish outliers from known categories, has gained prominence in practical scenarios. Recently, the advent of vision-language models (VLM) has heightened interest in enhancing OOD detection for VLM through few-shot tuning. However, existing methods mainly focus on optimizing global prompts, ignoring refined utilization of local information with regard to outliers. Motivated by this, we freeze global prompts and introduce a novel coarse-to-fine tuning paradigm to emphasize regional enhancement with local prompts. Our method comprises two integral components: global prompt guided negative augmentation and local prompt enhanced regional regularization. The former utilizes frozen, coarse global prompts as guiding cues to incorporate negative augmentation, thereby leveraging local outlier knowledge. The latter employs trainable local prompts and a regional regularization to capture local information effectively, aiding in outlier identification. We also propose regional-related metric to empower the enrichment of OOD detection. Moreover, since our approach explores enhancing local prompts only, it can be seamlessly integrated with trained global prompts during inference to boost the performance. Comprehensive experiments demonstrate the effectiveness and potential of our method. Notably, our method reduces average FPR95 by 5.17% against state-of-the-art method in 4-shot tuning on challenging ImageNet-1k dataset, even outperforming 16-shot results of previous methods.

Translated Abstract:
Out-of-Distribution (OOD) 탐지는 알려진 범주에서 벗어난 데이터를 구분하는 걸 목표로 하고 있어. 요즘에는 비전-언어 모델(VLM)의 발전 덕분에 OOD 탐지를 개선하려는 관심이 높아졌어. 특히, 몇 번의 샷으로 튜닝하는 방식이 주목받고 있지. 그런데 기존 방법들은 주로 전반적인 프롬프트를 최적화하는 데 집중하고, 이상치에 대한 지역 정보 활용은 소홀히 하고 있어. 

이런 문제를 해결하기 위해, 우리는 전반적인 프롬프트는 고정하고 새로운 거친-세밀 튜닝 방식을 도입했어. 이 방식은 지역 강화를 위해 지역 프롬프트에 집중해. 우리 방법은 두 가지 주요 요소로 구성되어 있어: 글로벌 프롬프트에 의해 안내되는 부정적 증강과 지역 프롬프트를 활용한 지역 정규화. 첫 번째는 고정된 거친 글로벌 프롬프트를 이용해 부정적 증강을 도입하고, 이를 통해 지역적인 이상치 정보를 활용하는 거야. 두 번째는 학습 가능한 지역 프롬프트와 지역 정규화를 사용해서 지역 정보를 효과적으로 잡아내고, 이상치 식별을 돕는 거지.

또한, OOD 탐지를 강화하기 위한 지역 관련 메트릭도 제안했어. 우리 접근 방식은 지역 프롬프트만 강화하기 때문에, 추론할 때 훈련된 글로벌 프롬프트와 쉽게 결합할 수 있어 성능을 높일 수 있어. 다양한 실험 결과에서 우리 방법의 효과성과 가능성을 입증했어. 특히, 우리 방법은 도전적인 ImageNet-1k 데이터셋에서 4샷 튜닝 시 최신 방법에 비해 평균 FPR95를 5.17% 줄였고, 이전 방법들의 16샷 결과보다도 더 나은 성능을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.04801
Title: SpotActor: Training-Free Layout-Controlled Consistent Image Generation

Original Abstract:
Text-to-image diffusion models significantly enhance the efficiency of artistic creation with high-fidelity image generation. However, in typical application scenarios like comic book production, they can neither place each subject into its expected spot nor maintain the consistent appearance of each subject across images. For these issues, we pioneer a novel task, Layout-to-Consistent-Image (L2CI) generation, which produces consistent and compositional images in accordance with the given layout conditions and text prompts. To accomplish this challenging task, we present a new formalization of dual energy guidance with optimization in a dual semantic-latent space and thus propose a training-free pipeline, SpotActor, which features a layout-conditioned backward update stage and a consistent forward sampling stage. In the backward stage, we innovate a nuanced layout energy function to mimic the attention activations with a sigmoid-like objective. While in the forward stage, we design Regional Interconnection Self-Attention (RISA) and Semantic Fusion Cross-Attention (SFCA) mechanisms that allow mutual interactions across images. To evaluate the performance, we present ActorBench, a specified benchmark with hundreds of reasonable prompt-box pairs stemming from object detection datasets. Comprehensive experiments are conducted to demonstrate the effectiveness of our method. The results prove that SpotActor fulfills the expectations of this task and showcases the potential for practical applications with superior layout alignment, subject consistency, prompt conformity and background diversity.

Translated Abstract:
텍스트-이미지 확산 모델은 고품질 이미지 생성을 통해 예술 창작의 효율성을 크게 향상시킵니다. 하지만 만화책 제작 같은 일반적인 상황에서는 각 주제를 예상한 위치에 배치하거나 이미지 간에 일관된 모습을 유지하는 데 어려움이 있습니다. 이 문제를 해결하기 위해 우리는 새로운 작업인 Layout-to-Consistent-Image (L2CI) 생성을 제안합니다. 이 작업은 주어진 레이아웃 조건과 텍스트 프롬프트에 따라 일관되고 구성적인 이미지를 생성합니다.

이 도전적인 작업을 수행하기 위해, 우리는 이중 에너지 가이드를 새로운 방식으로 정식화하고 이중 의미-잠재 공간에서 최적화를 진행합니다. 그리고 교육이 필요 없는 파이프라인인 SpotActor를 제안하는데, 여기에는 레이아웃 조건에 맞춘 역 업데이트 단계와 일관된 전방 샘플링 단계가 포함됩니다. 역 단계에서는 시그모이드 같은 목표를 사용하여 주목 활성화를 모방하는 미세한 레이아웃 에너지 함수를 혁신적으로 개발했습니다.

전방 단계에서는 이미지 간의 상호 작용을 가능하게 하는 지역 상호 연결 자기 주의(RISA)와 의미 융합 교차 주의(SFCA) 메커니즘을 설계했습니다. 성능 평가를 위해, 우리는 객체 탐지 데이터셋에서 나온 수백 개의 합리적인 프롬프트-박스 쌍으로 구성된 특정 벤치마크 ActorBench를 제시합니다. 다양한 실험을 통해 우리의 방법의 효과를 입증했습니다.

결과는 SpotActor가 이 작업의 기대를 충족시키고, 우수한 레이아웃 정렬, 주제 일관성, 프롬프트 적합성, 배경 다양성을 통해 실제 응용 가능성을 보여준다는 것을 증명합니다.

================================================================================

URL: https://arxiv.org/abs/2409.04812
Title: Power Line Aerial Image Restoration under dverse Weather: Datasets and Baselines

Original Abstract:
Power Line Autonomous Inspection (PLAI) plays a crucial role in the construction of smart grids due to its great advantages of low cost, high efficiency, and safe operation. PLAI is completed by accurately detecting the electrical components and defects in the aerial images captured by Unmanned Aerial Vehicles (UAVs). However, the visible quality of aerial images is inevitably degraded by adverse weather like haze, rain, or snow, which are found to drastically decrease the detection accuracy in our research. To circumvent this problem, we propose a new task of Power Line Aerial Image Restoration under Adverse Weather (PLAIR-AW), which aims to recover clean and high-quality images from degraded images with bad weather thus improving detection performance for PLAI. In this context, we are the first to release numerous corresponding datasets, namely, HazeCPLID, HazeTTPLA, HazeInsPLAD for power line aerial image dehazing, RainCPLID, RainTTPLA, RainInsPLAD for power line aerial image deraining, SnowCPLID, SnowInsPLAD for power line aerial image desnowing, which are synthesized upon the public power line aerial image datasets of CPLID, TTPLA, InsPLAD following the mathematical models. Meanwhile, we select numerous state-of-the-art methods from image restoration community as the baseline methods for PLAIR-AW. At last, we conduct large-scale empirical experiments to evaluate the performance of baseline methods on the proposed datasets. The proposed datasets and trained models are available at this https URL.

Translated Abstract:
전력선 자율 점검(PLAI)은 스마트 그리드 구축에 중요한 역할을 해. 비용이 적고, 효율이 높으며, 안전하게 운영할 수 있는 장점이 있거든. PLAI는 UAV(무인 항공기)가 촬영한 공중 이미지에서 전기 구성 요소와 결함을 정확하게 감지함으로써 이루어져. 하지만, 안개, 비, 눈 같은 나쁜 날씨 때문에 공중 이미지의 가시성이 떨어지게 되고, 이로 인해 탐지 정확도가 급격히 감소하는 문제가 있어.

이 문제를 해결하기 위해, 우리는 악천후에서 전력선 공중 이미지 복원(PLAIR-AW)이라는 새로운 작업을 제안해. 이 작업은 나쁜 날씨에서 손상된 이미지를 깨끗하고 고품질의 이미지로 복원하려고 하는 거야. 이렇게 하면 PLAI의 탐지 성능이 향상될 수 있어. 

우리는 전력선 공중 이미지 디헤이징, 디레인링, 디스노잉을 위한 HazeCPLID, HazeTTPLA, HazeInsPLAD, RainCPLID, RainTTPLA, RainInsPLAD, SnowCPLID, SnowInsPLAD 같은 여러 데이터셋을 처음으로 공개했어. 이 데이터셋들은 CPLID, TTPLA, InsPLAD라는 공개 전력선 공중 이미지 데이터셋을 바탕으로 수학적 모델을 통해 생성된 거야.

또한, 우리는 PLAIR-AW의 기준 방법으로 이미지 복원 분야의 최신 기법들을 선정했어. 마지막으로, 제안된 데이터셋에서 기준 방법들의 성능을 평가하기 위해 대규모 실험을 진행했어. 제안된 데이터셋과 훈련된 모델은 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04817
Title: SSFam: Scribble Supervised Salient Object Detection Family

Original Abstract:
Scribble supervised salient object detection (SSSOD) constructs segmentation ability of attractive objects from surroundings under the supervision of sparse scribble labels. For the better segmentation, depth and thermal infrared modalities serve as the supplement to RGB images in the complex scenes. Existing methods specifically design various feature extraction and multi-modal fusion strategies for RGB, RGB-Depth, RGB-Thermal, and Visual-Depth-Thermal image input respectively, leading to similar model flood. As the recently proposed Segment Anything Model (SAM) possesses extraordinary segmentation and prompt interactive capability, we propose an SSSOD family based on SAM, named SSFam, for the combination input with different modalities. Firstly, different modal-aware modulators are designed to attain modal-specific knowledge which cooperates with modal-agnostic information extracted from the frozen SAM encoder for the better feature ensemble. Secondly, a siamese decoder is tailored to bridge the gap between the training with scribble prompt and the testing with no prompt for the stronger decoding ability. Our model demonstrates the remarkable performance among combinations of different modalities and refreshes the highest level of scribble supervised methods and comes close to the ones of fully supervised methods. this https URL

Translated Abstract:
Scribble supervised salient object detection (SSSOD)는 주변에서 매력적인 객체를 구분할 수 있도록 희소한 스크리블 레이블의 도움을 받아 분할 능력을 개발하는 방법이야. 더 나은 분할을 위해 깊이(depth)와 열 적외선(thermal infrared) 모드가 복잡한 장면에서 RGB 이미지의 보조 역할을 해. 기존 방법들은 RGB, RGB-Depth, RGB-Thermal, Visual-Depth-Thermal 이미지 입력에 대해 다양한 특징 추출과 다중 모드 융합 전략을 특별히 설계했는데, 이로 인해 비슷한 모델이 많이 생겼어.

최근에 제안된 Segment Anything Model (SAM)은 뛰어난 분할 능력과 즉각적인 인터랙션 기능을 가지고 있어서, 우리는 SAM을 기반으로 한 SSSOD 계열인 SSFam을 제안해. 이 모델은 서로 다른 모드를 조합해서 입력받아. 먼저, 각 모드에 맞춘 모듈러를 설계해서 모드 특화 지식을 얻고, 이 정보는 고정된 SAM 인코더에서 추출된 모드 무관한 정보와 협력해서 더 나은 특징 조합을 만들어. 

두 번째로, 시암 디코더를 맞춤 설계해서 스크리블 프롬프트로 훈련하는 것과 프롬프트 없이 테스트하는 것 사이의 간극을 메워서 더 강력한 디코딩 능력을 보여줘. 우리의 모델은 다양한 모드 조합에서 뛰어난 성능을 보이고, 스크리블 감독 방법 중 최고 수준을 갱신하며, 완전 감독 방법의 성능에도 근접해.

================================================================================

URL: https://arxiv.org/abs/2409.04819
Title: Top-GAP: Integrating Size Priors in CNNs for more Interpretability, Robustness, and Bias Mitigation

Original Abstract:
This paper introduces Top-GAP, a novel regularization technique that enhances the explainability and robustness of convolutional neural networks. By constraining the spatial size of the learned feature representation, our method forces the network to focus on the most salient image regions, effectively reducing background influence. Using adversarial attacks and the Effective Receptive Field, we show that Top-GAP directs more attention towards object pixels rather than the background. This leads to enhanced interpretability and robustness. We achieve over 50% robust accuracy on CIFAR-10 with PGD $\epsilon=\frac{8}{255}$ and $20$ iterations while maintaining the original clean accuracy. Furthermore, we see increases of up to 5% accuracy against distribution shifts. Our approach also yields more precise object localization, as evidenced by up to 25% improvement in Intersection over Union (IOU) compared to methods like GradCAM and Recipro-CAM.

Translated Abstract:
이 논문에서는 Top-GAP이라는 새로운 정규화 기법을 소개해. 이 기법은 합성곱 신경망의 설명 가능성과 강건성을 높여줘. 우리가 배운 특징 표현의 공간 크기를 제한함으로써, 네트워크가 가장 중요한 이미지 영역에 집중하게 만들어 배경의 영향을 줄이는 거야.

적대적 공격과 효과적인 수용 영역(Effective Receptive Field)을 사용해서, Top-GAP이 배경보다 물체 픽셀에 더 많은 주의를 기울이게 만든다는 걸 보여줘. 이렇게 하면 해석 가능성과 강건성이 향상돼. 우리는 PGD에서 $\epsilon=\frac{8}{255}$, 20회 반복을 사용했을 때 CIFAR-10에서 50% 이상의 강건한 정확도를 달성했어. 원래의 깨끗한 정확도도 유지하면서 말이지. 

또한, 분포 변화에 대해서도 최대 5%의 정확도 향상을 확인했어. 우리의 접근 방식은 물체 위치를 더 정확하게 찾을 수 있게 해주고, GradCAM이나 Recipro-CAM 같은 방법에 비해 Intersection over Union (IOU)가 최대 25% 개선됐다는 결과도 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04820
Title: FreeAugment: Data Augmentation Search Across All Degrees of Freedom

Original Abstract:
Data augmentation has become an integral part of deep learning, as it is known to improve the generalization capabilities of neural networks. Since the most effective set of image transformations differs between tasks and domains, automatic data augmentation search aims to alleviate the extreme burden of manually finding the optimal image transformations. However, current methods are not able to jointly optimize all degrees of freedom: (1) the number of transformations to be applied, their (2) types, (3) order, and (4) magnitudes. Many existing methods risk picking the same transformation more than once, limit the search to two transformations only, or search for the number of transformations exhaustively or iteratively in a myopic manner. Our approach, FreeAugment, is the first to achieve global optimization of all four degrees of freedom simultaneously, using a fully differentiable method. It efficiently learns the number of transformations and a probability distribution over their permutations, inherently refraining from redundant repetition while sampling. Our experiments demonstrate that this joint learning of all degrees of freedom significantly improves performance, achieving state-of-the-art results on various natural image benchmarks and beyond across other domains. Project page at this https URL

Translated Abstract:
데이터 증강은 딥러닝에서 필수적인 부분이 되었어. 이 방법은 신경망의 일반화 능력을 향상시키는 데 도움이 되거든. 하지만 작업이나 분야마다 가장 효과적인 이미지 변환 세트가 다르기 때문에, 자동 데이터 증강 검색은 최적의 이미지 변환을 수동으로 찾는 부담을 덜어주는 걸 목표로 해.

현재의 방법들은 모든 자유도를 동시에 최적화할 수 없는데, 여기서 자유도란 (1) 적용할 변환의 수, (2) 변환의 종류, (3) 순서, (4) 크기를 말해. 기존의 많은 방법들은 같은 변환을 여러 번 선택할 위험이 있거나, 두 개의 변환만 탐색하거나, 변환의 수를 비효율적으로 검색하는 경우가 많아.

우리의 접근법인 FreeAugment는 이 네 가지 자유도를 동시에 전역 최적화하는 첫 번째 방법이야. 완전히 미분 가능한 방법을 사용해서 효과적으로 변환의 수와 그 순열에 대한 확률 분포를 학습해. 이 과정에서 중복 선택을 자연스럽게 피하면서 샘플링을 해. 실험 결과, 모든 자유도를 함께 학습하는 것이 성능을 크게 향상시키는 걸 보여줬고, 다양한 자연 이미지 벤치마크와 다른 분야에서 최첨단 결과를 달성했어. 프로젝트 페이지는 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04825
Title: Metadata augmented deep neural networks for wild animal classification

Original Abstract:
Camera trap imagery has become an invaluable asset in contemporary wildlife surveillance, enabling researchers to observe and investigate the behaviors of wild animals. While existing methods rely solely on image data for classification, this may not suffice in cases of suboptimal animal angles, lighting, or image quality. This study introduces a novel approach that enhances wild animal classification by combining specific metadata (temperature, location, time, etc) with image data. Using a dataset focused on the Norwegian climate, our models show an accuracy increase from 98.4% to 98.9% compared to existing methods. Notably, our approach also achieves high accuracy with metadata-only classification, highlighting its potential to reduce reliance on image quality. This work paves the way for integrated systems that advance wildlife classification technology.

Translated Abstract:
카메라 트랩 이미지가 요즘 야생 동물 감시에 정말 중요한 자원이 되고 있어. 연구자들이 야생 동물의 행동을 관찰하고 조사하는 데 큰 도움이 돼. 

기존 방법들은 이미지만 가지고 동물 분류를 하는데, 이게 동물 각도나 조명, 이미지 품질이 좋지 않을 때는 잘 안 될 수 있어. 그래서 이 연구에서는 특정 메타데이터(온도, 위치, 시간 등)를 이미지 데이터와 결합해서 야생 동물 분류를 향상시키는 새로운 방법을 소개해.

노르웨이 기후에 초점을 맞춘 데이터셋을 사용해서 우리의 모델은 기존 방법에 비해 정확도가 98.4%에서 98.9%로 증가했어. 특히 메타데이터만 가지고도 분류를 잘 할 수 있어서 이미지 품질에 대한 의존도를 줄일 수 있는 가능성을 보여줘. 

이 연구는 야생 동물 분류 기술을 발전시키는 통합 시스템으로 나아가는 길을 열어줘.

================================================================================

URL: https://arxiv.org/abs/2409.04828
Title: POINTS: Improving Your Vision-language Model with Affordable Strategies

Original Abstract:
In recent years, vision-language models have made significant strides, excelling in tasks like optical character recognition and geometric problem-solving. However, several critical issues remain: 1) Proprietary models often lack transparency about their architectures, while open-source models need more detailed ablations of their training strategies. 2) Pre-training data in open-source works is under-explored, with datasets added empirically, making the process cumbersome. 3) Fine-tuning often focuses on adding datasets, leading to diminishing returns. To address these issues, we propose the following contributions: 1) We trained a robust baseline model using the latest advancements in vision-language models, introducing effective improvements and conducting comprehensive ablation and validation for each technique. 2) Inspired by recent work on large language models, we filtered pre-training data using perplexity, selecting the lowest perplexity data for training. This approach allowed us to train on a curated 1M dataset, achieving competitive performance. 3) During visual instruction tuning, we used model soup on different datasets when adding more datasets yielded marginal improvements. These innovations resulted in a 9B parameter model that performs competitively with state-of-the-art models. Our strategies are efficient and lightweight, making them easily adoptable by the community.

Translated Abstract:
최근 몇 년 동안 비전-언어 모델이 큰 발전을 이루면서 광학 문자 인식이나 기하학 문제 해결 같은 작업에서 성과를 내고 있어. 하지만 몇 가지 중요한 문제가 여전히 남아있어:

1) 상용 모델은 구조에 대한 투명성이 부족하고, 오픈 소스 모델은 훈련 전략에 대한 더 상세한 분석이 필요해.
2) 오픈 소스 작업에서 사전 훈련 데이터가 충분히 탐구되지 않았고, 데이터셋이 경험적으로 추가되어서 프로세스가 번거로워.
3) 파인튜닝은 주로 데이터셋을 추가하는 데 집중해서 수익이 줄어드는 경향이 있어.

이런 문제를 해결하기 위해 우리는 다음과 같은 기여를 제안해:

1) 최신 비전-언어 모델의 발전을 활용해 강력한 기준 모델을 훈련했어. 각 기술에 대해 효과적인 개선을 도입하고, 포괄적인 분석과 검증을 진행했어.
2) 최근 대형 언어 모델에 대한 연구에서 영감을 받아, 혼란도(perplexity)를 사용해 사전 훈련 데이터를 필터링했어. 가장 낮은 혼란도를 가진 데이터로 훈련을 진행해서, 100만 개의 데이터셋으로 훈련하게 되었고, 경쟁력 있는 성능을 달성했어.
3) 비주얼 지침 튜닝에서는 데이터셋을 추가할 때 미미한 개선을 보일 경우 여러 데이터셋에서 모델 수프(model soup)를 사용했어.

이런 혁신 덕분에 90억 개의 매개변수를 가진 모델이 최첨단 모델들과 경쟁할 수 있는 성능을 갖게 되었어. 우리의 전략은 효율적이고 가벼워서 커뮤니티에서 쉽게 채택할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04847
Title: Rethinking The Training And Evaluation of Rich-Context Layout-to-Image Generation

Original Abstract:
Recent advancements in generative models have significantly enhanced their capacity for image generation, enabling a wide range of applications such as image editing, completion and video editing. A specialized area within generative modeling is layout-to-image (L2I) generation, where predefined layouts of objects guide the generative process. In this study, we introduce a novel regional cross-attention module tailored to enrich layout-to-image generation. This module notably improves the representation of layout regions, particularly in scenarios where existing methods struggle with highly complex and detailed textual descriptions. Moreover, while current open-vocabulary L2I methods are trained in an open-set setting, their evaluations often occur in closed-set environments. To bridge this gap, we propose two metrics to assess L2I performance in open-vocabulary scenarios. Additionally, we conduct a comprehensive user study to validate the consistency of these metrics with human preferences.

Translated Abstract:
최근 생성 모델의 발전 덕분에 이미지 생성 능력이 크게 향상되었고, 이로 인해 이미지 편집, 완성, 비디오 편집 같은 다양한 응용이 가능해졌어. 생성 모델의 한 분야인 레이아웃-투-이미지(L2I) 생성에서는 미리 정해진 객체의 레이아웃이 생성 과정을 안내해.

이번 연구에서는 레이아웃-투-이미지 생성을 더 풍부하게 해줄 새로운 지역 크로스 주의 모듈을 소개해. 이 모듈은 특히 기존 방법들이 복잡하고 상세한 텍스트 설명에 어려움을 겪는 상황에서 레이아웃 지역의 표현을 크게 개선해.

게다가 현재의 오픈-보캐불러리 L2I 방법들은 오픈 세트 환경에서 훈련되지만, 평가할 때는 종종 클로즈드 세트 환경에서 이루어져. 이 간극을 메우기 위해, 우리는 오픈-보캐불러리 상황에서 L2I 성능을 평가할 두 가지 지표를 제안해. 추가로, 이 지표들이 사람들의 선호와 얼마나 일치하는지를 확인하기 위해 종합적인 사용자 연구도 진행했어.

================================================================================

URL: https://arxiv.org/abs/2409.04850
Title: Deep Computer Vision for Solar Physics Big Data: Opportunities and Challenges

Original Abstract:
With recent missions such as advanced space-based observatories like the Solar Dynamics Observatory (SDO) and Parker Solar Probe, and ground-based telescopes like the Daniel K. Inouye Solar Telescope (DKIST), the volume, velocity, and variety of data have made solar physics enter a transformative era as solar physics big data (SPBD). With the recent advancement of deep computer vision, there are new opportunities in SPBD for tackling problems that were previously unsolvable. However, there are new challenges arising due to the inherent characteristics of SPBD and deep computer vision models. This vision paper presents an overview of the different types of SPBD, explores new opportunities in applying deep computer vision to SPBD, highlights the unique challenges, and outlines several potential future research directions.

Translated Abstract:
최근에 태양 동역학 관측소(SDO), 파커 태양 탐사선, 그리고 다니엘 K. 이노우에 태양 망원경(DKIST) 같은 고급 우주 관측 미션 덕분에 태양 물리학이 '태양 물리학 빅데이터(SPBD)'라는 새로운 시대에 접어들었어. 이제 데이터의 양, 속도, 다양성이 엄청나게 증가했지.

딥 컴퓨터 비전 기술이 발전하면서, SPBD에서 이전에는 해결할 수 없었던 문제들을 다룰 기회가 생겼어. 하지만 SPBD와 딥 컴퓨터 비전 모델의 고유한 특성 때문에 새로운 도전과제가 생기기도 해.

이 논문에서는 다양한 SPBD의 유형을 소개하고, 딥 컴퓨터 비전을 SPBD에 적용하는 새로운 기회를 탐구해. 또한, 독특한 도전과제를 강조하고, 향후 연구 방향에 대한 몇 가지 가능성을 제시할 거야.

================================================================================

URL: https://arxiv.org/abs/2409.04851
Title: AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction

Original Abstract:
Recent advancements in sensor technology and deep learning have led to significant progress in 3D human body reconstruction. However, most existing approaches rely on data from a specific sensor, which can be unreliable due to the inherent limitations of individual sensing modalities. On the other hand, existing multi-modal fusion methods generally require customized designs based on the specific sensor combinations or setups, which limits the flexibility and generality of these methods. Furthermore, conventional point-image projection-based and Transformer-based fusion networks are susceptible to the influence of noisy modalities and sensor poses. To address these limitations and achieve robust 3D human body reconstruction in various conditions, we propose AdaptiveFusion, a generic adaptive multi-modal multi-view fusion framework that can effectively incorporate arbitrary combinations of uncalibrated sensor inputs. By treating different modalities from various viewpoints as equal tokens, and our handcrafted modality sampling module by leveraging the inherent flexibility of Transformer models, AdaptiveFusion is able to cope with arbitrary numbers of inputs and accommodate noisy modalities with only a single training network. Extensive experiments on large-scale human datasets demonstrate the effectiveness of AdaptiveFusion in achieving high-quality 3D human body reconstruction in various environments. In addition, our method achieves superior accuracy compared to state-of-the-art fusion methods.

Translated Abstract:
최근 센서 기술과 딥러닝의 발전 덕분에 3D 인간 신체 재구성이 크게 발전했어. 하지만 대부분의 기존 방법은 특정 센서의 데이터에 의존하는데, 이 센서는 각 센서의 한계 때문에 신뢰성이 떨어질 수 있어. 반면, 기존의 다중 모달 융합 방법은 특정 센서 조합이나 설정에 따라 맞춤형 설계를 필요로 해서 유연성과 일반성이 떨어져.

또한 전통적인 포인트-이미지 프로젝션 기반과 트랜스포머 기반 융합 네트워크는 노이즈가 많은 모달리티나 센서 자세의 영향을 쉽게 받게 돼. 이런 한계를 극복하고 다양한 조건에서 견고한 3D 인간 신체 재구성을 달성하기 위해, 우리는 AdaptiveFusion을 제안해. 이건 일반적인 적응형 다중 모달 다중 뷰 융합 프레임워크로, 교정되지 않은 센서 입력의 임의 조합을 효과적으로 포함할 수 있어.

AdaptiveFusion은 다양한 관점에서 온 다른 모달리티를 동등한 토큰으로 취급하고, 트랜스포머 모델의 유연성을 활용한 우리의 맞춤형 모달리티 샘플링 모듈 덕분에, 다양한 입력 수를 처리하고 노이즈가 많은 모달리티를 단일 훈련 네트워크로도 수용할 수 있어. 대규모 인간 데이터셋에 대한 광범위한 실험 결과, AdaptiveFusion이 다양한 환경에서 고품질 3D 인간 신체 재구성을 달성하는 데 효과적임을 보여줘. 게다가, 우리의 방법은 최신 융합 방법들에 비해 뛰어난 정확도를 자랑해.

================================================================================

URL: https://arxiv.org/abs/2409.04867
Title: Contrastive Disentangling: Fine-grained representation learning through multi-level contrastive learning without class priors

Original Abstract:
Recent advancements in unsupervised representation learning often leverage class information to enhance feature extraction and clustering performance. However, this reliance on class priors limits the applicability of such methods in real-world scenarios where class information is unavailable or ambiguous. In this paper, we propose Contrastive Disentangling (CD), a simple and effective framework that learns representations without any reliance on class priors. Our framework employs a multi-level contrastive learning strategy that combines instance-level and feature-level losses with a normalized entropy loss to learn semantically rich and fine-grained representations. Specifically, (1) the instance-level contrastive loss encourages the separation of feature representations for different samples, (2) the feature-level contrastive loss promotes independence among the feature head predictions, and (3) the normalized entropy loss encourages the feature heads to capture meaningful and prevalent attributes from the data. These components work together to enable CD to significantly outperform existing methods, as demonstrated by extensive experiments on benchmark datasets including CIFAR-10, CIFAR-100, STL-10, and ImageNet-10, particularly in scenarios where class priors are absent. The code is available at this https URL.

Translated Abstract:
최근 비지도 표현 학습에서 클래스 정보를 활용해 특징 추출과 클러스터링 성능을 높이는 기술이 많이 발전했어. 하지만 클래스 정보에 의존하면 실제 상황에서는 적용하기 어려운 경우가 많아. 그래서 이 논문에서는 클래스 정보에 의존하지 않고 표현을 학습하는 간단하고 효과적인 프레임워크인 '대조적 분리(CD)'를 제안해.

우리 프레임워크는 다양한 수준의 대조 학습 전략을 사용해. 이건 인스턴스 수준과 특징 수준의 손실을 결합하고 정규화된 엔트로피 손실을 더해 의미 있게 세분화된 표현을 배우는 방식이야. 구체적으로는, (1) 인스턴스 수준 대조 손실이 서로 다른 샘플의 특징 표현을 분리하도록 유도하고, (2) 특징 수준 대조 손실이 특징 예측 간의 독립성을 촉진하며, (3) 정규화된 엔트로피 손실이 특징 헤드가 데이터에서 의미 있는 속성을 포착하도록 돕는 거야.

이런 요소들이 함께 작용해서 CD가 기존 방법들을 크게 초월할 수 있게 해. 우리는 CIFAR-10, CIFAR-100, STL-10, 그리고 ImageNet-10 같은 벤치마크 데이터셋에서 많은 실험을 통해 이걸 증명했어, 특히 클래스 정보가 없는 상황에서 더 효과적이었어. 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04888
Title: A Quantitative Approach for Evaluating Disease Focus and Interpretability of Deep Learning Models for Alzheimer's Disease Classification

Original Abstract:
Deep learning (DL) models have shown significant potential in Alzheimer's Disease (AD) classification. However, understanding and interpreting these models remains challenging, which hinders the adoption of these models in clinical practice. Techniques such as saliency maps have been proven effective in providing visual and empirical clues about how these models work, but there still remains a gap in understanding which specific brain regions DL models focus on and whether these brain regions are pathologically associated with AD.
To bridge such gap, in this study, we developed a quantitative disease-focusing strategy to first enhance the interpretability of DL models using saliency maps and brain segmentations; then we propose a disease-focus (DF) score that quantifies how much a DL model focuses on brain areas relevant to AD pathology based on clinically known MRI-based pathological regions of AD. Using this strategy, we compared several state-of-the-art DL models, including a baseline 3D ResNet model, a pretrained MedicalNet model, and a MedicalNet with data augmentation to classify patients with AD vs. cognitive normal patients using MRI data; then we evaluated these models in terms of their abilities to focus on disease-relevant regions. Our results show interesting disease-focusing patterns with different models, particularly characteristic patterns with the pretrained models and data augmentation, and also provide insight into their classification performance. These results suggest that the approach we developed for quantitatively assessing the abilities of DL models to focus on disease-relevant regions may help improve interpretability of these models for AD classification and facilitate their adoption for AD diagnosis in clinical practice. The code is publicly available at this https URL.

Translated Abstract:
딥러닝(DL) 모델은 알츠하이머병(AD) 분류에 큰 잠재력을 보여줬어. 하지만 이 모델들을 이해하고 해석하는 건 여전히 어려워서, 임상에서 사용하기 힘든 상황이야. 주목도 맵 같은 기술은 모델이 어떻게 작동하는지에 대한 시각적이고 실증적인 단서를 제공하는 데 효과적이지만, DL 모델이 어떤 특정한 뇌 영역에 집중하는지, 그리고 그 영역이 AD와 병리학적으로 관련이 있는지에 대한 이해는 부족해.

이런 간극을 메우기 위해, 우리는 이 연구에서 주목도 맵과 뇌 세분화를 사용해 DL 모델의 해석 가능성을 높이는 정량적인 질병 집중 전략을 개발했어. 그리고 AD의 병리학적 영역에 관련된 뇌 부위에 얼마나 집중하는지를 정량화하는 질병 집중(DF) 점수를 제안했어. 이 전략을 사용해서, 우리는 3D ResNet 모델, 미리 훈련된 MedicalNet 모델, 데이터 증강을 활용한 MedicalNet 모델 등 여러 최신 DL 모델을 비교했어. 그리고 MRI 데이터를 사용해서 AD 환자와 인지 정상 환자를 분류하는 데 있어서 이 모델들이 질병 관련 영역에 얼마나 집중하는지를 평가했어.

우리의 결과는 서로 다른 모델에서 흥미로운 질병 집중 패턴을 보여줬고, 특히 미리 훈련된 모델과 데이터 증강에서 독특한 패턴이 나타났어. 이 결과들은 DL 모델이 질병 관련 영역에 얼마나 집중하는지를 정량적으로 평가하는 우리가 개발한 접근법이 AD 분류의 해석 가능성을 개선하고, 임상에서 AD 진단에 활용될 수 있도록 도와줄 수 있다는 것을 시사해. 코드는 이 [https URL]에서 공개되어 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04915
Title: Activation Function Optimization Scheme for Image Classification

Original Abstract:
Activation function has a significant impact on the dynamics, convergence, and performance of deep neural networks. The search for a consistent and high-performing activation function has always been a pursuit during deep learning model development. Existing state-of-the-art activation functions are manually designed with human expertise except for Swish. Swish was developed using a reinforcement learning-based search strategy. In this study, we propose an evolutionary approach for optimizing activation functions specifically for image classification tasks, aiming to discover functions that outperform current state-of-the-art options. Through this optimization framework, we obtain a series of high-performing activation functions denoted as Exponential Error Linear Unit (EELU). The developed activation functions are evaluated for image classification tasks from two perspectives: (1) five state-of-the-art neural network architectures, such as ResNet50, AlexNet, VGG16, MobileNet, and Compact Convolutional Transformer which cover computationally heavy to light neural networks, and (2) eight standard datasets, including CIFAR10, Imagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15, and TinyImageNet which cover from typical machine vision benchmark, agricultural image applications to medical image applications. Finally, we statistically investigate the generalization of the resultant activation functions developed through the optimization scheme. With a Friedman test, we conclude that the optimization scheme is able to generate activation functions that outperform the existing standard ones in 92.8% cases among 28 different cases studied, and $-x\cdot erf(e^{-x})$ is found to be the best activation function for image classification generated by the optimization scheme.

Translated Abstract:
활성화 함수는 딥 뉴럴 네트워크의 동작, 수렴, 성능에 큰 영향을 미쳐. 일관되고 성능이 높은 활성화 함수를 찾는 건 딥 러닝 모델 개발 중 항상 중요한 목표였어. 현재 최고의 활성화 함수들은 대부분 인간의 전문 지식으로 수작업으로 디자인되었고, Swish만이 강화 학습 기반의 탐색 전략으로 개발됐어.

이번 연구에서는 이미지 분류 작업에 최적화된 활성화 함수를 찾기 위해 진화적 접근 방식을 제안해. 목표는 기존의 최첨단 옵션보다 우수한 함수를 발견하는 거야. 이 최적화 프레임워크를 통해 Exponential Error Linear Unit (EELU)라는 일련의 성능이 높은 활성화 함수를 얻었어.

개발된 활성화 함수는 두 가지 관점에서 이미지 분류 작업을 평가해. 첫 번째는 ResNet50, AlexNet, VGG16, MobileNet, Compact Convolutional Transformer 같은 다섯 가지 최첨단 뉴럴 네트워크 아키텍처를 다루고, 두 번째는 CIFAR10, Imagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15, TinyImageNet 같은 여덟 개의 표준 데이터셋을 포함해. 이 데이터셋들은 일반 머신 비전 벤치마크부터 농업 이미지, 의료 이미지 응용까지 다양해.

마지막으로, 최적화 스킴으로 개발된 활성화 함수의 일반화 가능성을 통계적으로 조사했어. Friedman 테스트를 통해, 이 최적화 스킴이 28개 다양한 사례 중 92.8%에서 기존 표준 함수를 능가하는 활성화 함수를 생성할 수 있다는 결론을 내렸고, $-x\cdot erf(e^{-x})$가 최적화 스킴으로 생성된 이미지 분류에 가장 좋은 활성화 함수로 발견됐어.

================================================================================

URL: https://arxiv.org/abs/2409.04918
Title: Training-free ZS-CIR via Weighted Modality Fusion and Similarity

Original Abstract:
Composed image retrieval (CIR), which formulates the query as a combination of a reference image and modified text, has emerged as a new form of image search due to its enhanced ability to capture users' intentions. However, training a CIR model in a supervised manner typically requires labor-intensive collection of (reference image, text modifier, target image) triplets. While existing zero-shot CIR (ZS-CIR) methods eliminate the need for training on specific downstream datasets, they still require additional pretraining with large-scale image-text pairs. In this paper, we introduce a training-free approach for ZS-CIR. Our approach, \textbf{Wei}ghted \textbf{Mo}dality fusion and similarity for \textbf{CIR} (WeiMoCIR), operates under the assumption that image and text modalities can be effectively combined using a simple weighted average. This allows the query representation to be constructed directly from the reference image and text modifier. To further enhance retrieval performance, we employ multimodal large language models (MLLMs) to generate image captions for the database images and incorporate these textual captions into the similarity computation by combining them with image information using a weighted average. Our approach is simple, easy to implement, and its effectiveness is validated through experiments on the FashionIQ and CIRR datasets.

Translated Abstract:
복합 이미지 검색(CIR)은 쿼리를 참조 이미지와 수정된 텍스트의 조합으로 구성하는 새로운 이미지 검색 방식이야. 이 방법은 사용자 의도를 더 잘 반영할 수 있어서 인기를 끌고 있어. 하지만 CIR 모델을 감독식으로 훈련하려면 (참조 이미지, 텍스트 수정자, 목표 이미지) 삼중 항목을 수집하는 데 많은 노력이 필요해. 기존의 제로샷 CIR(ZS-CIR) 방법은 특정 데이터셋에서 훈련할 필요가 없지만, 여전히 대규모 이미지-텍스트 쌍으로 사전 훈련이 필요해.

이 논문에서는 ZS-CIR을 위한 훈련 없이 사용할 수 있는 방법을 소개해. 우리의 방법인 **Wei**ghted **Mo**dality fusion and similarity for **CIR**(WeiMoCIR)은 이미지와 텍스트 모달리티를 간단한 가중 평균으로 효과적으로 결합할 수 있다고 가정해. 이걸 통해 쿼리 표현을 참조 이미지와 텍스트 수정자로부터 직접 만들 수 있어.

검색 성능을 더 높이기 위해 다중 모달 대형 언어 모델(MLLM)을 사용해서 데이터베이스 이미지에 대한 캡션을 생성하고, 이 텍스트 캡션을 이미지 정보와 결합해 가중 평균을 이용해 유사성 계산에 포함시켜. 우리의 방법은 간단하고 구현하기 쉬우며, FashionIQ와 CIRR 데이터셋을 통해 효과가 검증됐어.

================================================================================

URL: https://arxiv.org/abs/2409.04920
Title: MoistNet: Machine Vision-based Deep Learning Models for Wood Chip Moisture Content Measurement

Original Abstract:
Quick and reliable measurement of wood chip moisture content is an everlasting problem for numerous forest-reliant industries such as biofuel, pulp and paper, and bio-refineries. Moisture content is a critical attribute of wood chips due to its direct relationship with the final product quality. Conventional techniques for determining moisture content, such as oven-drying, possess some drawbacks in terms of their time-consuming nature, potential sample damage, and lack of real-time feasibility. Furthermore, alternative techniques, including NIR spectroscopy, electrical capacitance, X-rays, and microwaves, have demonstrated potential; nevertheless, they are still constrained by issues related to portability, precision, and the expense of the required equipment. Hence, there is a need for a moisture content determination method that is instant, portable, non-destructive, inexpensive, and precise. This study explores the use of deep learning and machine vision to predict moisture content classes from RGB images of wood chips. A large-scale image dataset comprising 1,600 RGB images of wood chips has been collected and annotated with ground truth labels, utilizing the results of the oven-drying technique. Two high-performing neural networks, MoistNetLite and MoistNetMax, have been developed leveraging Neural Architecture Search (NAS) and hyperparameter optimization. The developed models are evaluated and compared with state-of-the-art deep learning models. Results demonstrate that MoistNetLite achieves 87% accuracy with minimal computational overhead, while MoistNetMax exhibits exceptional precision with a 91% accuracy in wood chip moisture content class prediction. With improved accuracy and faster prediction speed, our proposed MoistNet models hold great promise for the wood chip processing industry.

Translated Abstract:
목재 칩의 수분 함량을 빠르고 정확하게 측정하는 건 바이오 연료, 펄프 및 종이, 바이오 정제소 같은 여러 숲 관련 산업에서 항상 고민되는 문제야. 수분 함량은 최종 제품의 품질과 직접적인 관계가 있기 때문에 매우 중요해. 

전통적인 수분 함량 측정 방법인 오븐 건조법은 시간이 오래 걸리고, 샘플을 손상시킬 수 있으며, 실시간으로 측정할 수 없다는 단점이 있어. 다른 대안으로 NIR 분광법, 전기 용량 측정, X선, 마이크로웨이브 같은 방법들도 가능성이 있지만, 휴대성, 정확성, 장비 비용 문제가 여전히 있지. 그래서 즉각적이고 휴대 가능하며 비파괴적이고 저렴하면서도 정확한 수분 함량 측정 방법이 필요해.

이 연구에서는 딥러닝과 머신 비전을 활용해 목재 칩의 RGB 이미지에서 수분 함량 클래스를 예측하는 방법을 탐구했어. 1,600개의 RGB 이미지로 구성된 대규모 이미지 데이터셋을 수집하고, 오븐 건조법의 결과를 활용해 진짜 라벨로 주석을 달았어. 두 가지 성능이 뛰어난 신경망, MoistNetLite와 MoistNetMax를 Neural Architecture Search(NAS)와 하이퍼파라미터 최적화를 통해 개발했어.

개발된 모델들은 최신 딥러닝 모델들과 평가하고 비교했어. 결과적으로 MoistNetLite는 최소한의 계산 오버헤드로 87%의 정확도를 달성했고, MoistNetMax는 목재 칩 수분 함량 클래스 예측에서 91%의 뛰어난 정확성을 보여줬어. 개선된 정확도와 빠른 예측 속도로, 우리가 제안한 MoistNet 모델들은 목재 칩 가공 산업에 큰 가능성을 가지고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04945
Title: Fast Deep Predictive Coding Networks for Videos Feature Extraction without Labels

Original Abstract:
Brain-inspired deep predictive coding networks (DPCNs) effectively model and capture video features through a bi-directional information flow, even without labels. They are based on an overcomplete description of video scenes, and one of the bottlenecks has been the lack of effective sparsification techniques to find discriminative and robust dictionaries. FISTA has been the best alternative. This paper proposes a DPCN with a fast inference of internal model variables (states and causes) that achieves high sparsity and accuracy of feature clustering. The proposed unsupervised learning procedure, inspired by adaptive dynamic programming with a majorization-minimization framework, and its convergence are rigorously analyzed. Experiments in the data sets CIFAR-10, Super Mario Bros video game, and Coil-100 validate the approach, which outperforms previous versions of DPCNs on learning rate, sparsity ratio, and feature clustering accuracy. Because of DCPN's solid foundation and explainability, this advance opens the door for general applications in object recognition in video without labels.

Translated Abstract:
뇌에서 영감을 받은 깊은 예측 코딩 네트워크(DPCN)는 비디오 특징을 효과적으로 모델링하고 포착할 수 있어. 이건 양방향 정보 흐름을 통해 이루어지는데, 라벨 없이도 가능해. DPCN은 비디오 장면에 대한 과잉 설명을 바탕으로 하고, 한 가지 문제점은 구별 가능하고 강력한 사전을 찾기 위한 효과적인 희소화 기술이 부족했다는 거야. FISTA가 그나마 가장 좋은 대안이었어.

이 논문에서는 내부 모델 변수(상태와 원인)를 빠르게 추론할 수 있는 DPCN을 제안해. 이 모델은 높은 희소성과 정확한 특징 클러스터링을 달성해. 제안된 비지도 학습 절차는 적응형 동적 프로그래밍에서 영감을 받았고, 주요화-최소화 프레임워크를 사용해. 이 절차의 수렴성도 엄밀하게 분석했어.

CIFAR-10, 슈퍼 마리오 브로스 비디오 게임, 그리고 Coil-100 데이터 세트에서 실험을 통해 이 방법이 이전 DPCN 버전보다 학습 속도, 희소 비율, 그리고 특징 클러스터링 정확도에서 더 뛰어난 성능을 보인다는 걸 검증했어. DPCN의 확고한 기초와 설명 가능성 덕분에, 이 발전은 라벨 없이 비디오에서 물체 인식과 같은 일반적인 응용을 위한 길을 열어줘.

================================================================================

URL: https://arxiv.org/abs/2409.04952
Title: Deep Bayesian Active Learning-to-Rank with Relative Annotation for Estimation of Ulcerative Colitis Severity

Original Abstract:
Automatic image-based severity estimation is an important task in computer-aided diagnosis. Severity estimation by deep learning requires a large amount of training data to achieve a high performance. In general, severity estimation uses training data annotated with discrete (i.e., quantized) severity labels. Annotating discrete labels is often difficult in images with ambiguous severity, and the annotation cost is high. In contrast, relative annotation, in which the severity between a pair of images is compared, can avoid quantizing severity and thus makes it easier. We can estimate relative disease severity using a learning-to-rank framework with relative annotations, but relative annotation has the problem of the enormous number of pairs that can be annotated. Therefore, the selection of appropriate pairs is essential for relative annotation. In this paper, we propose a deep Bayesian active learning-to-rank that automatically selects appropriate pairs for relative annotation. Our method preferentially annotates unlabeled pairs with high learning efficiency from the model uncertainty of the samples. We prove the theoretical basis for adapting Bayesian neural networks to pairwise learning-to-rank and demonstrate the efficiency of our method through experiments on endoscopic images of ulcerative colitis on both private and public datasets. We also show that our method achieves a high performance under conditions of significant class imbalance because it automatically selects samples from the minority classes.

Translated Abstract:
자동 이미지 기반 중증도 추정은 컴퓨터 보조 진단에서 중요한 작업이야. 딥러닝을 이용한 중증도 추정은 높은 성능을 내기 위해 많은 훈련 데이터가 필요해. 보통 중증도 추정은 구체적인 중증도 레이블이 붙은 훈련 데이터를 사용하는데, 중증도가 애매한 이미지에 레이블을 붙이는 건 어렵고 비용도 많이 들어.

반면에, 상대적 주석은 두 이미지의 중증도를 비교하는 방식으로, 중증도를 양자화할 필요가 없어서 더 쉬워. 우리는 상대적 주석을 이용해서 학습-순위 매기기 프레임워크를 통해 상대적 질병 중증도를 추정할 수 있지만, 주석을 달 수 있는 쌍의 수가 엄청 많아서 문제가 돼. 그래서 적절한 쌍을 선택하는 게 상대적 주석에서 중요해.

이 논문에서는 상대적 주석을 위한 적절한 쌍을 자동으로 선택하는 딥 베이지안 능동 학습-순위 매기기 방법을 제안해. 우리 방법은 모델의 불확실성으로부터 높은 학습 효율성을 가진 레이블이 없는 쌍을 우선적으로 주석 달아. 우리는 베이지안 신경망을 쌍별 학습-순위 매기기에 적용하는 이론적 근거를 증명하고, 염증성 장 질환의 내시경 이미지를 사용한 실험을 통해 우리 방법의 효율성을 보여줘. 또한, 우리 방법이 특정 클래스 불균형 조건에서도 높은 성능을 내는 걸 보여주는데, 이는 소수 클래스에서 샘플을 자동으로 선택하기 때문이야.

================================================================================

URL: https://arxiv.org/abs/2409.04958
Title: DDNet: Deformable Convolution and Dense FPN for Surface Defect Detection in Recycled Books

Original Abstract:
Recycled and recirculated books, such as ancient texts and reused textbooks, hold significant value in the second-hand goods market, with their worth largely dependent on surface preservation. However, accurately assessing surface defects is challenging due to the wide variations in shape, size, and the often imprecise detection of defects. To address these issues, we propose DDNet, an innovative detection model designed to enhance defect localization and classification. DDNet introduces a surface defect feature extraction module based on a deformable convolution operator (DC) and a densely connected FPN module (DFPN). The DC module dynamically adjusts the convolution grid to better align with object contours, capturing subtle shape variations and improving boundary delineation and prediction accuracy. Meanwhile, DFPN leverages dense skip connections to enhance feature fusion, constructing a hierarchical structure that generates multi-resolution, high-fidelity feature maps, thus effectively detecting defects of various sizes. In addition to the model, we present a comprehensive dataset specifically curated for surface defect detection in recycled and recirculated books. This dataset encompasses a diverse range of defect types, shapes, and sizes, making it ideal for evaluating the robustness and effectiveness of defect detection models. Through extensive evaluations, DDNet achieves precise localization and classification of surface defects, recording a mAP value of 46.7% on our proprietary dataset - an improvement of 14.2% over the baseline model - demonstrating its superior detection capabilities.

Translated Abstract:
재활용된 책, 예를 들어 고대 문서나 중고 교과서 같은 것들은 중고품 시장에서 꽤 중요한 가치를 가지고 있어. 이 가치가 주로 표면 상태에 따라 결정되는데, 표면 결함을 정확하게 평가하는 건 쉽지 않아. 왜냐하면 결함의 형태와 크기가 다양하고, 검출이 자주 부정확하거든. 

이 문제를 해결하기 위해 우리는 DDNet이라는 새로운 검출 모델을 제안해. DDNet은 결함 위치 파악과 분류를 개선하도록 설계되었어. 이 모델은 변형 컨볼루션 연산자(DC)를 기반으로 한 표면 결함 특징 추출 모듈과 밀집 연결된 FPN 모듈(DFPN)을 도입해. DC 모듈은 컨볼루션 그리드를 동적으로 조정해서 물체의 윤곽선에 더 잘 맞추게 해. 이렇게 해서 미세한 형태 변화도 잘 잡아내고, 경계 구분과 예측 정확도를 높여줘.

DFPN은 밀집 스킵 연결을 활용해서 특징 융합을 개선하고, 다층 구조를 만들어 다중 해상도와 높은 충실도의 특징 맵을 생성해. 그래서 다양한 크기의 결함을 효과적으로 검출할 수 있어. 

모델 외에도 우리는 재활용된 책의 표면 결함 검출을 위해 특별히 curated된 포괄적인 데이터셋도 제공해. 이 데이터셋은 다양한 결함 유형, 형태, 크기를 포함하고 있어서 결함 검출 모델의 강건성과 효과성을 평가하기에 아주 적합해. 

광범위한 평가를 통해 DDNet은 표면 결함의 정확한 위치 파악과 분류를 달성했고, 우리 독점 데이터셋에서 46.7%의 mAP 값을 기록했어. 이건 기본 모델보다 14.2% 향상된 거라 DDNet의 뛰어난 검출 능력을 잘 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.04963
Title: GS-PT: Exploiting 3D Gaussian Splatting for Comprehensive Point Cloud Understanding via Self-supervised Learning

Original Abstract:
Self-supervised learning of point cloud aims to leverage unlabeled 3D data to learn meaningful representations without reliance on manual annotations. However, current approaches face challenges such as limited data diversity and inadequate augmentation for effective feature learning. To address these challenges, we propose GS-PT, which integrates 3D Gaussian Splatting (3DGS) into point cloud self-supervised learning for the first time. Our pipeline utilizes transformers as the backbone for self-supervised pre-training and introduces novel contrastive learning tasks through 3DGS. Specifically, the transformers aim to reconstruct the masked point cloud. 3DGS utilizes multi-view rendered images as input to generate enhanced point cloud distributions and novel view images, facilitating data augmentation and cross-modal contrastive learning. Additionally, we incorporate features from depth maps. By optimizing these tasks collectively, our method enriches the tri-modal self-supervised learning process, enabling the model to leverage the correlation across 3D point clouds and 2D images from various modalities. We freeze the encoder after pre-training and test the model's performance on multiple downstream tasks. Experimental results indicate that GS-PT outperforms the off-the-shelf self-supervised learning methods on various downstream tasks including 3D object classification, real-world classifications, and few-shot learning and segmentation.

Translated Abstract:
포인트 클라우드의 자기 감독 학습은 라벨 없는 3D 데이터를 활용해서 수동 주석 없이 의미 있는 표현을 배우는 걸 목표로 해. 그런데 현재 방법들은 데이터 다양성이 부족하고 효과적인 특징 학습을 위한 증강이 부족한 문제에 직면해 있어.

이런 문제를 해결하기 위해 GS-PT라는 방법을 제안해. GS-PT는 3D 가우시안 스플래팅(3DGS)을 포인트 클라우드 자기 감독 학습에 처음으로 통합한 거야. 우리 파이프라인은 트랜스포머를 자기 감독 사전 학습의 기본 구조로 사용하고, 3DGS를 통해 새로운 대조 학습 작업을 도입해.

구체적으로, 트랜스포머는 마스킹된 포인트 클라우드를 재구성하는 걸 목표로 해. 3DGS는 멀티뷰 렌더링된 이미지를 입력으로 사용해서 향상된 포인트 클라우드 분포와 새로운 시각 이미지를 생성해. 이렇게 해서 데이터 증강과 크로스 모달 대조 학습이 가능해져.

또한, 우리는 깊이 맵의 특징도 포함했어. 이 작업들을 함께 최적화함으로써, 우리의 방법은 3D 포인트 클라우드와 다양한 모달리티의 2D 이미지 간의 상관관계를 활용할 수 있게 해줘. 사전 학습 후에는 인코더를 고정하고, 여러 다운스트림 작업에서 모델의 성능을 테스트해.

실험 결과, GS-PT는 3D 객체 분류, 실제 세계 분류, 그리고 몇 장의 샷 학습 및 분할 등 여러 다운스트림 작업에서 기존의 자기 감독 학습 방법보다 더 나은 성능을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.04968
Title: Natias: Neuron Attribution based Transferable Image Adversarial Steganography

Original Abstract:
Image steganography is a technique to conceal secret messages within digital images. Steganalysis, on the contrary, aims to detect the presence of secret messages within images. Recently, deep-learning-based steganalysis methods have achieved excellent detection performance. As a countermeasure, adversarial steganography has garnered considerable attention due to its ability to effectively deceive deep-learning-based steganalysis. However, steganalysts often employ unknown steganalytic models for detection. Therefore, the ability of adversarial steganography to deceive non-target steganalytic models, known as transferability, becomes especially important. Nevertheless, existing adversarial steganographic methods do not consider how to enhance transferability. To address this issue, we propose a novel adversarial steganographic scheme named Natias. Specifically, we first attribute the output of a steganalytic model to each neuron in the target middle layer to identify critical features. Next, we corrupt these critical features that may be adopted by diverse steganalytic models. Consequently, it can promote the transferability of adversarial steganography. Our proposed method can be seamlessly integrated with existing adversarial steganography frameworks. Thorough experimental analyses affirm that our proposed technique possesses improved transferability when contrasted with former approaches, and it attains heightened security in retraining scenarios.

Translated Abstract:
이미지 스테가노그래피는 디지털 이미지 안에 비밀 메시지를 숨기는 기술이에요. 반면, 스테가날리시스는 이미지 안에 비밀 메시지가 있는지를 찾는 거죠. 최근에는 딥러닝 기반의 스테가날리시스 방법들이 정말 뛰어난 탐지 성능을 보여주고 있어요.

그에 대한 대응책으로, 적대적 스테가노그래피가 주목받고 있는데, 이는 딥러닝 기반의 스테가날리시스를 효과적으로 속일 수 있는 능력 덕분이에요. 하지만 스테가날리스트들은 종종 탐지에 대해 잘 알려지지 않은 스테가날리틱 모델을 사용해요. 그래서 적대적 스테가노그래피가 비타겟 스테가날리틱 모델도 속일 수 있는 능력, 즉 전이 가능성이 특히 중요해져요.

그런데 기존의 적대적 스테가노그래픽 방법들은 전이 가능성을 높이는 방법을 고려하지 않았어요. 이 문제를 해결하기 위해, 우리는 Natias라는 새로운 적대적 스테가노그래픽 방식을 제안해요. 구체적으로, 먼저 스테가날리틱 모델의 출력을 목표 중간 레이어의 각 뉴런에 연결해서 중요한 특징들을 식별해요. 그런 다음, 다양한 스테가날리틱 모델에서 사용될 수 있는 이러한 중요한 특징들을 손상시켜요. 이렇게 하면 적대적 스테가노그래피의 전이 가능성을 높일 수 있어요.

우리의 제안된 방법은 기존의 적대적 스테가노그래피 프레임워크와 원활하게 통합될 수 있어요. 실험 분석 결과, 우리의 방법이 이전 접근 방식에 비해 전이 가능성이 향상되었고, 재훈련 상황에서도 보안성이 높아졌다는 것을 확인했어요.

================================================================================

URL: https://arxiv.org/abs/2409.04975
Title: PatchAlign:Fair and Accurate Skin Disease Image Classification by Alignment with Clinical Labels

Original Abstract:
Deep learning models have achieved great success in automating skin lesion diagnosis. However, the ethnic disparity in these models' predictions needs to be addressed before deploying them. We introduce a novel approach, PatchAlign, to enhance skin condition image classification accuracy and fairness by aligning with clinical text representations of skin conditions. PatchAlign uses Graph Optimal Transport (GOT) Loss as a regularizer to perform cross-domain alignment. The representations obtained are robust and generalize well across skin tones, even with limited training samples. To reduce the effect of noise and artifacts in clinical dermatology images, we propose a learnable Masked Graph Optimal Transport for cross-domain alignment that further improves fairness metrics.
We compare our model to the state-of-the-art FairDisCo on two skin lesion datasets with different skin types: Fitzpatrick17k and Diverse Dermatology Images (DDI). PatchAlign enhances the accuracy of skin condition image classification by 2.8% (in-domain) and 6.2% (out-domain) on Fitzpatrick17k, and 4.2% (in-domain) on DDI compared to FairDisCo. Additionally, it consistently improves the fairness of true positive rates across skin tones.
The source code for the implementation is available at the following GitHub repository: this https URL, enabling easy reproduction and further experimentation.

Translated Abstract:
딥러닝 모델이 피부 병변 진단 자동화에서 큰 성공을 거두었지만, 이 모델들이 예측하는 데 있어 인종 간 차이를 해결해야 해. 우리는 PatchAlign이라는 새로운 방법을 소개하는데, 이 방법은 피부 상태 이미지를 분류하는 정확성과 공정성을 높이기 위해 임상 텍스트 표현과 정렬하는 방식이야.

PatchAlign은 Graph Optimal Transport (GOT) Loss를 정규화기로 사용해서 서로 다른 분야 간의 정렬을 수행해. 이렇게 얻은 표현은 강력하고 다양한 피부 톤에 잘 일반화돼, 훈련 샘플이 적어도 괜찮아. 임상 피부과 이미지에서 노이즈와 인위적인 효과를 줄이기 위해, 우리는 배우는 Masked Graph Optimal Transport를 제안해서 교차 분야 정렬을 통해 공정성 지표를 더 개선해.

우리는 PatchAlign을 최신 기술인 FairDisCo와 비교했어. 두 가지 피부 병변 데이터셋, Fitzpatrick17k와 Diverse Dermatology Images (DDI)에서 성능을 평가했지. PatchAlign은 Fitzpatrick17k에서 피부 상태 이미지 분류의 정확도를 2.8% (내부 도메인)와 6.2% (외부 도메인) 높였고, DDI에서는 4.2% (내부 도메인)를 향상시켰어. 게다가, 다양한 피부 톤에서 진짜 양성 비율의 공정성을 꾸준히 개선했어.

구현을 위한 소스 코드는 이 GitHub 저장소에서 확인할 수 있어: 이 URL. 쉽게 재현하고 추가 실험을 할 수 있도록 제공되고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04978
Title: Time-independent Spiking Neuron via Membrane Potential Estimation for Efficient Spiking Neural Networks

Original Abstract:
The computational inefficiency of spiking neural networks (SNNs) is primarily due to the sequential updates of membrane potential, which becomes more pronounced during extended encoding periods compared to artificial neural networks (ANNs). This highlights the need to parallelize SNN computations effectively to leverage available hardware parallelism. To address this, we propose Membrane Potential Estimation Parallel Spiking Neurons (MPE-PSN), a parallel computation method for spiking neurons that enhances computational efficiency by enabling parallel processing while preserving the intrinsic dynamic characteristics of SNNs. Our approach exhibits promise for enhancing computational efficiency, particularly under conditions of elevated neuron density. Empirical experiments demonstrate that our method achieves state-of-the-art (SOTA) accuracy and efficiency on neuromorphic datasets without requiring additional training parameters. Codes are available at~\url{this https URL}.

Translated Abstract:
스파이킹 신경망(SNN)의 계산 비효율성은 주로 막 전위의 순차적 업데이트 때문인데, 이게 인공 신경망(ANN)보다 긴 인코딩 기간 동안 더 두드러져. 그래서 SNN의 계산을 효과적으로 병렬 처리할 필요가 있어. 

우리는 '막 전위 추정 병렬 스파이킹 뉴런(MPE-PSN)'이라는 병렬 계산 방법을 제안해. 이 방법은 스파이킹 뉴런의 동적 특성을 유지하면서 병렬 처리를 가능하게 해줘서 계산 효율성을 높여. 특히 뉴런 밀도가 높은 상황에서 효과적이야. 

실험 결과, 우리 방법이 신경형 데이터셋에서 최첨단(SOTA) 정확도와 효율성을 달성했어. 추가적인 훈련 파라미터 없이도 가능했어. 코드도 제공해.

================================================================================

URL: https://arxiv.org/abs/2409.04979
Title: RCBEVDet++: Toward High-accuracy Radar-Camera Fusion 3D Perception Network

Original Abstract:
Perceiving the surrounding environment is a fundamental task in autonomous driving. To obtain highly accurate perception results, modern autonomous driving systems typically employ multi-modal sensors to collect comprehensive environmental data. Among these, the radar-camera multi-modal perception system is especially favored for its excellent sensing capabilities and cost-effectiveness. However, the substantial modality differences between radar and camera sensors pose challenges in fusing information. To address this problem, this paper presents RCBEVDet, a radar-camera fusion 3D object detection framework. Specifically, RCBEVDet is developed from an existing camera-based 3D object detector, supplemented by a specially designed radar feature extractor, RadarBEVNet, and a Cross-Attention Multi-layer Fusion (CAMF) module. Firstly, RadarBEVNet encodes sparse radar points into a dense bird's-eye-view (BEV) feature using a dual-stream radar backbone and a Radar Cross Section aware BEV encoder. Secondly, the CAMF module utilizes a deformable attention mechanism to align radar and camera BEV features and adopts channel and spatial fusion layers to fuse them. To further enhance RCBEVDet's capabilities, we introduce RCBEVDet++, which advances the CAMF through sparse fusion, supports query-based multi-view camera perception models, and adapts to a broader range of perception tasks. Extensive experiments on the nuScenes show that our method integrates seamlessly with existing camera-based 3D perception models and improves their performance across various perception tasks. Furthermore, our method achieves state-of-the-art radar-camera fusion results in 3D object detection, BEV semantic segmentation, and 3D multi-object tracking tasks. Notably, with ViT-L as the image backbone, RCBEVDet++ achieves 72.73 NDS and 67.34 mAP in 3D object detection without test-time augmentation or model ensembling.

Translated Abstract:
주변 환경을 인식하는 것은 자율주행의 기본 작업이야. 정확한 인식 결과를 얻기 위해 현대의 자율주행 시스템은 보통 여러 종류의 센서를 사용해서 환경 데이터를 수집해. 그 중에서도 레이더-카메라 복합 인식 시스템은 뛰어난 감지 능력과 비용 효율성 덕분에 특히 인기가 많아. 하지만 레이더와 카메라 센서 간의 큰 차이 때문에 정보를 결합하는 데 어려움이 있어.

이 문제를 해결하기 위해 이 논문에서는 RCBEVDet라는 레이더-카메라 융합 3D 객체 감지 프레임워크를 제안해. RCBEVDet는 기존의 카메라 기반 3D 객체 감지기를 바탕으로 발전시킨 거고, 특별히 설계된 레이더 특징 추출기인 RadarBEVNet과 Cross-Attention Multi-layer Fusion (CAMF) 모듈이 추가돼.

먼저, RadarBEVNet은 희소한 레이더 포인트를 밀집된 조감도(BEV) 특징으로 변환하는데, 이때 이중 스트림 레이더 백본과 레이더 단면을 고려한 BEV 인코더를 사용해. 다음으로, CAMF 모듈은 변형 가능한 주의 메커니즘을 사용해서 레이더와 카메라의 BEV 특징을 정렬하고, 채널 및 공간 융합 레이어를 통해 이들을 결합해. 

또한, RCBEVDet의 능력을 더욱 강화하기 위해 RCBEVDet++를 소개해. 이건 CAMF의 희소 융합을 발전시키고, 쿼리 기반의 다중 시점 카메라 인식 모델을 지원하며, 더 다양한 인식 작업에 적응할 수 있어. nuScenes에서의 광범위한 실험 결과, 우리의 방법은 기존의 카메라 기반 3D 인식 모델과 매끄럽게 통합되고, 여러 인식 작업에서 성능을 향상시킨다는 걸 보여줬어.

게다가, 우리의 방법은 3D 객체 감지, BEV 의미 분할, 3D 다중 객체 추적 작업에서 최첨단 레이더-카메라 융합 결과를 달성했어. 특히, ViT-L을 이미지 백본으로 사용할 때, RCBEVDet++는 테스트 시간 증강이나 모델 앙상블 없이 3D 객체 감지에서 72.73 NDS와 67.34 mAP를 기록했어.

================================================================================

URL: https://arxiv.org/abs/2409.04980
Title: Multi-V2X: A Large Scale Multi-modal Multi-penetration-rate Dataset for Cooperative Perception

Original Abstract:
Cooperative perception through vehicle-to-everything (V2X) has garnered significant attention in recent years due to its potential to overcome occlusions and enhance long-distance perception. Great achievements have been made in both datasets and algorithms. However, existing real-world datasets are limited by the presence of few communicable agents, while synthetic datasets typically cover only vehicles. More importantly, the penetration rate of connected and autonomous vehicles (CAVs) , a critical factor for the deployment of cooperative perception technologies, has not been adequately addressed. To tackle these issues, we introduce Multi-V2X, a large-scale, multi-modal, multi-penetration-rate dataset for V2X perception. By co-simulating SUMO and CARLA, we equip a substantial number of cars and roadside units (RSUs) in simulated towns with sensor suites, and collect comprehensive sensing data. Datasets with specified CAV penetration rates can be obtained by masking some equipped cars as normal vehicles. In total, our Multi-V2X dataset comprises 549k RGB frames, 146k LiDAR frames, and 4,219k annotated 3D bounding boxes across six categories. The highest possible CAV penetration rate reaches 86.21%, with up to 31 agents in communication range, posing new challenges in selecting agents to collaborate with. We provide comprehensive benchmarks for cooperative 3D object detection tasks. Our data and code are available at this https URL .

Translated Abstract:
차량 간 협력 인식(V2X)을 통한 협력 인식이 최근 몇 년 동안 주목받고 있어. 이 기술은 시야가 가려지는 문제를 극복하고 먼 거리 인식을 개선할 수 있는 잠재력이 있기 때문이야. 데이터셋과 알고리즘에서 많은 성과가 있었지만, 기존의 실제 데이터셋은 소통할 수 있는 차량이 적어서 한계가 있어. 또, 합성 데이터셋은 보통 차량만 포함되고, 연결된 자율주행차(CAV)의 비율, 즉 협력 인식 기술이 실제로 활용될 수 있는 중요한 요소는 충분히 다뤄지지 않았어.

이 문제를 해결하기 위해 우리는 Multi-V2X라는 대규모 멀티모달 데이터셋을 소개해. 이 데이터셋은 V2X 인식을 위해 여러 가지 침투율을 가진 데이터로 구성되어 있어. SUMO와 CARLA를 동시에 시뮬레이션하여, 많은 수의 차량과 도로 옆 장비(RSU)를 장착한 도시에서 센서를 갖춘 차량들을 배치하고 종합적인 감지 데이터를 수집했어. 특정 CAV 침투율에 맞춘 데이터셋은 일부 장착된 차량을 일반 차량으로 마스킹해서 얻을 수 있어.

우리의 Multi-V2X 데이터셋은 총 549,000개의 RGB 프레임, 146,000개의 LiDAR 프레임, 그리고 4,219,000개의 주석이 달린 3D 바운딩 박스를 포함해 여섯 가지 카테고리로 나뉘어 있어. 가장 높은 CAV 침투율은 86.21%에 달하며, 최대 31개의 소통 가능한 에이전트가 있어 협력할 에이전트를 선택하는 데 새로운 도전 과제가 생기지. 우리는 협력 3D 객체 검출 작업을 위한 종합적인 벤치마크도 제공해. 우리의 데이터와 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04982
Title: 2DSig-Detect: a semi-supervised framework for anomaly detection on image data using 2D-signatures

Original Abstract:
The rapid advancement of machine learning technologies raises questions about the security of machine learning models, with respect to both training-time (poisoning) and test-time (evasion, impersonation, and inversion) attacks. Models performing image-related tasks, e.g. detection, and classification, are vulnerable to adversarial attacks that can degrade their performance and produce undesirable outcomes. This paper introduces a novel technique for anomaly detection in images called 2DSig-Detect, which uses a 2D-signature-embedded semi-supervised framework rooted in rough path theory. We demonstrate our method in adversarial settings for training-time and test-time attacks, and benchmark our framework against other state of the art methods. Using 2DSig-Detect for anomaly detection, we show both superior performance and a reduction in the computation time to detect the presence of adversarial perturbations in images.

Translated Abstract:
기계 학습 기술이 빠르게 발전하면서, 기계 학습 모델의 보안에 대한 의문이 생겼어. 여기에는 학습 시의 공격(독성 공격)과 테스트 시의 공격(회피, 사칭, 역전)이 포함돼. 이미지 관련 작업을 수행하는 모델들, 예를 들어 탐지나 분류 같은 것들은 적대적 공격에 취약해서 성능이 떨어지고 바람직하지 않은 결과를 낳을 수 있어.

이 논문에서는 이미지에서 이상을 탐지하기 위한 새로운 기술인 2DSig-Detect를 소개해. 이 기술은 거칠은 경로 이론에 기반한 2D 서명 임베디드 반지도 학습 프레임워크를 사용해. 우리는 이 방법을 학습 시와 테스트 시의 공격이 있는 환경에서 보여주고, 다른 최신 방법들과 비교해봤어.

2DSig-Detect를 사용해서 이상 탐지를 했을 때, 우리는 뛰어난 성능과 함께 이미지에서 적대적 왜곡을 탐지하는 데 걸리는 시간을 줄일 수 있었어.

================================================================================

URL: https://arxiv.org/abs/2409.04999
Title: Visual Grounding with Multi-modal Conditional Adaptation

Original Abstract:
Visual grounding is the task of locating objects specified by natural language expressions. Existing methods extend generic object detection frameworks to tackle this task. They typically extract visual and textual features separately using independent visual and textual encoders, then fuse these features in a multi-modal decoder for final prediction. However, visual grounding presents unique challenges. It often involves locating objects with different text descriptions within the same image. Existing methods struggle with this task because the independent visual encoder produces identical visual features for the same image, limiting detection performance. Some recently approaches propose various language-guided visual encoders to address this issue, but they mostly rely solely on textual information and require sophisticated designs. In this paper, we introduce Multi-modal Conditional Adaptation (MMCA), which enables the visual encoder to adaptively update weights, directing its focus towards text-relevant regions. Specifically, we first integrate information from different modalities to obtain multi-modal embeddings. Then we utilize a set of weighting coefficients, which generated from the multimodal embeddings, to reorganize the weight update matrices and apply them to the visual encoder of the visual grounding model. Extensive experiments on four widely used datasets demonstrate that MMCA achieves significant improvements and state-of-the-art results. Ablation experiments further demonstrate the lightweight and efficiency of our method. Our source code is available at: this https URL.

Translated Abstract:
비주얼 그라운딩은 자연어 표현으로 지정된 객체를 찾는 작업이야. 기존 방법들은 일반적인 객체 탐지 프레임워크를 확장해서 이 작업을 수행해. 이들은 보통 시각적 특성과 텍스트 특성을 따로 추출하고, 그런 다음 다중 모달 디코더에서 이 특성들을 합쳐서 최종 예측을 해. 

하지만 비주얼 그라운딩은 독특한 어려움이 있어. 같은 이미지 안에서 서로 다른 텍스트 설명으로 객체를 찾아야 하거든. 기존 방법들은 독립적인 시각 인코더가 같은 이미지에 대해 똑같은 시각적 특성을 만들어내기 때문에 이 작업에 힘들어해. 최근 몇몇 방법들은 이 문제를 해결하기 위해 다양한 언어 기반 시각 인코더를 제안했지만, 대부분 텍스트 정보에만 의존하고 복잡한 설계를 필요로 해. 

이 논문에서는 다중 모달 조건 적응(MMCA)을 소개해. 이 방법은 시각 인코더가 가중치를 적응적으로 업데이트하면서 텍스트와 관련된 영역에 더 집중할 수 있게 해. 먼저, 서로 다른 모달리티에서 정보를 통합해 다중 모달 임베딩을 얻고, 그런 다음 이 임베딩에서 생성된 가중치 계수를 이용해 가중치 업데이트 행렬을 재구성해 시각 인코더에 적용해. 

네 개의 널리 사용되는 데이터셋에서 광범위한 실험을 통해 MMCA가 상당한 개선을 이루고 최첨단 결과를 달성했음을 보여줬어. 제거 실험을 통해서도 우리의 방법이 경량화되고 효율적임을 입증했어. 우리의 소스 코드는 여기에서 확인할 수 있어: 이 https URL.

================================================================================

URL: https://arxiv.org/abs/2409.05005
Title: Towards Patronizing and Condescending Language in Chinese Videos: A Multimodal Dataset and Detector

Original Abstract:
Patronizing and Condescending Language (PCL) is a form of discriminatory toxic speech targeting vulnerable groups, threatening both online and offline safety. While toxic speech research has mainly focused on overt toxicity, such as hate speech, microaggressions in the form of PCL remain underexplored. Additionally, dominant groups' discriminatory facial expressions and attitudes toward vulnerable communities can be more impactful than verbal cues, yet these frame features are often overlooked. In this paper, we introduce the PCLMM dataset, the first Chinese multimodal dataset for PCL, consisting of 715 annotated videos from Bilibili, with high-quality PCL facial frame spans. We also propose the MultiPCL detector, featuring a facial expression detection module for PCL recognition, demonstrating the effectiveness of modality complementarity in this challenging task. Our work makes an important contribution to advancing microaggression detection within the domain of toxic speech.

Translated Abstract:
패트로나이징과 경멸적인 언어(PCL)는 취약한 그룹을 겨냥한 차별적인 독성 언어의 한 형태로, 온라인과 오프라인 안전 모두에 위협이 돼. 독성 언어 연구는 주로 증오 발언 같은 명백한 독성에 집중했지만, PCL 같은 미세 공격은 아직 많이 연구되지 않았어. 

또한, 지배 집단의 차별적인 표정과 태도가 취약한 커뮤니티에 미치는 영향은 언어적 신호보다 더 클 수 있는데, 이런 부분은 종종 간과돼. 이 논문에서는 PCLMM 데이터셋을 소개해. 이 데이터셋은 PCL에 대한 첫 번째 중국어 다중 모달 데이터셋으로, 715개의 주석이 달린 Bilibili 비디오로 구성돼. 여기에는 고품질의 PCL 얼굴 프레임도 포함돼.

우리는 또한 PCL 인식을 위한 얼굴 표정 감지 모듈을 가진 MultiPCL 탐지기를 제안해. 이 탐지기는 이 어려운 작업에서 모달리티 보완의 효과를 보여줘. 우리 연구는 독성 언어 분야에서 미세 공격 탐지를 발전시키는 데 중요한 기여를 해.

================================================================================

URL: https://arxiv.org/abs/2409.05024
Title: Deep Self-cleansing for Medical Image Segmentation with Noisy Labels

Original Abstract:
Medical image segmentation is crucial in the field of medical imaging, aiding in disease diagnosis and surgical planning. Most established segmentation methods rely on supervised deep learning, in which clean and precise labels are essential for supervision and significantly impact the performance of models. However, manually delineated labels often contain noise, such as missing labels and inaccurate boundary delineation, which can hinder networks from correctly modeling target characteristics. In this paper, we propose a deep self-cleansing segmentation framework that can preserve clean labels while cleansing noisy ones in the training phase. To achieve this, we devise a gaussian mixture model-based label filtering module that distinguishes noisy labels from clean labels. Additionally, we develop a label cleansing module to generate pseudo low-noise labels for identified noisy samples. The preserved clean labels and pseudo-labels are then used jointly to supervise the network. Validated on a clinical liver tumor dataset and a public cardiac diagnosis dataset, our method can effectively suppress the interference from noisy labels and achieve prominent segmentation performance.

Translated Abstract:
의료 이미지 분할은 의료 영상 분야에서 정말 중요해. 이게 질병 진단이나 수술 계획에 큰 도움이 되거든. 기존의 분할 방법들은 대부분 감독된 딥 러닝에 의존하는데, 여기서 정확하고 깨끗한 라벨이 필요해. 이 라벨이 모델 성능에 큰 영향을 미치거든. 하지만 수작업으로 만들어진 라벨은 종종 노이즈가 포함되어 있어. 예를 들어, 라벨이 빠지거나 경계가 제대로 표시되지 않는 경우가 많아. 이런 문제들은 네트워크가 목표 특성을 제대로 모델링하는 데 방해가 될 수 있어.

그래서 우리는 '딥 셀프 클렌징 분할 프레임워크'를 제안해. 이 프레임워크는 훈련 단계에서 깨끗한 라벨은 유지하면서 노이즈가 있는 라벨을 정리할 수 있어. 이를 위해, 우리는 가우시안 혼합 모델 기반의 라벨 필터링 모듈을 만들었어. 이 모듈은 노이즈가 있는 라벨과 깨끗한 라벨을 구별해. 그리고 노이즈가 있는 샘플에 대해 저노이즈의 가짜 라벨을 생성하는 라벨 클렌징 모듈도 개발했어. 이렇게 보존된 깨끗한 라벨과 가짜 라벨을 함께 사용해서 네트워크를 감독해.

우리는 이 방법을 임상 간종양 데이터셋과 공개 심장 진단 데이터셋에서 검증했어. 결과적으로, 우리의 방법은 노이즈가 있는 라벨의 간섭을 효과적으로 줄이고 뛰어난 분할 성능을 달성할 수 있었어.

================================================================================

URL: https://arxiv.org/abs/2409.05040
Title: Unsupervised Multimodal 3D Medical Image Registration with Multilevel Correlation Balanced Optimization

Original Abstract:
Surgical navigation based on multimodal image registration has played a significant role in providing intraoperative guidance to surgeons by showing the relative position of the target area to critical anatomical structures during surgery. However, due to the differences between multimodal images and intraoperative image deformation caused by tissue displacement and removal during the surgery, effective registration of preoperative and intraoperative multimodal images faces significant challenges. To address the multimodal image registration challenges in Learn2Reg 2024, an unsupervised multimodal medical image registration method based on multilevel correlation balanced optimization (MCBO) is designed to solve these problems. First, the features of each modality are extracted based on the modality independent neighborhood descriptor, and the multimodal images is mapped to the feature space. Second, a multilevel pyramidal fusion optimization mechanism is designed to achieve global optimization and local detail complementation of the deformation field through dense correlation analysis and weight-balanced coupled convex optimization for input features at different scales. For preoperative medical images in different modalities, the alignment and stacking of valid information between different modalities is achieved by the maximum fusion between deformation fields. Our method focuses on the ReMIND2Reg task in Learn2Reg 2024, and to verify the generality of the method, we also tested it on the COMULIS3DCLEM task. Based on the results, our method achieved second place in the validation of both two tasks.

Translated Abstract:
다중 모드 이미지 등록에 기반한 수술 내비게이션은 수술 중에 목표 영역과 중요한 해부학적 구조의 상대 위치를 보여줌으로써 외과 의사에게 큰 도움을 주고 있어. 하지만 다중 모드 이미지 간의 차이와 수술 중 조직 이동 및 제거로 인해 발생하는 이미지 변형 때문에, 수술 전후의 다중 모드 이미지를 효과적으로 등록하는 데는 많은 어려움이 있어. 

이런 문제를 해결하기 위해 Learn2Reg 2024에서 다중 모드 의료 이미지 등록을 위한 비지도 학습 방법인 다층 상관 균형 최적화(MCBO) 기반의 방법을 설계했어. 먼저, 각 모드의 특징을 모드 독립적인 이웃 설명자를 통해 추출하고, 다중 모드 이미지를 특징 공간으로 맵핑해. 두 번째로, 다층 피라미드 융합 최적화 메커니즘을 설계해서 변형 필드의 전역 최적화와 지역 세부 사항 보완을 달성해. 이 과정은 입력 특징을 다양한 스케일에서 밀집 상관 분석과 가중치 균형 결합 볼록 최적화를 통해 이루어져. 

수술 전 다양한 모드의 의료 이미지에 대해서는, 변형 필드 간의 최대 융합을 통해 서로 다른 모드 간의 유효 정보 정렬과 스태킹이 가능해. 우리 방법은 Learn2Reg 2024의 ReMIND2Reg 작업에 중점을 두었고, 방법의 일반성을 검증하기 위해 COMULIS3DCLEM 작업에서도 테스트했어. 결과적으로, 우리 방법은 두 개의 작업 검증에서 모두 2위를 차지했어.

================================================================================

URL: https://arxiv.org/abs/2409.05065
Title: Sight View Constraint for Robust Point Cloud Registration

Original Abstract:
Partial to Partial Point Cloud Registration (partial PCR) remains a challenging task, particularly when dealing with a low overlap rate. In comparison to the full-to-full registration task, we find that the objective of partial PCR is still not well-defined, indicating no metric can reliably identify the true transformation. We identify this as the most fundamental challenge in partial PCR tasks. In this paper, instead of directly seeking the optimal transformation, we propose a novel and general Sight View Constraint (SVC) to conclusively identify incorrect transformations, thereby enhancing the robustness of existing PCR methods. Extensive experiments validate the effectiveness of SVC on both indoor and outdoor scenes. On the challenging 3DLoMatch dataset, our approach increases the registration recall from 78\% to 82\%, achieving the state-of-the-art result. This research also highlights the significance of the decision version problem of partial PCR, which has the potential to provide novel insights into the partial PCR problem.

Translated Abstract:
부분 대 부분 포인트 클라우드 정합(Partial PCR)은 특히 겹치는 부분이 적을 때 여전히 어려운 과제야. 전체 대 전체 정합과 비교했을 때, 부분 PCR의 목적이 아직 잘 정의되지 않았다는 점을 발견했어. 그래서 어떤 기준도 진짜 변환을 신뢰성 있게 식별할 수 없다는 거지. 우리는 이 점이 부분 PCR 작업의 가장 근본적인 도전 과제라고 생각해.

이 논문에서는 최적의 변환을 직접 찾는 대신, 새로운 일반적인 시각적 제약(Sight View Constraint, SVC)을 제안해. 이 제약은 잘못된 변환을 확실히 식별할 수 있게 해줘, 그래서 기존의 PCR 방법들의 강건성을 높이는 데 도움을 줘. 실험을 통해 SVC가 실내 및 실외 장면 모두에서 효과적임을 입증했어.

어려운 3DLoMatch 데이터셋에서는 우리 방법이 정합 회수율을 78%에서 82%로 높여서 최신 기술 수준의 결과를 달성했어. 이 연구는 부분 PCR의 결정 버전 문제의 중요성도 강조하고, 이는 부분 PCR 문제에 대한 새로운 통찰을 제공할 가능성이 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05076
Title: PIP: Detecting Adversarial Examples in Large Vision-Language Models via Attention Patterns of Irrelevant Probe Questions

Original Abstract:
Large Vision-Language Models (LVLMs) have demonstrated their powerful multimodal capabilities. However, they also face serious safety problems, as adversaries can induce robustness issues in LVLMs through the use of well-designed adversarial examples. Therefore, LVLMs are in urgent need of detection tools for adversarial examples to prevent incorrect responses. In this work, we first discover that LVLMs exhibit regular attention patterns for clean images when presented with probe questions. We propose an unconventional method named PIP, which utilizes the attention patterns of one randomly selected irrelevant probe question (e.g., "Is there a clock?") to distinguish adversarial examples from clean examples. Regardless of the image to be tested and its corresponding question, PIP only needs to perform one additional inference of the image to be tested and the probe question, and then achieves successful detection of adversarial examples. Even under black-box attacks and open dataset scenarios, our PIP, coupled with a simple SVM, still achieves more than 98% recall and a precision of over 90%. Our PIP is the first attempt to detect adversarial attacks on LVLMs via simple irrelevant probe questions, shedding light on deeper understanding and introspection within LVLMs. The code is available at this https URL.

Translated Abstract:
대규모 비전-언어 모델(LVLMs)은 멀티모달 기능이 강력하다는 걸 보여줬어. 하지만, 이 모델들은 안전 문제도 심각해. 적들이 잘 설계된 적대적 예제를 이용해 LVLM의 강건성을 문제 삼을 수 있거든. 그래서 LVLM들은 잘못된 응답을 막기 위해 적대적 예제를 감지할 도구가 절실히 필요해.

이 연구에서는 LVLM이 깔끔한 이미지에 대해 probe 질문을 받았을 때 규칙적인 주의(attention) 패턴을 보인다는 걸 발견했어. 우리는 PIP라는 독특한 방법을 제안하는데, 이건 무작위로 선택한 관련 없는 probe 질문(예: "시계가 있나요?")의 주의 패턴을 이용해 적대적 예제를 깔끔한 예제와 구별하는 거야.

검사할 이미지와 그에 해당하는 질문에 관계없이, PIP는 검사할 이미지와 probe 질문에 대해 한 번만 추가 추론을 하면 적대적 예제를 성공적으로 감지할 수 있어. 블랙박스 공격과 열린 데이터셋 상황에서도, PIP는 간단한 SVM과 결합해 98% 이상의 재현율과 90% 이상의 정밀도를 달성해.

우리의 PIP는 간단한 관련 없는 probe 질문을 통해 LVLM에서 적대적 공격을 감지하는 첫 시도야. 이로 인해 LVLM에 대한 더 깊은 이해와 성찰의 빛을 비추게 돼. 코드도 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05088
Title: Transformer with Leveraged Masked Autoencoder for video-based Pain Assessment

Original Abstract:
Accurate pain assessment is crucial in healthcare for effective diagnosis and treatment; however, traditional methods relying on self-reporting are inadequate for populations unable to communicate their pain. Cutting-edge AI is promising for supporting clinicians in pain recognition using facial video data. In this paper, we enhance pain recognition by employing facial video analysis within a Transformer-based deep learning model. By combining a powerful Masked Autoencoder with a Transformers-based classifier, our model effectively captures pain level indicators through both expressions and micro-expressions. We conducted our experiment on the AI4Pain dataset, which produced promising results that pave the way for innovative healthcare solutions that are both comprehensive and objective.

Translated Abstract:
정확한 통증 평가가 의료에서 효과적인 진단과 치료에 매우 중요해. 하지만, 자신이 느끼는 통증을 표현할 수 없는 사람들을 위해 기존의 자가 보고 방식은 부족해. 최신 AI 기술은 얼굴 비디오 데이터를 활용해 임상의들이 통증을 인식하는 데 도움을 줄 수 있는 가능성이 있어.

이 논문에서는 Transformer 기반의 딥러닝 모델을 사용해 얼굴 비디오 분석을 통해 통증 인식을 더 개선했어. 강력한 Masked Autoencoder와 Transformer 기반의 분류기를 결합해서, 우리의 모델은 표정과 미세 표정을 통해 통증 수준 지표를 효과적으로 포착할 수 있어.

우리는 AI4Pain 데이터셋을 이용해 실험을 진행했고, 유망한 결과를 얻었어. 이 결과는 포괄적이고 객관적인 혁신적인 의료 솔루션을 위한 길을 열어줄 거야.

================================================================================

URL: https://arxiv.org/abs/2409.05089
Title: Leveraging WaveNet for Dynamic Listening Head Modeling from Speech

Original Abstract:
The creation of listener facial responses aims to simulate interactive communication feedback from a listener during a face-to-face conversation. Our goal is to generate believable videos of listeners' heads that respond authentically to a single speaker by a sequence-to-sequence model with an combination of WaveNet and Long short-term memory network. Our approach focuses on capturing the subtle nuances of listener feedback, ensuring the preservation of individual listener identity while expressing appropriate attitudes and viewpoints. Experiment results show that our method surpasses the baseline models on ViCo benchmark Dataset.

Translated Abstract:
청중의 얼굴 반응을 만드는 것은 대화 중에 청중이 어떻게 반응하는지를 시뮬레이션하는 거야. 우리의 목표는 한 사람의 말을 듣고 자연스럽게 반응하는 청중의 머리 영상을 만드는 거야. 이를 위해 WaveNet과 장단기 메모리 네트워크를 조합한 시퀀스-투-시퀀스 모델을 사용하고 있어.

우리는 청중의 미세한 반응을 잘 포착하는 데 초점을 맞추고, 각 청중의 정체성을 유지하면서 적절한 태도와 관점을 표현하도록 하고 있어. 실험 결과, 우리의 방법이 ViCo 벤치마크 데이터셋에서 기존 모델들보다 더 나은 성능을 보였어.

================================================================================

URL: https://arxiv.org/abs/2409.05099
Title: DreamMapping: High-Fidelity Text-to-3D Generation via Variational Distribution Mapping

Original Abstract:
Score Distillation Sampling (SDS) has emerged as a prevalent technique for text-to-3D generation, enabling 3D content creation by distilling view-dependent information from text-to-2D guidance. However, they frequently exhibit shortcomings such as over-saturated color and excess smoothness. In this paper, we conduct a thorough analysis of SDS and refine its formulation, finding that the core design is to model the distribution of rendered images. Following this insight, we introduce a novel strategy called Variational Distribution Mapping (VDM), which expedites the distribution modeling process by regarding the rendered images as instances of degradation from diffusion-based generation. This special design enables the efficient training of variational distribution by skipping the calculations of the Jacobians in the diffusion U-Net. We also introduce timestep-dependent Distribution Coefficient Annealing (DCA) to further improve distilling precision. Leveraging VDM and DCA, we use Gaussian Splatting as the 3D representation and build a text-to-3D generation framework. Extensive experiments and evaluations demonstrate the capability of VDM and DCA to generate high-fidelity and realistic assets with optimization efficiency.

Translated Abstract:
Score Distillation Sampling (SDS)는 텍스트에서 3D로 생성하는 데 많이 쓰이는 기법으로, 텍스트를 2D로 변환하는 과정에서 얻은 정보를 바탕으로 3D 콘텐츠를 만드는 방식이야. 하지만 이 방법은 색상이 과하게 풍부하거나 너무 매끄럽게 나오는 문제점이 자주 발생해.

이 논문에서는 SDS에 대한 자세한 분석을 하고, 그 정의를 개선했어. 핵심은 렌더링된 이미지의 분포를 모델링하는 거라는 걸 알게 됐어. 이 아이디어를 바탕으로 Variational Distribution Mapping (VDM)이라는 새로운 전략을 도입했는데, 이는 렌더링된 이미지를 확산 기반 생성의 열화 사례로 보고 분포 모델링 과정을 더 빠르게 해줘. 이 특별한 설계 덕분에 확산 U-Net에서 Jacobian 계산을 생략하면서 변분 분포를 효율적으로 훈련할 수 있어.

또한, distilling 정확도를 높이기 위해 timestep에 따라 달라지는 Distribution Coefficient Annealing (DCA)도 도입했어. VDM과 DCA를 활용해서 Gaussian Splatting을 3D 표현으로 사용하고, 텍스트에서 3D로 생성하는 프레임워크를 구축했어. 다양한 실험과 평가를 통해 VDM과 DCA가 최적화 효율성을 가지고 높은 충실도와 현실적인 자산을 생성할 수 있음을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.05122
Title: PMT: Progressive Mean Teacher via Exploring Temporal Consistency for Semi-Supervised Medical Image Segmentation

Original Abstract:
Semi-supervised learning has emerged as a widely adopted technique in the field of medical image segmentation. The existing works either focuses on the construction of consistency constraints or the generation of pseudo labels to provide high-quality supervisory signals, whose main challenge mainly comes from how to keep the continuous improvement of model capabilities. In this paper, we propose a simple yet effective semi-supervised learning framework, termed Progressive Mean Teachers (PMT), for medical image segmentation, whose goal is to generate high-fidelity pseudo labels by learning robust and diverse features in the training process. Specifically, our PMT employs a standard mean teacher to penalize the consistency of the current state and utilizes two sets of MT architectures for co-training. The two sets of MT architectures are individually updated for prolonged periods to maintain stable model diversity established through performance gaps generated by iteration differences. Additionally, a difference-driven alignment regularizer is employed to expedite the alignment of lagging models with the representation capabilities of leading models. Furthermore, a simple yet effective pseudo-label filtering algorithm is employed for facile evaluation of models and selection of high-fidelity pseudo-labels outputted when models are operating at high performance for co-training purposes. Experimental results on two datasets with different modalities, i.e., CT and MRI, demonstrate that our method outperforms the state-of-the-art medical image segmentation approaches across various dimensions. The code is available at this https URL.

Translated Abstract:
반지도 학습은 의료 이미지 분할 분야에서 널리 사용되는 기법으로 자리 잡았어. 기존 연구들은 주로 일관성 제약을 만드는 것에 집중하거나, 높은 품질의 감독 신호를 제공하기 위해 가짜 레이블을 생성하는 데 초점을 맞췄어. 이 과정에서 가장 큰 도전 과제는 모델의 능력을 지속적으로 향상시키는 방법이야.

이번 논문에서는 "진행형 평균 교사(Progressive Mean Teachers, PMT)"라는 간단하면서도 효과적인 반지도 학습 프레임워크를 제안해. 이 프레임워크의 목표는 훈련 과정에서 강력하고 다양한 특징을 학습해 고품질의 가짜 레이블을 생성하는 거야. 구체적으로, PMT는 표준 평균 교사를 사용해 현재 상태의 일관성을 제한하고, 두 세트의 평균 교사 아키텍처를 동시에 훈련하는 방식을 사용해.

이 두 세트의 평균 교사 아키텍처는 각각 오랜 기간 동안 개별적으로 업데이트되어서, 반복 차이로 인해 발생하는 성능 격차를 통해 안정적인 모델 다양성을 유지해. 또한, 뒤처진 모델이 앞선 모델의 표현 능력과 정렬되도록 돕는 차이 기반 정렬 정규화기도 사용해. 추가로, 모델을 쉽게 평가하고, 고성능으로 작동할 때 출력되는 고품질 가짜 레이블을 선택할 수 있는 간단하면서도 효과적인 가짜 레이블 필터링 알고리즘도 도입했어.

CT와 MRI라는 서로 다른 두 데이터셋에서 실험한 결과, 우리의 방법이 다양한 차원에서 최신 의료 이미지 분할 접근법보다 더 나은 성능을 보였어.코드는 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05125
Title: PdfTable: A Unified Toolkit for Deep Learning-Based Table Extraction

Original Abstract:
Currently, a substantial volume of document data exists in an unstructured format, encompassing Portable Document Format (PDF) files and images. Extracting information from these documents presents formidable challenges due to diverse table styles, complex forms, and the inclusion of different languages. Several open-source toolkits, such as Camelot, Plumb a PDF (pdfnumber), and Paddle Paddle Structure V2 (PP-StructureV2), have been developed to facilitate table extraction from PDFs or images. However, each toolkit has its limitations. Camelot and pdfnumber can solely extract tables from digital PDFs and cannot handle image-based PDFs and pictures. On the other hand, PP-StructureV2 can comprehensively extract image-based PDFs and tables from pictures. Nevertheless, it lacks the ability to differentiate between diverse application scenarios, such as wired tables and wireless tables, digital PDFs, and image-based PDFs. To address these issues, we have introduced the PDF table extraction (PdfTable) toolkit. This toolkit integrates numerous open-source models, including seven table recognition models, four Optical character recognition (OCR) recognition tools, and three layout analysis models. By refining the PDF table extraction process, PdfTable achieves adaptability across various application scenarios. We substantiate the efficacy of the PdfTable toolkit through verification on a self-labeled wired table dataset and the open-source wireless Publicly Table Reconition Dataset (PubTabNet). The PdfTable code will available on Github: this https URL.

Translated Abstract:
현재 많은 문서 데이터가 비구조화된 형식으로 존재해. 여기에는 PDF 파일과 이미지가 포함돼. 이런 문서에서 정보를 추출하는 건 다양한 표 스타일, 복잡한 양식, 여러 언어가 섞여 있어서 쉽지 않아. 

Camelot, Plumb a PDF (pdfnumber), Paddle Paddle Structure V2 (PP-StructureV2) 같은 여러 오픈소스 툴킷이 PDF나 이미지에서 표를 추출하기 위해 개발되었어. 하지만 각 툴킷마다 한계가 있어. Camelot과 pdfnumber는 디지털 PDF에서만 표를 추출할 수 있고, 이미지 기반 PDF나 사진은 처리할 수 없어. 반면에 PP-StructureV2는 이미지 기반 PDF와 사진에서 표를 잘 추출할 수 있지만, 유선 표와 무선 표, 디지털 PDF와 이미지 기반 PDF 같은 다양한 상황을 구분할 수 없어.

이런 문제를 해결하기 위해 우리는 PDF 표 추출(PdfTable) 툴킷을 소개했어. 이 툴킷은 7개의 표 인식 모델, 4개의 광학 문자 인식(OCR) 도구, 3개의 레이아웃 분석 모델 등 여러 오픈소스 모델을 통합하고 있어. PdfTable은 PDF 표 추출 과정을 개선해서 다양한 애플리케이션 상황에 적응할 수 있도록 만들었어.

우리는 PdfTable 툴킷의 효과를 자체 라벨링된 유선 표 데이터셋과 오픈소스 무선 Publicly Table Recognition Dataset (PubTabNet)에서 검증했어. PdfTable 코드는 Github에서 사용할 수 있어: 이 https URL.

================================================================================

URL: https://arxiv.org/abs/2409.05142
Title: TanDepth: Leveraging Global DEMs for Metric Monocular Depth Estimation in UAVs

Original Abstract:
Aerial scene understanding systems face stringent payload restrictions and must often rely on monocular depth estimation for modelling scene geometry, which is an inherently ill-posed problem. Moreover, obtaining accurate ground truth data required by learning-based methods raises significant additional challenges in the aerial domain. Self-supervised approaches can bypass this problem, at the cost of providing only up-to-scale results. Similarly, recent supervised solutions which make good progress towards zero-shot generalization also provide only relative depth values. This work presents TanDepth, a practical, online scale recovery method for obtaining metric depth results from relative estimations at inference-time, irrespective of the type of model generating them. Tailored for Unmanned Aerial Vehicle (UAV) applications, our method leverages sparse measurements from Global Digital Elevation Models (GDEM) by projecting them to the camera view using extrinsic and intrinsic information. An adaptation to the Cloth Simulation Filter is presented, which allows selecting ground points from the estimated depth map to then correlate with the projected reference points. We evaluate and compare our method against alternate scaling methods adapted for UAVs, on a variety of real-world scenes. Considering the limited availability of data for this domain, we construct and release a comprehensive, depth-focused extension to the popular UAVid dataset to further research.

Translated Abstract:
항공 장면 이해 시스템은 무게 제한이 엄격해서, 장면 기하학을 모델링할 때 종종 단안 깊이 추정에 의존해야 해. 그런데 이게 본질적으로 해결하기 어려운 문제야. 게다가, 학습 기반 방법에서 필요한 정확한 실제 데이터 수집은 항공 분야에서 큰 도전 과제가 돼. 

자기 감독(Self-supervised) 방식은 이 문제를 우회할 수 있지만, 결과가 스케일에 맞춰진 것만 제공해. 최근의 감독(supervised) 방식도 제로샷 일반화(zero-shot generalization)에서 좋은 성과를 내지만, 여전히 상대 깊이 값만 제공하고 있어. 

이 연구에서는 TanDepth라는 실용적이고 온라인으로 스케일을 회복하는 방법을 제안해. 이 방법은 어떤 모델이 생성하든 상대적인 추정값에서 메트릭 깊이 결과를 얻을 수 있어. UAV(무인 항공기) 응용을 위해 맞춤 설계되었고, Global Digital Elevation Models(GDEM)에서 얻은 희소 측정값을 외부 및 내부 정보를 사용해 카메라 뷰로 투영해. 

Cloth Simulation Filter에 대한 적응법도 제시하는데, 이걸 통해 추정된 깊이 맵에서 지점들을 선택하고, 그 지점들이 투영된 기준점과 연관되도록 할 수 있어. 우리는 다양한 실제 장면에서 UAV에 맞춘 다른 스케일링 방법들과 비교하고 평가했어. 이 분야의 데이터가 제한적이라는 점을 고려해서, 우리는 인기 있는 UAVid 데이터셋에 깊이에 초점을 맞춘 포괄적인 확장을 만들어서 공개했어.

================================================================================

URL: https://arxiv.org/abs/2409.05151
Title: Ultron: Enabling Temporal Geometry Compression of 3D Mesh Sequences using Temporal Correspondence and Mesh Deformation

Original Abstract:
With the advancement of computer vision, dynamic 3D reconstruction techniques have seen significant progress and found applications in various fields. However, these techniques generate large amounts of 3D data sequences, necessitating efficient storage and transmission methods. Existing 3D model compression methods primarily focus on static models and do not consider inter-frame information, limiting their ability to reduce data size. Temporal mesh compression, which has received less attention, often requires all input meshes to have the same topology, a condition rarely met in real-world applications. This research proposes a method to compress mesh sequences with arbitrary topology using temporal correspondence and mesh deformation. The method establishes temporal correspondence between consecutive frames, applies a deformation model to transform the mesh from one frame to subsequent frames, and replaces the original meshes with deformed ones if the quality meets a tolerance threshold. Extensive experiments demonstrate that this method can achieve state-of-the-art performance in terms of compression performance. The contributions of this paper include a geometry and motion-based model for establishing temporal correspondence between meshes, a mesh quality assessment for temporal mesh sequences, an entropy-based encoding and corner table-based method for compressing mesh sequences, and extensive experiments showing the effectiveness of the proposed method. All the code will be open-sourced at this https URL.

Translated Abstract:
컴퓨터 비전 기술이 발전하면서, 동적인 3D 재구성 기술도 크게 발전했고 다양한 분야에 응용되고 있어. 하지만 이런 기술들은 많은 양의 3D 데이터 시퀀스를 만들어내기 때문에 효율적인 저장과 전송 방법이 필요해. 기존의 3D 모델 압축 방법들은 주로 정적인 모델에 초점을 맞추고 있어서 프레임 간 정보는 고려하지 않아, 데이터 크기를 줄이는 데 한계가 있어. 

시간적 메쉬 압축은 덜 주목받고 있지만, 모든 입력 메쉬가 같은 형태(토폴로지)를 가져야 하는 조건이 있어. 이런 조건은 실제 응용에서는 잘 맞지 않아. 이 연구에서는 임의의 토폴로지를 가진 메쉬 시퀀스를 압축하는 방법을 제안해. 이 방법은 연속된 프레임 사이의 시간적 대응관계를 설정하고, 변형 모델을 적용해서 한 프레임의 메쉬를 다음 프레임으로 변형해. 그리고 품질이 허용 기준을 만족하면 원래의 메쉬를 변형된 메쉬로 바꿔. 

광범위한 실험을 통해 이 방법이 압축 성능 면에서 최첨단의 성능을 달성할 수 있음을 보여줬어. 이 논문의 기여는 메쉬 간의 시간적 대응관계를 설정하기 위한 기하학 및 동작 기반 모델, 시간적 메쉬 시퀀스에 대한 메쉬 품질 평가, 메쉬 시퀀스를 압축하기 위한 엔트로피 기반 인코딩 및 코너 테이블 기반 방법, 그리고 제안된 방법의 효과성을 보여주는 많은 실험들이 포함돼. 모든 코드는 이 https URL에서 오픈소스로 제공될 거야.

================================================================================

URL: https://arxiv.org/abs/2409.05159
Title: Image color consistency in datasets: the Smooth-TPS3D method

Original Abstract:
Image color consistency is the key problem in digital imaging consistency when creating datasets. Here, we propose an improved 3D Thin-Plate Splines (TPS3D) color correction method to be used, in conjunction with color charts (i.e. Macbeth ColorChecker) or other machine-readable patterns, to achieve image consistency by post-processing. Also, we benchmark our method against its former implementation and the alternative methods reported to date with an augmented dataset based on the Gehler's ColorChecker dataset. Benchmark includes how corrected images resemble the ground-truth images and how fast these implementations are. Results demonstrate that the TPS3D is the best candidate to achieve image consistency. Furthermore, our Smooth-TPS3D method shows equivalent results compared to the original method and reduced the 11-15% of ill-conditioned scenarios which the previous method failed to less than 1%. Moreover, we demonstrate that the Smooth-TPS method is 20% faster than the original method. Finally, we discuss how different methods offer different compromises between quality, correction accuracy and computational load.

Translated Abstract:
이미지 색상 일관성은 데이터셋을 만들 때 디지털 이미지의 일관성에서 중요한 문제야. 여기서 우리는 색상 차트(예: Macbeth ColorChecker)나 다른 기계로 읽을 수 있는 패턴과 함께 사용할 수 있는 개선된 3D 얇은 판 스플라인(TPS3D) 색상 보정 방법을 제안해. 이 방법은 후처리를 통해 이미지 일관성을 달성하는 데 도움을 줘.

우리는 이 방법을 이전 버전과 현재까지 보고된 다른 방법들과 비교해봤어. 비교는 Gehler의 ColorChecker 데이터셋을 기반으로 한 확장된 데이터셋을 사용했어. 여기에는 보정된 이미지가 실제 이미지와 얼마나 유사한지, 그리고 이 구현들이 얼마나 빠른지를 포함해. 결과적으로 TPS3D가 이미지 일관성을 달성하는 데 가장 좋은 후보라는 걸 보여줬어.

게다가 우리 Smooth-TPS3D 방법은 원래 방법과 비슷한 결과를 냈고, 이전 방법이 실패했던 불량 상황을 11-15% 줄여서 1% 미만으로 만들었어. 그리고 Smooth-TPS 방법은 원래 방법보다 20% 더 빠르다는 것도 보여줬어. 마지막으로, 다양한 방법들이 품질, 보정 정확도, 계산 부하 사이에서 다른 타협점을 제공한다는 점도 논의했어.

================================================================================

URL: https://arxiv.org/abs/2409.05162
Title: Can OOD Object Detectors Learn from Foundation Models?

Original Abstract:
Out-of-distribution (OOD) object detection is a challenging task due to the absence of open-set OOD data. Inspired by recent advancements in text-to-image generative models, such as Stable Diffusion, we study the potential of generative models trained on large-scale open-set data to synthesize OOD samples, thereby enhancing OOD object detection. We introduce SyncOOD, a simple data curation method that capitalizes on the capabilities of large foundation models to automatically extract meaningful OOD data from text-to-image generative models. This offers the model access to open-world knowledge encapsulated within off-the-shelf foundation models. The synthetic OOD samples are then employed to augment the training of a lightweight, plug-and-play OOD detector, thus effectively optimizing the in-distribution (ID)/OOD decision boundaries. Extensive experiments across multiple benchmarks demonstrate that SyncOOD significantly outperforms existing methods, establishing new state-of-the-art performance with minimal synthetic data usage.

Translated Abstract:
Out-of-distribution (OOD) 물체 탐지는 오픈셋 OOD 데이터가 없어서 어려운 과제야. 최근의 텍스트-이미지 생성 모델, 예를 들어 Stable Diffusion에서 영감을 받아서, 우리는 대규모 오픈셋 데이터로 훈련된 생성 모델이 OOD 샘플을 합성하는 가능성을 연구했어. 이렇게 하면 OOD 물체 탐지를 향상시킬 수 있어.

우리는 SyncOOD라는 간단한 데이터 큐레이션 방법을 소개해. 이 방법은 대형 기본 모델의 능력을 활용해서 텍스트-이미지 생성 모델에서 의미 있는 OOD 데이터를 자동으로 추출해. 이렇게 하면 모델이 기존의 기본 모델에 담긴 오픈월드 지식에 접근할 수 있어.

합성된 OOD 샘플은 경량의 플러그 앤 플레이 OOD 탐지기를 훈련하는 데 사용돼. 이 과정을 통해 인디스트리뷰션(ID)과 OOD의 결정 경계를 효과적으로 최적화할 수 있어. 여러 벤치마크에서 진행한 실험 결과, SyncOOD가 기존 방법들보다 훨씬 뛰어난 성능을 보여주었고, 최소한의 합성 데이터 사용으로 새로운 최첨단 성능을 확립했어.

================================================================================

URL: https://arxiv.org/abs/2409.05166
Title: CD-NGP: A Fast Scalable Continual Representation for Dynamic Scenes

Original Abstract:
We present CD-NGP, which is a fast and scalable representation for 3D reconstruction and novel view synthesis in dynamic scenes. Inspired by continual learning, our method first segments input videos into multiple chunks, followed by training the model chunk by chunk, and finally, fuses features of the first branch and subsequent branches. Experiments on the prevailing DyNeRF dataset demonstrate that our proposed novel representation reaches a great balance between memory consumption, model size, training speed, and rendering quality. Specifically, our method consumes $85\%$ less training memory ($<14$GB) than offline methods and requires significantly lower streaming bandwidth ($<0.4$MB/frame) than other online alternatives.

Translated Abstract:
우리는 CD-NGP를 소개하는데, 이건 동적 장면에서 3D 재구성과 새로운 뷰 합성을 위한 빠르고 확장 가능한 표현 방식이야. 지속적인 학습에서 영감을 받아, 우리의 방법은 먼저 입력 비디오를 여러 조각으로 나눈 다음, 조각별로 모델을 훈련하고, 마지막으로 첫 번째 가지와 이후 가지의 특징을 합치는 방식이야.

주요 DyNeRF 데이터셋에서 실험해본 결과, 우리가 제안한 새로운 표현 방식은 메모리 소비, 모델 크기, 훈련 속도, 렌더링 품질 사이에서 좋은 균형을 이루는 걸 보여줬어. 특히, 우리의 방법은 오프라인 방법보다 훈련 메모리를 $85\%$ 덜 사용하고(14GB 미만), 다른 온라인 대안보다 스트리밍 대역폭도 훨씬 낮아서(프레임당 0.4MB 미만) 효율적이야.

================================================================================

URL: https://arxiv.org/abs/2409.05174
Title: Advanced Machine Learning Framework for Efficient Plant Disease Prediction

Original Abstract:
Recently, Machine Learning (ML) methods are built-in as an important component in many smart agriculture platforms. In this paper, we explore the new combination of advanced ML methods for creating a smart agriculture platform where farmers could reach out for assistance from the public, or a closed circle of experts. Specifically, we focus on an easy way to assist the farmers in understanding plant diseases where the farmers can get help to solve the issues from the members of the community. The proposed system utilizes deep learning techniques for identifying the disease of the plant from the affected image, which acts as an initial identifier. Further, Natural Language Processing techniques are employed for ranking the solutions posted by the user community. In this paper, a message channel is built on top of Twitter, a popular social media platform to establish proper communication among farmers. Since the effect of the solutions can differ based on various other parameters, we extend the use of the concept drift approach and come up with a good solution and propose it to the farmer. We tested the proposed framework on the benchmark dataset, and it produces accurate and reliable results.

Translated Abstract:
최근에 기계 학습(ML) 방법이 많은 스마트 농업 플랫폼에서 중요한 요소로 자리 잡고 있어. 이 논문에서는 농부들이 대중이나 전문가 그룹에 도움을 요청할 수 있는 스마트 농업 플랫폼을 만들기 위해 고급 ML 방법의 새로운 조합을 탐구해.

특히, 우리는 농부들이 식물 질병을 이해하는 데 도움을 줄 수 있는 간단한 방법에 집중하고 있어. 농부들은 지역 사회의 멤버들로부터 문제를 해결할 도움을 받을 수 있지. 제안된 시스템은 딥 러닝 기술을 이용해 피해를 입은 이미지에서 식물의 질병을 식별하는데, 이게 초기 식별자로 작용해.

또한, 자연어 처리 기술을 사용해서 사용자 커뮤니티에서 게시된 해결책을 순위 매기는 방법도 도입했어. 이 논문에서는 농부들 간의 올바른 소통을 위해 인기 있는 소셜 미디어 플랫폼인 트위터를 기반으로 메시지 채널을 구축했어. 해결책의 효과는 다양한 다른 변수에 따라 달라질 수 있으니까, 개념 변화를 적용하고 좋은 해결책을 제안해.

우리는 제안한 프레임워크를 벤치마크 데이터셋에서 테스트했는데, 정확하고 신뢰할 수 있는 결과를 만들어냈어.

================================================================================

URL: https://arxiv.org/abs/2409.05200
Title: Lung-DETR: Deformable Detection Transformer for Sparse Lung Nodule Anomaly Detection

Original Abstract:
Accurate lung nodule detection for computed tomography (CT) scan imagery is challenging in real-world settings due to the sparse occurrence of nodules and similarity to other anatomical structures. In a typical positive case, nodules may appear in as few as 3% of CT slices, complicating detection. To address this, we reframe the problem as an anomaly detection task, targeting rare nodule occurrences in a predominantly normal dataset. We introduce a novel solution leveraging custom data preprocessing and Deformable Detection Transformer (Deformable- DETR). A 7.5mm Maximum Intensity Projection (MIP) is utilized to combine adjacent lung slices into single images, reducing the slice count and decreasing nodule sparsity. This enhances spatial context, allowing for better differentiation between nodules and other structures such as complex vascular structures and bronchioles. Deformable-DETR is employed to detect nodules, with a custom focal loss function to better handle the imbalanced dataset. Our model achieves state-of-the-art performance on the LUNA16 dataset with an F1 score of 94.2% (95.2% recall, 93.3% precision) on a dataset sparsely populated with lung nodules that is reflective of real-world clinical data.

Translated Abstract:
CT 스캔 이미지에서 폐 결절을 정확하게 찾는 건 현실에서는 쉽지 않아. 결절이 드물게 나타나고 다른 해부학적 구조와 비슷하게 생겨서 그래. 일반적으로 양성 사례에서는 CT 슬라이스의 3%밖에 결절이 없을 때도 있어서 탐지가 어려워.

이 문제를 해결하기 위해, 우리는 이걸 이상 탐지 작업으로 다시 설정했어. 즉, 정상 데이터셋에서 드물게 나타나는 결절을 목표로 한 거야. 우리는 새로운 방법을 제안하는데, 이는 맞춤형 데이터 전처리와 Deformable Detection Transformer(Deformable-DETR)를 활용해.

7.5mm 최대 강도 투영(MIP)을 사용해서 인접한 폐 슬라이스를 하나의 이미지로 합쳐. 이렇게 하면 슬라이스 수가 줄어들고 결절이 더 많이 나타나게 돼. 이게 공간적 맥락을 향상시켜서 결절과 복잡한 혈관 구조, 세기관지 같은 다른 구조를 더 잘 구분할 수 있게 해.

Deformable-DETR를 사용해서 결절을 탐지하고, 맞춤형 초점 손실 함수를 적용해 데이터셋 불균형을 더 잘 처리할 수 있도록 했어. 우리 모델은 LUNA16 데이터셋에서 94.2%의 F1 점수(95.2% 재현율, 93.3% 정밀도)를 기록하며 최신 기술의 성능을 보여줬어. 이 데이터셋은 실제 임상 데이터를 반영하는 폐 결절이 드물게 포함된 거야.

================================================================================

URL: https://arxiv.org/abs/2409.05225
Title: Comparison of Two Augmentation Methods in Improving Detection Accuracy of Hemarthrosis

Original Abstract:
With the increase of computing power, machine learning models in medical imaging have been introduced to help in rending medical diagnosis and inspection, like hemophilia, a rare disorder in which blood cannot clot normally. Often, one of the bottlenecks of detecting hemophilia is the lack of data available to train the algorithm to increase the accuracy. As a possible solution, this research investigated whether introducing augmented data by data synthesis or traditional augmentation techniques can improve model accuracy, helping to diagnose the diseases. To tackle this research, features of ultrasound images were extracted by the pre-trained VGG-16, and similarities were compared by cosine similarity measure based on extracted features in different distributions among real images, synthetic images, and augmentation images (Real vs. Real, Syn vs. Syn, Real vs. Different Batches of Syn, Real vs. Augmentation Techniques). Model testing performance was investigated using EffientNet-B4 to recognize "blood" images with two augmentation methods. In addition, a gradient-weighted class activation mapping (Grad-CAM) visualization was used to interpret the unexpected results like loss of accuracy. Synthetic and real images do not show high similarity, with a mean similarity score of 0.4737. Synthetic batch 1 dataset and images by horizontal flip are more similar to the original images. Classic augmentation techniques and data synthesis can improve model accuracy, and data by traditional augmentation techniques have a better performance than synthetic data. In addition, the Grad-CAM heatmap figured out the loss of accuracy is due to a shift in the domain. Overall, this research found that two augmentation methods, data synthesis and traditional augmentation techniques, both can improve accuracy to a certain extent to help to diagnose rare diseases.

Translated Abstract:
컴퓨터 성능이 높아지면서, 의료 영상에서 머신러닝 모델이 도입되어 진단과 검사를 도와주고 있어. 예를 들어, 혈액이 정상적으로 응고되지 않는 희귀 질환인 혈우병 같은 경우 말이야. 혈우병을 감지하는 데 가장 큰 문제 중 하나는 알고리즘을 훈련할 데이터가 부족하다는 거야. 그래서 이 연구에서는 데이터 합성이나 전통적인 증강 기법을 통해 데이터를 늘리면 모델의 정확도를 높일 수 있는지 살펴봤어.

이 연구에서는 초음파 이미지의 특징을 사전 훈련된 VGG-16 모델로 추출하고, 실제 이미지, 합성 이미지, 증강 이미지 사이의 유사성을 코사인 유사도 측정으로 비교했어. 모델 테스트 성능은 "혈액" 이미지를 인식하기 위해 EffientNet-B4를 사용해서 두 가지 증강 방법으로 조사했어. 또한, 예상치 못한 결과를 해석하기 위해 그래디언트 가중치 클래스 활성화 맵(Grad-CAM) 시각화를 사용했어.

합성 이미지와 실제 이미지는 유사성이 높지 않았고, 평균 유사도 점수는 0.4737이었어. 합성 배치 1 데이터셋과 수평 뒤집기를 한 이미지는 원본 이미지와 더 유사했어. 전통적인 증강 기법과 데이터 합성이 모델의 정확도를 높일 수 있었고, 전통적인 증강 기법에서 나온 데이터가 합성 데이터보다 성능이 더 좋았어. Grad-CAM 히트맵을 통해 정확도 손실이 도메인 변화 때문이라는 것도 확인했어.

결론적으로, 이 연구는 데이터 합성과 전통적인 증강 기법 두 가지 모두 희귀 질환 진단에 도움이 될 만큼 정확도를 높일 수 있다는 것을 발견했어.

================================================================================

URL: https://arxiv.org/abs/2409.05230
Title: A Low-Computational Video Synopsis Framework with a Standard Dataset

Original Abstract:
Video synopsis is an efficient method for condensing surveillance videos. This technique begins with the detection and tracking of objects, followed by the creation of object tubes. These tubes consist of sequences, each containing chronologically ordered bounding boxes of a unique object. To generate a condensed video, the first step involves rearranging the object tubes to maximize the number of non-overlapping objects in each frame. Then, these tubes are stitched to a background image extracted from the source video. The lack of a standard dataset for the video synopsis task hinders the comparison of different video synopsis models. This paper addresses this issue by introducing a standard dataset, called SynoClip, designed specifically for the video synopsis task. SynoClip includes all the necessary features needed to evaluate various models directly and effectively. Additionally, this work introduces a video synopsis model, called FGS, with low computational cost. The model includes an empty-frame object detector to identify frames empty of any objects, facilitating efficient utilization of the deep object detector. Moreover, a tube grouping algorithm is proposed to maintain relationships among tubes in the synthesized video. This is followed by a greedy tube rearrangement algorithm, which efficiently determines the start time of each tube. Finally, the proposed model is evaluated using the proposed dataset. The source code, fine-tuned object detection model, and tutorials are available at this https URL.

Translated Abstract:
비디오 요약은 감시 비디오를 간편하게 압축하는 방법이야. 이 기술은 먼저 물체를 감지하고 추적하는 것으로 시작해. 그 다음에 물체 튜브를 만들어. 이 튜브는 고유한 물체의 경계 상자가 시간 순서대로 나열된 시퀀스로 구성돼. 

압축된 비디오를 만들기 위해서는 첫 번째 단계로 각 프레임에서 겹치지 않는 물체의 수를 최대화하도록 물체 튜브를 재배열해. 그 후에, 이 튜브를 원본 비디오에서 추출한 배경 이미지에 붙여넣어. 그런데 비디오 요약 작업을 위한 표준 데이터셋이 없어서 다양한 비디오 요약 모델을 비교하는 데 어려움이 있어. 이 논문은 비디오 요약 작업을 위해 특별히 설계된 표준 데이터셋인 SynoClip을 소개하면서 이 문제를 해결해. SynoClip은 다양한 모델을 직접적으로 그리고 효과적으로 평가하는 데 필요한 모든 기능을 포함하고 있어.

또한, 이 연구는 낮은 계산 비용의 비디오 요약 모델인 FGS를 소개해. 이 모델은 물체가 없는 빈 프레임을 감지하는 빈 프레임 물체 탐지기를 포함해서 딥 오브젝트 탐지기를 효율적으로 활용할 수 있도록 해. 게다가, 합성된 비디오에서 튜브 간의 관계를 유지하기 위한 튜브 그룹화 알고리즘도 제안해. 그 다음으로는 각 튜브의 시작 시간을 효율적으로 결정하는 탐욕적인 튜브 재배열 알고리즘이 이어져. 마지막으로, 제안된 데이터셋을 사용해서 모델을 평가해. 소스 코드, 미세 조정된 물체 탐지 모델, 그리고 튜토리얼은 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05243
Title: Mamba-Enhanced Text-Audio-Video Alignment Network for Emotion Recognition in Conversations

Original Abstract:
Emotion Recognition in Conversations (ERCs) is a vital area within multimodal interaction research, dedicated to accurately identifying and classifying the emotions expressed by speakers throughout a conversation. Traditional ERC approaches predominantly rely on unimodal cues\-such as text, audio, or visual data\-leading to limitations in their effectiveness. These methods encounter two significant challenges: 1) Consistency in multimodal information. Before integrating various modalities, it is crucial to ensure that the data from different sources is aligned and coherent. 2) Contextual information capture. Successfully fusing multimodal features requires a keen understanding of the evolving emotional tone, especially in lengthy dialogues where emotions may shift and develop over time. To address these limitations, we propose a novel Mamba-enhanced Text-Audio-Video alignment network (MaTAV) for the ERC task. MaTAV is with the advantages of aligning unimodal features to ensure consistency across different modalities and handling long input sequences to better capture contextual multimodal information. The extensive experiments on the MELD and IEMOCAP datasets demonstrate that MaTAV significantly outperforms existing state-of-the-art methods on the ERC task with a big margin.

Translated Abstract:
대화에서 감정 인식(ERC)은 멀티모달 상호작용 연구에서 중요한 분야야. 이 분야는 대화 중에 화자가 표현하는 감정을 정확하게 식별하고 분류하는 데 초점을 맞추고 있어. 전통적인 ERC 접근법은 주로 텍스트, 오디오, 시각 데이터와 같은 단일 모달 신호에 의존하는데, 이로 인해 효과에서 한계가 생겨.

이 방법들은 두 가지 큰 도전에 직면해 있어:  
1) 멀티모달 정보의 일관성. 다양한 모달리티를 통합하기 전에 서로 다른 출처의 데이터가 정렬되고 일관되도록 하는 게 중요해.  
2) 맥락 정보를 포착하는 것. 멀티모달 특징을 성공적으로 융합하려면, 특히 긴 대화에서 감정이 변화하고 발전할 때 그 감정의 흐름을 잘 이해해야 해.

이런 한계를 해결하기 위해, 우리는 ERC 작업을 위한 새로운 Mamba 강화 텍스트-오디오-비디오 정렬 네트워크(MaTAV)를 제안해. MaTAV는 단일 모달 특징을 정렬하여 다양한 모달리티 간의 일관성을 보장하고, 긴 입력 시퀀스를 처리해 맥락적 멀티모달 정보를 더 잘 포착할 수 있는 장점이 있어.  

MELD와 IEMOCAP 데이터셋에서의 광범위한 실험 결과, MaTAV는 ERC 작업에서 기존의 최첨단 방법들보다 큰 차이로 더 우수한 성능을 보였어.

================================================================================

URL: https://arxiv.org/abs/2409.05250
Title: MRStyle: A Unified Framework for Color Style Transfer with Multi-Modality Reference

Original Abstract:
In this paper, we introduce MRStyle, a comprehensive framework that enables color style transfer using multi-modality reference, including image and text. To achieve a unified style feature space for both modalities, we first develop a neural network called IRStyle, which generates stylized 3D lookup tables for image reference. This is accomplished by integrating an interaction dual-mapping network with a combined supervised learning pipeline, resulting in three key benefits: elimination of visual artifacts, efficient handling of high-resolution images with low memory usage, and maintenance of style consistency even in situations with significant color style variations. For text reference, we align the text feature of stable diffusion priors with the style feature of our IRStyle to perform text-guided color style transfer (TRStyle). Our TRStyle method is highly efficient in both training and inference, producing notable open-set text-guided transfer results. Extensive experiments in both image and text settings demonstrate that our proposed method outperforms the state-of-the-art in both qualitative and quantitative evaluations.

Translated Abstract:
이 논문에서는 MRStyle이라는 색상 스타일 전송을 위한 종합 프레임워크를 소개해. 이 프레임워크는 이미지와 텍스트 같은 여러 모달리티를 사용할 수 있어.

먼저, 두 가지 모달리티에 대해 통합된 스타일 특징 공간을 만들기 위해 IRStyle이라는 신경망을 개발했어. 이 네트워크는 이미지 참고를 위해 스타일화된 3D 룩업 테이블을 생성해. 이걸 위해 상호작용 이중 맵핑 네트워크와 결합된 감독 학습 파이프라인을 통합했어. 이렇게 해서 세 가지 주요 장점이 생겼어: 시각적 아티팩트 제거, 고해상도 이미지를 메모리 사용을 줄이면서 효율적으로 처리, 그리고 큰 색상 스타일 변동이 있어도 스타일 일관성을 유지하는 거야.

텍스트 참고를 위해서는, 안정적인 확산 선행 모델의 텍스트 특징을 IRStyle의 스타일 특징과 정렬해서 텍스트 유도 색상 스타일 전송(TRStyle)을 수행해. 우리 TRStyle 방법은 훈련과 추론 모두에서 매우 효율적이어서, 주목할 만한 개방형 텍스트 유도 전송 결과를 만들어.

이미지와 텍스트 설정 모두에서 광범위한 실험을 통해, 우리 방법이 정성적이고 정량적인 평가 모두에서 최신 기술보다 더 뛰어나다는 것을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.05260
Title: Scalable Frame Sampling for Video Classification: A Semi-Optimal Policy Approach with Reduced Search Space

Original Abstract:
Given a video with $T$ frames, frame sampling is a task to select $N \ll T$ frames, so as to maximize the performance of a fixed video classifier. Not just brute-force search, but most existing methods suffer from its vast search space of $\binom{T}{N}$, especially when $N$ gets large. To address this challenge, we introduce a novel perspective of reducing the search space from $O(T^N)$ to $O(T)$. Instead of exploring the entire $O(T^N)$ space, our proposed semi-optimal policy selects the top $N$ frames based on the independently estimated value of each frame using per-frame confidence, significantly reducing the computational complexity. We verify that our semi-optimal policy can efficiently approximate the optimal policy, particularly under practical settings. Additionally, through extensive experiments on various datasets and model architectures, we demonstrate that learning our semi-optimal policy ensures stable and high performance regardless of the size of $N$ and $T$.

Translated Abstract:
주어진 비디오가 $T$ 프레임으로 구성되어 있을 때, 프레임 샘플링은 $N \ll T$ 프레임을 선택해서 고정된 비디오 분류기의 성능을 극대화하는 작업이야. 단순히 모든 경우를 다 시도하는 게 아니라, 기존의 방법들은 $\binom{T}{N}$이라는 방대한 검색 공간 때문에 어려움을 겪고 있어, 특히 $N$이 커질수록 더 그렇지.

이 문제를 해결하기 위해 우리는 검색 공간을 $O(T^N)$에서 $O(T)$로 줄이는 새로운 접근 방식을 제안해. 전체 $O(T^N)$ 공간을 탐색하는 대신, 우리 방법은 각 프레임의 신뢰도를 독립적으로 추정해서 상위 $N$ 프레임을 선택해. 이렇게 하면 계산 복잡도가 크게 줄어들어.

우리는 우리의 반최적 정책이 실제 환경에서도 최적 정책을 효율적으로 근사할 수 있음을 검증했어. 그리고 다양한 데이터셋과 모델 아키텍처에 대한 광범위한 실험을 통해, 우리의 반최적 정책을 학습하면 $N$과 $T$의 크기에 관계없이 안정적이고 높은 성능을 보장한다는 것을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.05277
Title: Disentangled Representations for Short-Term and Long-Term Person Re-Identification

Original Abstract:
We address the problem of person re-identification (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. A key challenge is to learn person representations robust to intra-class variations, as different persons could have the same attribute, and persons' appearances look different, e.g., with viewpoint changes. Recent reID methods focus on learning person features discriminative only for a particular factor of variations (e.g., human pose), which also requires corresponding supervisory signals (e.g., pose annotations). To tackle this problem, we propose to factorize person images into identity-related and unrelated features. Identity-related features contain information useful for specifying a particular person (e.g., clothing), while identity-unrelated ones hold other factors (e.g., human pose). To this end, we propose a new generative adversarial network, dubbed identity shuffle GAN (IS-GAN). It disentangles identity-related and unrelated features from person images through an identity-shuffling technique that exploits identification labels alone without any auxiliary supervisory signals. We restrict the distribution of identity-unrelated features or encourage the identity-related and unrelated features to be uncorrelated, facilitating the disentanglement process. Experimental results validate the effectiveness of IS-GAN, showing state-of-the-art performance on standard reID benchmarks, including Market-1501, CUHK03, and DukeMTMC-reID. We further demonstrate the advantages of disentangling person representations on a long-term reID task, setting a new state of the art on a Celeb-reID dataset.

Translated Abstract:
우리는 사람 재식별(person re-identification, reID) 문제를 다뤄. 이건 특정 사람의 이미지를 쿼리 이미지로 주었을 때, 큰 데이터셋에서 그 사람의 이미지를 찾는 거야. 

주요 도전 과제는 클래스 내 변동성에 강한 사람 표현을 배우는 거야. 같은 속성을 가진 다른 사람들이 있을 수 있고, 사람들의 외모가 다르게 보일 수 있거든, 예를 들어 시점이 바뀌면 말이야. 최근 reID 방법들은 특정 변동 요인(예: 사람의 포즈)만을 구별할 수 있는 사람 특징을 배우는 데 집중하고, 이 과정에서 해당 감독 신호(예: 포즈 주석)가 필요해.

이 문제를 해결하기 위해, 우리는 사람 이미지를 정체성과 관련된 특징과 관련이 없는 특징으로 나누는 방법을 제안해. 정체성과 관련된 특징은 특정 사람을 지정하는 데 유용한 정보(예: 옷)들을 포함하고, 정체성과 관련이 없는 특징은 다른 요인(예: 사람의 포즈)을 담고 있어. 

이를 위해, 우리는 identity shuffle GAN (IS-GAN)이라고 불리는 새로운 생성적 적대 신경망을 제안해. 이 네트워크는 식별 레이블만을 이용해 정체성과 관련된 특징과 관련이 없는 특징을 사람 이미지에서 분리해. 우리는 정체성과 관련이 없는 특징의 분포를 제한하거나, 정체성과 관련된 특징과 관련이 없는 특징이 서로 상관이 없도록 유도해, 분리 과정을 쉽게 만들어.

실험 결과 IS-GAN의 효과를 입증했어. Market-1501, CUHK03, DukeMTMC-reID 같은 표준 reID 기준에서 최첨단 성능을 보여줬어. 또한, 우리는 사람 표현을 분리하는 것의 장점을 장기 reID 작업에서도 증명하며, Celeb-reID 데이터셋에서 새로운 최첨단 기록을 세웠어.

================================================================================

URL: https://arxiv.org/abs/2409.05279
Title: BrainDecoder: Style-Based Visual Decoding of EEG Signals

Original Abstract:
Decoding neural representations of visual stimuli from electroencephalography (EEG) offers valuable insights into brain activity and cognition. Recent advancements in deep learning have significantly enhanced the field of visual decoding of EEG, primarily focusing on reconstructing the semantic content of visual stimuli. In this paper, we present a novel visual decoding pipeline that, in addition to recovering the content, emphasizes the reconstruction of the style, such as color and texture, of images viewed by the subject. Unlike previous methods, this ``style-based'' approach learns in the CLIP spaces of image and text separately, facilitating a more nuanced extraction of information from EEG signals. We also use captions for text alignment simpler than previously employed, which we find work better. Both quantitative and qualitative evaluations show that our method better preserves the style of visual stimuli and extracts more fine-grained semantic information from neural signals. Notably, it achieves significant improvements in quantitative results and sets a new state-of-the-art on the popular Brain2Image dataset.

Translated Abstract:
신경망으로부터 시각 자극의 표현을 해독하는 것은 뇌 활동과 인지에 대한 귀중한 통찰을 제공합니다. 최근 딥러닝의 발전 덕분에 EEG의 시각 해독 분야가 크게 향상되었고, 주로 시각 자극의 의미 내용을 복원하는 데 집중하고 있습니다. 

이 논문에서는 새로운 시각 해독 파이프라인을 제안합니다. 이 방법은 내용 복원뿐만 아니라, 피험자가 본 이미지의 색상이나 질감 같은 스타일 복원도 강조합니다. 이전 방법들과 달리, 이 "스타일 기반" 접근법은 이미지와 텍스트의 CLIP 공간에서 각각 학습하여 EEG 신호에서 정보를 더 세밀하게 추출할 수 있게 합니다. 

또한, 우리는 이전보다 더 간단한 텍스트 정렬을 위해 캡션을 사용하고, 이 방법이 더 잘 작동한다는 것을 발견했습니다. 정량적 및 정성적 평가 모두에서 우리의 방법이 시각 자극의 스타일을 더 잘 보존하고 신경 신호에서 더 세밀한 의미 정보를 추출한다는 것을 보여줍니다. 특히, 정량적 결과에서 큰 개선을 이루었고, 인기 있는 Brain2Image 데이터셋에서 새로운 최첨단 성과를 달성했습니다.

================================================================================

URL: https://arxiv.org/abs/2409.05280
Title: RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation

Original Abstract:
Cardiovascular disease is a major global health concern, contributing significantly to global mortality. Accurately segmenting cardiac medical imaging data is crucial for reducing fatality rates associated with these conditions. However, current state-of-the-art (SOTA) neural networks, including CNN-based and Transformer-based approaches, face challenges in capturing both inter-slice connections and intra-slice details, especially in datasets featuring intricate, long-range details along the z-axis like coronary arteries. Existing methods also struggle with differentiating non-cardiac components from the myocardium, resulting in segmentation inaccuracies and the "spraying" phenomenon. To address these issues, we introduce RotCAtt-TransUNet++, a novel architecture designed for robust segmentation of intricate cardiac structures. Our approach enhances global context modeling through multiscale feature aggregation and nested skip connections in the encoder. Transformer layers facilitate capturing intra-slice interactions, while a rotatory attention mechanism handles inter-slice connectivity. A channel-wise cross-attention gate integrates multiscale information and decoder features, effectively bridging semantic gaps. Experimental results across multiple datasets demonstrate superior performance over current methods, achieving near-perfect annotation of coronary arteries and myocardium. Ablation studies confirm that our rotatory attention mechanism significantly improves segmentation accuracy by transforming embedded vectorized patches in semantic dimensional space.

Translated Abstract:
심혈관 질환은 전 세계적으로 큰 건강 문제로, 사망률에 큰 영향을 미치고 있어. 심장 의료 이미지를 정확하게 분할하는 건 이러한 질환과 관련된 사망률을 줄이는 데 매우 중요해. 하지만 현재의 최신 신경망 모델, 특히 CNN 기반과 Transformer 기반 방법들은 슬라이스 간의 연결과 슬라이스 내부의 세부 사항을 잘 잡아내는 데 어려움을 겪고 있어. 특히 심장 동맥 같은 복잡하고 긴 세부 사항이 포함된 데이터셋에서는 더욱 그렇고. 기존 방법들은 심근과 비심장 성분을 구분하는 데도 힘들어서 분할 오류와 "스프레이" 현상이 발생해.

이런 문제를 해결하기 위해 우리는 RotCAtt-TransUNet++라는 새로운 구조를 제안해. 이 구조는 복잡한 심장 구조를 견고하게 분할하도록 설계됐어. 우리 방법은 멀티스케일 특징 집합과 중첩된 스킵 연결을 통해 전반적인 맥락 모델링을 개선해. Transformer 레이어는 슬라이스 내부 상호작용을 잡아내는 데 도움을 주고, 회전 주의 메커니즘은 슬라이스 간 연결성을 처리해.

채널 별 교차 주의 게이트는 멀티스케일 정보와 디코더 특징을 통합해, 의미적 간극을 효과적으로 메워. 여러 데이터셋에서 실험 결과를 보면 현재 방법들보다 성능이 훨씬 뛰어나서 심장 동맥과 심근에 대해 거의 완벽한 주석을 달 수 있었어. 또한, 소거 연구를 통해 우리의 회전 주의 메커니즘이 의미적 차원 공간에서 벡터화된 패치를 변환함으로써 분할 정확도를 크게 향상시킨다는 것을 확인했어.

================================================================================

URL: https://arxiv.org/abs/2409.05307
Title: RAL:Redundancy-Aware Lipreading Model Based on Differential Learning with Symmetric Views

Original Abstract:
Lip reading involves interpreting a speaker's speech by analyzing sequences of lip movements. Currently, most models regard the left and right halves of the lips as a symmetrical whole, lacking a thorough investigation of their differences. However, the left and right halves of the lips are not always symmetrical, and the subtle differences between them contain rich semantic information. In this paper, we propose a differential learning strategy with symmetric views (DLSV) to address this issue. Additionally, input images often contain a lot of redundant information unrelated to recognition results, which can degrade the model's performance. We present a redundancy-aware operation (RAO) to reduce it. Finally, to leverage the relational information between symmetric views and within each view, we further design an adaptive cross-view interaction module (ACVI). Experiments on LRW and LRW-1000 datasets fully demonstrate the effectiveness of our approach.

Translated Abstract:
입술 읽기란 말하는 사람의 입술 움직임을 분석해서 말을 이해하는 방법이야. 지금까지 대부분의 모델은 입술의 왼쪽과 오른쪽을 대칭적인 하나로 보고, 그 차이에 대해 깊이 있게 연구하지 않았어. 하지만 왼쪽과 오른쪽 입술은 항상 대칭적이지 않고, 그 사이의 미세한 차이가 중요한 의미 정보를 담고 있어.

이 논문에서는 이런 문제를 해결하기 위해 대칭적 관점을 가진 차별적 학습 전략(DLSV)을 제안해. 게다가 입력 이미지에는 인식 결과와 관련 없는 불필요한 정보가 많이 포함되어 있어서 모델 성능이 떨어질 수 있어. 이를 줄이기 위해 불필요한 정보 인식 작업(RAO)을 제시해.

마지막으로 대칭적 관점 간의 관계 정보와 각 관점 내의 관계 정보를 활용하기 위해 적응형 교차 관점 상호작용 모듈(ACVI)을 추가로 디자인했어. LRW와 LRW-1000 데이터셋에서 실험한 결과, 우리의 접근법이 효과적이라는 걸 충분히 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.05311
Title: Fitting Skeletal Models via Graph-based Learning

Original Abstract:
Skeletonization is a popular shape analysis technique that models an object's interior as opposed to just its boundary. Fitting template-based skeletal models is a time-consuming process requiring much manual parameter tuning. Recently, machine learning-based methods have shown promise for generating s-reps from object boundaries. In this work, we propose a new skeletonization method which leverages graph convolutional networks to produce skeletal representations (s-reps) from dense segmentation masks. The method is evaluated on both synthetic data and real hippocampus segmentations, achieving promising results and fast inference.

Translated Abstract:
스켈레타이제이션은 객체의 경계뿐만 아니라 내부를 모델링하는 인기 있는 형태 분석 기법이야. 기존의 템플릿 기반 스켈레탈 모델을 맞추는 건 시간이 많이 걸리고 수동으로 매개변수를 조정해야 해서 귀찮아.

최근에는 머신러닝 기반 방법들이 객체 경계에서 s-rep을 생성하는 데 가능성을 보여줬어. 이 연구에서는 그래프 컨볼루션 네트워크를 활용해서 밀집 분할 마스크에서 스켈레탈 표현(s-reps)을 만드는 새로운 스켈레타이제이션 방법을 제안해. 

이 방법은 합성 데이터와 실제 해마 분할에 대해 평가했는데, 괜찮은 결과와 빠른 추론 속도를 기록했어.

================================================================================

URL: https://arxiv.org/abs/2409.05312
Title: Open-World Dynamic Prompt and Continual Visual Representation Learning

Original Abstract:
The open world is inherently dynamic, characterized by ever-evolving concepts and distributions. Continual learning (CL) in this dynamic open-world environment presents a significant challenge in effectively generalizing to unseen test-time classes. To address this challenge, we introduce a new practical CL setting tailored for open-world visual representation learning. In this setting, subsequent data streams systematically introduce novel classes that are disjoint from those seen in previous training phases, while also remaining distinct from the unseen test classes. In response, we present Dynamic Prompt and Representation Learner (DPaRL), a simple yet effective Prompt-based CL (PCL) method. Our DPaRL learns to generate dynamic prompts for inference, as opposed to relying on a static prompt pool in previous PCL methods. In addition, DPaRL jointly learns dynamic prompt generation and discriminative representation at each training stage whereas prior PCL methods only refine the prompt learning throughout the process. Our experimental results demonstrate the superiority of our approach, surpassing state-of-the-art methods on well-established open-world image retrieval benchmarks by an average of 4.7\% improvement in Recall@1 performance.

Translated Abstract:
열린 세계는 본질적으로 역동적이며, 끊임없이 변화하는 개념과 분포로 특징지어져 있어. 이런 역동적인 열린 세계 환경에서 지속적인 학습(Continual Learning, CL)은 보지 못한 테스트 클래스에 효과적으로 일반화하는 데 큰 도전이 돼. 이 문제를 해결하기 위해, 우리는 열린 세계 시각 표현 학습에 맞춘 새로운 실용적인 CL 설정을 소개해.

이 설정에서는 이후의 데이터 스트림이 이전 훈련 단계에서 본 클래스와는 전혀 다른 새로운 클래스를 체계적으로 도입해. 그리고 이 새로운 클래스는 보지 못한 테스트 클래스와도 구별돼. 이에 대한 대응으로, 우리는 동적 프롬프트와 표현 학습기(Dynamic Prompt and Representation Learner, DPaRL)를 제안해. DPaRL은 기존의 정적인 프롬프트 풀에 의존하는 대신, 추론을 위한 동적 프롬프트를 생성하는 방법이야.

또한, DPaRL은 각 훈련 단계에서 동적 프롬프트 생성과 판별 표현을 함께 학습해. 기존의 PCL 방법들은 과정 내내 프롬프트 학습만 다듬었던 반면, DPaRL은 다르게 접근해. 실험 결과, 우리의 방법이 잘 알려진 열린 세계 이미지 검색 기준에서 평균 4.7%의 Recall@1 성능 향상을 보여주며 최신 기술들을 능가하는 것을 입증했어.

================================================================================

URL: https://arxiv.org/abs/2409.05324
Title: FIF-UNet: An Efficient UNet Using Feature Interaction and Fusion for Medical Image Segmentation

Original Abstract:
Nowadays, pre-trained encoders are widely used in medical image segmentation because of their ability to capture complex feature representations. However, the existing models fail to effectively utilize the rich features obtained by the pre-trained encoder, resulting in suboptimal segmentation results. In this work, a novel U-shaped model, called FIF-UNet, is proposed to address the above issue, including three plug-and-play modules. A channel spatial interaction module (CSI) is proposed to obtain informative features by establishing the interaction between encoder stages and corresponding decoder stages. A cascaded conv-SE module (CoSE) is designed to enhance the representation of critical features by adaptively assigning importance weights on different feature channels. A multi-level fusion module (MLF) is proposed to fuse the multi-scale features from the decoder stages, ensuring accurate and robust final segmentation. Comprehensive experiments on the Synapse and ACDC datasets demonstrate that the proposed FIF-UNet outperforms existing state-of-the-art methods, which achieves the highest average DICE of 86.05% and 92.58%, respectively.

Translated Abstract:
요즘 의료 이미지 분할에서는 복잡한 특징 표현을 잘 포착할 수 있는 사전 훈련된 인코더가 많이 사용되고 있어. 하지만 기존 모델들은 이 사전 훈련된 인코더가 얻은 풍부한 특징을 효과적으로 활용하지 못해서 분할 결과가 최적이 아닌 경우가 많아. 

이 연구에서는 이런 문제를 해결하기 위해 FIF-UNet이라는 새로운 U자형 모델을 제안해. 이 모델에는 세 가지 플러그 앤 플레이 모듈이 포함되어 있어. 첫 번째는 채널 공간 상호작용 모듈(CSI)로, 인코더 단계와 해당 디코더 단계 간의 상호작용을 통해 유용한 특징을 얻는 데 도움을 줘. 두 번째는 연속된 conv-SE 모듈(CoSE)로, 중요한 특징의 표현을 강화하기 위해 다양한 특징 채널에 중요 가중치를 적응적으로 부여해. 마지막으로 다중 수준 융합 모듈(MLF)은 디코더 단계에서 나온 다중 스케일 특징을 융합해서 정확하고 강력한 최종 분할을 보장해.

Synapse와 ACDC 데이터셋에서 실시한 종합적인 실험 결과, 제안한 FIF-UNet이 기존 최첨단 방법들보다 뛰어난 성능을 보였고, 각각 86.05%와 92.58%의 최고 평균 DICE를 달성했어.

================================================================================

URL: https://arxiv.org/abs/2409.05327
Title: ICPR 2024 Competition on Safe Segmentation of Drive Scenes in Unstructured Traffic and Adverse Weather Conditions

Original Abstract:
The ICPR 2024 Competition on Safe Segmentation of Drive Scenes in Unstructured Traffic and Adverse Weather Conditions served as a rigorous platform to evaluate and benchmark state-of-the-art semantic segmentation models under challenging conditions for autonomous driving. Over several months, participants were provided with the IDD-AW dataset, consisting of 5000 high-quality RGB-NIR image pairs, each annotated at the pixel level and captured under adverse weather conditions such as rain, fog, low light, and snow. A key aspect of the competition was the use and improvement of the Safe mean Intersection over Union (Safe mIoU) metric, designed to penalize unsafe incorrect predictions that could be overlooked by traditional mIoU. This innovative metric emphasized the importance of safety in developing autonomous driving systems. The competition showed significant advancements in the field, with participants demonstrating models that excelled in semantic segmentation and prioritized safety and robustness in unstructured and adverse conditions. The results of the competition set new benchmarks in the domain, highlighting the critical role of safety in deploying autonomous vehicles in real-world scenarios. The contributions from this competition are expected to drive further innovation in autonomous driving technology, addressing the critical challenges of operating in diverse and unpredictable environments.

Translated Abstract:
ICPR 2024 대회는 자율주행에서의 안전한 장면 분할을 평가하는 플랫폼이었어. 이 대회는 비정형 교통 상황과 나쁜 날씨에서 최첨단 의미 분할 모델을 테스트하는 데 초점을 맞췄어.

참가자들은 몇 달 동안 IDD-AW 데이터셋을 받았고, 이 데이터셋은 5000개의 고화질 RGB-NIR 이미지 쌍으로 구성되어 있었어. 각 이미지는 픽셀 단위로 주석이 달려 있었고, 비, 안개, 저조도, 눈 같은 힘든 날씨 조건에서 촬영됐어.

대회의 중요한 부분은 '안전 평균 교차 비율(Safe mIoU)'이라는 새로운 평가 지표였어. 이 지표는 전통적인 mIoU가 간과할 수 있는 위험한 잘못된 예측에 대해 벌점을 주는 방식이야. 이 혁신적인 지표는 자율주행 시스템 개발에서 안전이 얼마나 중요한지를 강조했어.

대회 결과, 참가자들은 의미 분할에서 뛰어난 모델을 선보였고, 비정형 및 나쁜 조건에서도 안전성과 견고함을 우선시했어. 이 대회는 새로운 기준을 세웠고, 실제 상황에서 자율주행 차량을 배치하는 데 안전이 얼마나 중요한지를 부각했어.

이 대회에서 나온 기여는 자율주행 기술의 혁신을 이끌 것으로 기대돼, 다양한 예측 불가능한 환경에서 작동하는 데 필요한 중요한 문제들을 해결하는 데 도움이 될 거야.

================================================================================

URL: https://arxiv.org/abs/2409.05330
Title: KAN-Based Fusion of Dual-Domain for Audio-Driven Facial Landmarks Generation

Original Abstract:
Audio-driven talking face generation is a widely researched topic due to its high applicability. Reconstructing a talking face using audio significantly contributes to fields such as education, healthcare, online conversations, virtual assistants, and virtual reality. Early studies often focused solely on changing the mouth movements, which resulted in outcomes with limited practical applications. Recently, researchers have proposed a new approach of constructing the entire face, including face pose, neck, and shoulders. To achieve this, they need to generate through landmarks. However, creating stable landmarks that align well with the audio is a challenge. In this paper, we propose the KFusion of Dual-Domain model, a robust model that generates landmarks from audio. We separate the audio into two distinct domains to learn emotional information and facial context, then use a fusion mechanism based on the KAN model. Our model demonstrates high efficiency compared to recent models. This will lay the groundwork for the development of the audio-driven talking face generation problem in the future.

Translated Abstract:
오디오 기반의 말하는 얼굴 생성은 적용 가능성이 높아서 많이 연구되고 있는 주제야. 오디오를 이용해 말하는 얼굴을 재구성하는 건 교육, 헬스케어, 온라인 대화, 가상 비서, 가상 현실 같은 분야에 큰 도움이 돼. 

초기 연구들은 주로 입 움직임만 변화시키는 데 집중했는데, 이로 인해 실제 응용이 제한된 결과가 나왔어. 최근에 연구자들은 얼굴 포즈, 목, 어깨까지 포함해서 전체 얼굴을 만드는 새로운 접근법을 제안했어. 이를 위해 랜드마크를 통해 생성해야 하는데, 오디오와 잘 맞는 안정적인 랜드마크를 만드는 게 도전 과제가 되었지.

이 논문에서는 KFusion of Dual-Domain 모델을 제안해. 이 모델은 오디오로부터 랜드마크를 생성하는 강력한 모델이야. 우리는 오디오를 두 개의 다른 영역으로 나누어서 감정 정보와 얼굴 맥락을 학습한 다음, KAN 모델을 기반으로 한 융합 메커니즘을 사용해. 

우리 모델은 최근 모델들에 비해 높은 효율성을 보여줘. 이 연구는 앞으로 오디오 기반 말하는 얼굴 생성 문제의 발전을 위한 기초가 될 거야.

================================================================================

URL: https://arxiv.org/abs/2409.05334
Title: Lagrangian Hashing for Compressed Neural Field Representations

Original Abstract:
We present Lagrangian Hashing, a representation for neural fields combining the characteristics of fast training NeRF methods that rely on Eulerian grids (i.e.~InstantNGP), with those that employ points equipped with features as a way to represent information (e.g. 3D Gaussian Splatting or PointNeRF). We achieve this by incorporating a point-based representation into the high-resolution layers of the hierarchical hash tables of an InstantNGP representation. As our points are equipped with a field of influence, our representation can be interpreted as a mixture of Gaussians stored within the hash table. We propose a loss that encourages the movement of our Gaussians towards regions that require more representation budget to be sufficiently well represented. Our main finding is that our representation allows the reconstruction of signals using a more compact representation without compromising quality.

Translated Abstract:
우리는 Lagrangian Hashing이라는 걸 소개해. 이건 신경 필드를 표현하는 방법인데, 빠르게 학습할 수 있는 NeRF 방식과 포인트 기반의 정보 표현 방식을 결합한 거야. 예를 들어, InstantNGP 같은 방식은 유러리안 그리드를 사용하고, 3D Gaussian Splatting이나 PointNeRF 같은 방식은 특징이 있는 포인트를 사용하는 방식이야.

우리는 InstantNGP의 계층적 해시 테이블의 고해상도 레이어에 포인트 기반 표현을 통합해서 이걸 이루었어. 우리의 포인트는 영향 범위를 가지기 때문에, 이 표현은 해시 테이블에 저장된 가우시안 혼합물로 해석될 수 있어. 우리는 우리의 가우시안이 더 많은 표현 예산이 필요한 지역으로 이동하도록 유도하는 손실 함수를 제안했어.

우리의 주요 발견은 이 표현 방식이 품질을 해치지 않으면서 더 컴팩트한 표현으로 신호를 재구성할 수 있게 해준다는 거야.

================================================================================

URL: https://arxiv.org/abs/2409.05336
Title: Early-exit Convolutional Neural Networks

Original Abstract:
This paper is aimed at developing a method that reduces the computational cost of convolutional neural networks (CNN) during inference. Conventionally, the input data pass through a fixed neural network architecture. However, easy examples can be classified at early stages of processing and conventional networks do not take this into account. In this paper, we introduce 'Early-exit CNNs', EENets for short, which adapt their computational cost based on the input by stopping the inference process at certain exit locations. In EENets, there are a number of exit blocks each of which consists of a confidence branch and a softmax branch. The confidence branch computes the confidence score of exiting (i.e. stopping the inference process) at that location; while the softmax branch outputs a classification probability vector. Both branches are learnable and their parameters are separate. During training of EENets, in addition to the classical classification loss, the computational cost of inference is taken into account as well. As a result, the network adapts its many confidence branches to the inputs so that less computation is spent for easy examples. Inference works as in conventional feed-forward networks, however, when the output of a confidence branch is larger than a certain threshold, the inference stops for that specific example. The idea of EENets is applicable to available CNN architectures such as ResNets. Through comprehensive experiments on MNIST, SVHN, CIFAR10 and Tiny-ImageNet datasets, we show that early-exit (EE) ResNets achieve similar accuracy with their non-EE versions while reducing the computational cost to 20% of the original. Code is available at this https URL

Translated Abstract:
이 논문은 추론 과정에서 컨볼루션 신경망(CNN)의 계산 비용을 줄이는 방법을 개발하는 게 목표야. 일반적으로 입력 데이터는 고정된 신경망 구조를 통과하는데, 쉬운 예제들은 처리 초기 단계에서 분류될 수 있어. 하지만 기존 네트워크는 이걸 고려하지 않아.

그래서 우리는 '얼리 엑시트 CNNs', 줄여서 EENets를 소개해. EENets는 입력에 따라 계산 비용을 조절하면서 특정 엑시트 위치에서 추론 과정을 멈출 수 있어. EENets에는 여러 개의 엑시트 블록이 있는데, 각 블록은 신뢰도 브랜치와 소프트맥스 브랜치로 구성돼. 신뢰도 브랜치는 해당 위치에서 추론을 멈출 확률 점수를 계산하고, 소프트맥스 브랜치는 분류 확률 벡터를 출력해. 두 브랜치는 학습 가능하고, 파라미터가 따로 있어.

EENets 훈련할 때는 전통적인 분류 손실 외에도 추론의 계산 비용도 고려해. 그래서 네트워크는 많은 신뢰도 브랜치를 입력에 맞춰 조정해서 쉬운 예제에 대해선 덜 계산하게 돼. 추론은 기존의 피드포워드 네트워크처럼 작동하지만, 신뢰도 브랜치의 출력이 특정 임계값보다 크면 그 특정 예제에 대해 추론이 멈춰.

EENets 아이디어는 ResNets와 같은 기존 CNN 아키텍처에 적용할 수 있어. MNIST, SVHN, CIFAR10, Tiny-ImageNet 데이터셋에서 포괄적인 실험을 통해, 얼리 엑시트(EE) ResNets가 비-EE 버전과 비슷한 정확도를 달성하면서도 계산 비용을 원래의 20%로 줄일 수 있음을 보여줬어. 코드는 이 https URL에서 볼 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05352
Title: Driving with Prior Maps: Unified Vector Prior Encoding for Autonomous Vehicle Mapping

Original Abstract:
High-Definition Maps (HD maps) are essential for the precise navigation and decision-making of autonomous vehicles, yet their creation and upkeep present significant cost and timeliness challenges. The online construction of HD maps using on-board sensors has emerged as a promising solution; however, these methods can be impeded by incomplete data due to occlusions and inclement weather. This paper proposes the PriorDrive framework to addresses these limitations by harnessing the power of prior maps, significantly enhancing the robustness and accuracy of online HD map construction. Our approach integrates a variety of prior maps, such as OpenStreetMap's Standard Definition Maps (SD maps), outdated HD maps from vendors, and locally constructed maps from historical vehicle data. To effectively encode this prior information into online mapping models, we introduce a Hybrid Prior Representation (HPQuery) that standardizes the representation of diverse map elements. At the core of PriorDrive is the Unified Vector Encoder (UVE), which employs a dual encoding mechanism to process vector data. The intra-vector encoder captures fine-grained local features, while the inter-vector encoder integrates global context. Furthermore, we propose a segment-level and point-level pre-training strategy that enables the UVE to learn the prior distribution of vector data, thereby improving the encoder's generalizability and performance. Through extensive testing on the nuScenes dataset, we demonstrate that PriorDrive is highly compatible with various online mapping models and substantially improves map prediction capabilities. The integration of prior maps through the PriorDrive framework offers a robust solution to the challenges of single-perception data, paving the way for more reliable autonomous vehicle navigation.

Translated Abstract:
고해상도 지도(HD 맵)는 자율주행차의 정확한 내비게이션과 의사결정에 꼭 필요해. 하지만 HD 맵을 만들고 유지하는 데는 비용과 시간 문제가 크지. onboard 센서를 이용한 온라인 HD 맵 생성이 좋은 해결책으로 떠오르고 있지만, 가려짐이나 나쁜 날씨 때문에 데이터가 불완전해질 수 있어. 

이 논문에서는 PriorDrive라는 프레임워크를 제안해서 이런 한계를 해결하려고 해. PriorDrive는 이전 지도의 힘을 활용해 온라인 HD 맵 생성의 견고함과 정확성을 크게 높여. OpenStreetMap의 표준 정의 맵(SD 맵), 오래된 HD 맵, 그리고 역사적 차량 데이터로 만든 지역 지도를 포함한 다양한 이전 지도를 통합해. 

이전 정보를 온라인 맵 모델에 효과적으로 인코딩하기 위해 Hybrid Prior Representation(HPQuery)을 도입했어. 이건 다양한 맵 요소의 표현을 표준화하는 거야. PriorDrive의 핵심은 Unified Vector Encoder(UVE)인데, 이건 벡터 데이터를 처리하기 위해 이중 인코딩 메커니즘을 사용해. intra-vector encoder는 세밀한 지역 특징을 잡고, inter-vector encoder는 전반적인 맥락을 통합해. 

또한, UVE가 벡터 데이터의 이전 분포를 학습할 수 있게 하는 segment-level과 point-level의 사전 훈련 전략도 제안했어. 이렇게 하면 인코더의 일반화 능력과 성능이 향상돼. nuScenes 데이터셋에서 여러 번 테스트해본 결과, PriorDrive가 다양한 온라인 맵 모델과 잘 호환되고 맵 예측 능력을 크게 개선한다는 걸 보여줬어. 

PriorDrive 프레임워크를 통해 이전 지도를 통합하면 단일 감지 데이터의 문제를 해결할 수 있는 강력한 솔루션이 되고, 자율주행차 내비게이션의 신뢰성을 높일 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05359
Title: FedBrain-Distill: Communication-Efficient Federated Brain Tumor Classification Using Ensemble Knowledge Distillation on Non-IID Data

Original Abstract:
Brain is one the most complex organs in the human body. Due to its complexity, classification of brain tumors still poses a significant challenge, making brain tumors a particularly serious medical issue. Techniques such as Machine Learning (ML) coupled with Magnetic Resonance Imaging (MRI) have paved the way for doctors and medical institutions to classify different types of tumors. However, these techniques suffer from limitations that violate patients privacy. Federated Learning (FL) has recently been introduced to solve such an issue, but the FL itself suffers from limitations like communication costs and dependencies on model architecture, forcing all models to have identical architectures. In this paper, we propose FedBrain-Distill, an approach that leverages Knowledge Distillation (KD) in an FL setting that maintains the users privacy and ensures the independence of FL clients in terms of model architecture. FedBrain-Distill uses an ensemble of teachers that distill their knowledge to a simple student model. The evaluation of FedBrain-Distill demonstrated high-accuracy results for both Independent and Identically Distributed (IID) and non-IID data with substantial low communication costs on the real-world Figshare brain tumor dataset. It is worth mentioning that we used Dirichlet distribution to partition the data into IID and non-IID data. All the implementation details are accessible through our Github repository.

Translated Abstract:
뇌는 인체에서 가장 복잡한 기관 중 하나야. 이 복잡성 때문에 뇌종양을 분류하는 게 여전히 큰 도전 과제가 되고 있어. 그래서 뇌종양은 특히 심각한 의료 문제로 여겨져. 

기계 학습(ML)과 자기 공명 영상(MRI) 같은 기술들이 의사들과 의료 기관들이 다양한 종류의 종양을 분류하는 데 도움이 되고 있어. 하지만 이러한 기술들은 환자의 개인정보를 침해하는 한계가 있어. 최근에는 연합 학습(FL)이 이런 문제를 해결하기 위해 도입되었지만, FL 자체도 통신 비용이나 모델 아키텍처에 대한 의존성과 같은 한계가 있어. 그래서 모든 모델이 똑같은 아키텍처를 가져야 해. 

그래서 우리는 FedBrain-Distill이라는 방법을 제안해. 이 방법은 FL 환경에서 지식 증류(KD)를 활용해서 사용자 개인정보를 유지하고 FL 클라이언트의 모델 아키텍처 독립성을 보장해. FedBrain-Distill은 여러 개의 선생님 모델이 간단한 학생 모델에게 지식을 증류하는 방식을 사용해. 

FedBrain-Distill의 평가 결과는 독립적이고 동일하게 분포된(IID) 데이터와 비동일하게 분포된(non-IID) 데이터 모두에서 높은 정확도를 보여줬고, 실제 Figshare 뇌종양 데이터셋에서 상당히 낮은 통신 비용을 기록했어. 데이터를 IID와 non-IID로 나누기 위해 디리클레 분포를 사용했다는 것도 언급할 만해. 모든 구현 세부 사항은 우리의 깃허브 저장소에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05370
Title: KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models

Original Abstract:
Harnessing the robust capabilities of Large Language Models (LLMs) for narrative generation, logical reasoning, and common-sense knowledge integration, this study delves into utilizing LLMs to enhance automated radiology report generation (R2Gen). Despite the wealth of knowledge within LLMs, efficiently triggering relevant knowledge within these large models for specific tasks like R2Gen poses a critical research challenge. This paper presents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration framework based on LLMs. Utilizing a frozen LLM to generate reports, the framework integrates a knowledge graph to unlock chest disease-related knowledge within the LLM to enhance the clinical utility of generated reports. This is achieved by leveraging the knowledge graph to distill disease-related features in a designed way. Since a radiology report encompasses both normal and disease-related findings, the extracted graph-enhanced disease-related features are integrated with regional image features, attending to both aspects. We explore two fusion methods to automatically prioritize and select the most relevant features. The fused features are employed by LLM to generate reports that are more sensitive to diseases and of improved quality. Our approach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.

Translated Abstract:
이 연구는 대형 언어 모델(LLM)의 강력한 기능을 활용해 내러티브 생성, 논리적 추론, 상식 지식 통합을 다루고 있어. 여기서는 LLM을 이용해서 자동화된 방사선 보고서 생성을 향상시키는 방법을 살펴보려고 해(R2Gen). LLM 안에는 많은 지식이 있지만, R2Gen 같은 특정 작업을 위해 이 큰 모델에서 관련 지식을 효과적으로 끌어내는 건 중요한 연구 과제가 되고 있어.

이 논문에서는 KARGEN이라는 LLM 기반의 지식 강화 자동 방사선 보고서 생성 프레임워크를 소개해. 이 프레임워크는 고정된 LLM을 사용해 보고서를 생성하고, 지식 그래프를 통합해서 흉부 질환과 관련된 지식을 LLM에서 끌어내서 생성된 보고서의 임상적 유용성을 높여. 이는 지식 그래프를 활용해 질환 관련 특징을 설계된 방식으로 추출함으로써 이루어져.

방사선 보고서는 정상 소견과 질병 관련 소견을 모두 포함하기 때문에, 추출된 그래프 강화 질병 관련 특징을 지역 이미지 특징과 결합해 두 가지 측면을 모두 고려해. 우리는 자동으로 가장 관련성이 높은 특징을 우선순위에 따라 선택하고 결합하는 두 가지 방법을 탐구해. 결합된 특징은 LLM에 의해 질병에 더 민감하고 품질이 향상된 보고서를 생성하는 데 사용돼. 우리의 접근 방식은 MIMIC-CXR와 IU-Xray 데이터셋에서 유망한 결과를 보여주고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05378
Title: Memoryless Multimodal Anomaly Detection via Student-Teacher Network and Signed Distance Learning

Original Abstract:
Unsupervised anomaly detection is a challenging computer vision task, in which 2D-based anomaly detection methods have been extensively studied. However, multimodal anomaly detection based on RGB images and 3D point clouds requires further investigation. The existing methods are mainly inspired by memory bank based methods commonly used in 2D-based anomaly detection, which may cost extra memory for storing mutimodal features. In present study, a novel memoryless method MDSS is proposed for multimodal anomaly detection, which employs a light-weighted student-teacher network and a signed distance function to learn from RGB images and 3D point clouds respectively, and complements the anomaly information from the two modalities. Specifically, a student-teacher network is trained with normal RGB images and masks generated from point clouds by a dynamic loss, and the anomaly score map could be obtained from the discrepancy between the output of student and teacher. Furthermore, the signed distance function learns from normal point clouds to predict the signed distances between points and surface, and the obtained signed distances are used to generate anomaly score map. Subsequently, the anomaly score maps are aligned to generate the final anomaly score map for detection. The experimental results indicate that MDSS is comparable but more stable than the SOTA memory bank based method Shape-guided, and furthermore performs better than other baseline methods.

Translated Abstract:
비지도 이상 탐지는 어려운 컴퓨터 비전 작업인데, 2D 기반의 이상 탐지 방법이 많이 연구되어 왔어. 하지만 RGB 이미지랑 3D 포인트 클라우드를 기반으로 한 다중 모달 이상 탐지는 더 많은 연구가 필요해. 기존의 방법들은 주로 2D 기반 이상 탐지에서 많이 쓰이는 메모리 뱅크 방식에서 영감을 받아서, 다중 모달 특징을 저장하는 데 추가 메모리가 필요할 수 있어.

이번 연구에서는 다중 모달 이상 탐지를 위한 새로운 메모리 없는 방법 MDSS를 제안해. 이 방법은 가벼운 학생-교사 네트워크와 서명 거리 함수를 사용해서 RGB 이미지와 3D 포인트 클라우드에서 각각 학습하고, 두 가지 모달리티에서 이상 정보가 보완되도록 해. 구체적으로, 학생-교사 네트워크는 정상 RGB 이미지와 포인트 클라우드에서 생성된 마스크로 훈련되는데, 동적 손실을 사용해서 진행해. 그리고 학생과 교사의 출력 차이로부터 이상 점수 맵을 얻을 수 있어.

또한, 서명 거리 함수는 정상 포인트 클라우드에서 학습해서 점들과 표면 사이의 서명 거리를 예측해. 이렇게 얻어진 서명 거리들은 이상 점수 맵을 생성하는 데 사용돼. 이후에는 이상 점수 맵들을 정렬해서 최종 이상 점수 맵을 만들어 탐지에 사용해. 실험 결과를 보면, MDSS는 SOTA 메모리 뱅크 기반 방법인 Shape-guided와 비슷하지만 더 안정적이고, 다른 기준 방법들보다도 성능이 더 좋았어.

================================================================================

URL: https://arxiv.org/abs/2409.05379
Title: PersonaTalk: Bring Attention to Your Persona in Visual Dubbing

Original Abstract:
For audio-driven visual dubbing, it remains a considerable challenge to uphold and highlight speaker's persona while synthesizing accurate lip synchronization. Existing methods fall short of capturing speaker's unique speaking style or preserving facial details. In this paper, we present PersonaTalk, an attention-based two-stage framework, including geometry construction and face rendering, for high-fidelity and personalized visual dubbing. In the first stage, we propose a style-aware audio encoding module that injects speaking style into audio features through a cross-attention layer. The stylized audio features are then used to drive speaker's template geometry to obtain lip-synced geometries. In the second stage, a dual-attention face renderer is introduced to render textures for the target geometries. It consists of two parallel cross-attention layers, namely Lip-Attention and Face-Attention, which respectively sample textures from different reference frames to render the entire face. With our innovative design, intricate facial details can be well preserved. Comprehensive experiments and user studies demonstrate our advantages over other state-of-the-art methods in terms of visual quality, lip-sync accuracy and persona preservation. Furthermore, as a person-generic framework, PersonaTalk can achieve competitive performance as state-of-the-art person-specific methods. Project Page: this https URL.

Translated Abstract:
오디오 기반의 비주얼 더빙에서는 화자의 개성을 유지하면서 정확한 입술 동기화를 하는 게 여전히 큰 도전이야. 기존 방법들은 화자의 독특한 말투나 얼굴의 디테일을 잘 잡아내지 못해. 

이 논문에서는 PersonaTalk라는 주목 기반의 두 단계 프레임워크를 소개해. 이 프레임워크는 고품질의 개인화된 비주얼 더빙을 위해 기하학적 구조와 얼굴 렌더링을 포함하고 있어. 첫 번째 단계에서는 스타일을 인지하는 오디오 인코딩 모듈을 제안하는데, 이 모듈은 교차 주의 레이어를 통해 오디오 특징에 말투를 주입해. 스타일이 적용된 오디오 특징은 화자의 템플릿 기하학을 구동하는 데 사용돼서 입술이 동기화된 기하학을 얻을 수 있어.

두 번째 단계에서는 이중 주의 얼굴 렌더러를 도입해서 목표 기하학에 대한 텍스처를 렌더링해. 이 렌더러는 각각 다른 참조 프레임에서 텍스처를 샘플링하는 두 개의 평행한 교차 주의 레이어, 즉 Lip-Attention과 Face-Attention으로 구성돼. 이렇게 해서 전체 얼굴을 렌더링할 수 있어. 우리의 혁신적인 디자인 덕분에 복잡한 얼굴 디테일도 잘 유지할 수 있어.

종합적인 실험과 사용자 연구 결과, 우리는 다른 최신 방법들에 비해 시각적 품질, 입술 동기화 정확성, 화자 개성 유지 측면에서 우수한 성능을 보여줬어. 게다가 PersonaTalk는 사람에 구애받지 않는 프레임워크라서 최신 개인화된 방법들과 경쟁할 만한 성능을 발휘할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05380
Title: Prim2Room: Layout-Controllable Room Mesh Generation from Primitives

Original Abstract:
We propose Prim2Room, a novel framework for controllable room mesh generation leveraging 2D layout conditions and 3D primitive retrieval to facilitate precise 3D layout specification. Diverging from existing methods that lack control and precision, our approach allows for detailed customization of room-scale environments. To overcome the limitations of previous methods, we introduce an adaptive viewpoint selection algorithm that allows the system to generate the furniture texture and geometry from more favorable views than predefined camera trajectories. Additionally, we employ non-rigid depth registration to ensure alignment between generated objects and their corresponding primitive while allowing for shape variations to maintain diversity. Our method not only enhances the accuracy and aesthetic appeal of generated 3D scenes but also provides a user-friendly platform for detailed room design.

Translated Abstract:
우리는 Prim2Room이라는 새로운 프레임워크를 제안해. 이건 2D 레이아웃 조건과 3D 원시 객체를 활용해서 제어 가능한 방 메쉬 생성을 도와줘. 기존 방법들은 제어와 정밀성이 부족했는데, 우리의 접근법은 방 크기 환경을 자세하게 맞춤 설정할 수 있게 해.

이전 방법의 한계를 극복하기 위해, 우리는 적응형 시점 선택 알고리즘을 도입했어. 이 알고리즘은 시스템이 미리 정해진 카메라 경로보다 더 좋은 시점에서 가구의 텍스처와 기하학을 생성할 수 있게 해. 게다가 비강체 깊이 정합을 사용해서 생성된 객체와 해당 원시 객체 간의 정렬을 보장하면서도 형태 변화를 허용해 다양성을 유지해.

우리 방법은 생성된 3D 장면의 정확성과 미적 매력을 높여줄 뿐만 아니라, 자세한 방 디자인을 위한 사용자 친화적인 플랫폼도 제공해.

================================================================================

URL: https://arxiv.org/abs/2409.05381
Title: Boosting CLIP Adaptation for Image Quality Assessment via Meta-Prompt Learning and Gradient Regularization

Original Abstract:
Image Quality Assessment (IQA) remains an unresolved challenge in the field of computer vision, due to complex distortion conditions, diverse image content, and limited data availability. The existing Blind IQA (BIQA) methods heavily rely on extensive human annotations to train models, which is both labor-intensive and costly due to the demanding nature of creating IQA datasets. To mitigate the dependence on labeled samples, this paper introduces a novel Gradient-Regulated Meta-Prompt IQA Framework (GRMP-IQA). This framework aims to fast adapt the powerful visual-language pre-trained model, CLIP, to downstream IQA tasks, significantly improving accuracy in scenarios with limited data. Specifically, the GRMP-IQA comprises two key modules: Meta-Prompt Pre-training Module and Quality-Aware Gradient Regularization. The Meta Prompt Pre-training Module leverages a meta-learning paradigm to pre-train soft prompts with shared meta-knowledge across different distortions, enabling rapid adaptation to various IQA tasks. On the other hand, the Quality-Aware Gradient Regularization is designed to adjust the update gradients during fine-tuning, focusing the model's attention on quality-relevant features and preventing overfitting to semantic information. Extensive experiments on five standard BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA methods under limited data setting, i.e., achieving SRCC values of 0.836 (vs. 0.760 on LIVEC) and 0.853 (vs. 0.812 on KonIQ). Notably, utilizing just 20\% of the training data, our GRMP-IQA outperforms most existing fully supervised BIQA methods.

Translated Abstract:
이미지 품질 평가(IQA)는 컴퓨터 비전 분야에서 여전히 해결되지 않은 문제야. 복잡한 왜곡 조건, 다양한 이미지 내용, 그리고 제한된 데이터로 인해 어려움이 커. 기존의 비공식 IQA(BIQA) 방법들은 모델 훈련을 위해 많은 사람의 주석이 필요해, 그래서 데이터셋을 만드는 게 힘들고 비용도 많이 들어.

이런 한계를 극복하기 위해, 이 논문에서는 새로운 그라디언트 조절 메타 프롬프트 IQA 프레임워크(GRMP-IQA)를 소개해. 이 프레임워크는 강력한 시각-언어 사전 훈련 모델인 CLIP을 빠르게 IQA 작업에 적응시켜서, 데이터가 적은 상황에서도 정확도를 크게 높이는 것을 목표로 해.

GRMP-IQA는 두 가지 주요 모듈로 구성되어 있어: 메타 프롬프트 사전 훈련 모듈과 품질 인식 그라디언트 정규화. 메타 프롬프트 사전 훈련 모듈은 메타 학습 패러다임을 활용해 다양한 왜곡에 대한 공유 메타 지식을 가진 소프트 프롬프트를 사전 훈련해, 여러 IQA 작업에 빠르게 적응할 수 있게 해. 반면에, 품질 인식 그라디언트 정규화는 미세 조정 중 업데이트 그라디언트를 조절해, 모델이 품질 관련 특징에 집중하도록 하고, 의미 정보에 대한 과적합을 방지해.

다섯 개의 표준 BIQA 데이터셋에 대한 광범위한 실험 결과, 제한된 데이터 환경에서 기존의 최첨단 BIQA 방법들보다 뛰어난 성능을 보였어. 예를 들어, LIVEC에서는 SRCC 값이 0.836(기존의 0.760)이고, KonIQ에서는 0.853(기존의 0.812)를 달성했어. 특히, 훈련 데이터의 단 20%만 사용해도 GRMP-IQA는 대부분의 기존 완전 감독 BIQA 방법들을 능가하는 성과를 거두었어.

================================================================================

URL: https://arxiv.org/abs/2409.05383
Title: Deep Learning for Video Anomaly Detection: A Review

Original Abstract:
Video anomaly detection (VAD) aims to discover behaviors or events deviating from the normality in videos. As a long-standing task in the field of computer vision, VAD has witnessed much good progress. In the era of deep learning, with the explosion of architectures of continuously growing capability and capacity, a great variety of deep learning based methods are constantly emerging for the VAD task, greatly improving the generalization ability of detection algorithms and broadening the application scenarios. Therefore, such a multitude of methods and a large body of literature make a comprehensive survey a pressing necessity. In this paper, we present an extensive and comprehensive research review, covering the spectrum of five different categories, namely, semi-supervised, weakly supervised, fully supervised, unsupervised and open-set supervised VAD, and we also delve into the latest VAD works based on pre-trained large models, remedying the limitations of past reviews in terms of only focusing on semi-supervised VAD and small model based methods. For the VAD task with different levels of supervision, we construct a well-organized taxonomy, profoundly discuss the characteristics of different types of methods, and show their performance comparisons. In addition, this review involves the public datasets, open-source codes, and evaluation metrics covering all the aforementioned VAD tasks. Finally, we provide several important research directions for the VAD community.

Translated Abstract:
비디오 이상 탐지(VAD)는 비디오에서 정상적인 행동이나 사건에서 벗어난 것들을 찾아내는 작업이야. 컴퓨터 비전 분야에서 오랫동안 연구되어 온 이 과제는 많은 발전을 이뤘어. 딥러닝 시대에 들어서면서, 성능이 점점 더 좋아지는 다양한 구조들이 등장하고, 이로 인해 VAD를 위한 딥러닝 기반 방법들이 계속 생겨나고 있어. 덕분에 탐지 알고리즘의 일반화 능력이 크게 향상되고, 적용 가능한 분야도 넓어졌지.

하지만 이렇게 많은 방법과 문헌이 생기다 보니, 종합적인 리뷰가 필요해졌어. 그래서 이 논문에서는 반지도학(semi-supervised), 약지도학(weakly supervised), 완전지도학(fully supervised), 비지도학(unsupervised), 그리고 오픈셋 지도학(open-set supervised) VAD의 다섯 가지 카테고리를 포괄하는 광범위하고 포괄적인 연구 리뷰를 제공해. 그리고 최근의 대규모 모델을 기반으로 한 VAD 작업들도 다루어서, 이전 리뷰들이 반지도 VAD와 소규모 모델에만 집중했던 한계를 보완했어.

각기 다른 수준의 감독을 가진 VAD 작업을 위해서 잘 정리된 분류 체계를 만들고, 다양한 방법의 특징을 깊이 있게 논의하며 성능 비교도 보여줄 거야. 또한, 이 리뷰는 공공 데이터셋, 오픈소스 코드, 그리고 앞서 언급한 모든 VAD 작업을 포함하는 평가 지표도 포함해. 마지막으로, VAD 커뮤니티를 위한 몇 가지 중요한 연구 방향도 제시할 거야.

================================================================================

URL: https://arxiv.org/abs/2409.05384
Title: Look One and More: Distilling Hybrid Order Relational Knowledge for Cross-Resolution Image Recognition

Original Abstract:
In spite of great success in many image recognition tasks achieved by recent deep models, directly applying them to recognize low-resolution images may suffer from low accuracy due to the missing of informative details during resolution degradation. However, these images are still recognizable for subjects who are familiar with the corresponding high-resolution ones. Inspired by that, we propose a teacher-student learning approach to facilitate low-resolution image recognition via hybrid order relational knowledge distillation. The approach refers to three streams: the teacher stream is pretrained to recognize high-resolution images in high accuracy, the student stream is learned to identify low-resolution images by mimicking the teacher's behaviors, and the extra assistant stream is introduced as bridge to help knowledge transfer across the teacher to the student. To extract sufficient knowledge for reducing the loss in accuracy, the learning of student is supervised with multiple losses, which preserves the similarities in various order relational structures. In this way, the capability of recovering missing details of familiar low-resolution images can be effectively enhanced, leading to a better knowledge transfer. Extensive experiments on metric learning, low-resolution image classification and low-resolution face recognition tasks show the effectiveness of our approach, while taking reduced models.

Translated Abstract:
최근 딥러닝 모델들이 이미지 인식 작업에서 큰 성공을 거두긴 했지만, 저해상도 이미지를 인식하는 데 직접 적용하면 정보가 부족해 정확도가 떨어질 수 있어. 그래도 이런 이미지들은 고해상도 이미지를 잘 아는 사람들에게는 여전히 인식 가능해. 

이런 점에서 영감을 받아, 우리는 하이브리드 순서 관계 지식 증류를 통해 저해상도 이미지 인식을 도와주는 교사-학생 학습 방법을 제안해. 이 방법은 세 가지 흐름으로 나뉘어져 있어: 교사 흐름은 고해상도 이미지를 높은 정확도로 인식하도록 사전 훈련되고, 학생 흐름은 교사의 행동을 모방하면서 저해상도 이미지를 인식하도록 학습해. 그리고 추가적인 보조 흐름은 교사에서 학생으로 지식 전달을 도와주는 역할을 해.

정확도 손실을 줄이기 위해 학생의 학습은 여러 손실을 통해 감독되며, 이는 다양한 순서 관계 구조의 유사성을 유지해. 이렇게 하면 익숙한 저해상도 이미지의 누락된 세부사항을 효과적으로 복원할 수 있는 능력이 향상돼, 결과적으로 더 나은 지식 전달이 이루어져. 

메트릭 학습, 저해상도 이미지 분류, 저해상도 얼굴 인식 작업에서 우리의 방법의 효과를 보여주는 다양한 실험을 진행했어.

================================================================================

URL: https://arxiv.org/abs/2409.05387
Title: Decoupling Contact for Fine-Grained Motion Style Transfer

Original Abstract:
Motion style transfer changes the style of a motion while retaining its content and is useful in computer animations and games. Contact is an essential component of motion style transfer that should be controlled explicitly in order to express the style vividly while enhancing motion naturalness and quality. However, it is unknown how to decouple and control contact to achieve fine-grained control in motion style transfer. In this paper, we present a novel style transfer method for fine-grained control over contacts while achieving both motion naturalness and spatial-temporal variations of style. Based on our empirical evidence, we propose controlling contact indirectly through the hip velocity, which can be further decomposed into the trajectory and contact timing, respectively. To this end, we propose a new model that explicitly models the correlations between motions and trajectory/contact timing/style, allowing us to decouple and control each separately. Our approach is built around a motion manifold, where hip controls can be easily integrated into a Transformer-based decoder. It is versatile in that it can generate motions directly as well as be used as post-processing for existing methods to improve quality and contact controllability. In addition, we propose a new metric that measures a correlation pattern of motions based on our empirical evidence, aligning well with human perception in terms of motion naturalness. Based on extensive evaluation, our method outperforms existing methods in terms of style expressivity and motion quality.

Translated Abstract:
모션 스타일 전이는 모션의 스타일을 바꾸면서 내용을 유지하는 기술로, 컴퓨터 애니메이션과 게임에서 유용해. 여기서 '접촉'은 스타일을 생생하게 표현하고 모션의 자연스러움과 품질을 높이기 위해 꼭 필요한 요소야. 하지만, 접촉을 어떻게 분리하고 조절할 수 있는지에 대한 연구는 부족해.

이 논문에서는 접촉에 대한 세밀한 제어를 가능하게 하면서도 모션의 자연스러움과 스타일의 시간적, 공간적 변화를 동시에 이룰 수 있는 새로운 스타일 전이 방법을 제안해. 우리의 실험 결과에 따르면, 접촉을 간접적으로 엉덩이 속도로 조절하는 게 좋다고 해. 이 속도는 경로와 접촉 타이밍으로 더 나눌 수 있어.

이를 위해, 우리는 모션과 경로/접촉 타이밍/스타일 간의 상관관계를 명확히 모델링하는 새로운 모델을 제안해. 이렇게 하면 각각을 따로 조절할 수 있어. 우리의 접근 방식은 모션 매니폴드 주변에서 구성되어 있고, 엉덩이 제어가 Transformer 기반 디코더에 쉽게 통합될 수 있어. 이 방법은 직접적으로 모션을 생성할 수도 있고, 기존 방법의 품질과 접촉 제어 능력을 개선하는 후처리로도 사용할 수 있어.

또한, 우리는 모션의 상관관계 패턴을 측정하는 새로운 메트릭도 제안해. 이 메트릭은 우리의 실험 결과에 기반하고, 모션의 자연스러움에 대한 인간의 인식과 잘 맞아. 광범위한 평가를 바탕으로, 우리의 방법은 스타일 표현력과 모션 품질 면에서 기존 방법들을 능가해.

================================================================================

URL: https://arxiv.org/abs/2409.05389
Title: A Novel Representation of Periodic Pattern and Its Application to Untrained Anomaly Detection

Original Abstract:
There are a variety of industrial products that possess periodic textures or surfaces, such as carbon fiber textiles and display panels. Traditional image-based quality inspection methods for these products require identifying the periodic patterns from normal images (without anomaly and noise) and subsequently detecting anomaly pixels with inconsistent appearances. However, it remains challenging to accurately extract the periodic pattern from a single image in the presence of unknown anomalies and measurement noise. To deal with this challenge, this paper proposes a novel self-representation of the periodic image defined on a set of continuous parameters. In this way, periodic pattern learning can be embedded into a joint optimization framework, which is named periodic-sparse decomposition, with simultaneously modeling the sparse anomalies and Gaussian noise. Finally, for the real-world industrial images that may not strictly satisfy the periodic assumption, we propose a novel pixel-level anomaly scoring strategy to enhance the performance of anomaly detection. Both simulated and real-world case studies demonstrate the effectiveness of the proposed methodology for periodic pattern learning and anomaly detection.

Translated Abstract:
산업 제품에는 주기적인 질감이나 표면을 가진 것들이 많아. 예를 들면, 탄소 섬유 직물이나 디스플레이 패널 같은 것들이야. 전통적인 이미지 기반 품질 검사 방법은 정상적인 이미지에서 주기적인 패턴을 찾아내고, 그 다음에 일관성이 없는 픽셀을 찾아내는 방식이야. 하지만, 알 수 없는 이상 현상이나 측정 노이즈가 있을 때 단일 이미지에서 주기적인 패턴을 정확하게 추출하는 건 여전히 어려워.

이런 문제를 해결하기 위해, 이 논문에서는 연속적인 매개변수 집합 위에서 정의된 주기적 이미지의 새로운 자기 표현 방법을 제안해. 이렇게 하면 주기적인 패턴 학습을 하나의 최적화 프레임워크에 통합할 수 있어. 이걸 '주기적 희소 분해'라고 부르는데, 여기서 희소한 이상 현상과 가우시안 노이즈를 동시에 모델링할 수 있어.

마지막으로, 주기적 가정이 항상 정확히 맞지 않을 수 있는 실제 산업 이미지에 대해서는 새로운 픽셀 수준의 이상 점수 전략을 제안해. 이 방법이 이상 탐지 성능을 향상시킬 수 있어. 시뮬레이션과 실제 사례 연구 모두에서 이 방법이 주기적 패턴 학습과 이상 탐지에 효과적이라는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.05393
Title: TAVP: Task-Adaptive Visual Prompt for Cross-domain Few-shot Segmentation

Original Abstract:
Under the backdrop of large-scale pre-training, large visual models (LVM) have demonstrated significant potential in image understanding. The recent emergence of the Segment Anything Model (SAM) has brought a qualitative shift in the field of image segmentation, supporting flexible interactive cues and strong learning capabilities. However, its performance often falls short in cross-domain and few-shot applications. Transferring prior knowledge from foundation models to new applications while preserving learning capabilities is worth exploring. This work proposes a task-adaptive prompt framework based on SAM, a new paradigm for Cross-dominan few-shot segmentation (CD-FSS). First, a Multi-level Feature Fusion (MFF) was used for integrated feature extraction. Besides, an additional Class Domain Task-Adaptive Auto-Prompt (CDTAP) module was combined with the segmentation branch for class-domain agnostic feature extraction and high-quality learnable prompt production. This significant advancement uses a unique generative approach to prompts alongside a comprehensive model structure and specialized prototype computation. While ensuring that the prior knowledge of SAM is not discarded, the new branch disentangles category and domain information through prototypes, guiding it in adapting the CD-FSS. We have achieved the best results on three benchmarks compared to the recent state-of-the-art (SOTA) methods. Comprehensive experiments showed that after task-specific and weighted guidance, the abundant feature information of SAM can be better learned for CD-FSS.

Translated Abstract:
대규모 사전 훈련을 바탕으로, 대형 시각 모델(LVM)은 이미지 이해에서 큰 가능성을 보여주고 있어. 최근에 등장한 Segment Anything Model(SAM)은 이미지 분할 분야에서 질적인 변화를 가져왔고, 유연한 상호작용 신호와 강력한 학습 능력을 지원해. 하지만 이 모델은 도메인 간 전이 및 몇 번의 샷이 필요한 응용에서는 성능이 부족한 경우가 많아. 그래서 기존 모델의 지식을 새로운 응용으로 옮기면서도 학습 능력을 유지하는 방법을 탐구하는 게 중요해.

이번 연구는 SAM을 기반으로 한 작업 적응형 프롬프트 프레임워크를 제안해. 이건 Cross-domain few-shot segmentation (CD-FSS)이라는 새로운 패러다임이야. 먼저, Multi-level Feature Fusion(MFF)를 사용해서 통합된 특징 추출을 했어. 그리고 추가로 Class Domain Task-Adaptive Auto-Prompt(CDTAP) 모듈을 세그멘테이션 브랜치와 결합해서 클래스 도메인에 구애받지 않는 특징 추출과 고품질의 학습 가능한 프롬프트 생성을 했어. 

이 연구는 독특한 생성 접근 방식을 사용해서 프롬프트를 만들고, 포괄적인 모델 구조와 전문화된 프로토타입 계산을 통해 큰 발전을 이루었어. SAM의 기존 지식을 버리지 않으면서 새로운 브랜치는 프로토타입을 통해 카테고리와 도메인 정보를 분리해, CD-FSS에 적응하도록 돕고 있어. 우리는 최근의 최첨단(SOTA) 방법들과 비교했을 때 세 가지 벤치마크에서 최고의 결과를 얻었어. 포괄적인 실험 결과, 작업 특정과 가중치 지침 후에 SAM의 풍부한 특징 정보를 CD-FSS에 더 잘 학습할 수 있음을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.05395
Title: Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling

Original Abstract:
This study explores replacing Transformers in Visual Language Models (VLMs) with Mamba, a recent structured state space model (SSM) that demonstrates promising performance in sequence modeling. We test models up to 3B parameters under controlled conditions, showing that Mamba-based VLMs outperforms Transformers-based VLMs in captioning, question answering, and reading comprehension. However, we find that Transformers achieve greater performance in visual grounding and the performance gap widens with scale. We explore two hypotheses to explain this phenomenon: 1) the effect of task-agnostic visual encoding on the updates of the hidden states, and 2) the difficulty in performing visual grounding from the perspective of in-context multimodal retrieval. Our results indicate that a task-aware encoding yields minimal performance gains on grounding, however, Transformers significantly outperform Mamba at in-context multimodal retrieval. Overall, Mamba shows promising performance on tasks where the correct output relies on a summary of the image but struggles when retrieval of explicit information from the context is required.

Translated Abstract:
이 연구는 비주얼 언어 모델(VLM)에서 변환기를 Mamba라는 최근의 구조적 상태 공간 모델(SSM)로 바꾸는 방법을 탐구해. Mamba는 시퀀스 모델링에서 좋은 성능을 보여줘. 우리는 최대 30억 개의 매개변수를 가진 모델을 통제된 조건에서 테스트했어. 그 결과, Mamba 기반 VLM이 변환기 기반 VLM보다 캡션 생성, 질문 답변, 독해에서 더 나은 성능을 보였어.

하지만 변환기는 시각적 기반 설정에서 더 좋은 성능을 보이고, 그 성능 차이는 모델의 크기가 커질수록 더 커져. 이 현상을 설명하기 위해 두 가지 가설을 살펴봤어: 1) 작업에 구애받지 않는 시각적 인코딩이 숨겨진 상태 업데이트에 미치는 영향, 2) 맥락에서 다중 모드 검색 관점에서 시각적 기반 설정을 수행하는 데 어려움.

결과적으로, 작업 인식 인코딩은 기반 설정에서 성능 향상을 거의 가져오지 않지만, 변환기는 맥락에서 다중 모드 검색에서 Mamba보다 훨씬 더 좋은 성능을 보여. 전반적으로 Mamba는 이미지 요약에 의존하는 작업에서는 좋은 성능을 보이지만, 맥락에서 명시적 정보를 검색해야 할 때는 성능이 떨어져.

================================================================================

URL: https://arxiv.org/abs/2409.05396
Title: FacialFlowNet: Advancing Facial Optical Flow Estimation with a Diverse Dataset and a Decomposed Model

Original Abstract:
Facial movements play a crucial role in conveying altitude and intentions, and facial optical flow provides a dynamic and detailed representation of it. However, the scarcity of datasets and a modern baseline hinders the progress in facial optical flow research. This paper proposes FacialFlowNet (FFN), a novel large-scale facial optical flow dataset, and the Decomposed Facial Flow Model (DecFlow), the first method capable of decomposing facial flow. FFN comprises 9,635 identities and 105,970 image pairs, offering unprecedented diversity for detailed facial and head motion analysis. DecFlow features a facial semantic-aware encoder and a decomposed flow decoder, excelling in accurately estimating and decomposing facial flow into head and expression components. Comprehensive experiments demonstrate that FFN significantly enhances the accuracy of facial flow estimation across various optical flow methods, achieving up to an 11% reduction in Endpoint Error (EPE) (from 3.91 to 3.48). Moreover, DecFlow, when coupled with FFN, outperforms existing methods in both synthetic and real-world scenarios, enhancing facial expression analysis. The decomposed expression flow achieves a substantial accuracy improvement of 18% (from 69.1% to 82.1%) in micro-expressions recognition. These contributions represent a significant advancement in facial motion analysis and optical flow estimation. Codes and datasets can be found.

Translated Abstract:
얼굴 움직임은 감정과 의도를 전달하는 데 중요한 역할을 해. 얼굴의 광학 흐름은 이걸 동적으로 잘 보여주는 방법이야. 그런데 데이터셋이 부족하고 현대적인 기준이 없어서 얼굴 광학 흐름 연구가 잘 진행되지 않고 있어. 

이 논문에서는 FacialFlowNet (FFN)이라는 새로운 대규모 얼굴 광학 흐름 데이터셋과 Decomposed Facial Flow Model (DecFlow)이라는 방법을 제안해. DecFlow는 얼굴 흐름을 분해할 수 있는 첫 번째 방법이야. FFN은 9,635개의 인물과 105,970개의 이미지 쌍으로 구성되어 있어서, 얼굴과 머리 움직임 분석에 있어 전례 없는 다양성을 제공해.

DecFlow는 얼굴 의미를 인식하는 인코더와 분해된 흐름 디코더를 특징으로 해. 이 두 가지가 결합되면 얼굴 흐름을 정확하게 추정하고, 머리와 표정 요소로 분해하는 데 뛰어나. 다양한 실험 결과, FFN이 여러 광학 흐름 방법에서 얼굴 흐름 추정의 정확성을 크게 향상시켜서 Endpoint Error (EPE)를 11% 줄여줬어 (3.91에서 3.48로).

게다가 DecFlow가 FFN과 함께 사용될 때, 합성 및 실제 상황 모두에서 기존 방법보다 더 뛰어난 성능을 보여줬고, 얼굴 표정 분석도 개선했어. 분해된 표정 흐름은 미세 표정 인식에서 18%의 정확도 향상(69.1%에서 82.1%로)을 이뤘어. 이런 기여는 얼굴 움직임 분석과 광학 흐름 추정에서 큰 발전을 의미해. 코드와 데이터셋은 찾아볼 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05399
Title: Sequential Posterior Sampling with Diffusion Models

Original Abstract:
Diffusion models have quickly risen in popularity for their ability to model complex distributions and perform effective posterior sampling. Unfortunately, the iterative nature of these generative models makes them computationally expensive and unsuitable for real-time sequential inverse problems such as ultrasound imaging. Considering the strong temporal structure across sequences of frames, we propose a novel approach that models the transition dynamics to improve the efficiency of sequential diffusion posterior sampling in conditional image synthesis. Through modeling sequence data using a video vision transformer (ViViT) transition model based on previous diffusion outputs, we can initialize the reverse diffusion trajectory at a lower noise scale, greatly reducing the number of iterations required for convergence. We demonstrate the effectiveness of our approach on a real-world dataset of high frame rate cardiac ultrasound images and show that it achieves the same performance as a full diffusion trajectory while accelerating inference 25$\times$, enabling real-time posterior sampling. Furthermore, we show that the addition of a transition model improves the PSNR up to 8\% in cases with severe motion. Our method opens up new possibilities for real-time applications of diffusion models in imaging and other domains requiring real-time inference.

Translated Abstract:
확산 모델은 복잡한 분포를 모델링하고 효과적인 후방 샘플링을 수행할 수 있어서 인기가 급상승했어. 하지만 이 생성 모델은 반복적인 특성 때문에 계산 비용이 많이 들고 초음파 영상 같은 실시간 순차 역문제에는 적합하지 않아.

우리는 프레임 시퀀스 간의 강한 시간 구조를 고려해서, 조건부 이미지 합성을 위한 순차적 확산 후방 샘플링의 효율성을 높이는 새로운 접근 방식을 제안해. 이전 확산 결과를 기반으로 한 비디오 비전 변환기(ViViT) 전이 모델을 사용해 시퀀스 데이터를 모델링하면, 역 확산 궤적을 더 낮은 노이즈 스케일로 초기화할 수 있어. 이렇게 하면 수렴을 위해 필요한 반복 횟수를 크게 줄일 수 있어.

우리는 고프레임 속도의 심장 초음파 이미지 데이터셋에서 우리의 방법의 효과를 증명했어. 이 방법은 전체 확산 궤적과 같은 성능을 내면서도 추론 속도를 25배나 높여서 실시간 후방 샘플링이 가능해. 게다가 전이 모델을 추가하면 심한 움직임이 있는 경우 PSNR이 최대 8%까지 개선되는 것도 보여줬어. 

우리의 방법은 영상 및 실시간 추론이 필요한 다른 분야에서 확산 모델의 실시간 응용 가능성을 열어줘.

================================================================================

URL: https://arxiv.org/abs/2409.05405
Title: A Survey of Multimodal Composite Editing and Retrieval

Original Abstract:
In the real world, where information is abundant and diverse across different modalities, understanding and utilizing various data types to improve retrieval systems is a key focus of research. Multimodal composite retrieval integrates diverse modalities such as text, image and audio, etc. to provide more accurate, personalized, and contextually relevant results. To facilitate a deeper understanding of this promising direction, this survey explores multimodal composite editing and retrieval in depth, covering image-text composite editing, image-text composite retrieval, and other multimodal composite retrieval. In this survey, we systematically organize the application scenarios, methods, benchmarks, experiments, and future directions. Multimodal learning is a hot topic in large model era, and have also witnessed some surveys in multimodal learning and vision-language models with transformers published in the PAMI journal. To the best of our knowledge, this survey is the first comprehensive review of the literature on multimodal composite retrieval, which is a timely complement of multimodal fusion to existing reviews. To help readers' quickly track this field, we build the project page for this survey, which can be found at this https URL.

Translated Abstract:
현실 세계는 정보가 풍부하고 다양한 양식으로 가득 차 있어, 여러 데이터 유형을 이해하고 활용해 검색 시스템을 개선하는 것이 연구의 중요한 초점이야. 멀티모달 복합 검색은 텍스트, 이미지, 오디오 등 다양한 양식을 통합해서 더 정확하고 개인화된, 그리고 맥락에 맞는 결과를 제공해.

이 연구에서는 멀티모달 복합 편집과 검색에 대해 깊이 탐구해. 여기에는 이미지-텍스트 복합 편집, 이미지-텍스트 복합 검색, 그리고 다른 멀티모달 복합 검색이 포함돼. 우리는 응용 시나리오, 방법, 벤치마크, 실험, 그리고 미래 방향을 체계적으로 정리했어. 멀티모달 학습은 대형 모델 시대의 핫한 주제고, PAMI 저널에는 멀티모달 학습과 비전-언어 모델에 대한 몇 가지 리뷰도 발표됐어.

우리의 연구는 멀티모달 복합 검색에 관한 문헌을 종합적으로 리뷰한 첫 번째 논문이야. 기존 리뷰에 멀티모달 융합을 시기 적절하게 보완해. 이 분야를 빠르게 따라잡을 수 있도록, 이 연구를 위한 프로젝트 페이지도 만들었어. (URL을 통해 확인할 수 있어.)

================================================================================

URL: https://arxiv.org/abs/2409.05407
Title: KRONC: Keypoint-based Robust Camera Optimization for 3D Car Reconstruction

Original Abstract:
The three-dimensional representation of objects or scenes starting from a set of images has been a widely discussed topic for years and has gained additional attention after the diffusion of NeRF-based approaches. However, an underestimated prerequisite is the knowledge of camera poses or, more specifically, the estimation of the extrinsic calibration parameters. Although excellent general-purpose Structure-from-Motion methods are available as a pre-processing step, their computational load is high and they require a lot of frames to guarantee sufficient overlapping among the views. This paper introduces KRONC, a novel approach aimed at inferring view poses by leveraging prior knowledge about the object to reconstruct and its representation through semantic keypoints. With a focus on vehicle scenes, KRONC is able to estimate the position of the views as a solution to a light optimization problem targeting the convergence of keypoints' back-projections to a singular point. To validate the method, a specific dataset of real-world car scenes has been collected. Experiments confirm KRONC's ability to generate excellent estimates of camera poses starting from very coarse initialization. Results are comparable with Structure-from-Motion methods with huge savings in computation. Code and data will be made publicly available.

Translated Abstract:
3D 객체나 장면을 여러 이미지에서 시작해서 표현하는 건 오랫동안 이야기되어온 주제야. 최근에는 NeRF 기반 방법들이 인기를 끌면서 더 주목받고 있어. 하지만, 카메라 포즈에 대한 지식, 특히 외부 캘리브레이션 파라미터를 추정하는 게 중요해. 일반적으로 쓰이는 Structure-from-Motion 방법들이 있지만, 이건 계산량이 많고 여러 프레임이 필요해서 각 뷰 간의 충분한 겹침을 보장해야 해.

이 논문에서는 KRONC라는 새로운 접근법을 소개해. 이 방법은 객체에 대한 사전 지식을 활용해서 뷰 포즈를 추정하고, 이를 통해 세맨틱 키포인트로 재구성해. 차량 장면에 초점을 맞추고 있는 KRONC는 키포인트의 역투영이 하나의 점으로 수렴하도록 하는 가벼운 최적화 문제를 해결하면서 뷰의 위치를 추정할 수 있어.

이 방법을 검증하기 위해 실제 자동차 장면의 특정 데이터셋을 수집했어. 실험 결과, KRONC는 매우 기본적인 초기화에서 시작해도 훌륭한 카메라 포즈 추정이 가능하다는 걸 보여줬어. 결과는 Structure-from-Motion 방법과 비슷하면서도 계산량은 크게 줄일 수 있었어. 코드와 데이터는 공개될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.05413
Title: From Words to Poses: Enhancing Novel Object Pose Estimation with Vision Language Models

Original Abstract:
Robots are increasingly envisioned to interact in real-world scenarios, where they must continuously adapt to new situations. To detect and grasp novel objects, zero-shot pose estimators determine poses without prior knowledge. Recently, vision language models (VLMs) have shown considerable advances in robotics applications by establishing an understanding between language input and image input. In our work, we take advantage of VLMs zero-shot capabilities and translate this ability to 6D object pose estimation. We propose a novel framework for promptable zero-shot 6D object pose estimation using language embeddings. The idea is to derive a coarse location of an object based on the relevancy map of a language-embedded NeRF reconstruction and to compute the pose estimate with a point cloud registration method. Additionally, we provide an analysis of LERF's suitability for open-set object pose estimation. We examine hyperparameters, such as activation thresholds for relevancy maps and investigate the zero-shot capabilities on an instance- and category-level. Furthermore, we plan to conduct robotic grasping experiments in a real-world setting.

Translated Abstract:
로봇들이 실제 상황에서 상호작용하는 모습이 점점 더 많이 그려지고 있어. 이 로봇들은 새로운 상황에 계속 적응해야 해. 새로운 물체를 감지하고 잡기 위해, 제로샷 포즈 추정기가 사전 지식 없이 포즈를 결정해. 최근 비전 언어 모델(VLMs)이 언어 입력과 이미지 입력 간의 이해를 통해 로봇 응용 분야에서 큰 발전을 보여줬어.

우리 연구에서는 VLM의 제로샷 기능을 활용해서 6D 물체 포즈 추정에 적용하고 있어. 언어 임베딩을 사용한 프롬프트 가능한 제로샷 6D 물체 포즈 추정을 위한 새로운 프레임워크를 제안해. 이 아이디어는 언어 임베딩이 포함된 NeRF 재구성의 관련성 맵을 기반으로 물체의 대략적인 위치를 추정하고, 포인트 클라우드 등록 방법으로 포즈 추정을 계산하는 거야.

또한, LERF의 오픈셋 물체 포즈 추정에 대한 적합성을 분석했어. 관련성 맵에 대한 활성화 임계값 같은 하이퍼파라미터를 살펴보고, 인스턴스 및 카테고리 수준에서 제로샷 기능을 조사했어. 그리고 실제 환경에서 로봇 그랩 실험도 계획하고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05420
Title: AD-Net: Attention-based dilated convolutional residual network with guided decoder for robust skin lesion segmentation

Original Abstract:
In computer-aided diagnosis tools employed for skin cancer treatment and early diagnosis, skin lesion segmentation is important. However, achieving precise segmentation is challenging due to inherent variations in appearance, contrast, texture, and blurry lesion boundaries. This research presents a robust approach utilizing a dilated convolutional residual network, which incorporates an attention-based spatial feature enhancement block (ASFEB) and employs a guided decoder strategy. In each dilated convolutional residual block, dilated convolution is employed to broaden the receptive field with varying dilation rates. To improve the spatial feature information of the encoder, we employed an attention-based spatial feature enhancement block in the skip connections. The ASFEB in our proposed method combines feature maps obtained from average and maximum-pooling operations. These combined features are then weighted using the active outcome of global average pooling and convolution operations. Additionally, we have incorporated a guided decoder strategy, where each decoder block is optimized using an individual loss function to enhance the feature learning process in the proposed AD-Net. The proposed AD-Net presents a significant benefit by necessitating fewer model parameters compared to its peer methods. This reduction in parameters directly impacts the number of labeled data required for training, facilitating faster convergence during the training process. The effectiveness of the proposed AD-Net was evaluated using four public benchmark datasets. We conducted a Wilcoxon signed-rank test to verify the efficiency of the AD-Net. The outcomes suggest that our method surpasses other cutting-edge methods in performance, even without the implementation of data augmentation strategies.

Translated Abstract:
피부암 치료와 조기 진단에 사용되는 컴퓨터 보조 진단 도구에서, 피부 병변 분할은 매우 중요해. 하지만 정확한 분할을 하는 게 쉽지 않은데, 그 이유는 외관, 대비, 질감, 그리고 흐릿한 병변 경계 같은 다양한 변수가 있기 때문이야.

이 연구에서는 주목 기반 공간 특징 향상 블록(ASFEB)을 포함한 팽창 합성곱 잔차 네트워크를 이용한 강력한 접근 방식을 제안해. 각 팽창 합성곱 잔차 블록에서는 다양한 팽창 비율로 수용 영역을 넓히기 위해 팽창 합성곱을 사용해. 인코더의 공간 특징 정보를 개선하기 위해, 우리는 스킵 연결에서 주목 기반 공간 특징 향상 블록을 사용했어. 제안된 방법의 ASFEB는 평균 풀링과 최대 풀링 작업을 통해 얻은 특징 맵을 결합해. 이렇게 결합된 특징들은 글로벌 평균 풀링과 합성곱 작업의 활성화 결과를 사용해 가중치를 부여해.

또한, 우리는 각 디코더 블록이 개별 손실 함수를 사용해 최적화되는 가이드 디코더 전략을 도입했어. 이 방법은 제안된 AD-Net의 특징 학습 과정을 향상시키는 데 도움을 줘. AD-Net은 다른 방법들에 비해 모델 파라미터가 적게 필요하다는 큰 장점이 있어. 이렇게 파라미터를 줄이면 훈련에 필요한 레이블 데이터 양에도 직접적인 영향을 미쳐서 훈련 과정이 더 빠르게 수렴할 수 있게 해.

제안된 AD-Net의 효과는 네 개의 공공 벤치마크 데이터셋을 사용해 평가했어. AD-Net의 효율성을 확인하기 위해 윌콕슨 부호 순위 검정을 진행했어. 결과는 우리 방법이 데이터 증강 전략을 사용하지 않고도 최신 방법들보다 성능이 뛰어나다는 걸 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.05425
Title: Distribution Discrepancy and Feature Heterogeneity for Active 3D Object Detection

Original Abstract:
LiDAR-based 3D object detection is a critical technology for the development of autonomous driving and robotics. However, the high cost of data annotation limits its advancement. We propose a novel and effective active learning (AL) method called Distribution Discrepancy and Feature Heterogeneity (DDFH), which simultaneously considers geometric features and model embeddings, assessing information from both the instance-level and frame-level perspectives. Distribution Discrepancy evaluates the difference and novelty of instances within the unlabeled and labeled distributions, enabling the model to learn efficiently with limited data. Feature Heterogeneity ensures the heterogeneity of intra-frame instance features, maintaining feature diversity while avoiding redundant or similar instances, thus minimizing annotation costs. Finally, multiple indicators are efficiently aggregated using Quantile Transform, providing a unified measure of informativeness. Extensive experiments demonstrate that DDFH outperforms the current state-of-the-art (SOTA) methods on the KITTI and Waymo datasets, effectively reducing the bounding box annotation cost by 56.3% and showing robustness when working with both one-stage and two-stage models.

Translated Abstract:
LiDAR 기반 3D 물체 감지는 자율주행차와 로봇 기술 개발에 중요한 기술이야. 하지만 데이터 주석 비용이 너무 높아서 발전이 제한되고 있어. 우리는 새로운 능동 학습(Active Learning, AL) 방법인 분포 불일치 및 특징 이질성(Distribution Discrepancy and Feature Heterogeneity, DDFH)을 제안해. 이 방법은 기하학적 특징과 모델 임베딩을 동시에 고려해서 인스턴스 수준과 프레임 수준에서 정보를 평가해.

분포 불일치는 레이블이 없는 데이터와 레이블이 있는 데이터의 차이와 새로움을 평가해. 이 덕분에 모델이 제한된 데이터로도 효율적으로 학습할 수 있어. 특징 이질성은 프레임 내 인스턴스 특징의 다양성을 유지하면서 중복되거나 비슷한 인스턴스를 피하게 해주어, 주석 비용을 최소화해.

마지막으로, 여러 지표를 효율적으로 집계하기 위해 분위수 변환(Quantile Transform)을 사용해, 정보의 통합된 척도를 제공해. 다양한 실험 결과에 따르면, DDFH는 KITTI와 Waymo 데이터셋에서 현재 최고 성능(SOTA) 방법들보다 더 나은 성능을 보여주고, 바운딩 박스 주석 비용을 56.3% 줄이는 데 성공했어. 또한, 일단계와 이단계 모델 모두에서 강인함을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.05427
Title: TextToucher: Fine-Grained Text-to-Touch Generation

Original Abstract:
Tactile sensation plays a crucial role in the development of multi-modal large models and embodied intelligence. To collect tactile data with minimal cost as possible, a series of studies have attempted to generate tactile images by vision-to-touch image translation. However, compared to text modality, visual modality-driven tactile generation cannot accurately depict human tactile sensation. In this work, we analyze the characteristics of tactile images in detail from two granularities: object-level (tactile texture, tactile shape), and sensor-level (gel status). We model these granularities of information through text descriptions and propose a fine-grained Text-to-Touch generation method (TextToucher) to generate high-quality tactile samples. Specifically, we introduce a multimodal large language model to build the text sentences about object-level tactile information and employ a set of learnable text prompts to represent the sensor-level tactile information. To better guide the tactile generation process with the built text information, we fuse the dual grains of text information and explore various dual-grain text conditioning methods within the diffusion transformer architecture. Furthermore, we propose a Contrastive Text-Touch Pre-training (CTTP) metric to precisely evaluate the quality of text-driven generated tactile data. Extensive experiments demonstrate the superiority of our TextToucher method. The source codes will be available at \url{this https URL}.

Translated Abstract:
촉각 감각은 다중 모달 대형 모델과 구현된 지능의 발전에 중요한 역할을 해. 최소한의 비용으로 촉각 데이터를 수집하려는 여러 연구들이 시도되었고, 그중 하나가 시각에서 촉각 이미지로 변환하는 거야. 하지만 텍스트 기반의 촉각 생성에 비해, 시각 기반의 촉각 생성은 사람의 촉각 감각을 제대로 표현하지 못해.

이 연구에서는 촉각 이미지의 특성을 두 가지 관점에서 자세히 분석했어: 객체 수준(촉각 질감, 촉각 형태)과 센서 수준(젤 상태). 우리는 이 정보의 두 가지 수준을 텍스트 설명을 통해 모델링하고, 고품질 촉각 샘플을 생성하기 위한 세밀한 텍스트-투-촉각 생성 방법(TextToucher)을 제안해. 구체적으로는, 객체 수준의 촉각 정보에 대한 텍스트 문장을 만드는 멀티모달 대형 언어 모델을 도입하고, 센서 수준의 촉각 정보를 나타내기 위해 학습 가능한 텍스트 프롬프트 세트를 사용해.

생성 과정을 더 잘 안내하기 위해 두 가지 텍스트 정보를 융합하고, 확산 변환기 구조 내에서 다양한 이중 텍스트 조건화 방법을 탐색해. 또한 텍스트 기반으로 생성된 촉각 데이터의 품질을 정확하게 평가하기 위한 대조적 텍스트-촉각 사전 훈련(CTTP) 메트릭을 제안해. 다양한 실험 결과는 우리의 TextToucher 방법이 우수하다는 것을 보여줘. 소스 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05442
Title: EndoOmni: Zero-Shot Cross-Dataset Depth Estimation in Endoscopy by Robust Self-Learning from Noisy Labels

Original Abstract:
Single-image depth estimation is essential for endoscopy tasks such as localization, reconstruction, and augmented reality. Most existing methods in surgical scenes focus on in-domain depth estimation, limiting their real-world applicability. This constraint stems from the scarcity and inferior labeling quality of medical data for training. In this work, we present EndoOmni, the first foundation model for zero-shot cross-domain depth estimation for endoscopy. To harness the potential of diverse training data, we refine the advanced self-learning paradigm that employs a teacher model to generate pseudo-labels, guiding a student model trained on large-scale labeled and unlabeled data. To address training disturbance caused by inherent noise in depth labels, we propose a robust training framework that leverages both depth labels and estimated confidence from the teacher model to jointly guide the student model training. Moreover, we propose a weighted scale-and-shift invariant loss to adaptively adjust learning weights based on label confidence, thus imposing learning bias towards cleaner label pixels while reducing the influence of highly noisy pixels. Experiments on zero-shot relative depth estimation show that our EndoOmni improves state-of-the-art methods in medical imaging for 41\% and existing foundation models for 25\% in terms of absolute relative error on specific dataset. Furthermore, our model provides strong initialization for fine-tuning to metric depth estimation, maintaining superior performance in both in-domain and out-of-domain scenarios. The source code will be publicly available.

Translated Abstract:
단일 이미지 깊이 추정은 내시경 작업에서 위치 파악, 재구성, 증강 현실 등에 매우 중요해. 기존의 방법들은 주로 수술 장면에서 깊이를 추정하는 데 집중하고 있는데, 이게 실제 상황에선 제한적이야. 그 이유는 의료 데이터가 부족하고 라벨링 품질이 별로 좋지 않기 때문이야.

이번 연구에서 우리는 EndoOmni라는 새로운 모델을 소개해. 이건 내시경을 위한 제로샷 크로스 도메인 깊이 추정의 첫 번째 기반 모델이야. 다양한 훈련 데이터의 장점을 활용하기 위해, 우리는 선생님 모델이 가짜 라벨을 생성하고, 이를 통해 대규모로 라벨이 있는 데이터와 없는 데이터로 훈련되는 학생 모델을 가이드하는 고급 자기 학습 패러다임을 개선했어.

깊이 라벨의 고유한 노이즈로 인해 훈련에 방해가 되는 문제를 해결하기 위해, 우리는 깊이 라벨과 선생님 모델이 추정한 신뢰도를 함께 활용해 학생 모델 훈련을 안내하는 강력한 훈련 프레임워크를 제안했어. 게다가, 우리는 라벨 신뢰도를 기반으로 학습 가중치를 조정할 수 있는 가중치 스케일 및 이동 불변 손실을 제안해서, 깨끗한 라벨 픽셀에 대한 학습 편향을 부여하고, 노이즈가 심한 픽셀의 영향을 줄이고 있어.

제로샷 상대 깊이 추정 실험에서, EndoOmni는 특정 데이터셋에서 의료 이미징의 최신 방법보다 41% 더 향상되었고, 기존의 기반 모델보다 25% 더 나은 성능을 보여줬어. 또한, 우리 모델은 메트릭 깊이 추정을 위한 파인튜닝에서도 강력한 초기화를 제공하며, 도메인 내외에서 모두 뛰어난 성능을 유지해. 소스 코드는 공개될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.05463
Title: DriveScape: Towards High-Resolution Controllable Multi-View Driving Video Generation

Original Abstract:
Recent advancements in generative models have provided promising solutions for synthesizing realistic driving videos, which are crucial for training autonomous driving perception models. However, existing approaches often struggle with multi-view video generation due to the challenges of integrating 3D information while maintaining spatial-temporal consistency and effectively learning from a unified model. In this paper, we propose an end-to-end framework named DriveScape for multi-view, 3D condition-guided video generation. DriveScape not only streamlines the process by integrating camera data to ensure comprehensive spatial-temporal coverage, but also introduces a Bi-Directional Modulated Transformer module to effectively align 3D road structural information. As a result, our approach enables precise control over video generation, significantly enhancing realism and providing a robust solution for generating multi-view driving videos. Our framework achieves state-of-the-art results on the nuScenes dataset, demonstrating impressive generative quality metrics with an FID score of 8.34 and an FVD score of 76.39, as well as superior performance across various perception tasks. This paves the way for more accurate environmental simulations in autonomous driving. Code will be available at our project homepage.

Translated Abstract:
최근 생성 모델의 발전 덕분에 현실감 있는 운전 비디오를 합성하는 데 유망한 솔루션이 생겼어. 이 비디오는 자율주행 인식 모델을 훈련하는 데 꼭 필요해. 하지만 기존 방법들은 3D 정보를 통합하면서도 공간-시간 일관성을 유지하고 통합 모델에서 효과적으로 학습하는 데 어려움을 겪고 있어.

이 논문에서는 DriveScape라는 이름의 엔드 투 엔드 프레임워크를 제안해. 이 프레임워크는 다중 시점 3D 조건에 따라 비디오를 생성할 수 있어. DriveScape는 카메라 데이터를 통합하여 포괄적인 공간-시간 범위를 보장하고, 3D 도로 구조 정보를 효과적으로 정렬하기 위해 Bi-Directional Modulated Transformer 모듈을 도입했어. 덕분에 비디오 생성을 정확하게 제어할 수 있게 되고, 현실감이 크게 향상돼. 

우리의 접근 방식은 nuScenes 데이터셋에서 최첨단 결과를 달성했어. 인상적인 생성 품질 지표로 FID 점수가 8.34, FVD 점수가 76.39를 기록했으며, 다양한 인식 작업에서도 뛰어난 성능을 보여줬어. 이로 인해 자율주행에서 더 정확한 환경 시뮬레이션이 가능해질 거야. 코드도 프로젝트 홈페이지에서 확인할 수 있을 거야.

================================================================================

URL: https://arxiv.org/abs/2409.05466
Title: Proto-OOD: Enhancing OOD Object Detection with Prototype Feature Similarity

Original Abstract:
The limited training samples for object detectors commonly result in low accuracy out-of-distribution (OOD) object detection. We have observed that feature vectors of the same class tend to cluster tightly in feature space, whereas those of different classes are more scattered. This insight motivates us to leverage feature similarity for OOD detection. Drawing on the concept of prototypes prevalent in few-shot learning, we introduce a novel network architecture, Proto-OOD, designed for this purpose. Proto-OOD enhances prototype representativeness through contrastive loss and identifies OOD data by assessing the similarity between input features and prototypes. It employs a negative embedding generator to create negative embedding, which are then used to train the similarity module. Proto-OOD achieves significantly lower FPR95 in MS-COCO dataset and higher mAP for Pascal VOC dataset, when utilizing Pascal VOC as ID dataset and MS-COCO as OOD dataset. Additionally, we identify limitations in existing evaluation metrics and propose an enhanced evaluation protocol.

Translated Abstract:
물체 감지를 위한 훈련 샘플이 제한적이면, 분포 밖(OUT-OF-DISTRIBUTION, OOD) 물체 감지의 정확도가 낮아지는 경우가 많아. 우리는 같은 클래스의 특징 벡터가 특징 공간에서 서로 가까이 모이는 경향이 있고, 다른 클래스의 벡터는 더 흩어져 있음을 발견했어. 이 통찰력은 OOD 감지를 위해 특징 유사성을 활용하게 만들었지.

우리는 몇 샷 학습에서 자주 사용되는 프로토타입 개념을 바탕으로, Proto-OOD라는 새로운 네트워크 아키텍처를 도입했어. Proto-OOD는 대조 손실을 통해 프로토타입의 대표성을 높이고, 입력 특징과 프로토타입 간의 유사성을 평가하여 OOD 데이터를 식별해. 여기서 음성 임베딩 생성기를 사용해 음성 임베딩을 만들어내고, 이를 통해 유사성 모듈을 훈련시켜.

Proto-OOD는 MS-COCO 데이터셋에서 FPR95를 크게 낮추고, Pascal VOC 데이터셋에서는 mAP를 높이는 성과를 보여줘. 여기서 Pascal VOC는 ID 데이터셋으로, MS-COCO는 OOD 데이터셋으로 사용했어. 그리고 기존 평가 지표의 한계를 확인하고, 개선된 평가 프로토콜도 제안했어.

================================================================================

URL: https://arxiv.org/abs/2409.05474
Title: PVP-Recon: Progressive View Planning via Warping Consistency for Sparse-View Surface Reconstruction

Original Abstract:
Neural implicit representations have revolutionized dense multi-view surface reconstruction, yet their performance significantly diminishes with sparse input views. A few pioneering works have sought to tackle the challenge of sparse-view reconstruction by leveraging additional geometric priors or multi-scene generalizability. However, they are still hindered by the imperfect choice of input views, using images under empirically determined viewpoints to provide considerable overlap. We propose PVP-Recon, a novel and effective sparse-view surface reconstruction method that progressively plans the next best views to form an optimal set of sparse viewpoints for image capturing. PVP-Recon starts initial surface reconstruction with as few as 3 views and progressively adds new views which are determined based on a novel warping score that reflects the information gain of each newly added view. This progressive view planning progress is interleaved with a neural SDF-based reconstruction module that utilizes multi-resolution hash features, enhanced by a progressive training scheme and a directional Hessian loss. Quantitative and qualitative experiments on three benchmark datasets show that our framework achieves high-quality reconstruction with a constrained input budget and outperforms existing baselines.

Translated Abstract:
신경 임플리시트 표현 방식은 밀집 멀티뷰 표면 재구성을 혁신적으로 변화시켰지만, 입력 뷰가 sparse할 때 성능이 크게 떨어져. 몇몇 선구적인 연구들이 추가적인 기하학적 정보를 활용하거나 여러 장면에서의 일반화 가능성을 통해 sparse-view 재구성 문제를 해결하려고 했어. 하지만 여전히 입력 뷰 선택이 완벽하지 않아서 경험적으로 정해진 시점에서 이미지를 사용해 많은 겹침이 있도록 하고 있어.

우리는 PVP-Recon이라는 새로운 sparse-view 표면 재구성 방법을 제안해. 이 방법은 다음에 어떤 뷰를 잡아야 할지를 계획하면서 최적의 sparse 뷰 세트를 형성해. PVP-Recon은 처음에 3개의 뷰로 표면 재구성을 시작하고, 새로운 뷰를 점진적으로 추가하는데, 이때 각 새로 추가된 뷰의 정보 이득을 반영하는 새로운 워핑 점수를 기반으로 결정해. 이렇게 진행되는 뷰 계획 과정은 멀티 해상도 해시 특징을 활용하는 신경 SDF 재구성 모듈과 교차하면서 진행돼. 이 모듈은 점진적인 훈련 방식과 방향성 헤시안 손실로 향상돼.

세 가지 벤치마크 데이터셋에서 진행한 정량적, 정성적 실험 결과, 우리의 프레임워크가 제한된 입력 예산으로도 고품질 재구성을 달성하고 기존의 기준보다 더 나은 성능을 보인다는 걸 확인했어.

================================================================================

URL: https://arxiv.org/abs/2409.05494
Title: An Atmospheric Correction Integrated LULC Segmentation Model for High-Resolution Satellite Imagery

Original Abstract:
The integration of fine-scale multispectral imagery with deep learning models has revolutionized land use and land cover (LULC) classification. However, the atmospheric effects present in Top-of-Atmosphere sensor measured Digital Number values must be corrected to retrieve accurate Bottom-of-Atmosphere surface reflectance for reliable analysis. This study employs look-up-table-based radiative transfer simulations to estimate the atmospheric path reflectance and transmittance for atmospherically correcting high-resolution CARTOSAT-3 Multispectral (MX) imagery for several Indian cities. The corrected surface reflectance data were subsequently used in supervised and semi-supervised segmentation models, demonstrating stability in multi-class (buildings, roads, trees and water bodies) LULC segmentation accuracy, particularly in scenarios with sparsely labelled data.

Translated Abstract:
정밀 다중 분광 이미지를 딥러닝 모델과 결합하는 것이 토지 이용 및 토지 피복(LULC) 분류에 큰 변화를 가져왔어. 하지만 대기에서 측정된 디지털 숫자 값에는 대기 효과가 있어서, 정확한 바닥 반사율을 얻기 위해서는 이를 보정해야 해. 

이 연구에서는 대기 경로 반사율과 투과율을 추정하기 위해 룩업 테이블 기반의 방사선 전이 시뮬레이션을 사용했어. 이 방법을 통해 인도 여러 도시의 고해상도 CARTOSAT-3 다중 분광 이미지를 대기 보정했어. 

그 다음, 보정된 표면 반사율 데이터를 감독 학습과 반감독 학습 세분화 모델에 사용했어. 그 결과, 여러 클래스(건물, 도로, 나무, 수역)의 LULC 세분화 정확도가 안정적으로 나타났고, 특히 레이블이 sparsely한 데이터 상황에서도 좋은 성과를 보였어.

================================================================================

URL: https://arxiv.org/abs/2409.05531
Title: HMAFlow: Learning More Accurate Optical Flow via Hierarchical Motion Field Alignment

Original Abstract:
Optical flow estimation is a fundamental and long-standing visual task. In this work, we present a novel method, dubbed HMAFlow, to improve optical flow estimation in these tough scenes, especially with small objects. The proposed model mainly consists of two core components: a Hierarchical Motion Field Alignment (HMA) module and a Correlation Self-Attention (CSA) module. In addition, we rebuild 4D cost volumes by employing a Multi-Scale Correlation Search (MCS) layer and replacing average pooling in common cost volumes with an search strategy using multiple search ranges. Experimental results demonstrate that our model achieves the best generalization performance in comparison to other state-of-the-art methods. Specifically, compared with RAFT, our method achieves relative error reductions of 14.2% and 3.4% on the clean pass and final pass of the Sintel online benchmark, respectively. On the KITTI test benchmark, HMAFlow surpasses RAFT and GMA in the Fl-all metric by a relative margin of 6.8% and 7.7%, respectively. To facilitate future research, our code will be made available at this https URL.

Translated Abstract:
광학 흐름 추정은 기본적이고 오랫동안 연구된 시각적 작업이야. 이 연구에서는 HMAFlow라는 새로운 방법을 소개해. 이 방법은 특히 작은 물체가 많은 어려운 장면에서 광학 흐름 추정을 개선하는 데 초점을 맞추고 있어.

제안된 모델은 두 가지 핵심 구성 요소로 이루어져 있어: 계층적 운동 필드 정렬(HMA) 모듈과 상관 자기 주의(CSA) 모듈. 그리고 4D 비용 볼륨을 재구성하기 위해 다중 스케일 상관 검색(MCS) 계층을 사용하고, 일반적인 비용 볼륨에서 평균 풀링을 여러 검색 범위를 사용하는 검색 전략으로 대체했어.

실험 결과를 보면, 우리 모델이 다른 최첨단 방법들과 비교했을 때 가장 좋은 일반화 성능을 보여줬어. 특히 RAFT와 비교했을 때, 우리 방법은 Sintel 온라인 벤치마크의 클린 패스와 최종 패스에서 각각 14.2%와 3.4%의 상대 오류 감소를 달성했어. KITTI 테스트 벤치마크에서는 HMAFlow가 RAFT와 GMA를 Fl-all 지표에서 각각 6.8%와 7.7%의 상대적인 차이로 초월했어.

앞으로의 연구를 위해, 우리의 코드는 이 URL에서 제공될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.05540
Title: Exploring Rich Subjective Quality Information for Image Quality Assessment in the Wild

Original Abstract:
Traditional in the wild image quality assessment (IQA) models are generally trained with the quality labels of mean opinion score (MOS), while missing the rich subjective quality information contained in the quality ratings, for example, the standard deviation of opinion scores (SOS) or even distribution of opinion scores (DOS). In this paper, we propose a novel IQA method named RichIQA to explore the rich subjective rating information beyond MOS to predict image quality in the wild. RichIQA is characterized by two key novel designs: (1) a three-stage image quality prediction network which exploits the powerful feature representation capability of the Convolutional vision Transformer (CvT) and mimics the short-term and long-term memory mechanisms of human brain; (2) a multi-label training strategy in which rich subjective quality information like MOS, SOS and DOS are concurrently used to train the quality prediction network. Powered by these two novel designs, RichIQA is able to predict the image quality in terms of a distribution, from which the mean image quality can be subsequently obtained. Extensive experimental results verify that the three-stage network is tailored to predict rich quality information, while the multi-label training strategy can fully exploit the potentials within subjective quality rating and enhance the prediction performance and generalizability of the network. RichIQA outperforms state-of-the-art competitors on multiple large-scale in the wild IQA databases with rich subjective rating labels. The code of RichIQA will be made publicly available on GitHub.

Translated Abstract:
전통적인 야외 이미지 품질 평가(IQA) 모델은 일반적으로 평균 의견 점수(MOS)라는 품질 레이블로 학습되는데, 이 과정에서 의견 점수의 표준 편차(SOS)나 의견 점수의 분포(DOS) 같은 다양한 주관적 품질 정보를 놓치고 있어. 

이번 논문에서는 MOS를 넘어서는 다양한 주관적 평가 정보를 활용해 야외 이미지 품질을 예측하는 새로운 IQA 방법인 RichIQA를 제안해. RichIQA는 두 가지 주요한 새로운 설계가 특징이야: 

1. 강력한 특징 표현 능력을 가진 합성곱 비전 트랜스포머(CvT)를 활용하고, 인간의 단기 및 장기 기억 메커니즘을 모방한 3단계 이미지 품질 예측 네트워크.
2. MOS, SOS, DOS 같은 다양한 주관적 품질 정보를 동시에 사용해 품질 예측 네트워크를 훈련하는 다중 라벨 훈련 전략.

이 두 가지 새로운 설계를 통해 RichIQA는 이미지 품질을 분포 형태로 예측할 수 있고, 여기서 평균 이미지 품질도 얻을 수 있어. 다양한 실험 결과는 3단계 네트워크가 풍부한 품질 정보를 예측하는 데 적합하고, 다중 라벨 훈련 전략이 주관적 품질 평가의 잠재력을 최대한 활용해 예측 성능과 일반화 능력을 향상시킬 수 있음을 보여줘. 

RichIQA는 여러 대규모 야외 IQA 데이터베이스에서 최신 기술을 앞서는 성능을 보였고, RichIQA의 코드는 GitHub에 공개될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.05552
Title: Seeing is Believing? Enhancing Vision-Language Navigation using Visual Perturbations

Original Abstract:
Autonomous navigation for an embodied agent guided by natural language instructions remains a formidable challenge in vision-and-language navigation (VLN). Despite remarkable recent progress in learning fine-grained and multifarious visual representations, the tendency to overfit to the training environments leads to unsatisfactory generalization performance. In this work, we present a versatile Multi-Branch Architecture (MBA) aimed at exploring and exploiting diverse visual inputs. Specifically, we introduce three distinct visual variants: ground-truth depth images, visual inputs integrated with incongruent views, and those infused with random noise to enrich the diversity of visual input representation and prevent overfitting to the original RGB observations. To adaptively fuse these varied inputs, the proposed MBA extend a base agent model into a multi-branch variant, where each branch processes a different visual input. Surprisingly, even random noise can further enhance navigation performance in unseen environments. Extensive experiments conducted on three VLN benchmarks (R2R, REVERIE, SOON) demonstrate that our proposed method equals or even surpasses state-of-the-art results. The source code will be publicly available.

Translated Abstract:
자연어 지침에 따라 움직이는 자율 탐색은 비전-언어 탐색(VLN)에서 여전히 큰 도전 과제가 되고 있어. 최근에 시각적 표현을 배우는 데 많은 발전이 있었지만, 훈련 환경에 너무 맞춰버리는 경향 때문에 일반화 성능이 좋지 않아. 

이 연구에서는 다양한 시각적 입력을 탐색하고 활용하기 위해 다목적 멀티 브랜치 아키텍처(MBA)를 소개해. 특히, 세 가지 다른 시각적 변형을 도입했어: 실제 깊이 이미지, 서로 다른 시점이 결합된 시각적 입력, 그리고 무작위 노이즈가 섞인 입력이야. 이렇게 해서 시각적 입력 표현의 다양성을 높이고 원래 RGB 관측에 과적합되는 걸 방지하려고 해. 

이 다양한 입력을 적절히 결합하기 위해, 제안한 MBA는 기본 에이전트 모델을 멀티 브랜치 변형으로 확장해. 각 브랜치는 다른 시각적 입력을 처리해. 놀랍게도, 무작위 노이즈조차도 보지 못한 환경에서 탐색 성능을 높일 수 있어. 

세 가지 VLN 벤치마크(R2R, REVERIE, SOON)에서 진행한 실험 결과, 우리 방법이 최신 기술과 동등하거나 더 나은 성능을 보여줬어. 소스 코드는 공개될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.05558
Title: Seeing Through the Mask: Rethinking Adversarial Examples for CAPTCHAs

Original Abstract:
Modern CAPTCHAs rely heavily on vision tasks that are supposedly hard for computers but easy for humans. However, advances in image recognition models pose a significant threat to such CAPTCHAs. These models can easily be fooled by generating some well-hidden "random" noise and adding it to the image, or hiding objects in the image. However, these methods are model-specific and thus can not aid CAPTCHAs in fooling all models. We show in this work that by allowing for more significant changes to the images while preserving the semantic information and keeping it solvable by humans, we can fool many state-of-the-art models. Specifically, we demonstrate that by adding masks of various intensities the Accuracy @ 1 (Acc@1) drops by more than 50%-points for all models, and supposedly robust models such as vision transformers see an Acc@1 drop of 80%-points.
These masks can therefore effectively fool modern image classifiers, thus showing that machines have not caught up with humans -- yet.

Translated Abstract:
현대의 CAPTCHA는 컴퓨터에게 어렵고 인간에게는 쉬운 시각적 작업에 크게 의존하고 있어. 그런데 이미지 인식 모델의 발전이 이런 CAPTCHA에 큰 위협이 되고 있어. 이 모델들은 잘 숨겨진 "무작위" 노이즈를 이미지에 추가하거나 이미지 속 물체를 숨기면 쉽게 속일 수 있어. 하지만 이런 방법은 특정 모델에만 적용되기 때문에 모든 모델을 속이는 데는 도움이 되지 않아.

이번 연구에서는 이미지의 의미 정보를 유지하면서도 더 큰 변화를 허용하면 많은 최신 모델을 속일 수 있다는 걸 보여줘. 특히 다양한 강도의 마스크를 추가하면 모든 모델에서 정확도(Acc@1)가 50% 이상 떨어지고, 비전 트랜스포머 같은 꽤 강력한 모델은 80%까지 떨어지는 걸 확인했어.

이 마스크는 현대 이미지 분류기를 효과적으로 속일 수 있어서, 기계가 아직 인간을 따라잡지 못했다는 걸 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.05564
Title: LEROjD: Lidar Extended Radar-Only Object Detection

Original Abstract:
Accurate 3D object detection is vital for automated driving. While lidar sensors are well suited for this task, they are expensive and have limitations in adverse weather conditions. 3+1D imaging radar sensors offer a cost-effective, robust alternative but face challenges due to their low resolution and high measurement noise. Existing 3+1D imaging radar datasets include radar and lidar data, enabling cross-modal model improvements. Although lidar should not be used during inference, it can aid the training of radar-only object detectors. We explore two strategies to transfer knowledge from the lidar to the radar domain and radar-only object detectors: 1. multi-stage training with sequential lidar point cloud thin-out, and 2. cross-modal knowledge distillation. In the multi-stage process, three thin-out methods are examined. Our results show significant performance gains of up to 4.2 percentage points in mean Average Precision with multi-stage training and up to 3.9 percentage points with knowledge distillation by initializing the student with the teacher's weights. The main benefit of these approaches is their applicability to other 3D object detection networks without altering their architecture, as we show by analyzing it on two different object detectors. Our code is available at this https URL

Translated Abstract:
정확한 3D 물체 감지는 자율 주행에 정말 중요해. 라이다 센서는 이 작업에 잘 맞지만, 가격이 비싸고 나쁜 날씨에서는 한계가 있어. 3+1D 이미징 레이더 센서는 비용 효과적이고 튼튼한 대안이지만, 해상도가 낮고 측정 노이즈가 많아서 어려움이 있어. 

현재 있는 3+1D 이미징 레이더 데이터셋은 레이더와 라이다 데이터를 포함하고 있어서, 서로 다른 데이터의 모델 성능을 개선할 수 있어. 라이다는 추론할 때 사용하면 안 되지만, 레이더 전용 물체 탐지기를 훈련하는 데는 도움이 될 수 있어. 우리는 라이다에서 레이더로 지식을 전이하는 두 가지 전략을 살펴봤어: 1. 연속적인 라이다 포인트 클라우드 얇게 만들기(multi-stage training)와 2. 교차 모달 지식 증류(cross-modal knowledge distillation). 

다단계 과정에서는 세 가지 얇게 만드는 방법을 검토했어. 우리 결과는 다단계 훈련을 통해 평균 정밀도(Mean Average Precision)가 최대 4.2% 향상되었고, 지식 증류를 통해서는 최대 3.9% 향상되었다는 걸 보여줬어. 이 방법들의 가장 큰 장점은 다른 3D 물체 탐지 네트워크에 아키텍처를 바꾸지 않고도 적용할 수 있다는 거야. 우리는 두 가지 다른 물체 탐지기를 분석하면서 이를 보여줬어. 우리 코드는 이 https URL에서 사용할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05585
Title: Latent 3D Brain MRI Counterfactual

Original Abstract:
The number of samples in structural brain MRI studies is often too small to properly train deep learning models. Generative models show promise in addressing this issue by effectively learning the data distribution and generating high-fidelity MRI. However, they struggle to produce diverse, high-quality data outside the distribution defined by the training data. One way to address the issue is using causal models developed for 3D volume counterfactuals. However, accurately modeling causality in high-dimensional spaces is a challenge so that these models generally generate 3D brain MRIS of lower quality. To address these challenges, we propose a two-stage method that constructs a Structural Causal Model (SCM) within the latent space. In the first stage, we employ a VQ-VAE to learn a compact embedding of the MRI volume. Subsequently, we integrate our causal model into this latent space and execute a three-step counterfactual procedure using a closed-form Generalized Linear Model (GLM). Our experiments conducted on real-world high-resolution MRI data (1mm) demonstrate that our method can generate high-quality 3D MRI counterfactuals.

Translated Abstract:
구조적 뇌 MRI 연구에서 샘플 수가 너무 작아서 딥러닝 모델을 제대로 훈련하기 어려워. 생성 모델은 데이터 분포를 잘 학습하고 고품질의 MRI를 생성하는 데 도움을 줄 수 있지만, 훈련 데이터로 정의된 분포 밖에서 다양한 고품질 데이터를 만드는 데는 한계가 있어. 

이 문제를 해결하는 한 가지 방법은 3D 볼륨 반사 모델을 위해 개발된 인과 모델을 사용하는 거야. 하지만 고차원 공간에서 인과 관계를 정확하게 모델링하는 건 쉽지 않아서, 이런 모델들이 일반적으로 낮은 품질의 3D 뇌 MRI를 생성하게 돼. 

이런 문제를 해결하기 위해, 우리는 잠재 공간에서 구조적 인과 모델(SCM)을 만드는 두 단계 방법을 제안해. 첫 번째 단계에서는 VQ-VAE를 사용해서 MRI 볼륨의 컴팩트한 임베딩을 학습해. 그 다음에는 이 잠재 공간에 우리의 인과 모델을 통합하고, 닫힌 형태의 일반화 선형 모델(GLM)을 사용해 세 단계의 반사 절차를 실행해. 

실제 고해상도 MRI 데이터(1mm)에서 진행한 실험 결과, 우리의 방법이 고품질의 3D MRI 반사 데이터를 생성할 수 있다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.05587
Title: DSDFormer: An Innovative Transformer-Mamba Framework for Robust High-Precision Driver Distraction Identification

Original Abstract:
Driver distraction remains a leading cause of traffic accidents, posing a critical threat to road safety globally. As intelligent transportation systems evolve, accurate and real-time identification of driver distraction has become essential. However, existing methods struggle to capture both global contextual and fine-grained local features while contending with noisy labels in training datasets. To address these challenges, we propose DSDFormer, a novel framework that integrates the strengths of Transformer and Mamba architectures through a Dual State Domain Attention (DSDA) mechanism, enabling a balance between long-range dependencies and detailed feature extraction for robust driver behavior recognition. Additionally, we introduce Temporal Reasoning Confident Learning (TRCL), an unsupervised approach that refines noisy labels by leveraging spatiotemporal correlations in video sequences. Our model achieves state-of-the-art performance on the AUC-V1, AUC-V2, and 100-Driver datasets and demonstrates real-time processing efficiency on the NVIDIA Jetson AGX Orin platform. Extensive experimental results confirm that DSDFormer and TRCL significantly improve both the accuracy and robustness of driver distraction detection, offering a scalable solution to enhance road safety.

Translated Abstract:
운전자의 주의 분산은 교통사고의 주요 원인 중 하나로, 전 세계적으로 도로 안전에 큰 위협이 되고 있어. 똑똑한 교통 시스템이 발전하면서 운전자의 주의 분산을 정확하게 그리고 실시간으로 파악하는 게 점점 더 중요해지고 있어. 하지만 기존 방법들은 전반적인 맥락과 세부적인 특징을 동시에 잘 잡아내지 못하고, 훈련 데이터셋의 노이즈가 많은 라벨 문제 때문에 어려움을 겪고 있어.

이런 문제를 해결하기 위해 우리는 DSDFormer라는 새로운 프레임워크를 제안해. 이건 Transformer와 Mamba 구조의 장점을 결합한 Dual State Domain Attention (DSDA) 메커니즘을 통해 장거리 의존성과 세부적인 특징 추출의 균형을 맞춰서 운전자의 행동 인식에 강력한 성능을 발휘할 수 있게 해줘. 

또한, Temporal Reasoning Confident Learning (TRCL)이라는 비지도 학습 방법을 도입했어. 이 방법은 비디오 시퀀스의 시공간적 상관관계를 이용해 노이즈가 있는 라벨을 개선해. 우리의 모델은 AUC-V1, AUC-V2, 100-Driver 데이터셋에서 최첨단 성능을 달성했고, NVIDIA Jetson AGX Orin 플랫폼에서 실시간 처리 효율성도 보여줬어. 

많은 실험 결과에서 DSDFormer와 TRCL이 운전자의 주의 분산 탐지의 정확도와 견고성을 크게 개선한다는 걸 확인했어. 이건 도로 안전을 높이기 위한 확장 가능한 솔루션을 제공해.

================================================================================

URL: https://arxiv.org/abs/2409.05595
Title: SynMorph: Generating Synthetic Face Morphing Dataset with Mated Samples

Original Abstract:
Face morphing attack detection (MAD) algorithms have become essential to overcome the vulnerability of face recognition systems. To solve the lack of large-scale and public-available datasets due to privacy concerns and restrictions, in this work we propose a new method to generate a synthetic face morphing dataset with 2450 identities and more than 100k morphs. The proposed synthetic face morphing dataset is unique for its high-quality samples, different types of morphing algorithms, and the generalization for both single and differential morphing attack detection algorithms. For experiments, we apply face image quality assessment and vulnerability analysis to evaluate the proposed synthetic face morphing dataset from the perspective of biometric sample quality and morphing attack potential on face recognition systems. The results are benchmarked with an existing SOTA synthetic dataset and a representative non-synthetic and indicate improvement compared with the SOTA. Additionally, we design different protocols and study the applicability of using the proposed synthetic dataset on training morphing attack detection algorithms.

Translated Abstract:
얼굴 변형 공격 탐지(MAD) 알고리즘은 얼굴 인식 시스템의 취약점을 극복하는 데 필수적이야. 하지만 개인정보 보호 문제 때문에 대규모 공개 데이터셋이 부족해. 그래서 우리는 2450개의 아이디와 10만 개 이상의 변형을 가진 합성 얼굴 변형 데이터셋을 생성하는 새로운 방법을 제안해.

이 합성 얼굴 변형 데이터셋은 고품질 샘플, 다양한 변형 알고리즘, 그리고 단일 및 차별적 변형 공격 탐지 알고리즘 모두에 대해 일반화된 점에서 독특해. 실험을 위해 우리는 얼굴 이미지 품질 평가와 취약성 분석을 적용해서 제안한 합성 얼굴 변형 데이터셋의 생체 샘플 품질과 얼굴 인식 시스템의 변형 공격 가능성을 평가했어. 결과는 기존의 최신 합성 데이터셋과 대표적인 비합성 데이터셋과 비교했을 때 개선된 것으로 나타났어.

또한, 우리는 다양한 프로토콜을 설계하고 제안한 합성 데이터셋이 변형 공격 탐지 알고리즘 훈련에 어떻게 적용될 수 있는지를 연구했어.

================================================================================

URL: https://arxiv.org/abs/2409.05606
Title: CustomContrast: A Multilevel Contrastive Perspective For Subject-Driven Text-to-Image Customization

Original Abstract:
Subject-driven text-to-image (T2I) customization has drawn significant interest in academia and industry. This task enables pre-trained models to generate novel images based on unique subjects. Existing studies adopt a self-reconstructive perspective, focusing on capturing all details of a single image, which will misconstrue the specific image's irrelevant attributes (e.g., view, pose, and background) as the subject intrinsic attributes. This misconstruction leads to both overfitting or underfitting of irrelevant and intrinsic attributes of the subject, i.e., these attributes are over-represented or under-represented simultaneously, causing a trade-off between similarity and controllability. In this study, we argue an ideal subject representation can be achieved by a cross-differential perspective, i.e., decoupling subject intrinsic attributes from irrelevant attributes via contrastive learning, which allows the model to focus more on intrinsic attributes through intra-consistency (features of the same subject are spatially closer) and inter-distinctiveness (features of different subjects have distinguished differences). Specifically, we propose CustomContrast, a novel framework, which includes a Multilevel Contrastive Learning (MCL) paradigm and a Multimodal Feature Injection (MFI) Encoder. The MCL paradigm is used to extract intrinsic features of subjects from high-level semantics to low-level appearance through crossmodal semantic contrastive learning and multiscale appearance contrastive learning. To facilitate contrastive learning, we introduce the MFI encoder to capture cross-modal representations. Extensive experiments show the effectiveness of CustomContrast in subject similarity and text controllability.

Translated Abstract:
주제 기반 텍스트-이미지(T2I) 맞춤화가 학계와 산업에서 큰 관심을 받고 있어. 이 작업은 미리 훈련된 모델이 독특한 주제를 바탕으로 새로운 이미지를 생성할 수 있게 해줘. 기존 연구들은 자기 재구성 관점에서 접근하는데, 이는 단일 이미지의 모든 세부사항을 포착하는 데 집중해. 이 과정에서 특정 이미지의 관련 없는 속성들(예: 시각, 자세, 배경)을 주제의 본질적인 속성으로 잘못 해석하게 돼. 이런 오해는 주제의 관련 없는 속성과 본질적인 속성이 각각 과대 또는 과소 표현되게 만들어. 그래서 유사성과 제어 가능성 사이에 균형을 이룰 수 없게 되지.

이번 연구에서는 이상적인 주제 표현이 교차 차별적 관점을 통해 이뤄질 수 있다고 주장해. 즉, 대조 학습을 통해 주제의 본질적인 속성과 관련 없는 속성을 분리하는 거야. 이렇게 하면 모델이 본질적인 속성에 더 집중할 수 있어. 내부 일관성(같은 주제의 특징들이 공간적으로 가깝게 위치)과 외부 독특함(다른 주제의 특징들이 뚜렷한 차이를 보임)을 통해 가능해.

특히, 우리는 CustomContrast라는 새로운 프레임워크를 제안해. 이 프레임워크에는 다수준 대조 학습(MCL) 패러다임과 다중 모달 특징 주입(MFI) 인코더가 포함돼. MCL 패러다임은 고수준 의미에서 저수준 외형까지 주제의 본질적인 특징을 추출하는 데 사용돼. 이를 위해 대조 학습을 용이하게 하는 MFI 인코더를 도입해 다양한 모달 표현을 캡처해. 여러 실험 결과 CustomContrast가 주제 유사성과 텍스트 제어 가능성에서 효과적이라는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.05611
Title: Adapted-MoE: Mixture of Experts with Test-Time Adaption for Anomaly Detection

Original Abstract:
Most unsupervised anomaly detection methods based on representations of normal samples to distinguish anomalies have recently made remarkable progress. However, existing methods only learn a single decision boundary for distinguishing the samples within the training dataset, neglecting the variation in feature distribution for normal samples even in the same category in the real world. Furthermore, it was not considered that a distribution bias still exists between the test set and the train set. Therefore, we propose an Adapted-MoE which contains a routing network and a series of expert models to handle multiple distributions of same-category samples by divide and conquer. Specifically, we propose a routing network based on representation learning to route same-category samples into the subclasses feature space. Then, a series of expert models are utilized to learn the representation of various normal samples and construct several independent decision boundaries. We propose the test-time adaption to eliminate the bias between the unseen test sample representation and the feature distribution learned by the expert model. Our experiments are conducted on a dataset that provides multiple subclasses from three categories, namely Texture AD benchmark. The Adapted-MoE significantly improves the performance of the baseline model, achieving 2.18%-7.20% and 1.57%-16.30% increase in I-AUROC and P-AUROC, which outperforms the current state-of-the-art methods. Our code is available at this https URL.

Translated Abstract:
최근에 정상 샘플의 표현을 기반으로 이상치를 구별하는 대부분의 비지도 이상 탐지 방법들이 눈에 띄는 발전을 이뤘어. 하지만 기존 방법들은 훈련 데이터셋 내의 샘플들을 구별하기 위해 단일 결정 경계만 배우고, 같은 카테고리의 정상 샘플들 간의 피처 분포 변화는 무시하고 있어. 또한, 테스트 세트와 훈련 세트 간에도 여전히 분포 편향이 존재한다는 점도 고려하지 않았어.

그래서 우리는 Adapted-MoE라는 방법을 제안해. 이건 라우팅 네트워크와 여러 전문가 모델로 구성되어, 같은 카테고리의 샘플들을 나누어서 처리할 수 있어. 구체적으로는, 같은 카테고리의 샘플들을 서브클래스 피처 공간으로 라우팅하기 위해 표현 학습 기반의 라우팅 네트워크를 제안해. 그리고 여러 전문가 모델을 활용해서 다양한 정상 샘플의 표현을 배우고 여러 독립적인 결정 경계를 생성해.

우리는 테스트 시간 적응 방식을 제안해서 보지 못한 테스트 샘플 표현과 전문가 모델이 학습한 피처 분포 간의 편향을 없애려고 해. 우리의 실험은 Texture AD 벤치마크라는 데이터셋에서 세 가지 카테고리의 여러 서브클래스를 제공하는 데이터를 바탕으로 진행했어. Adapted-MoE는 기본 모델의 성능을 크게 개선해서 I-AUROC에서 2.18%-7.20% 증가, P-AUROC에서 1.57%-16.30% 증가를 달성했고, 현재의 최신 방법들을 뛰어넘었어. 우리의 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05617
Title: G-NeLF: Memory- and Data-Efficient Hybrid Neural Light Field for Novel View Synthesis

Original Abstract:
Following the burgeoning interest in implicit neural representation, Neural Light Field (NeLF) has been introduced to predict the color of a ray directly. Unlike Neural Radiance Field (NeRF), NeLF does not create a point-wise representation by predicting color and volume density for each point in space. However, the current NeLF methods face a challenge as they need to train a NeRF model first and then synthesize over 10K views to train NeLF for improved performance. Additionally, the rendering quality of NeLF methods is lower compared to NeRF methods. In this paper, we propose G-NeLF, a versatile grid-based NeLF approach that utilizes spatial-aware features to unleash the potential of the neural network's inference capability, and consequently overcome the difficulties of NeLF training. Specifically, we employ a spatial-aware feature sequence derived from a meticulously crafted grid as the ray's representation. Drawing from our empirical studies on the adaptability of multi-resolution hash tables, we introduce a novel grid-based ray representation for NeLF that can represent the entire space with a very limited number of parameters. To better utilize the sequence feature, we design a lightweight ray color decoder that simulates the ray propagation process, enabling a more efficient inference of the ray's color. G-NeLF can be trained without necessitating significant storage overhead and with the model size of only 0.95 MB to surpass previous state-of-the-art NeLF. Moreover, compared with grid-based NeRF methods, e.g., Instant-NGP, we only utilize one-tenth of its parameters to achieve higher performance. Our code will be released upon acceptance.

Translated Abstract:
최근 암묵적 신경 표현에 대한 관심이 커지면서, 신경 광장(Neural Light Field, NeLF)이 광선의 색상을 직접 예측하는 방식으로 소개되었어. Neural Radiance Field (NeRF)와는 다르게, NeLF는 공간의 각 점에 대해 색상과 볼륨 밀도를 예측하는 포인트 기반 표현을 만들지 않아. 하지만 현재의 NeLF 방법들은 먼저 NeRF 모델을 훈련해야 하고, 그 뒤에 10,000개 이상의 뷰를 합성해서 NeLF를 훈련해야 하는 어려움이 있어. 또한, NeLF 방법들의 렌더링 품질이 NeRF 방법들보다 낮아.

이 논문에서는 G-NeLF라는 다양한 기능을 가진 격자 기반 NeLF 접근법을 제안해. 이 방법은 공간 인지 기능을 활용해 신경망의 추론 능력을 최대한 활용하고, NeLF 훈련의 어려움을 극복할 수 있어. 구체적으로, 우리는 정교하게 제작된 격자에서 유도된 공간 인지 기능 시퀀스를 광선의 표현으로 사용해.

우리가 다중 해상도 해시 테이블의 적응성에 대한 실험적 연구를 바탕으로, 전체 공간을 매우 적은 수의 매개변수로 표현할 수 있는 새로운 격자 기반 광선 표현을 도입했어. 시퀀스 기능을 더 잘 활용하기 위해, 우리는 광선 전파 과정을 시뮬레이션하는 경량의 광선 색상 디코더를 설계했어. G-NeLF는 많은 저장 공간을 필요로 하지 않고, 모델 크기가 단 0.95MB로 이전의 최첨단 NeLF를 초월할 수 있어. 게다가, Instant-NGP 같은 격자 기반 NeRF 방법들과 비교했을 때, 우리는 더 높은 성능을 달성하기 위해 그 매개변수의 10분의 1만 사용해. 우리의 코드는 수락되면 공개할 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.05624
Title: Renormalized Connection for Scale-preferred Object Detection in Satellite Imagery

Original Abstract:
Satellite imagery, due to its long-range imaging, brings with it a variety of scale-preferred tasks, such as the detection of tiny/small objects, making the precise localization and detection of small objects of interest a challenging task. In this article, we design a Knowledge Discovery Network (KDN) to implement the renormalization group theory in terms of efficient feature extraction. Renormalized connection (RC) on the KDN enables ``synergistic focusing'' of multi-scale features. Based on our observations of KDN, we abstract a class of RCs with different connection strengths, called n21C, and generalize it to FPN-based multi-branch detectors. In a series of FPN experiments on the scale-preferred tasks, we found that the ``divide-and-conquer'' idea of FPN severely hampers the detector's learning in the right direction due to the large number of large-scale negative samples and interference from background noise. Moreover, these negative samples cannot be eliminated by the focal loss function. The RCs extends the multi-level feature's ``divide-and-conquer'' mechanism of the FPN-based detectors to a wide range of scale-preferred tasks, and enables synergistic effects of multi-level features on the specific learning goal. In addition, interference activations in two aspects are greatly reduced and the detector learns in a more correct direction. Extensive experiments of 17 well-designed detection architectures embedded with n21s on five different levels of scale-preferred tasks validate the effectiveness and efficiency of the RCs. Especially the simplest linear form of RC, E421C performs well in all tasks and it satisfies the scaling property of RGT. We hope that our approach will transfer a large number of well-designed detectors from the computer vision community to the remote sensing community.

Translated Abstract:
위성 이미지는 원거리 촬영 덕분에 다양한 규모의 작업을 다룰 수 있지만, 작은 물체를 정확하게 찾아내고 위치를 파악하는 게 정말 어려워. 이 논문에서는 효율적인 특징 추출을 위해 지식 발견 네트워크(KDN)를 설계했어. KDN의 재정규화 연결(RC)은 다중 스케일 특징의 "시너지 집중"을 가능하게 해.

KDN을 관찰하면서, 여러 연결 강도를 가진 RC 클래스인 n21C를 추상화했어. 그리고 이걸 FPN 기반의 다중 분기 탐지기로 일반화했지. 여러 FPN 실험을 통해, FPN의 "분할 및 정복" 아이디어가 많은 대규모 음성 샘플과 배경 잡음 때문에 탐지기의 학습 방향에 큰 방해가 된다는 걸 발견했어. 게다가, 이런 음성 샘플은 초점 손실 함수로도 없앨 수 없어.

RC는 FPN 기반 탐지기의 다중 레벨 특징 "분할 및 정복" 메커니즘을 여러 규모의 작업에 확장하고, 특정 학습 목표에 대해 다중 레벨 특징의 시너지 효과를 가능하게 해. 또한, 두 가지 측면에서의 간섭 활성화가 크게 줄어들고 탐지기가 더 올바른 방향으로 학습할 수 있어.

n21s가 내장된 17개의 잘 설계된 탐지 아키텍처에 대한 광범위한 실험을 통해 RC의 효과성과 효율성을 검증했어. 특히, 가장 간단한 선형 형태인 E421C가 모든 작업에서 잘 수행하고 RGT의 스케일링 속성도 만족해. 우리는 우리의 접근 방식이 컴퓨터 비전 커뮤니티에서 원거리 센싱 커뮤니티로 많은 잘 설계된 탐지기를 전이할 수 있기를 바래.

================================================================================

URL: https://arxiv.org/abs/2409.05636
Title: 3D-SAR Tomography and Machine Learning for High-Resolution Tree Height Estimation

Original Abstract:
Accurately estimating forest biomass is crucial for global carbon cycle modelling and climate change mitigation. Tree height, a key factor in biomass calculations, can be measured using Synthetic Aperture Radar (SAR) technology. This study applies machine learning to extract forest height data from two SAR products: Single Look Complex (SLC) images and tomographic cubes, in preparation for the ESA Biomass Satellite mission. We use the TomoSense dataset, containing SAR and LiDAR data from Germany's Eifel National Park, to develop and evaluate height estimation models. Our approach includes classical methods, deep learning with a 3D U-Net, and Bayesian-optimized techniques. By testing various SAR frequencies and polarimetries, we establish a baseline for future height and biomass modelling. Best-performing models predict forest height to be within 2.82m mean absolute error for canopies around 30m, advancing our ability to measure global carbon stocks and support climate action.

Translated Abstract:
숲의 바이오매스를 정확하게 추정하는 것은 전 세계 탄소 순환 모델링과 기후 변화 완화에 매우 중요해. 나무 높이는 바이오매스 계산에서 핵심적인 요소인데, 이걸 합성 개구 레이더(SAR) 기술을 사용해서 측정할 수 있어. 

이 연구는 ESA 바이오매스 위성 임무를 준비하기 위해 두 개의 SAR 제품, 즉 단일 시점 복합(SLC) 이미지와 단층 큐브에서 숲 높이 데이터를 추출하기 위해 머신 러닝을 적용하고 있어. 우리는 독일의 아이펠 국립공원에서 SAR과 LiDAR 데이터를 포함한 TomoSense 데이터셋을 사용해서 높이 추정 모델을 개발하고 평가해. 

우리의 접근 방식은 전통적인 방법, 3D U-Net을 이용한 딥러닝, 그리고 베이지안 최적화 기법을 포함해. 다양한 SAR 주파수와 편파를 테스트하면서 미래의 높이와 바이오매스 모델링을 위한 기준선을 설정했어. 성능이 가장 좋은 모델은 약 30m 높이의 나무들에 대해 평균 절대 오차가 2.82m 이내로 숲의 높이를 예측하고, 이를 통해 전 세계 탄소 저장량을 측정하고 기후 행동을 지원하는 데 도움을 줄 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05642
Title: Prototype-Driven Multi-Feature Generation for Visible-Infrared Person Re-identification

Original Abstract:
The primary challenges in visible-infrared person re-identification arise from the differences between visible (vis) and infrared (ir) images, including inter-modal and intra-modal variations. These challenges are further complicated by varying viewpoints and irregular movements. Existing methods often rely on horizontal partitioning to align part-level features, which can introduce inaccuracies and have limited effectiveness in reducing modality discrepancies. In this paper, we propose a novel Prototype-Driven Multi-feature generation framework (PDM) aimed at mitigating cross-modal discrepancies by constructing diversified features and mining latent semantically similar features for modal alignment. PDM comprises two key components: Multi-Feature Generation Module (MFGM) and Prototype Learning Module (PLM). The MFGM generates diversity features closely distributed from modality-shared features to represent pedestrians. Additionally, the PLM utilizes learnable prototypes to excavate latent semantic similarities among local features between visible and infrared modalities, thereby facilitating cross-modal instance-level alignment. We introduce the cosine heterogeneity loss to enhance prototype diversity for extracting rich local features. Extensive experiments conducted on the SYSU-MM01 and LLCM datasets demonstrate that our approach achieves state-of-the-art performance. Our codes are available at this https URL.

Translated Abstract:
가시광선과 적외선 이미지를 이용한 사람 재식별에서는 여러 가지 어려움이 있어. 그중에는 가시광선 이미지와 적외선 이미지 간의 차이, 즉 모달 간의 변동과 모달 내 변동이 포함돼. 그리고 다양한 시점과 불규칙한 움직임 때문에 문제가 더 복잡해져. 

기존 방법들은 보통 수평 분할을 이용해 부분별 특징을 정렬하는데, 이 방식은 정확도를 떨어뜨리고 모달 간의 차이를 줄이는 데 한계가 있어. 그래서 우리는 새로운 프로토타입 기반 다중 특징 생성 프레임워크(PDM)를 제안해. 이 프레임워크는 다양한 특징을 만들고, 모달 정렬을 위해 의미상 유사한 숨겨진 특징을 찾는 데 초점을 맞추고 있어.

PDM은 두 가지 주요 구성 요소로 이루어져 있어: 다중 특징 생성 모듈(MFGM)과 프로토타입 학습 모듈(PLM). MFGM은 보행자를 표현하기 위해 모달 공유 특징에서 가까운 다양한 특징을 생성해. PLM은 학습 가능한 프로토타입을 활용해 가시광선과 적외선 모달 간의 지역 특징들 사이에서 숨겨진 의미적 유사성을 찾아내고, 이를 통해 모달 간 인스턴스 레벨 정렬을 도와줘.

또한, 우리는 프로토타입의 다양성을 높이기 위해 코사인 이질성 손실을 도입했어, 이로 인해 풍부한 지역 특징을 추출할 수 있어. SYSU-MM01과 LLCM 데이터셋에서 진행한 광범위한 실험 결과, 우리 방법이 최신 기술 수준의 성능을 달성했음을 보여줬어. 우리의 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05650
Title: Replay Consolidation with Label Propagation for Continual Object Detection

Original Abstract:
Object Detection is a highly relevant computer vision problem with many applications such as robotics and autonomous driving. Continual Learning~(CL) considers a setting where a model incrementally learns new information while retaining previously acquired knowledge. This is particularly challenging since Deep Learning models tend to catastrophically forget old knowledge while training on new data. In particular, Continual Learning for Object Detection~(CLOD) poses additional difficulties compared to CL for Classification. In CLOD, images from previous tasks may contain unknown classes that could reappear labeled in future tasks. These missing annotations cause task interference issues for replay-based approaches. As a result, most works in the literature have focused on distillation-based approaches. However, these approaches are effective only when there is a strong overlap of classes across tasks. To address the issues of current methodologies, we propose a novel technique to solve CLOD called Replay Consolidation with Label Propagation for Object Detection (RCLPOD). Based on the replay method, our solution avoids task interference issues by enhancing the buffer memory samples. Our method is evaluated against existing techniques in CLOD literature, demonstrating its superior performance on established benchmarks like VOC and COCO.

Translated Abstract:
물체 탐지는 로봇 공학이나 자율 주행 같은 여러 응용 분야와 관련된 중요한 컴퓨터 비전 문제야. 계속 학습(Continual Learning, CL)은 모델이 새로운 정보를 점진적으로 배우면서 이전에 얻은 지식을 유지하는 상황을 고려해. 그런데 딥러닝 모델은 새로운 데이터를 학습할 때 옛 지식을 잊어버리기 쉬워서 이게 특히 어려워.

특히, 물체 탐지에 대한 계속 학습(Continual Learning for Object Detection, CLOD)은 분류에 대한 계속 학습(CL)보다 추가적인 어려움이 있어. CLOD에서는 이전 작업의 이미지에 미래 작업에서 레이블이 붙을 수 있는 미지의 클래스가 포함될 수 있어. 이런 누락된 레이블은 재생 기반 접근 방식에 문제를 일으켜. 그래서 기존 문헌의 대부분 연구는 증류 기반 접근 방식에 초점을 맞췄어. 하지만 이런 방법은 작업 간 클래스가 많이 겹칠 때만 효과적이야.

현재 방법론의 문제를 해결하기 위해, 우리는 물체 탐지를 위한 레이블 전파와 재생 통합(Replay Consolidation with Label Propagation for Object Detection, RCLPOD)이라는 새로운 기술을 제안해. 이 방법은 재생 방식에 기반해서 작업 간의 간섭 문제를 피하기 위해 버퍼 메모리 샘플을 강화해. 우리는 기존 CLOD 문헌의 기술들과 비교해서 이 방법을 평가했는데, VOC와 COCO 같은 잘 알려진 벤치마크에서 우수한 성능을 보였어.

================================================================================

URL: https://arxiv.org/abs/2409.05662
Title: Real-Time Human Action Recognition on Embedded Platforms

Original Abstract:
With advancements in computer vision and deep learning, video-based human action recognition (HAR) has become practical. However, due to the complexity of the computation pipeline, running HAR on live video streams incurs excessive delays on embedded platforms. This work tackles the real-time performance challenges of HAR with four contributions: 1) an experimental study identifying a standard Optical Flow (OF) extraction technique as the latency bottleneck in a state-of-the-art HAR pipeline, 2) an exploration of the latency-accuracy tradeoff between the standard and deep learning approaches to OF extraction, which highlights the need for a novel, efficient motion feature extractor, 3) the design of Integrated Motion Feature Extractor (IMFE), a novel single-shot neural network architecture for motion feature extraction with drastic improvement in latency, 4) the development of RT-HARE, a real-time HAR system tailored for embedded platforms. Experimental results on an Nvidia Jetson Xavier NX platform demonstrated that RT-HARE realizes real-time HAR at a video frame rate of 30 frames per second while delivering high levels of recognition accuracy.

Translated Abstract:
컴퓨터 비전과 딥러닝의 발전 덕분에 비디오 기반 인간 행동 인식(HAR)이 실제로 가능해졌어. 하지만 계산 과정이 복잡해서, 라이브 비디오 스트림에서 HAR을 실행하면 임베디드 플랫폼에서 너무 큰 지연이 발생해. 이 연구는 HAR의 실시간 성능 문제를 해결하기 위해 네 가지 기여를 해:

1) 최첨단 HAR 파이프라인에서 표준 광학 흐름(OF) 추출 기술이 지연의 병목 현상이라는 걸 밝힌 실험 연구.
2) 표준 방법과 딥러닝 접근 방식 간의 지연-정확도 트레이드오프를 탐구하여, 새로운 효율적인 모션 특징 추출기가 필요하다는 점을 강조.
3) 지연이 크게 개선된 새로운 단일 샷 신경망 아키텍처인 통합 모션 특징 추출기(IMFE)를 설계.
4) 임베디드 플랫폼에 맞춰진 실시간 HAR 시스템인 RT-HARE 개발.

Nvidia Jetson Xavier NX 플랫폼에서의 실험 결과는 RT-HARE가 초당 30프레임의 비디오 프레임 속도로 실시간 HAR을 실현하면서 높은 인식 정확도를 제공한다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.05679
Title: AnomalyCD: A benchmark for Earth anomaly change detection with high-resolution and time-series observations

Original Abstract:
Various Earth anomalies have destroyed the stable, balanced state, resulting in fatalities and serious destruction of property. With the advantages of large-scale and precise observation, high-resolution remote sensing images have been widely used for anomaly monitoring and localization. Powered by the deep representation, the existing methods have achieved remarkable advances, primarily in classification and change detection techniques. However, labeled samples are difficult to acquire due to the low probability of anomaly occurrence, and the trained models are limited to fixed anomaly categories, which hinders the application for anomalies with few samples or unknown anomalies. In this paper, to tackle this problem, we propose the anomaly change detection (AnomalyCD) technique, which accepts time-series observations and learns to identify anomalous changes by learning from the historical normal change pattern. Compared to the existing techniques, AnomalyCD processes an unfixed number of time steps and can localize the various anomalies in a unified manner, without human supervision. To benchmark AnomalyCD, we constructed a high-resolution dataset with time-series images dedicated to various Earth anomalies (the AnomalyCDD dataset). AnomalyCDD contains high-resolution (from 0.15 to 2.39 m/pixel), time-series (from 3 to 7 time steps), and large-scale images (1927.93 km2 in total) collected globally Furthermore, we developed a zero-shot baseline model (AnomalyCDM), which implements the AnomalyCD technique by extracting a general representation from the segment anything model (SAM) and conducting temporal comparison to distinguish the anomalous changes from normal changes. AnomalyCDM is designed as a two-stage workflow to enhance the efficiency, and has the ability to process the unseen images directly, without retraining for each scene.

Translated Abstract:
지구에서 다양한 이상 징후들이 발생하면서 안정적이고 균형 잡힌 상태가 깨졌고, 이로 인해 인명 피해와 재산 파괴가 심각하게 일어났어. 고해상도의 원격 센싱 이미지는 대규모로 정밀하게 관측할 수 있는 장점 덕분에 이상 징후 모니터링과 위치 파악에 널리 사용되고 있어. 

딥러닝 기반의 기존 방법들은 주로 분류와 변화 감지 기술에서 눈에 띄는 발전을 이뤘지만, 이상 징후가 발생할 확률이 낮아서 레이블이 달린 샘플을 얻기가 힘들어. 이 때문에 훈련된 모델들은 고정된 이상 카테고리에만 한정되어 있어서, 샘플이 적거나 아예 알려지지 않은 이상 징후에 적용하기가 어려워.

이 문제를 해결하기 위해 우리는 이상 변화 감지(AnomalyCD) 기술을 제안해. 이 기술은 시계열 관측 데이터를 받아서, 과거의 정상 변화 패턴을 학습하면서 이상 변화를 식별하는 방법이야. 기존 기술들과 비교했을 때, AnomalyCD는 고정되지 않은 시간 단계를 처리할 수 있고, 사람의 감독 없이 다양한 이상 징후를 통합적으로 위치 파악할 수 있어.

AnomalyCD를 평가하기 위해 우리는 다양한 지구 이상 징후에 맞춘 시계열 이미지를 포함한 고해상도 데이터셋(AnomalyCDD 데이터셋)을 만들었어. AnomalyCDD는 전 세계에서 수집된 고해상도(픽셀당 0.15m에서 2.39m), 시계열(3~7개 시간 단계), 대규모(총 1927.93 km²) 이미지를 포함하고 있어.

게다가 우리는 AnomalyCD 기술을 구현하는 제로샷 기본 모델(AnomalyCDM)을 개발했어. 이 모델은 '무엇이든 분할하기' 모델(SAM)에서 일반적인 표현을 추출하고, 정상 변화와 이상 변화를 구별하기 위해 시간적 비교를 하는 방식이야. AnomalyCDM은 효율성을 높이기 위해 두 단계로 설계되었고, 각 장면에 대해 다시 훈련하지 않고도 보지 못한 이미지를 직접 처리할 수 있는 능력을 가지고 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05681
Title: SX-Stitch: An Efficient VMS-UNet Based Framework for Intraoperative Scoliosis X-Ray Image Stitching

Original Abstract:
In scoliosis surgery, the limited field of view of the C-arm X-ray machine restricts the surgeons' holistic analysis of spinal structures .This paper presents an end-to-end efficient and robust intraoperative X-ray image stitching method for scoliosis surgery,named SX-Stitch. The method is divided into two stages:segmentation and stitching. In the segmentation stage, We propose a medical image segmentation model named Vision Mamba of Spine-UNet (VMS-UNet), which utilizes the state space Mamba to capture long-distance contextual information while maintaining linear computational complexity, and incorporates the SimAM attention mechanism, significantly improving the segmentation this http URL the stitching stage, we simplify the alignment process between images to the minimization of a registration energy function. The total energy function is then optimized to order unordered images, and a hybrid energy function is introduced to optimize the best seam, effectively eliminating parallax artifacts. On the clinical dataset, Sx-Stitch demonstrates superiority over SOTA schemes both qualitatively and quantitatively.

Translated Abstract:
척추측만증 수술에서 C-팔 엑스레이 기계의 제한된 시야는 외과의사들이 척추 구조를 전체적으로 분석하는 데 방해가 된다. 이 논문에서는 척추측만증 수술을 위한 효율적이고 강력한 수술 중 엑스레이 이미지 스티칭 방법인 SX-Stitch를 소개한다.

이 방법은 두 단계로 나뉜다: 세분화와 스티칭. 세분화 단계에서는 Vision Mamba of Spine-UNet (VMS-UNet)이라는 의료 이미지 세분화 모델을 제안한다. 이 모델은 Mamba라는 상태 공간을 활용해 먼 거리의 맥락 정보를 포착하면서도 계산의 복잡성을 낮춘다. 또한, SimAM 주의 메커니즘을 포함해 세분화 성능을 크게 향상시킨다.

스티칭 단계에서는 이미지 간 정렬 과정을 등록 에너지 함수의 최소화로 단순화한다. 그런 다음 총 에너지 함수를 최적화해 순서가 없는 이미지를 정렬하고, 최적의 이음새를 찾기 위해 혼합 에너지 함수를 도입해 시차 아티팩트를 효과적으로 제거한다. 

클리닉 데이터셋에서 SX-Stitch는 기존 최첨단 기법들보다 질적으로나 양적으로 우수한 성능을 보여준다.

================================================================================

URL: https://arxiv.org/abs/2409.05688
Title: LayeredFlow: A Real-World Benchmark for Non-Lambertian Multi-Layer Optical Flow

Original Abstract:
Achieving 3D understanding of non-Lambertian objects is an important task with many useful applications, but most existing algorithms struggle to deal with such objects. One major obstacle towards progress in this field is the lack of holistic non-Lambertian benchmarks -- most benchmarks have low scene and object diversity, and none provide multi-layer 3D annotations for objects occluded by transparent surfaces. In this paper, we introduce LayeredFlow, a real world benchmark containing multi-layer ground truth annotation for optical flow of non-Lambertian objects. Compared to previous benchmarks, our benchmark exhibits greater scene and object diversity, with 150k high quality optical flow and stereo pairs taken over 185 indoor and outdoor scenes and 360 unique objects. Using LayeredFlow as evaluation data, we propose a new task called multi-layer optical flow. To provide training data for this task, we introduce a large-scale densely-annotated synthetic dataset containing 60k images within 30 scenes tailored for non-Lambertian objects. Training on our synthetic dataset enables model to predict multi-layer optical flow, while fine-tuning existing optical flow methods on the dataset notably boosts their performance on non-Lambertian objects without compromising the performance on diffuse objects. Data is available at this https URL.

Translated Abstract:
비라멜트 물체의 3D 이해는 여러 유용한 응용 프로그램이 있어서 중요한 과제인데, 기존의 대부분 알고리즘은 이런 물체를 다루는 데 어려움을 겪고 있어. 이 분야의 발전을 가로막는 주요 장애물 중 하나는 전체적인 비라멜트 벤치마크가 부족하다는 거야. 대부분의 벤치마크는 장면과 물체의 다양성이 낮고, 투명한 표면에 가려진 물체에 대한 다층 3D 주석을 제공하는 곳이 없어.

이 논문에서는 LayeredFlow라는 실제 벤치마크를 소개해. 이 벤치마크는 비라멜트 물체의 광학 흐름에 대한 다층 실제 주석을 포함하고 있어. 이전 벤치마크와 비교했을 때, 우리 벤치마크는 185개의 실내외 장면과 360개의 독특한 물체에서 촬영한 15만 개의 고품질 광학 흐름과 스테레오 쌍을 제공해서 장면과 물체의 다양성이 더 커.

LayeredFlow를 평가 데이터로 사용해서 다층 광학 흐름이라는 새로운 작업을 제안해. 이 작업을 위한 훈련 데이터를 제공하기 위해, 비라멜트 물체에 맞춘 30개의 장면 내 6만 개의 이미지로 이루어진 대규모 밀집 주석 합성 데이터셋을 소개해. 이 합성 데이터셋으로 훈련하면 모델이 다층 광학 흐름을 예측할 수 있게 되고, 기존의 광학 흐름 방법을 이 데이터셋에서 미세 조정하면 비라멜트 물체에 대한 성능이 크게 향상되면서도 퍼지 물체에 대한 성능은 유지할 수 있어. 데이터는 이 URL에서 사용할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05697
Title: Segmentation by Factorization: Unsupervised Semantic Segmentation for Pathology by Factorizing Foundation Model Features

Original Abstract:
We introduce Segmentation by Factorization (F-SEG), an unsupervised segmentation method for pathology that generates segmentation masks from pre-trained deep learning models. F-SEG allows the use of pre-trained deep neural networks, including recently developed pathology foundation models, for semantic segmentation. It achieves this without requiring additional training or finetuning, by factorizing the spatial features extracted by the models into segmentation masks and their associated concept features. We create generic tissue phenotypes for H&E images by training clustering models for multiple numbers of clusters on features extracted from several deep learning models on The Cancer Genome Atlas Program (TCGA), and then show how the clusters can be used for factorizing corresponding segmentation masks using off-the-shelf deep learning models. Our results show that F-SEG provides robust unsupervised segmentation capabilities for H&E pathology images, and that the segmentation quality is greatly improved by utilizing pathology foundation models. We discuss and propose methods for evaluating the performance of unsupervised segmentation in pathology.

Translated Abstract:
우리는 F-SEG라는 이름의 비지도 분할 방법을 소개해. 이 방법은 사전 훈련된 딥러닝 모델에서 분할 마스크를 생성해. F-SEG는 최근에 개발된 병리학 기초 모델을 포함한 사전 훈련된 딥 신경망을 사용해서 의미론적 분할을 할 수 있게 해줘. 추가적인 훈련이나 미세 조정 없이도 모델이 추출한 공간적 특징을 분할 마스크와 관련된 개념 특징으로 분해해서 이걸 가능하게 해.

우리는 H&E 이미지에 대한 일반적인 조직 표현형을 만들기 위해 여러 딥러닝 모델에서 추출한 특징들로 여러 개의 클러스터 수에 대한 군집화 모델을 훈련했어. 그 다음, 이 클러스터들이 오프더셀프 딥러닝 모델을 사용해서 해당 분할 마스크를 분해하는 데 어떻게 쓰일 수 있는지 보여줬어. 우리의 결과는 F-SEG가 H&E 병리 이미지에 대해 강력한 비지도 분할 능력을 제공하고, 병리학 기초 모델을 활용하면 분할 품질이 크게 개선된다는 걸 보여줘.

마지막으로, 우리는 병리학에서 비지도 분할 성능을 평가하는 방법에 대해 논의하고 제안할 거야.

================================================================================

URL: https://arxiv.org/abs/2409.05699
Title: Boosting CNN-based Handwriting Recognition Systems with Learnable Relaxation Labeling

Original Abstract:
The primary challenge for handwriting recognition systems lies in managing long-range contextual dependencies, an issue that traditional models often struggle with. To mitigate it, attention mechanisms have recently been employed to enhance context-aware labelling, thereby achieving state-of-the-art performance. In the field of pattern recognition and image analysis, however, the use of contextual information in labelling problems has a long history and goes back at least to the early 1970's. Among the various approaches developed in those years, Relaxation Labelling (RL) processes have played a prominent role and have been the method of choice in the field for more than a decade. Contrary to recent transformer-based architectures, RL processes offer a principled approach to the use of contextual constraints, having a solid theoretic foundation grounded on variational inequality and game theory, as well as effective algorithms with convergence guarantees. In this paper, we propose a novel approach to handwriting recognition that integrates the strengths of two distinct methodologies. In particular, we propose integrating (trainable) RL processes with various well-established neural architectures and we introduce a sparsification technique that accelerates the convergence of the algorithm and enhances the overall system's performance. Experiments over several benchmark datasets show that RL processes can improve the generalisation ability, even surpassing in some cases transformer-based architectures.

Translated Abstract:
손글씨 인식 시스템의 주요 문제는 긴 범위의 맥락 의존성을 관리하는 거예요. 전통적인 모델들은 이 문제를 잘 해결하지 못하는 경우가 많죠. 이를 해결하기 위해 최근에는 주의(attention) 메커니즘을 사용해서 맥락을 고려한 라벨링을 향상시켜, 최첨단 성능을 달성하고 있어요.

하지만 패턴 인식과 이미지 분석 분야에서는 맥락 정보를 라벨링 문제에 사용하는 역사가 오래됐고, 최소한 1970년대 초반까지 거슬러 올라가요. 그 시기에 개발된 다양한 접근 방식 중에서 Relaxation Labelling (RL) 프로세스가 중요한 역할을 해왔고, 10년 이상 이 분야에서 선호되는 방법이에요.

최근의 트랜스포머 기반 아키텍처와는 다르게, RL 프로세스는 맥락 제약을 사용하는 데 있어 이론적으로 기초가 확고하고, 변칙 불평등과 게임 이론에 기반하고 있어요. 또, 수렴 보장이 있는 효과적인 알고리즘도 가지고 있죠.

이 논문에서는 두 가지 다른 방법론의 강점을 통합한 새로운 손글씨 인식 접근 방식을 제안해요. 특히, (훈련 가능한) RL 프로세스를 다양한 잘 알려진 신경망 아키텍처와 통합하는 방법을 제안하고, 알고리즘의 수렴을 가속하고 전체 시스템 성능을 향상시키는 희소화 기법도 소개해요.

여러 벤치마크 데이터셋에서 실험해본 결과, RL 프로세스가 일반화 능력을 향상시킬 수 있고, 어떤 경우에는 트랜스포머 기반 아키텍처를 초월하기도 했어요.

================================================================================

URL: https://arxiv.org/abs/2409.05749
Title: ReL-SAR: Representation Learning for Skeleton Action Recognition with Convolutional Transformers and BYOL

Original Abstract:
To extract robust and generalizable skeleton action recognition features, large amounts of well-curated data are typically required, which is a challenging task hindered by annotation and computation costs. Therefore, unsupervised representation learning is of prime importance to leverage unlabeled skeleton data. In this work, we investigate unsupervised representation learning for skeleton action recognition. For this purpose, we designed a lightweight convolutional transformer framework, named ReL-SAR, exploiting the complementarity of convolutional and attention layers for jointly modeling spatial and temporal cues in skeleton sequences. We also use a Selection-Permutation strategy for skeleton joints to ensure more informative descriptions from skeletal data. Finally, we capitalize on Bootstrap Your Own Latent (BYOL) to learn robust representations from unlabeled skeleton sequence data. We achieved very competitive results on limited-size datasets: MCAD, IXMAS, JHMDB, and NW-UCLA, showing the effectiveness of our proposed method against state-of-the-art methods in terms of both performance and computational efficiency. To ensure reproducibility and reusability, the source code including all implementation parameters is provided at: this https URL

Translated Abstract:
강력하고 일반화 가능한 골격 행동 인식 기능을 추출하려면 대량의 잘 정리된 데이터가 필요해. 그런데 이게 쉽지 않은 게, 주석 작업과 계산 비용 때문에 어려움이 많아. 그래서 레이블이 없는 골격 데이터를 활용하기 위해 비지도 표현 학습이 매우 중요해. 

이 연구에서는 골격 행동 인식을 위한 비지도 표현 학습을 조사했어. 이를 위해 우리는 ReL-SAR라는 가벼운 컨볼루션 변환기 프레임워크를 설계했는데, 이건 골격 시퀀스의 공간적 및 시간적 단서를 함께 모델링하기 위해 컨볼루션 층과 주의(attention) 층의 보완성을 활용해. 

또한, 골격 관절에 대한 선택-순열 전략을 사용해서 골격 데이터로부터 더 많은 정보를 제공하는 설명을 보장했어. 마지막으로, Bootstrap Your Own Latent (BYOL)을 이용해 레이블이 없는 골격 시퀀스 데이터에서 강력한 표현을 학습했어. 

우리는 제한된 크기의 데이터셋인 MCAD, IXMAS, JHMDB, NW-UCLA에서 매우 경쟁력 있는 결과를 얻었어. 이 결과는 성능과 계산 효율성 모두에서 최신 방법들과 비교해도 효과적임을 보여줘. 재현성과 재사용성을 보장하기 위해 모든 구현 매개변수를 포함한 소스 코드는 이 링크에 제공돼: 이 https URL

================================================================================

URL: https://arxiv.org/abs/2409.05786
Title: Leveraging Object Priors for Point Tracking

Original Abstract:
Point tracking is a fundamental problem in computer vision with numerous applications in AR and robotics. A common failure mode in long-term point tracking occurs when the predicted point leaves the object it belongs to and lands on the background or another object. We identify this as the failure to correctly capture objectness properties in learning to track. To address this limitation of prior work, we propose a novel objectness regularization approach that guides points to be aware of object priors by forcing them to stay inside the the boundaries of object instances. By capturing objectness cues at training time, we avoid the need to compute object masks during testing. In addition, we leverage contextual attention to enhance the feature representation for capturing objectness at the feature level more effectively. As a result, our approach achieves state-of-the-art performance on three point tracking benchmarks, and we further validate the effectiveness of our components via ablation studies. The source code is available at: this https URL

Translated Abstract:
포인트 추적은 컴퓨터 비전에서 기본적인 문제로, 증강 현실(AR)과 로봇 공학에서 많은 응용 프로그램이 있어. 장기 포인트 추적에서 흔히 발생하는 문제는 예측된 포인트가 속한 물체를 벗어나서 배경이나 다른 물체에 위치하게 될 때야. 우리는 이걸 추적하는 데 필요한 물체의 특성을 제대로 포착하지 못한 문제로 보고 있어.

이런 이전 연구의 한계를 해결하기 위해, 우리는 포인트들이 물체의 경계 안에 머물도록 강제하는 새로운 물체 특성 규제 방식을 제안해. 이렇게 하면 포인트들이 물체의 사전 정보를 인식하게 돼. 훈련할 때 물체 특성 신호를 포착함으로써, 테스트할 때 물체 마스크를 계산할 필요가 없어져.

또한, 우리는 맥락적 주의를 활용해서 물체 특성을 더 효과적으로 포착할 수 있도록 특징 표현을 향상시켜. 결과적으로, 우리의 접근 방식은 세 가지 포인트 추적 기준에서 최첨단 성능을 달성했고, 컴포넌트의 효과를 확인하기 위해 소거 연구도 진행했어. 소스 코드는 이 링크에서 확인할 수 있어: this https URL

================================================================================

URL: https://arxiv.org/abs/2409.05817
Title: VFA: Vision Frequency Analysis of Foundation Models and Human

Original Abstract:
Machine learning models often struggle with distribution shifts in real-world scenarios, whereas humans exhibit robust adaptation. Models that better align with human perception may achieve higher out-of-distribution generalization. In this study, we investigate how various characteristics of large-scale computer vision models influence their alignment with human capabilities and robustness. Our findings indicate that increasing model and data size and incorporating rich semantic information and multiple modalities enhance models' alignment with human perception and their overall robustness. Our empirical analysis demonstrates a strong correlation between out-of-distribution accuracy and human alignment.

Translated Abstract:
기계 학습 모델은 실제 상황에서 분포 변화에 잘 적응하지 못하는 경우가 많아. 반면에 사람은 잘 적응하지. 사람의 인식과 더 잘 맞는 모델은 새로운 상황에서도 더 잘 작동할 수 있어. 

이 연구에서는 대규모 컴퓨터 비전 모델의 다양한 특성이 인간의 능력과 얼마나 잘 맞는지, 그리고 얼마나 강한지를 조사했어. 우리의 결과는 모델과 데이터의 크기를 키우고, 풍부한 의미 정보를 포함시키고, 여러 가지 방식을 사용하면 모델이 사람의 인식과 더 잘 맞아지고 전반적인 강인성도 향상된다는 걸 보여줘. 

실험 분석을 통해, 새로운 상황에서의 정확성과 인간의 적합성 사이에 강한 상관관계가 있다는 걸 발견했어.

================================================================================

URL: https://arxiv.org/abs/2409.05819
Title: GASP: Gaussian Splatting for Physic-Based Simulations

Original Abstract:
Physics simulation is paramount for modeling and utilization of 3D scenes in various real-world applications. However, its integration with state-of-the-art 3D scene rendering techniques such as Gaussian Splatting (GS) remains challenging. Existing models use additional meshing mechanisms, including triangle or tetrahedron meshing, marching cubes, or cage meshes. As an alternative, we can modify the physics grounded Newtonian dynamics to align with 3D Gaussian components. Current models take the first-order approximation of a deformation map, which locally approximates the dynamics by linear transformations. In contrast, our Gaussian Splatting for Physics-Based Simulations (GASP) model uses such a map (without any modifications) and flat Gaussian distributions, which are parameterized by three points (mesh faces). Subsequently, each 3D point (mesh face node) is treated as a discrete entity within a 3D space. Consequently, the problem of modeling Gaussian components is reduced to working with 3D points. Additionally, the information on mesh faces can be used to incorporate further properties into the physics model, facilitating the use of triangles. Resulting solution can be integrated into any physics engine that can be treated as a black box. As demonstrated in our studies, the proposed model exhibits superior performance on a diverse range of benchmark datasets designed for 3D object rendering.

Translated Abstract:
물리 시뮬레이션은 다양한 실제 응용 프로그램에서 3D 장면을 모델링하고 활용하는 데 중요해. 하지만 최신 3D 장면 렌더링 기술인 가우시안 스플래팅(GS)과 결합하는 건 여전히 어려운 과제야. 기존 모델들은 삼각형이나 사면체 메싱, 마칭 큐브, 또는 케이지 메쉬 같은 추가 메싱 기법을 사용해.

대안으로는 물리 기반 뉴턴 역학을 수정해서 3D 가우시안 요소에 맞출 수 있어. 현재 모델들은 변형 맵의 1차 근사를 사용해서 동역학을 선형 변환으로 근사해. 반면에, 우리 연구의 가우시안 스플래팅 물리 기반 시뮬레이션(GASP) 모델은 그런 변형 맵을 수정 없이 사용하고, 세 점(메쉬 면)으로 매개변수화된 평면 가우시안 분포를 사용해. 그래서 각 3D 점(메쉬 면 노드)은 3D 공간에서 개별적인 개체로 다뤄져. 이 덕분에 가우시안 요소를 모델링하는 문제가 3D 점 작업으로 줄어들어.

또한, 메쉬 면에 대한 정보는 물리 모델에 추가 속성을 통합하는 데 사용될 수 있어, 삼각형을 활용하는 것도 가능해. 이렇게 나온 솔루션은 블랙 박스처럼 다룰 수 있는 어떤 물리 엔진에도 통합할 수 있어. 우리 연구에서 보여준 바와 같이, 제안한 모델은 3D 객체 렌더링을 위해 설계된 다양한 벤치마크 데이터셋에서 뛰어난 성능을 보였어.

================================================================================

URL: https://arxiv.org/abs/2409.05834
Title: Vision-Driven 2D Supervised Fine-Tuning Framework for Bird's Eye View Perception

Original Abstract:
Visual bird's eye view (BEV) perception, due to its excellent perceptual capabilities, is progressively replacing costly LiDAR-based perception systems, especially in the realm of urban intelligent driving. However, this type of perception still relies on LiDAR data to construct ground truth databases, a process that is both cumbersome and time-consuming. Moreover, most massproduced autonomous driving systems are only equipped with surround camera sensors and lack LiDAR data for precise annotation. To tackle this challenge, we propose a fine-tuning method for BEV perception network based on visual 2D semantic perception, aimed at enhancing the model's generalization capabilities in new scene data. Considering the maturity and development of 2D perception technologies, our method significantly reduces the dependency on high-cost BEV ground truths and shows promising industrial application prospects. Extensive experiments and comparative analyses conducted on the nuScenes and Waymo public datasets demonstrate the effectiveness of our proposed method.

Translated Abstract:
시각적 조감도(BEV) 인식은 뛰어난 인식 능력 덕분에 비싼 LiDAR 기반 인식 시스템을 점점 대체하고 있어. 특히 도시 지능형 주행 분야에서 그렇지. 하지만 이 인식 방식은 여전히 LiDAR 데이터를 바탕으로 실제 데이터베이스를 만드는 데 의존하고 있는데, 이 과정이 번거롭고 시간이 많이 걸려. 게다가, 대부분의 양산형 자율주행 시스템은 주변 카메라 센서만 장착되어 있어서 정확한 주석을 위한 LiDAR 데이터가 부족해.

이 문제를 해결하기 위해 우리는 시각적 2D 의미 인식을 기반으로 한 BEV 인식 네트워크의 미세 조정 방법을 제안해. 이 방법은 새로운 장면 데이터에서 모델의 일반화 능력을 향상시키는 걸 목표로 해. 2D 인식 기술이 성숙하고 발전한 점을 고려할 때, 우리의 방법은 비싼 BEV 실제 데이터에 대한 의존도를 크게 줄여주고, 산업적 응용 가능성도 밝은 편이야.

nuScenes와 Waymo의 공개 데이터셋에서 진행한 다양한 실험과 비교 분석 결과, 우리가 제안한 방법의 효과가 입증되었어.

================================================================================

URL: https://arxiv.org/abs/2409.05847
Title: LSVOS Challenge Report: Large-scale Complex and Long Video Object Segmentation

Original Abstract:
Despite the promising performance of current video segmentation models on existing benchmarks, these models still struggle with complex scenes. In this paper, we introduce the 6th Large-scale Video Object Segmentation (LSVOS) challenge in conjunction with ECCV 2024 workshop. This year's challenge includes two tasks: Video Object Segmentation (VOS) and Referring Video Object Segmentation (RVOS). In this year, we replace the classic YouTube-VOS and YouTube-RVOS benchmark with latest datasets MOSE, LVOS, and MeViS to assess VOS under more challenging complex environments. This year's challenge attracted 129 registered teams from more than 20 institutes across over 8 countries. This report include the challenge and dataset introduction, and the methods used by top 7 teams in two tracks. More details can be found in our homepage this https URL.

Translated Abstract:
현재의 비디오 분할 모델들이 기존 벤치마크에서는 좋은 성능을 보이고 있지만, 복잡한 장면에서는 여전히 어려움을 겪고 있어. 이번 논문에서는 ECCV 2024 워크숍과 함께 6번째 대규모 비디오 객체 분할(LSVOS) 챌린지를 소개할 거야. 

올해 챌린지에는 두 가지 작업이 있어: 비디오 객체 분할(VOS)과 언급된 비디오 객체 분할(RVOS)이지. 올해는 기존의 YouTube-VOS와 YouTube-RVOS 벤치마크를 최신 데이터셋인 MOSE, LVOS, MeViS로 교체해서 더 복잡한 환경에서 VOS를 평가할 거야. 

올해 챌린지에는 8개국 이상에서 20개 이상의 기관에서 129개 등록 팀이 참가했어. 이 보고서에는 챌린지와 데이터셋 소개, 그리고 두 트랙에서 상위 7개 팀이 사용한 방법들이 포함되어 있어. 더 많은 세부사항은 우리 홈페이지에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05862
Title: Evaluating Multiview Object Consistency in Humans and Image Models

Original Abstract:
We introduce a benchmark to directly evaluate the alignment between human observers and vision models on a 3D shape inference task. We leverage an experimental design from the cognitive sciences which requires zero-shot visual inferences about object shape: given a set of images, participants identify which contain the same/different objects, despite considerable viewpoint variation. We draw from a diverse range of images that include common objects (e.g., chairs) as well as abstract shapes (i.e., procedurally generated `nonsense' objects). After constructing over 2000 unique image sets, we administer these tasks to human participants, collecting 35K trials of behavioral data from over 500 participants. This includes explicit choice behaviors as well as intermediate measures, such as reaction time and gaze data. We then evaluate the performance of common vision models (e.g., DINOv2, MAE, CLIP). We find that humans outperform all models by a wide margin. Using a multi-scale evaluation approach, we identify underlying similarities and differences between models and humans: while human-model performance is correlated, humans allocate more time/processing on challenging trials. All images, data, and code can be accessed via our project page.

Translated Abstract:
우리는 3D 형태 추론 작업에서 인간 관찰자와 비전 모델 간의 정렬을 직접 평가할 수 있는 벤치마크를 소개해. 

이 실험은 인지 과학에서 가져온 디자인을 활용하는데, 여기서는 객체 형태에 대한 제로샷 시각 추론이 필요해. 주어진 이미지 세트에서 참가자들은 서로 다른 물체를 식별해야 해. 이 과정에서 다양한 시점에서의 변화가 있어도 같은 물체와 다른 물체를 구분해야 해. 

우리는 일반적인 물체(예: 의자)와 추상적인 형태(즉, 절차적으로 생성된 ‘무의미한’ 객체)를 포함한 다양한 이미지를 수집했어. 2000개 이상의 독특한 이미지 세트를 만든 후, 이 작업을 500명 이상의 참가자에게 수행하게 했고, 35,000회의 행동 데이터를 수집했어. 여기에는 명확한 선택 행동뿐만 아니라 반응 시간이나 시선 데이터 같은 중간 측정도 포함되어 있어. 

그 다음으로 일반적인 비전 모델들(예: DINOv2, MAE, CLIP)의 성능을 평가했어. 결과적으로, 인간이 모든 모델보다 훨씬 더 뛰어난 성과를 보였어. 다중 스케일 평가 접근법을 사용해서 모델과 인간 간의 유사점과 차이점을 확인했어. 인간과 모델의 성능은 상관관계가 있지만, 인간은 어려운 시험에서는 더 많은 시간과 처리를 할애하는 경향이 있어. 

모든 이미지, 데이터, 코드 등은 우리 프로젝트 페이지에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05863
Title: Promptable Closed-loop Traffic Simulation

Original Abstract:
Simulation stands as a cornerstone for safe and efficient autonomous driving development. At its core a simulation system ought to produce realistic, reactive, and controllable traffic patterns. In this paper, we propose ProSim, a multimodal promptable closed-loop traffic simulation framework. ProSim allows the user to give a complex set of numerical, categorical or textual prompts to instruct each agent's behavior and intention. ProSim then rolls out a traffic scenario in a closed-loop manner, modeling each agent's interaction with other traffic participants. Our experiments show that ProSim achieves high prompt controllability given different user prompts, while reaching competitive performance on the Waymo Sim Agents Challenge when no prompt is given. To support research on promptable traffic simulation, we create ProSim-Instruct-520k, a multimodal prompt-scenario paired driving dataset with over 10M text prompts for over 520k real-world driving scenarios. We will release code of ProSim as well as data and labeling tools of ProSim-Instruct-520k at this https URL.

Translated Abstract:
시뮬레이션은 안전하고 효율적인 자율주행 개발의 핵심이야. 시뮬레이션 시스템은 현실적이고 반응이 빠르며 조절 가능한 교통 패턴을 만들어야 해. 

이 논문에서는 ProSim이라는 다중 모드 프롬프트 가능한 폐쇄 루프 교통 시뮬레이션 프레임워크를 제안해. ProSim은 사용자가 복잡한 숫자, 범주, 또는 텍스트 프롬프트를 줘서 각 에이전트의 행동과 의도를 지시할 수 있도록 해. 그런 다음 ProSim은 폐쇄 루프 방식으로 교통 시나리오를 전개하고, 각 에이전트가 다른 교통 참여자와 어떻게 상호작용하는지를 모델링해.

실험 결과, ProSim은 다양한 사용자 프롬프트에 대해 높은 조절 가능성을 보여줬고, 프롬프트가 없는 경우에도 Waymo Sim Agents Challenge에서 경쟁력 있는 성과를 달성했어. 프롬프트 가능한 교통 시뮬레이션 연구를 지원하기 위해, ProSim-Instruct-520k라는 다중 모드 프롬프트-시나리오 쌍 운전 데이터셋을 만들었어. 이 데이터셋은 520k개의 실제 운전 시나리오에 대해 10M 이상의 텍스트 프롬프트를 포함하고 있어.

ProSim의 코드와 ProSim-Instruct-520k의 데이터 및 레이블링 도구는 이 URL에서 공개할 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.04456
Title: Pattern based learning and optimisation through pricing for bin packing problem

Original Abstract:
As a popular form of knowledge and experience, patterns and their identification have been critical tasks in most data mining applications. However, as far as we are aware, no study has systematically examined the dynamics of pattern values and their reuse under varying conditions. We argue that when problem conditions such as the distributions of random variables change, the patterns that performed well in previous circumstances may become less effective and adoption of these patterns would result in sub-optimal solutions. In response, we make a connection between data mining and the duality theory in operations research and propose a novel scheme to efficiently identify patterns and dynamically quantify their values for each specific condition. Our method quantifies the value of patterns based on their ability to satisfy stochastic constraints and their effects on the objective value, allowing high-quality patterns and their combinations to be detected. We use the online bin packing problem to evaluate the effectiveness of the proposed scheme and illustrate the online packing procedure with the guidance of patterns that address the inherent uncertainty of the problem. Results show that the proposed algorithm significantly outperforms the state-of-the-art methods. We also analysed in detail the distinctive features of the proposed methods that lead to performance improvement and the special cases where our method can be further improved.

Translated Abstract:
패턴과 그 식별은 데이터 마이닝에서 중요한 작업이야. 하지만 우리가 아는 한, 다양한 조건에서 패턴 값의 동적인 변화와 재사용에 대해 체계적으로 연구한 사례는 없어. 문제 조건, 예를 들어 랜덤 변수의 분포가 변하면, 이전에 잘 작동했던 패턴들이 효과가 떨어질 수 있어. 이런 패턴을 사용하면 최적이 아닌 해결책을 만들 수 있지.

그래서 우리는 데이터 마이닝과 운영 연구의 이중성 이론을 연결해봤어. 그리고 특정 조건에 맞게 패턴을 효율적으로 식별하고 그 가치를 동적으로 측정할 수 있는 새로운 방안을 제안했어. 우리의 방법은 패턴이 확률적 제약을 얼마나 잘 만족시키는지와 목표 값에 미치는 영향을 기반으로 패턴의 가치를 정량화해. 이 덕분에 고품질 패턴과 그 조합을 찾아낼 수 있어.

우리는 온라인 빈 패킹 문제를 사용해서 제안한 방법의 효과를 평가했어. 그리고 문제의 본질적인 불확실성을 다루는 패턴의 안내에 따라 온라인 패킹 절차를 설명했지. 결과적으로, 제안한 알고리즘이 최신 기술보다 훨씬 더 뛰어난 성능을 보였어. 우리는 또한 성능 향상으로 이어지는 제안된 방법의 독특한 특징과 방법을 더 개선할 수 있는 특별한 경우에 대해 자세히 분석했어.

================================================================================

URL: https://arxiv.org/abs/2409.04494
Title: Diff-INR: Generative Regularization for Electrical Impedance Tomography

Original Abstract:
Electrical Impedance Tomography (EIT) is a non-invasive imaging technique that reconstructs conductivity distributions within a body from boundary measurements. However, EIT reconstruction is hindered by its ill-posed nonlinear inverse problem, which complicates accurate results. To tackle this, we propose Diff-INR, a novel method that combines generative regularization with Implicit Neural Representations (INR) through a diffusion model. Diff-INR introduces geometric priors to guide the reconstruction, effectively addressing the shortcomings of traditional regularization methods. By integrating a pre-trained diffusion regularizer with INR, our approach achieves state-of-the-art reconstruction accuracy in both simulation and experimental data. The method demonstrates robust performance across various mesh densities and hyperparameter settings, highlighting its flexibility and efficiency. This advancement represents a significant improvement in managing the ill-posed nature of EIT. Furthermore, the method's principles are applicable to other imaging modalities facing similar challenges with ill-posed inverse problems.

Translated Abstract:
전기 임피던스 단층 촬영(EIT)은 신체의 경계 측정값을 바탕으로 전도도 분포를 재구성하는 비침습적 이미지 촬영 기법이야. 하지만 EIT 재구성은 잘 정의되지 않은 비선형 역문제로 인해 정확한 결과를 내기가 어려워. 

이 문제를 해결하기 위해 우리는 Diff-INR이라는 새로운 방법을 제안해. 이 방법은 생성적 정규화와 암묵적 신경 표현(INR)을 확산 모델을 통해 결합한 거야. Diff-INR은 재구성을 안내하기 위해 기하학적 정보(prior)를 도입해서 전통적인 정규화 방법의 단점을 잘 해결해. 

사전 훈련된 확산 정규화기를 INR과 통합하면, 우리의 접근 방식은 시뮬레이션 데이터와 실험 데이터 모두에서 최신 재구성 정확도를 달성해. 이 방법은 다양한 메쉬 밀도와 하이퍼파라미터 설정에서도 강력한 성능을 보여줘, 유연성과 효율성을 강조하는 거지. 

이 발전은 EIT의 잘 정의되지 않은 특성을 관리하는 데 상당한 개선을 나타내. 게다가 이 방법의 원리는 비슷한 문제에 직면한 다른 이미지 촬영 방식에도 적용할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04596
Title: NeCA: 3D Coronary Artery Tree Reconstruction from Two 2D Projections by Neural Implicit Representation

Original Abstract:
Cardiovascular diseases (CVDs) are the most common health threats worldwide. 2D x-ray invasive coronary angiography (ICA) remains as the most widely adopted imaging modality for CVDs diagnosis. However, in current clinical practice, it is often difficult for the cardiologists to interpret the 3D geometry of coronary vessels based on 2D planes. Moreover, due to the radiation limit, in general only two angiographic projections are acquired, providing limited information of the vessel geometry and necessitating 3D coronary tree reconstruction based only on two ICA projections. In this paper, we propose a self-supervised deep learning method called NeCA, which is based on implicit neural representation using the multiresolution hash encoder and differentiable cone-beam forward projector layer in order to achieve 3D coronary artery tree reconstruction from two projections. We validate our method using six different metrics on coronary computed tomography angiography data in terms of right coronary artery and left anterior descending respectively. The evaluation results demonstrate that our NeCA method, without 3D ground truth for supervision and large datasets for training, achieves promising performance in both vessel topology preservation and branch-connectivity maintaining compared to the supervised deep learning model.

Translated Abstract:
심혈관 질환(CVDs)은 전 세계에서 가장 흔한 건강 위협이야. 2D X-ray 침습적 관상동맥 조영술(ICA)은 CVD 진단을 위한 가장 널리 사용되는 영상 기법이야. 하지만 현재 임상에서는 심장 전문의들이 2D 평면을 바탕으로 관상 혈관의 3D 구조를 해석하기가 어려운 경우가 많아. 게다가 방사선 노출 제한 때문에 일반적으로 두 개의 조영 영상을 촬영하는데, 이러면 혈관 구조에 대한 정보가 제한적이어서 두 개의 ICA 투영만으로 3D 관상 동맥 나무를 재구성해야 해.

이 논문에서는 NeCA라는 자기 감독 딥러닝 방법을 제안해. 이 방법은 다중 해상도 해시 인코더와 미분 가능한 원추 빔 전방 프로젝터 층을 사용한 암묵적 신경 표현에 기반해 2개의 투영으로부터 3D 관상 동맥 나무 재구성을 이루는 거야. 우리는 오른쪽 관상 동맥과 왼쪽 하행동맥에 대한 관상 컴퓨터 단층촬영 조영술 데이터를 사용해서 여섯 가지 다른 지표로 우리 방법을 검증했어. 평가 결과, NeCA 방법은 감독용 3D 기준 진실이나 대규모 데이터셋 없이도 혈관 구조 보존과 가지 연결 유지에서 감독 딥러닝 모델에 비해 괜찮은 성능을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.04615
Title: A Short Survey on Set-Based Aggregation Techniques for Single-Vector WSI Representation in Digital Pathology

Original Abstract:
Digital pathology is revolutionizing the field of pathology by enabling the digitization, storage, and analysis of tissue samples as whole slide images (WSIs). WSIs are gigapixel files that capture the intricate details of tissue samples, providing a rich source of information for diagnostic and research purposes. However, due to their enormous size, representing these images as one compact vector is essential for many computational pathology tasks, such as search and retrieval, to ensure efficiency and scalability. Most current methods are "patch-oriented," meaning they divide WSIs into smaller patches for processing, which prevents a holistic analysis of the entire slide. Additionally, the necessity for compact representation is driven by the expensive high-performance storage required for WSIs. Not all hospitals have access to such extensive storage solutions, leading to potential disparities in healthcare quality and accessibility. This paper provides an overview of existing set-based approaches to single-vector WSI representation, highlighting the innovations that allow for more efficient and effective use of these complex images in digital pathology, thus addressing both computational challenges and storage limitations.

Translated Abstract:
디지털 병리학은 조직 샘플을 전체 슬라이드 이미지(WSI)로 디지털화, 저장, 분석할 수 있게 하면서 병리학 분야에 혁신을 가져오고 있어. WSI는 조직 샘플의 복잡한 세부 정보를 담고 있는 기가픽셀 파일로, 진단과 연구에 유용한 정보를 많이 제공해. 하지만 이 이미지들이 크기가 너무 커서, 많은 컴퓨터 병리학 작업에서 이 이미지를 하나의 간결한 벡터로 표현하는 게 중요해. 이렇게 해야 효율적이고 확장성 있는 검색과 조회가 가능해.

현재 대부분의 방법은 "패치 중심" 방식이야. 이건 WSI를 더 작은 패치로 나눠서 처리하는 방식인데, 이러면 전체 슬라이드를 한 번에 분석하는 게 어려워. 게다가 WSI를 저장하려면 비싼 고성능 스토리지가 필요해서 간결한 표현이 꼭 필요해. 모든 병원이 이렇게 큰 저장 솔루션을 갖추고 있는 건 아니어서, 의료 품질과 접근성에 차이가 생길 수 있어.

이 논문은 단일 벡터 WSI 표현을 위한 기존의 집합 기반 접근 방식을 개관하고, 디지털 병리학에서 이 복잡한 이미지를 더 효율적이고 효과적으로 사용할 수 있도록 하는 혁신들을 강조해. 이렇게 해서 컴퓨터 관련 문제와 저장 한계를 동시에 해결할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04631
Title: Zero-Shot Whole Slide Image Retrieval in Histopathology Using Embeddings of Foundation Models

Original Abstract:
We have tested recently published foundation models for histopathology for image retrieval. We report macro average of F1 score for top-1 retrieval, majority of top-3 retrievals, and majority of top-5 retrievals. We perform zero-shot retrievals, i.e., we do not alter embeddings and we do not train any classifier. As test data, we used diagnostic slides of TCGA, The Cancer Genome Atlas, consisting of 23 organs and 117 cancer subtypes. As a search platform we used Yottixel that enabled us to perform WSI search using patches. Achieved F1 scores show low performance, e.g., for top-5 retrievals, 27% +/- 13% (Yottixel-DenseNet), 42% +/- 14% (Yottixel-UNI), 40%+/-13% (Yottixel-Virchow), and 41%+/-13% (Yottixel-GigaPath). The results for GigaPath WSI will be delayed due to the significant computational resources required for processing

Translated Abstract:
최근에 발표된 기초 모델들을 사용해서 조직병리학의 이미지를 검색하는 테스트를 했어요. 우리는 top-1 검색의 F1 점수의 매크로 평균과, top-3 및 top-5 검색의 다수 결과를 보고했어요. 

우리는 제로샷 검색을 수행했는데, 이건 임베딩을 바꾸지 않고 어떤 분류기도 훈련하지 않는다는 뜻이에요. 테스트 데이터로는 TCGA(암 게놈 아틀라스)의 진단 슬라이드를 사용했는데, 여기에는 23개의 장기와 117개의 암 아형이 포함되어 있어요. 검색 플랫폼으로는 패치를 사용해 WSI 검색을 가능하게 해준 Yottixel을 사용했어요. 

F1 점수 결과는 낮은 성능을 보여줬어요. 예를 들면, top-5 검색에서 Yottixel-DenseNet은 27% +/- 13%, Yottixel-UNI는 42% +/- 14%, Yottixel-Virchow는 40% +/- 13%, Yottixel-GigaPath는 41% +/- 13%였어요. GigaPath WSI의 결과는 처리하는 데 필요한 컴퓨팅 자원이 많아서 지연될 거예요.

================================================================================

URL: https://arxiv.org/abs/2409.04633
Title: Structure-Invariant Range-Visual-Inertial Odometry

Original Abstract:
The Mars Science Helicopter (MSH) mission aims to deploy the next generation of unmanned helicopters on Mars, targeting landing sites in highly irregular terrain such as Valles Marineris, the largest canyons in the Solar system with elevation variances of up to 8000 meters. Unlike its predecessor, the Mars 2020 mission, which relied on a state estimation system assuming planar terrain, MSH requires a novel approach due to the complex topography of the landing site. This work introduces a novel range-visual-inertial odometry system tailored for the unique challenges of the MSH mission. Our system extends the state-of-the-art xVIO framework by fusing consistent range information with visual and inertial measurements, preventing metric scale drift in the absence of visual-inertial excitation (mono camera and constant velocity descent), and enabling landing on any terrain structure, without requiring any planar terrain assumption. Through extensive testing in image-based simulations using actual terrain structure and textures collected in Mars orbit, we demonstrate that our range-VIO approach estimates terrain-relative velocity meeting the stringent mission requirements, and outperforming existing methods.

Translated Abstract:
화성 과학 헬리콥터(MSH) 미션은 화성에서 무인 헬리콥터의 다음 세대를 배치하는 걸 목표로 해. 이 헬리콥터는 고르지 않은 지형인 발레스 마리네리스를 포함한 지역에 착륙할 예정인데, 이곳은 태양계에서 가장 큰 협곡으로 고도가 8000미터까지 차이가 나.

이전의 화성 2020 미션은 평면 지형을 가정한 상태 추정 시스템에 의존했지만, MSH는 착륙지가 복잡한 형태라서 새로운 접근 방식이 필요해. 그래서 이 연구에서는 MSH 미션의 독특한 도전에 맞춘 새로운 거리-시각-관성 오도메트리 시스템을 소개해.

우리 시스템은 최신 xVIO 프레임워크를 확장해서 일관된 거리 정보를 시각적, 관성 측정과 융합해. 이렇게 하면 시각-관성 자극이 없을 때(모노 카메라와 일정한 속도로 하강할 때) 메트릭 스케일 드리프트를 방지하고, 평면 지형 가정 없이 어떤 지형 구조에서도 착륙할 수 있게 해.

우리는 실제 화성 궤도에서 수집한 지형 구조와 텍스처를 사용한 이미지 기반 시뮬레이션에서 광범위한 테스트를 통해, 우리의 거리-VIO 접근 방식이 지형 상대 속도를 추정하고 미션의 엄격한 요구 사항을 충족한다는 걸 보여줬어. 기존 방법보다 성능도 더 뛰어난 걸 확인했어.

================================================================================

URL: https://arxiv.org/abs/2409.04949
Title: Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings

Original Abstract:
In this research, we present an innovative, parameter-efficient model that utilizes the attention U-Net architecture for the automatic detection and eradication of non-speech vocal sounds, specifically breath sounds, in vocal recordings. This task is of paramount importance in the field of sound engineering, despite being relatively under-explored. The conventional manual process for detecting and eliminating these sounds requires significant expertise and is extremely time-intensive. Existing automated detection and removal methods often fall short in terms of efficiency and precision. Our proposed model addresses these limitations by offering a streamlined process and superior accuracy, achieved through the application of advanced deep learning techniques. A unique dataset, derived from Device and Produced Speech (DAPS), was employed for this purpose. The training phase of the model emphasizes a log spectrogram and integrates an early stopping mechanism to prevent overfitting. Our model not only conserves precious time for sound engineers but also enhances the quality and consistency of audio production. This constitutes a significant breakthrough, as evidenced by its comparative efficiency, necessitating only 1.9M parameters and a training duration of 3.2 hours - markedly less than the top-performing models in this domain. The model is capable of generating identical outputs as previous models with drastically improved precision, making it an optimal choice.

Translated Abstract:
이번 연구에서는 음성 녹음에서 비음성 발음 소리, 특히 호흡 소리를 자동으로 감지하고 제거하는 새로운 모델을 소개해. 이 모델은 attention U-Net 구조를 사용하고, 파라미터 효율성도 뛰어나.

이 작업은 음향 공학 분야에서 매우 중요하지만, 아직 많이 연구되지 않았어. 기존의 수동 방식은 전문가의 도움이 필요하고, 시간이 많이 걸려. 자동으로 감지하고 제거하는 방법들도 효율성과 정확성이 부족한 경우가 많아.

우리가 제안한 모델은 이런 한계를 해결해. 고급 딥러닝 기법을 사용해서 간편한 과정과 뛰어난 정확성을 제공해. 이를 위해 Device and Produced Speech (DAPS)에서 나온 독특한 데이터셋을 사용했어. 모델 훈련 단계에서는 로그 스펙트로그램을 강조하고, 과적합을 방지하기 위해 조기 중단 메커니즘도 통합했어.

우리 모델은 음향 엔지니어들에게 귀중한 시간을 절약해 주고, 오디오 제작 품질과 일관성을 높여줘. 이건 상당한 혁신으로, 비교적 효율적이어서 단 1.9M 파라미터와 3.2시간의 훈련 시간만 필요해. 이건 이 분야에서 가장 성능이 좋은 모델들보다 훨씬 적은 거야. 

우리 모델은 이전 모델과 동일한 출력을 생성하면서도 정확도가 크게 향상되어, 최적의 선택이 될 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.04977
Title: Enhancing Convolutional Neural Networks with Higher-Order Numerical Difference Methods

Original Abstract:
With the rise of deep learning technology in practical applications, Convolutional Neural Networks (CNNs) have been able to assist humans in solving many real-world problems. To enhance the performance of CNNs, numerous network architectures have been explored. Some of these architectures are designed based on the accumulated experience of researchers over time, while others are designed through neural architecture search methods. The improvements made to CNNs by the aforementioned methods are quite significant, but most of the improvement methods are limited in reality by model size and environmental constraints, making it difficult to fully realize the improved performance. In recent years, research has found that many CNN structures can be explained by the discretization of ordinary differential equations. This implies that we can design theoretically supported deep network structures using higher-order numerical difference methods. It should be noted that most of the previous CNN model structures are based on low-order numerical methods. Therefore, considering that the accuracy of linear multi-step numerical difference methods is higher than that of the forward Euler method, this paper proposes a stacking scheme based on the linear multi-step method. This scheme enhances the performance of ResNet without increasing the model size and compares it with the Runge-Kutta scheme. The experimental results show that the performance of the stacking scheme proposed in this paper is superior to existing stacking schemes (ResNet and HO-ResNet), and it has the capability to be extended to other types of neural networks.

Translated Abstract:
딥러닝 기술이 실제 응용 분야에서 발전하면서, 합성곱 신경망(CNN)은 많은 현실 문제를 해결하는 데 도움을 주고 있어. CNN의 성능을 높이기 위해 여러 네트워크 구조가 연구되었어. 어떤 구조는 연구자들의 경험을 바탕으로 만들어졌고, 다른 구조는 신경망 구조 검색 방법으로 설계되었지.

이런 방법들로 CNN을 개선한 건 확실히 큰 성과지만, 대부분의 개선 방법은 모델 크기와 환경 제약 때문에 실제로는 성능을 충분히 발휘하기 어려워. 최근 연구에 따르면, 많은 CNN 구조는 일반 미분 방정식의 이산화로 설명될 수 있다고 해. 이건 우리가 더 높은 차수의 수치 차분 방법을 사용해서 이론적으로 뒷받침된 딥 네트워크 구조를 설계할 수 있다는 뜻이야.

대부분의 기존 CNN 모델 구조가 저차 수치 방법에 기반하고 있다는 것도 주목할 만해. 그러니까, 선형 다단계 수치 차분 방법의 정확도가 오일러 전진법보다 높다는 점을 생각하면, 이 논문에서는 선형 다단계 방법을 기반으로 한 스태킹 방식을 제안해. 이 방식은 모델 크기를 늘리지 않고도 ResNet의 성능을 향상시키고, 룽게-쿠타 방법과 비교해봤어.

실험 결과, 이 논문에서 제안한 스태킹 방식이 기존 스태킹 방식(ResNet과 HO-ResNet)보다 성능이 우수하다는 걸 보여줘. 그리고 이 방식은 다른 종류의 신경망에도 확장할 수 있는 가능성이 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05137
Title: READoc: A Unified Benchmark for Realistic Document Structured Extraction

Original Abstract:
Document Structured Extraction (DSE) aims to extract structured content from raw documents. Despite the emergence of numerous DSE systems, their unified evaluation remains inadequate, significantly hindering the field's advancement. This problem is largely attributed to existing benchmark paradigms, which exhibit fragmented and localized characteristics. To address these limitations and offer a thorough evaluation of DSE systems, we introduce a novel benchmark named READoc, which defines DSE as a realistic task of converting unstructured PDFs into semantically rich Markdown. The READoc dataset is derived from 2,233 diverse and real-world documents from arXiv and GitHub. In addition, we develop a DSE Evaluation S$^3$uite comprising Standardization, Segmentation and Scoring modules, to conduct a unified evaluation of state-of-the-art DSE approaches. By evaluating a range of pipeline tools, expert visual models, and general VLMs, we identify the gap between current work and the unified, realistic DSE objective for the first time. We aspire that READoc will catalyze future research in DSE, fostering more comprehensive and practical solutions.

Translated Abstract:
문서 구조 추출(Document Structured Extraction, DSE)은 원본 문서에서 구조화된 내용을 추출하는 걸 목표로 해. 여러 DSE 시스템이 등장했지만, 이들을 통합적으로 평가하는 시스템이 부족해서 이 분야의 발전에 큰 걸림돌이 되고 있어. 이 문제는 기존 벤치마크 체계가 조각나고 지역화된 특성을 가지고 있기 때문이야.

이 제한점을 해결하고 DSE 시스템을 철저하게 평가하기 위해, 우리는 READoc이라는 새로운 벤치마크를 소개해. 이건 비구조화된 PDF를 의미가 풍부한 Markdown으로 변환하는 현실적인 과제로 DSE를 정의해. READoc 데이터셋은 arXiv와 GitHub에서 가져온 2,233개의 다양한 실제 문서로 구성되어 있어.

또한, 우리는 DSE 평가를 위한 표준화, 분할 및 점수 매기기 모듈로 구성된 DSE 평가 S$^3$uite도 개발했어. 이를 통해 최신 DSE 접근 방식을 통합적으로 평가할 수 있게 됐어. 여러 파이프라인 도구, 전문가 시각 모델, 일반 VLM을 평가하면서, 현재 작업과 통합적이고 현실적인 DSE 목표 사이의 간극을 처음으로 확인했어. READoc이 DSE 연구의 촉매제가 되어 더 포괄적이고 실용적인 솔루션을 촉진하기를 희망해.

================================================================================

URL: https://arxiv.org/abs/2409.05148
Title: Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis

Original Abstract:
Within the context of creating new Socially Assistive Robots, emotion recognition has become a key development factor, as it allows the robot to adapt to the user's emotional state in the wild. In this work, we focused on the analysis of two voice recording Spanish datasets: ELRA-S0329 and EmoMatchSpanishDB. Specifically, we centered our work in the paralanguage, e.~g. the vocal characteristics that go along with the message and clarifies the meaning. We proposed the use of the DeepSpectrum method, which consists of extracting a visual representation of the audio tracks and feeding them to a pretrained CNN model. For the classification task, DeepSpectrum is often paired with a Support Vector Classifier --DS-SVC--, or a Fully-Connected deep-learning classifier --DS-FC--. We compared the results of the DS-SVC and DS-FC architectures with the state-of-the-art (SOTA) for ELRA-S0329 and EmoMatchSpanishDB. Moreover, we proposed our own classifier based upon Attention Mechanisms, namely DS-AM. We trained all models against both datasets, and we found that our DS-AM model outperforms the SOTA models for the datasets and the SOTA DeepSpectrum architectures. Finally, we trained our DS-AM model in one dataset and tested it in the other, to simulate real-world conditions on how biased is the model to the dataset.

Translated Abstract:
사회적 보조 로봇을 만드는 과정에서 감정 인식이 중요한 개발 요소로 떠올랐어. 이 기술 덕분에 로봇이 사용자의 감정 상태에 맞춰 적응할 수 있게 되거든. 

이 연구에서는 두 개의 스페인어 음성 녹음 데이터셋인 ELRA-S0329와 EmoMatchSpanishDB를 분석했어. 특히, 메시지와 함께 전달되는 음성의 특징, 즉 파라랭귀지(paralanguage)에 초점을 맞췄지. 우리는 DeepSpectrum 방법을 제안했는데, 이건 음성 트랙에서 시각적 표현을 뽑아내고, 이를 미리 학습된 CNN 모델에 입력하는 방식이야. 

분류 작업에서는 DeepSpectrum을 Support Vector Classifier (DS-SVC)나 Fully-Connected 딥러닝 분류기 (DS-FC)와 함께 사용하는 경우가 많아. 우리는 ELRA-S0329와 EmoMatchSpanishDB에 대한 DS-SVC와 DS-FC 아키텍처의 결과를 최신 기술(SOTA)과 비교했어. 게다가 Attention Mechanisms를 기반으로 한 우리만의 분류기인 DS-AM도 제안했어. 

모든 모델을 두 데이터셋에 대해 학습시킨 결과, DS-AM 모델이 두 데이터셋과 SOTA DeepSpectrum 아키텍처보다 더 성능이 좋다는 걸 발견했어. 마지막으로, DS-AM 모델을 하나의 데이터셋에서 학습하고, 다른 데이터셋에서 테스트하면서 실제 환경에서 모델이 데이터셋에 얼마나 편향되는지를 시뮬레이션했어.

================================================================================

URL: https://arxiv.org/abs/2409.05171
Title: Exploring Fungal Morphology Simulation and Dynamic Light Containment from a Graphics Generation Perspective

Original Abstract:
Fungal simulation and control are considered crucial techniques in Bio-Art creation. However, coding algorithms for reliable fungal simulations have posed significant challenges for artists. This study equates fungal morphology simulation to a two-dimensional graphic time-series generation problem. We propose a zero-coding, neural network-driven cellular automaton. Fungal spread patterns are learned through an image segmentation model and a time-series prediction model, which then supervise the training of neural network cells, enabling them to replicate real-world spreading behaviors. We further implemented dynamic containment of fungal boundaries with lasers. Synchronized with the automaton, the fungus successfully spreads into pre-designed complex shapes in reality.

Translated Abstract:
버섯 시뮬레이션과 제어는 바이오 아트 창작에 중요한 기술로 여겨져. 하지만, 믿을 수 있는 버섯 시뮬레이션을 위한 코딩 알고리즘을 만드는 건 아티스트들에게 큰 도전이었어. 

이 연구에서는 버섯 형태 시뮬레이션을 2차원 그래픽 시계열 생성 문제와 동일시해. 우리는 제로 코딩으로 작동하는 신경망 기반의 세포 자동자를 제안해. 버섯 확산 패턴은 이미지 분할 모델과 시계열 예측 모델을 통해 학습되고, 이 모델들이 신경망 세포의 훈련을 감독해서 실제 세계의 확산 행동을 복제할 수 있게 해. 

게다가 우리는 레이저를 사용해서 버섯 경계를 동적으로 제어하는 방법도 구현했어. 이 자동자와 동기화되어, 버섯은 실제로 미리 설계된 복잡한 형태로 성공적으로 확산돼.

================================================================================

URL: https://arxiv.org/abs/2409.05202
Title: A Survey on Mixup Augmentations and Beyond

Original Abstract:
As Deep Neural Networks have achieved thrilling breakthroughs in the past decade, data augmentations have garnered increasing attention as regularization techniques when massive labeled data are unavailable. Among existing augmentations, Mixup and relevant data-mixing methods that convexly combine selected samples and the corresponding labels are widely adopted because they yield high performances by generating data-dependent virtual data while easily migrating to various domains. This survey presents a comprehensive review of foundational mixup methods and their applications. We first elaborate on the training pipeline with mixup augmentations as a unified framework containing modules. A reformulated framework could contain various mixup methods and give intuitive operational procedures. Then, we systematically investigate the applications of mixup augmentations on vision downstream tasks, various data modalities, and some analysis \& theorems of mixup. Meanwhile, we conclude the current status and limitations of mixup research and point out further work for effective and efficient mixup augmentations. This survey can provide researchers with the current state of the art in mixup methods and provide some insights and guidance roles in the mixup arena. An online project with this survey is available at \url{this https URL}.

Translated Abstract:
딥 뉴럴 네트워크가 지난 10년 동안 놀라운 발전을 이룬 덕분에, 대량의 레이블이 없는 데이터가 없을 때 데이터 증강 기법이 주목받고 있어. 기존의 증강 기법 중에서 Mixup과 관련된 데이터 혼합 방법이 널리 사용되는데, 이 방법은 선택된 샘플과 그에 맞는 레이블을 결합해서 가상의 데이터를 만들어내. 이 방식은 고성능을 내면서 다양한 도메인으로 쉽게 이전할 수 있어서 좋아.

이 조사는 기본적인 Mixup 방법과 그 응용에 대한 포괄적인 리뷰를 제공해. 먼저, Mixup 증강을 포함한 훈련 파이프라인을 모듈화된 통합 프레임워크로 설명해. 이렇게 다시 구성된 프레임워크는 다양한 Mixup 방법을 포함할 수 있고, 직관적인 작업 절차를 제공할 수 있어.

그 다음에는 Mixup 증강이 비전 관련 다운스트림 작업, 다양한 데이터 양식, 그리고 Mixup에 대한 분석 및 정리된 이론에 어떻게 적용되는지를 체계적으로 조사해. 또한, 현재 Mixup 연구의 상태와 한계를 정리하고, 효과적이고 효율적인 Mixup 증강을 위한 추가 작업을 제안해. 이 조사는 연구자들에게 Mixup 방법의 최신 동향을 제공하고, Mixup 분야에서 통찰과 가이드를 줄 수 있어. 이 조사와 관련된 온라인 프로젝트는 \url{this https URL}에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05255
Title: Label-free evaluation of lung and heart transplant biopsies using virtual staining

Original Abstract:
Organ transplantation serves as the primary therapeutic strategy for end-stage organ failures. However, allograft rejection is a common complication of organ transplantation. Histological assessment is essential for the timely detection and diagnosis of transplant rejection and remains the gold standard. Nevertheless, the traditional histochemical staining process is time-consuming, costly, and labor-intensive. Here, we present a panel of virtual staining neural networks for lung and heart transplant biopsies, which digitally convert autofluorescence microscopic images of label-free tissue sections into their brightfield histologically stained counterparts, bypassing the traditional histochemical staining process. Specifically, we virtually generated Hematoxylin and Eosin (H&E), Masson's Trichrome (MT), and Elastic Verhoeff-Van Gieson (EVG) stains for label-free transplant lung tissue, along with H&E and MT stains for label-free transplant heart tissue. Subsequent blind evaluations conducted by three board-certified pathologists have confirmed that the virtual staining networks consistently produce high-quality histology images with high color uniformity, closely resembling their well-stained histochemical counterparts across various tissue features. The use of virtually stained images for the evaluation of transplant biopsies achieved comparable diagnostic outcomes to those obtained via traditional histochemical staining, with a concordance rate of 82.4% for lung samples and 91.7% for heart samples. Moreover, virtual staining models create multiple stains from the same autofluorescence input, eliminating structural mismatches observed between adjacent sections stained in the traditional workflow, while also saving tissue, expert time, and staining costs.

Translated Abstract:
장기 이식은 말기 장기 부전 치료의 주요 방법이야. 하지만 이식된 장기가 거부되는 건 흔한 문제야. 이식 거부 반응을 제때 발견하고 진단하기 위해 조직학적 평가가 중요하고, 이게 가장 신뢰할 수 있는 방법이야. 하지만 전통적인 조직염색 과정은 시간이 많이 걸리고, 비용도 비싸고, 노동집약적이야.

그래서 우리는 폐와 심장 이식 생검을 위한 가상 염색 신경망 패널을 제안해. 이건 라벨이 없는 조직 절편의 자가 형광 현미경 이미지를 밝은 필드 조직학적 염색 이미지로 디지털 변환하는 방법이야. 전통적인 조직염색 과정을 건너뛰는 거지. 구체적으로, 우리는 라벨이 없는 이식 폐 조직에 대해 헤마톡실린과 에오신(H&E), 마쏘니의 삼색염색(MT), 엘라스틱 베로헤프-반 기손(EVG) 염색을 가상 생성했어. 그리고 라벨이 없는 이식 심장 조직에 대해서도 H&E와 MT 염색을 만들었지.

세 명의 인증된 병리학자가 진행한 맹검 평가 결과, 이 가상 염색 네트워크가 고품질의 조직학 이미지를 일관되게 만들어내고, 색상 균일성이 높아서 잘 염색된 전통적 조직학적 이미지와 매우 비슷하다는 걸 확인했어. 가상 염색 이미지를 사용한 이식 생검 평가 결과는 전통적인 조직염색을 통해 얻은 진단 결과와 비슷했어. 폐 샘플에 대해서는 82.4%, 심장 샘플에 대해서는 91.7%의 일치율이 나왔어. 게다가 가상 염색 모델은 같은 자가 형광 입력으로부터 여러 염색을 만들어내기 때문에, 전통적인 방법에서 인접 절편 간에 발생할 수 있는 구조적 일치 문제를 없애주고, 조직도 아낄 수 있고, 전문가의 시간과 염색 비용도 절약할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05258
Title: Towards Automated Machine Learning Research

Original Abstract:
This paper explores a top-down approach to automating incremental advances in machine learning research through component-level innovation, facilitated by Large Language Models (LLMs). Our framework systematically generates novel components, validates their feasibility, and evaluates their performance against existing baselines. A key distinction of this approach lies in how these novel components are generated. Unlike traditional AutoML and NAS methods, which often rely on a bottom-up combinatorial search over predefined, hardcoded base components, our method leverages the cross-domain knowledge embedded in LLMs to propose new components that may not be confined to any hard-coded predefined set. By incorporating a reward model to prioritize promising hypotheses, we aim to improve the efficiency of the hypothesis generation and evaluation process. We hope this approach offers a new avenue for exploration and contributes to the ongoing dialogue in the field.

Translated Abstract:
이 논문에서는 대형 언어 모델(LLMs)을 활용한 머신러닝 연구의 점진적인 발전을 자동화하는 방법을 위에서 아래로 접근하는 방식으로 탐구해. 우리의 프레임워크는 새롭고 혁신적인 구성 요소를 체계적으로 생성하고, 그 가능성을 검증하며, 기존 기준과 성능을 비교해.

이 접근 방식의 주요 차별점은 새로운 구성 요소가 생성되는 방식이야. 전통적인 AutoML이나 NAS 방법은 미리 정의된 하드코딩된 기본 구성 요소를 조합하는 하향식 검색에 의존하는 경우가 많아. 하지만 우리 방법은 LLM에 내재된 다양한 도메인 지식을 활용해서, 하드코딩된 기본 세트에 국한되지 않는 새로운 구성 요소를 제안해.

또한, 유망한 가설을 우선시하는 보상 모델을 포함시켜 가설 생성과 평가 프로세스의 효율성을 높이는 걸 목표로 하고 있어. 이 접근 방식이 새로운 탐구의 길을 열고, 이 분야에서의 지속적인 대화에 기여하기를 바래.

================================================================================

URL: https://arxiv.org/abs/2409.05274
Title: Rethinking the Atmospheric Scattering-driven Attention via Channel and Gamma Correction Priors for Low-Light Image Enhancement

Original Abstract:
Low-light image enhancement remains a critical challenge in computer vision, as does the lightweight design for edge devices with the computational burden for deep learning models. In this article, we introduce an extended version of Channel-Prior and Gamma-Estimation Network (CPGA-Net), termed CPGA-Net+, which incorporates an attention mechanism driven by a reformulated Atmospheric Scattering Model and effectively addresses both global and local image processing through Plug-in Attention with gamma correction. These innovations enable CPGA-Net+ to achieve superior performance on image enhancement tasks, surpassing lightweight state-of-the-art methods with high efficiency. Our results demonstrate the model's effectiveness and show the potential applications in resource-constrained environments.

Translated Abstract:
저조도 이미지 향상은 컴퓨터 비전에서 여전히 큰 도전 과제야. 또, 깊은 학습 모델을 사용하는 엣지 디바이스의 경량 설계도 마찬가지로 어려워. 

이 논문에서는 CPGA-Net이라는 채널 우선순위와 감마 추정 네트워크의 확장 버전인 CPGA-Net+를 소개해. 이 모델은 재정립된 대기 산란 모델에 기반한 주의(attention) 메커니즘을 포함하고 있어. 그리고 플러그인 어텐션과 감마 보정을 통해 이미지 처리를 전 세계적이고 지역적으로 잘 해결해. 

이런 혁신 덕분에 CPGA-Net+는 이미지 향상 작업에서 뛰어난 성능을 보여주고, 경량 최신 방법들을 능가하면서도 높은 효율성을 자랑해. 우리의 결과는 이 모델의 효과성을 보여주고, 자원이 제한된 환경에서도 활용 가능성을 나타내.

================================================================================

URL: https://arxiv.org/abs/2409.05310
Title: Neural Surface Reconstruction and Rendering for LiDAR-Visual Systems

Original Abstract:
This paper presents a unified surface reconstruction and rendering framework for LiDAR-visual systems, integrating Neural Radiance Fields (NeRF) and Neural Distance Fields (NDF) to recover both appearance and structural information from posed images and point clouds. We address the structural visible gap between NeRF and NDF by utilizing a visible-aware occupancy map to classify space into the free, occupied, visible unknown, and background regions. This classification facilitates the recovery of a complete appearance and structure of the scene. We unify the training of the NDF and NeRF using a spatial-varying scale SDF-to-density transformation for levels of detail for both structure and appearance. The proposed method leverages the learned NDF for structure-aware NeRF training by an adaptive sphere tracing sampling strategy for accurate structure rendering. In return, NeRF further refines structural in recovering missing or fuzzy structures in the NDF. Extensive experiments demonstrate the superior quality and versatility of the proposed method across various scenarios. To benefit the community, the codes will be released at \url{this https URL}.

Translated Abstract:
이 논문은 LiDAR-비주얼 시스템을 위한 통합 표면 재구성 및 렌더링 프레임워크를 소개해. 여기서는 Neural Radiance Fields (NeRF)와 Neural Distance Fields (NDF)를 통합해서 위치가 정해진 이미지와 포인트 클라우드에서 외형과 구조 정보를 모두 복구할 수 있어.

NeRF와 NDF 사이의 구조적 격차를 해결하기 위해, 공간을 자유, 점유, 보이는 미지수, 배경 지역으로 분류하는 가시성 인식 점유 맵을 사용해. 이런 분류 덕분에 장면의 완전한 외형과 구조를 복구할 수 있게 돼.

NDF와 NeRF의 훈련을 통합하기 위해 공간에 따라 변하는 스케일 SDF-밀도 변환을 사용해서 구조와 외형 모두에 대한 세부 수준을 조절해. 제안한 방법은 구조 인식 NDF를 활용해서 NeRF 훈련을 하는데, 이는 정확한 구조 렌더링을 위한 적응형 구형 추적 샘플링 전략을 사용해.

결과적으로 NeRF는 NDF에서 누락되거나 흐릿한 구조를 복구하는 데 더 정교함을 더해. 다양한 실험을 통해 제안한 방법의 뛰어난 품질과 다양성을 입증했어. 커뮤니티에 도움이 되도록 코드는 이 URL에서 공개할 예정이야.

================================================================================

URL: https://arxiv.org/abs/2409.05335
Title: A Multi-Modal Deep Learning Based Approach for House Price Prediction

Original Abstract:
Accurate prediction of house price, a vital aspect of the residential real estate sector, is of substantial interest for a wide range of stakeholders. However, predicting house prices is a complex task due to the significant variability influenced by factors such as house features, location, neighborhood, and many others. Despite numerous attempts utilizing a wide array of algorithms, including recent deep learning techniques, to predict house prices accurately, existing approaches have fallen short of considering a wide range of factors such as textual and visual features. This paper addresses this gap by comprehensively incorporating attributes, such as features, textual descriptions, geo-spatial neighborhood, and house images, typically showcased in real estate listings in a house price prediction system. Specifically, we propose a multi-modal deep learning approach that leverages different types of data to learn more accurate representation of the house. In particular, we learn a joint embedding of raw house attributes, geo-spatial neighborhood, and most importantly from textual description and images representing the house; and finally use a downstream regression model to predict the house price from this jointly learned embedding vector. Our experimental results with a real-world dataset show that the text embedding of the house advertisement description and image embedding of the house pictures in addition to raw attributes and geo-spatial embedding, can significantly improve the house price prediction accuracy. The relevant source code and dataset are publicly accessible at the following URL: this https URL

Translated Abstract:
주택 가격 예측은 주거용 부동산 분야에서 매우 중요한 부분인데, 다양한 이해관계자들에게 큰 관심을 받고 있어. 하지만 주택 가격을 예측하는 건 복잡한 작업이야. 집의 특성, 위치, 이웃 등 여러 요인에 의해 큰 변동성이 생기거든. 최근 딥러닝 기법을 포함한 다양한 알고리즘을 사용해도, 기존의 방법들은 텍스트나 이미지 같은 다양한 요인을 충분히 고려하지 못했어.

이 논문은 그런 문제를 해결하기 위해, 주택 가격 예측 시스템에서 부동산 목록에 보통 나타나는 특성, 텍스트 설명, 지리적 이웃, 그리고 집 이미지 같은 속성들을 포괄적으로 포함하는 방법을 제안해. 구체적으로, 우리는 다양한 데이터 유형을 활용해 집에 대한 더 정확한 표현을 학습하는 다중 모달 딥러닝 접근법을 제안해. 특히, 원시 집 속성, 지리적 이웃, 그리고 가장 중요한 텍스트 설명과 집을 나타내는 이미지의 공동 임베딩을 학습하고, 이 공동 학습된 임베딩 벡터를 사용해 주택 가격을 예측하는 회귀 모델을 적용해.

실제 데이터셋을 활용한 실험 결과에 따르면, 집 광고 설명의 텍스트 임베딩과 집 사진의 이미지 임베딩이 원시 속성과 지리적 임베딩과 함께 사용될 때 주택 가격 예측의 정확성을 크게 향상시킬 수 있다는 걸 보여줬어. 관련 소스 코드와 데이터셋은 다음 URL에서 공개되어 있어: this https URL

================================================================================

URL: https://arxiv.org/abs/2409.05414
Title: CipherDM: Secure Three-Party Inference for Diffusion Model Sampling

Original Abstract:
Diffusion Models (DMs) achieve state-of-the-art synthesis results in image generation and have been applied to various fields. However, DMs sometimes seriously violate user privacy during usage, making the protection of privacy an urgent issue. Using traditional privacy computing schemes like Secure Multi-Party Computation (MPC) directly in DMs faces significant computation and communication challenges. To address these issues, we propose CipherDM, the first novel, versatile and universal framework applying MPC technology to DMs for secure sampling, which can be widely implemented on multiple DM based tasks. We thoroughly analyze sampling latency breakdown, find time-consuming parts and design corresponding secure MPC protocols for computing nonlinear activations including SoftMax, SiLU and Mish. CipherDM is evaluated on popular architectures (DDPM, DDIM) using MNIST dataset and on SD deployed by diffusers. Compared to direct implementation on SPU, our approach improves running time by approximately 1.084\times \sim 2.328\times, and reduces communication costs by approximately 1.212\times \sim 1.791\times.

Translated Abstract:
확산 모델(Diffusion Models, DMs)은 이미지 생성에서 최첨단 합성 결과를 달성하고 여러 분야에 적용되고 있어. 하지만 DMs를 사용할 때 사용자 프라이버시를 심각하게 침해할 때가 있어, 그래서 프라이버시 보호가 긴급한 문제로 떠오르고 있어. 

전통적인 프라이버시 컴퓨팅 방식인 안전 다자간 계산(Secure Multi-Party Computation, MPC)을 DMs에 직접 적용하는 건 계산과 통신에서 큰 도전이 있어. 이런 문제를 해결하기 위해 우리는 CipherDM을 제안해. 이는 MPC 기술을 DMs에 적용한 첫 번째 새로운 프레임워크로, 안전한 샘플링을 가능하게 해. 이건 여러 DM 기반 작업에 널리 적용할 수 있어.

우리는 샘플링 지연 시간의 분석을 철저히 하고, 시간이 많이 걸리는 부분을 찾아내서 SoftMax, SiLU, Mish 같은 비선형 활성화를 계산하기 위한 안전한 MPC 프로토콜을 설계했어. CipherDM은 MNIST 데이터셋을 사용하여 인기 있는 아키텍처(DDPM, DDIM)에서 평가되었고, diffusers에 의해 배포된 SD에서도 테스트했어. 

직접 SPU에 구현한 것과 비교했을 때, 우리의 접근법은 실행 시간을 약 1.084배에서 2.328배 개선하고, 통신 비용은 약 1.212배에서 1.791배 줄였어.

================================================================================

URL: https://arxiv.org/abs/2409.05490
Title: A Taxonomy of Miscompressions: Preparing Image Forensics for Neural Compression

Original Abstract:
Neural compression has the potential to revolutionize lossy image compression. Based on generative models, recent schemes achieve unprecedented compression rates at high perceptual quality but compromise semantic fidelity. Details of decompressed images may appear optically flawless but semantically different from the originals, making compression errors difficult or impossible to detect. We explore the problem space and propose a provisional taxonomy of miscompressions. It defines three types of 'what happens' and has a binary 'high impact' flag indicating miscompressions that alter symbols. We discuss how the taxonomy can facilitate risk communication and research into mitigations.

Translated Abstract:
신경 압축은 손실 이미지 압축을 혁신할 가능성이 있어. 생성 모델을 기반으로 한 최근 기법들은 높은 인식 품질에서 이전에 없던 압축률을 달성하고 있지만, 의미적인 정확성을 희생하고 있어.

압축을 푼 이미지의 세부 사항은 시각적으로 완벽해 보일 수 있지만 원본과는 의미가 다르기 때문에 압축 오류를 감지하기 어렵거나 아예 불가능할 수 있어. 우리는 이러한 문제를 탐구하고 잘못된 압축의 임시 분류 체계를 제안해.

이 분류 체계는 '무슨 일이 발생하는지'를 세 가지 유형으로 정의하고, 기호를 변경하는 잘못된 압축을 나타내는 이진 '높은 영향' 플래그를 가지고 있어. 우리는 이 분류 체계가 위험 소통과 완화 연구에 어떻게 도움이 될 수 있는지 논의해.

================================================================================

URL: https://arxiv.org/abs/2409.05666
Title: Robust Real-time Segmentation of Bio-Morphological Features in Human Cherenkov Imaging during Radiotherapy via Deep Learning

Original Abstract:
Cherenkov imaging enables real-time visualization of megavoltage X-ray or electron beam delivery to the patient during Radiation Therapy (RT). Bio-morphological features, such as vasculature, seen in these images are patient-specific signatures that can be used for verification of positioning and motion management that are essential to precise RT treatment. However until now, no concerted analysis of this biological feature-based tracking was utilized because of the slow speed and accuracy of conventional image processing for feature segmentation. This study demonstrated the first deep learning framework for such an application, achieving video frame rate processing. To address the challenge of limited annotation of these features in Cherenkov images, a transfer learning strategy was applied. A fundus photography dataset including 20,529 patch retina images with ground-truth vessel annotation was used to pre-train a ResNet segmentation framework. Subsequently, a small Cherenkov dataset (1,483 images from 212 treatment fractions of 19 breast cancer patients) with known annotated vasculature masks was used to fine-tune the model for accurate segmentation prediction. This deep learning framework achieved consistent and rapid segmentation of Cherenkov-imaged bio-morphological features on another 19 patients, including subcutaneous veins, scars, and pigmented skin. Average segmentation by the model achieved Dice score of 0.85 and required less than 0.7 milliseconds processing time per instance. The model demonstrated outstanding consistency against input image variances and speed compared to conventional manual segmentation methods, laying the foundation for online segmentation in real-time monitoring in a prospective setting.

Translated Abstract:
체렌코프 이미징은 방사선 치료(RT) 중에 환자에게 메가볼트 X선이나 전자빔을 전달하는 과정을 실시간으로 시각화할 수 있게 해줘. 이 이미지에서 보이는 혈관 구조 같은 생물학적 형태는 환자별 고유의 특징으로, 정확한 RT 치료를 위해 필수적인 위치 확인과 움직임 관리에 사용될 수 있어. 

하지만 지금까지 이런 생물학적 특징을 바탕으로 한 추적 분석은 일반 이미지 처리의 느린 속도와 정확성 때문에 제대로 활용되지 않았어. 이번 연구에서는 이런 용도로 깊은 학습 프레임워크를 처음으로 제시했는데, 비디오 프레임 속도로 처리할 수 있었어. 체렌코프 이미지에서 이 특징에 대한 주석이 제한적인 문제를 해결하기 위해 전이 학습 전략을 사용했어. 20,529개의 패치 망막 이미지와 혈관 주석이 포함된 안저 사진 데이터셋을 사용해 ResNet 분할 프레임워크를 사전 훈련했어. 그 다음, 알려진 혈관 마스크로 주석된 작은 체렌코프 데이터셋(19명의 유방암 환자의 212개 치료 분획에서 1,483장 이미지)을 사용해 모델을 미세 조정했어. 

이 깊은 학습 프레임워크는 19명의 다른 환자에서 체렌코프 이미징 생물학적 형태의 일관되고 빠른 분할을 달성했어. 여기에는 피하 정맥, 흉터, 색소가 있는 피부가 포함돼. 모델의 평균 분할 성능은 Dice 점수 0.85를 기록했으며, 인스턴스당 처리 시간은 0.7 밀리초 이하였어. 이 모델은 입력 이미지의 변동성에 대해 뛰어난 일관성과 속도를 보여주었고, 기존의 수동 분할 방법에 비해 훨씬 빠른 성능을 보였어. 이 연구는 실시간 모니터링의 온라인 분할을 위한 기초를 마련했어.

================================================================================

URL: https://arxiv.org/abs/2409.05680
Title: Cherenkov Imaged Bio-morphological Features Verify Patient Positioning with Deformable Tissue Translocation in Breast Radiotherapy

Original Abstract:
Accurate patient positioning is critical for precise radiotherapy dose delivery, as positioning errors can significantly affect treatment outcomes. This study introduces a novel method for tracking loco-regional tissue deformation through Cherenkov image analysis during fractionated breast cancer radiotherapy. The primary goal was to develop and test an algorithm for Cherenkov-based regional position accuracy quantification, specifically for loco-regional deformations, which lack ideal quantification methods in radiotherapy. Blood vessel detection and segmentation were developed in Cherenkov images using a tissue phantom with incremental movements, and later applied to images from fractionated whole breast radiotherapy in human patients (n=10). A combined rigid and non-rigid registration technique was used to detect inter- and intra-fractional positioning variations. This approach quantified positioning variations in two parts: a global shift from rigid registration and a two-dimensional variation map of loco-regional deformation from non-rigid registration. The methodology was validated using an anthropomorphic chest phantom experiment, where known treatment couch translations and respiratory motion were simulated to assess inter- and intra-fractional uncertainties, yielding an average accuracy of 0.83 mm for couch translations up to 20 mm. Analysis of clinical Cherenkov data from ten breast cancer patients showed an inter-fraction setup variation of 3.7 plus minus 2.4 mm relative to the first fraction and loco-regional deformations (95th percentile) of up to 3.3 plus minus 1.9 mm. This study presents a Cherenkov-based approach to quantify global and local positioning variations, demonstrating feasibility in addressing loco-regional deformations that conventional imaging techniques fail to capture.

Translated Abstract:
정확한 환자 위치 설정은 방사선 치료에서 정확한 용량 전달을 위해 정말 중요해. 위치 오류가 치료 결과에 큰 영향을 미칠 수 있으니까. 이 연구는 유방암 방사선 치료 중에 체내 조직 변형을 추적하는 새로운 방법을 소개해. 

주된 목표는 체내 변형을 위해 체렌코프 이미지를 활용한 지역 위치 정확도를 측정하는 알고리즘을 개발하고 테스트하는 거야. 방사선 치료에서 이상적인 측정 방법이 부족하거든. 체렌코프 이미지에서 혈관을 감지하고 분할하는 방법을 개발하고, 이걸 조금씩 움직이는 조직 모형에 적용했어. 그 후, 10명의 유방암 환자에게서 나온 이미지에도 적용했지.

이 방법은 고정된 등록 기술과 비고정 등록 기술을 결합해서 환자 위치의 변화를 감지했어. 두 가지 부분으로 위치 변화를 정량화했는데, 하나는 고정 등록에서의 전반적인 이동량, 다른 하나는 비고정 등록에서의 지역 변형의 2D 변화를 보여주는 거야. 

방법론은 인체 모형 실험을 통해 검증되었고, 여기서 알려진 치료용 소파 이동과 호흡 운동을 시뮬레이션해서 위치의 불확실성을 평가했어. 그 결과, 소파 이동에 대해 평균 0.83 mm의 정확도를 얻었지. 

10명의 유방암 환자에서의 체렌코프 데이터 분석 결과, 첫 번째 치료에 대비한 치료 간 위치 설정 변동이 3.7 ± 2.4 mm였고, 지역 변형은 최대 3.3 ± 1.9 mm였어. 이 연구는 체렌코프 기반의 접근 방식을 통해 전반적인 위치 변동과 지역 변동을 정량화할 수 있음을 보여주고, 기존 이미징 기술이 포착하지 못하는 지역 변형을 해결하는 데 가능성을 나타내.

================================================================================

URL: https://arxiv.org/abs/2409.05721
Title: Referring Expression Generation in Visually Grounded Dialogue with Discourse-aware Comprehension Guiding

Original Abstract:
We propose an approach to referring expression generation (REG) in visually grounded dialogue that is meant to produce referring expressions (REs) that are both discriminative and discourse-appropriate. Our method constitutes a two-stage process. First, we model REG as a text- and image-conditioned next-token prediction task. REs are autoregressively generated based on their preceding linguistic context and a visual representation of the referent. Second, we propose the use of discourse-aware comprehension guiding as part of a generate-and-rerank strategy through which candidate REs generated with our REG model are reranked based on their discourse-dependent discriminatory power. Results from our human evaluation indicate that our proposed two-stage approach is effective in producing discriminative REs, with higher performance in terms of text-image retrieval accuracy for reranked REs compared to those generated using greedy decoding.

Translated Abstract:
우리는 시각적으로 기반한 대화에서 참조 표현 생성(REF)을 위한 방법을 제안해. 이 방법은 구별력이 있고 대화에 적합한 참조 표현(REs)을 만드는 데 초점을 맞추고 있어.

우리 방법은 두 단계로 나뉘어져 있어. 첫 번째로, REF를 텍스트와 이미지에 기반한 다음 토큰 예측 작업으로 모델링해. REs는 앞선 언어적 문맥과 참조 대상의 시각적 표현을 바탕으로 순차적으로 생성돼. 두 번째로, 대화 인식을 고려한 이해 가이드를 사용하는데, 이는 생성 후 재순위 지정 전략의 일환이야. 이 과정을 통해 우리 REF 모델로 생성된 후보 RE들이 대화에 따라 구별력이 얼마나 있는지를 기준으로 재순위가 돼.

인간 평가 결과에 따르면, 우리가 제안한 이 두 단계 접근 방식이 구별력이 있는 RE를 만드는 데 효과적이라는 것을 보여줘. 재순위된 RE들은 탐색 정확도 면에서도 그리디 디코딩으로 생성된 것보다 더 높은 성능을 보였어.

================================================================================

URL: https://arxiv.org/abs/2409.05742
Title: Robust Loss Functions for Object Grasping under Limited Ground Truth

Original Abstract:
Object grasping is a crucial technology enabling robots to perceive and interact with the environment sufficiently. However, in practical applications, researchers are faced with missing or noisy ground truth while training the convolutional neural network, which decreases the accuracy of the model. Therefore, different loss functions are proposed to deal with these problems to improve the accuracy of the neural network. For missing ground truth, a new predicted category probability method is defined for unlabeled samples, which works effectively in conjunction with the pseudo-labeling method. Furthermore, for noisy ground truth, a symmetric loss function is introduced to resist the corruption of label noises. The proposed loss functions are powerful, robust, and easy to use. Experimental results based on the typical grasping neural network show that our method can improve performance by 2 to 13 percent.

Translated Abstract:
물체 잡기는 로봇이 환경을 인식하고 상호작용하는 데 중요한 기술이야. 하지만 실제로 연구를 하다 보면, 합성곱 신경망을 훈련할 때 결측값이나 노이즈가 있는 데이터 때문에 모델의 정확도가 떨어지는 문제가 생겨. 그래서 이런 문제를 해결하기 위해 다양한 손실 함수가 제안돼서 신경망의 정확도를 높이려고 해.

결측값이 있을 경우, 라벨이 없는 샘플에 대해 새로운 예측 카테고리 확률 방법을 정의했어. 이 방법은 의사 라벨링 기법과 함께 효과적으로 작동해. 그리고 노이즈가 있는 데이터에 대해서는 라벨 노이즈의 영향을 저항할 수 있는 대칭 손실 함수를 도입했어.

제안된 손실 함수는 강력하고, 견고하며, 사용하기 쉬워. 일반적인 잡기 신경망을 기반으로 한 실험 결과에 따르면, 우리의 방법이 성능을 2%에서 13%까지 향상시킬 수 있음을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.05770
Title: Consensus-based Distributed Quantum Kernel Learning for Speech Recognition

Original Abstract:
This paper presents a Consensus-based Distributed Quantum Kernel Learning (CDQKL) framework aimed at improving speech recognition through distributed quantum computing.CDQKL addresses the challenges of scalability and data privacy in centralized quantum kernel learning. It does this by distributing computational tasks across quantum terminals, which are connected through classical channels. This approach enables the exchange of model parameters without sharing local training data, thereby maintaining data privacy and enhancing computational efficiency. Experimental evaluations on benchmark speech emotion recognition datasets demonstrate that CDQKL achieves competitive classification accuracy and scalability compared to centralized and local quantum kernel learning models. The distributed nature of CDQKL offers advantages in privacy preservation and computational efficiency, making it suitable for data-sensitive fields such as telecommunications, automotive, and finance. The findings suggest that CDQKL can effectively leverage distributed quantum computing for large-scale machine-learning tasks.

Translated Abstract:
이 논문은 분산 양자 커널 학습(CDQKL) 프레임워크를 소개해. 이건 분산 양자 컴퓨팅을 통해 음성 인식을 개선하는 걸 목표로 하고 있어. CDQKL은 중앙 집중식 양자 커널 학습에서 발생하는 확장성 문제와 데이터 프라이버시 문제를 해결해.

이 방법은 양자 터미널에 계산 작업을 분산시키고, 이 터미널들은 고전적인 채널로 연결돼. 이렇게 하면 로컬 훈련 데이터를 공유하지 않고도 모델 파라미터를 교환할 수 있어서 데이터 프라이버시를 유지하면서 계산 효율성을 높일 수 있어.

실험 결과, CDQKL은 벤치마크 음성 감정 인식 데이터셋에서 중앙 집중식이나 로컬 양자 커널 학습 모델에 비해 경쟁력 있는 분류 정확도와 확장성을 보여줬어. CDQKL의 분산 특성 덕분에 프라이버시 보호와 계산 효율성이 향상돼. 그래서 통신, 자동차, 금융 같은 데이터 민감한 분야에 적합해.

결론적으로, CDQKL은 대규모 머신러닝 작업을 위해 분산 양자 컴퓨팅을 효과적으로 활용할 수 있다는 걸 보여줘.

================================================================================

URL: https://arxiv.org/abs/2409.05773
Title: Creativity and Visual Communication from Machine to Musician: Sharing a Score through a Robotic Camera

Original Abstract:
This paper explores the integration of visual communication and musical interaction by implementing a robotic camera within a "Guided Harmony" musical game. We aim to examine co-creative behaviors between human musicians and robotic systems. Our research explores existing methodologies like improvisational game pieces and extends these concepts to include robotic participation using a PTZ camera. The robotic system interprets and responds to nonverbal cues from musicians, creating a collaborative and adaptive musical experience. This initial case study underscores the importance of intuitive visual communication channels. We also propose future research directions, including parameters for refining the visual cue toolkit and data collection methods to understand human-machine co-creativity further. Our findings contribute to the broader understanding of machine intelligence in augmenting human creativity, particularly in musical settings.

Translated Abstract:
이 논문은 "Guided Harmony"라는 음악 게임에서 로봇 카메라를 활용해 시각적 소통과 음악적 상호작용을 어떻게 통합할 수 있는지 탐구해. 우리는 인간 음악가와 로봇 시스템 간의 협력적인 행동을 조사하려고 해.

이 연구는 즉흥 게임 조각 같은 기존 방법론을 살펴보고, 여기에 PTZ 카메라를 사용해 로봇의 참여를 추가하는 개념을 확장해. 로봇 시스템은 음악가의 비언어적 신호를 해석하고 이에 반응하면서 협력적이고 적응적인 음악 경험을 만들어.

이 초기 사례 연구는 직관적인 시각적 소통 채널의 중요성을 강조해. 앞으로의 연구 방향으로는 시각적 신호 도구 키트를 정제하는 매개변수와 인간-기계 공동 창작을 더 이해하기 위한 데이터 수집 방법을 제안해. 우리의 결과는 기계 지능이 인간의 창의성을 어떻게 증대시킬 수 있는지를 더 넓게 이해하는 데 기여해, 특히 음악적 환경에서 말이야.

================================================================================

URL: https://arxiv.org/abs/2409.05800
Title: Input Space Mode Connectivity in Deep Neural Networks

Original Abstract:
We extend the concept of loss landscape mode connectivity to the input space of deep neural networks. Mode connectivity was originally studied within parameter space, where it describes the existence of low-loss paths between different solutions (loss minimizers) obtained through gradient descent. We present theoretical and empirical evidence of its presence in the input space of deep networks, thereby highlighting the broader nature of the phenomenon. We observe that different input images with similar predictions are generally connected, and for trained models, the path tends to be simple, with only a small deviation from being a linear path. Our methodology utilizes real, interpolated, and synthetic inputs created using the input optimization technique for feature visualization. We conjecture that input space mode connectivity in high-dimensional spaces is a geometric effect that takes place even in untrained models and can be explained through percolation theory. We exploit mode connectivity to obtain new insights about adversarial examples and demonstrate its potential for adversarial detection. Additionally, we discuss applications for the interpretability of deep networks.

Translated Abstract:
우리는 딥 뉴럴 네트워크의 입력 공간에서 손실 경관 모드 연결성 개념을 확장했어. 모드 연결성은 원래 파라미터 공간에서 연구되었는데, 여기서 서로 다른 솔루션(손실 최소화기) 사이에 낮은 손실 경로가 존재하는 걸 설명해. 

우리는 딥 네트워크의 입력 공간에서도 모드 연결성이 존재한다는 이론적 및 실증적 증거를 보여줘. 이로 인해 이 현상의 더 넓은 성격을 강조할 수 있어. 비슷한 예측을 가진 서로 다른 입력 이미지들이 일반적으로 연결되어 있는 걸 관찰했고, 훈련된 모델의 경우 경로는 간단하고 거의 직선 경로와 유사하게 나타나. 

우리의 방법론은 입력 최적화 기법을 사용해서 만든 실제, 보간된, 그리고 합성된 입력을 활용해. 우리는 고차원 공간에서의 입력 공간 모드 연결성이 훈련되지 않은 모델에서도 발생하는 기하학적 효과라고 추측하고, 이를 침투 이론으로 설명할 수 있어. 

모드 연결성을 이용해 적대적 예제에 대한 새로운 통찰을 얻고, 적대적 탐지를 위한 잠재력을 보여줘. 또한, 딥 네트워크의 해석 가능성에 대한 응용도 논의해.

================================================================================

URL: https://arxiv.org/abs/2409.05809
Title: A Flexible Framework for Universal Computational Aberration Correction via Automatic Lens Library Generation and Domain Adaptation

Original Abstract:
Emerging universal Computational Aberration Correction (CAC) paradigms provide an inspiring solution to light-weight and high-quality imaging without repeated data preparation and model training to accommodate new lens designs. However, the training databases in these approaches, i.e., the lens libraries (LensLibs), suffer from their limited coverage of real-world aberration behaviors. In this work, we set up an OmniLens framework for universal CAC, considering both the generalization ability and flexibility. OmniLens extends the idea of universal CAC to a broader concept, where a base model is trained for three cases, including zero-shot CAC with the pre-trained model, few-shot CAC with a little lens-specific data for fine-tuning, and domain adaptive CAC using domain adaptation for lens-descriptions-unknown lens. In terms of OmniLens's data foundation, we first propose an Evolution-based Automatic Optical Design (EAOD) pipeline to construct LensLib automatically, coined AODLib, whose diversity is enriched by an evolution framework, with comprehensive constraints and a hybrid optimization strategy for achieving realistic aberration behaviors. For network design, we introduce the guidance of high-quality codebook priors to facilitate zero-shot CAC and few-shot CAC, which enhances the model's generalization ability, while also boosting its convergence in a few-shot case. Furthermore, based on the statistical observation of dark channel priors in optical degradation, we design an unsupervised regularization term to adapt the base model to the target descriptions-unknown lens using its aberration images without ground truth. We validate OmniLens on 4 manually designed low-end lenses with various structures and aberration behaviors. Remarkably, the base model trained on AODLib exhibits strong generalization capabilities, achieving 97% of the lens-specific performance in a zero-shot setting.

Translated Abstract:
새로운 범용 컴퓨테이셔널 비정상 교정(CAC) 패러다임은 반복적인 데이터 준비나 모델 훈련 없이도 가볍고 고품질 이미지를 얻는 멋진 해결책을 제공해. 하지만 이런 방식에서 사용되는 훈련 데이터베이스, 즉 렌즈 라이브러리(LensLibs)는 실제 세계의 비정상 행동을 충분히 반영하지 못하는 한계가 있어. 

이 연구에서는 범용 CAC를 위한 OmniLens 프레임워크를 설정했어. 여기서는 일반화 능력과 유연성을 모두 고려했어. OmniLens는 범용 CAC의 개념을 더 넓은 개념으로 확장해. 기본 모델은 세 가지 경우에 대해 훈련되는데, 여기에는 사전 훈련된 모델을 이용한 제로샷 CAC, 조금의 렌즈 특정 데이터를 사용한 몇 샷 CAC, 그리고 렌즈 설명이 없는 경우에 대한 도메인 적응 CAC가 포함돼.

OmniLens의 데이터 기초를 위해, 우리는 렌즈 라이브러리를 자동으로 구성하는 진화 기반 자동 광학 설계(EAOD) 파이프라인인 AODLib를 제안해. 이 파이프라인은 진화 프레임워크를 통해 다양성을 높이고, 현실적인 비정상 행동을 달성하기 위한 포괄적인 제약조건과 하이브리드 최적화 전략을 사용해.

네트워크 설계 측면에서는, 제로샷 CAC와 몇 샷 CAC를 돕기 위해 고품질 코드북 프라이어의 가이드를 도입했어. 이건 모델의 일반화 능력을 향상시키고, 몇 샷 경우에서는 수렴을 더 빠르게 만들어. 또한, 광학 열화에서 어두운 채널 프라이어의 통계적 관찰을 바탕으로, 기준 모델이 타겟 설명이 없는 렌즈에 적응할 수 있도록 하는 비지도 정규화 항을 설계했어. 이건 비정상 이미지를 사용해서 진실 데이터 없이도 가능해.

우리는 다양한 구조와 비정상 행동을 가진 4개의 수동 설계 저가 렌즈에서 OmniLens를 검증했어. AODLib에서 훈련된 기본 모델은 강력한 일반화 능력을 보여줘, 제로샷 설정에서 렌즈 특정 성능의 97%를 달성했어.

================================================================================

URL: https://arxiv.org/abs/2409.05864
Title: Neural MP: A Generalist Neural Motion Planner

Original Abstract:
The current paradigm for motion planning generates solutions from scratch for every new problem, which consumes significant amounts of time and computational resources. For complex, cluttered scenes, motion planning approaches can often take minutes to produce a solution, while humans are able to accurately and safely reach any goal in seconds by leveraging their prior experience. We seek to do the same by applying data-driven learning at scale to the problem of motion planning. Our approach builds a large number of complex scenes in simulation, collects expert data from a motion planner, then distills it into a reactive generalist policy. We then combine this with lightweight optimization to obtain a safe path for real world deployment. We perform a thorough evaluation of our method on 64 motion planning tasks across four diverse environments with randomized poses, scenes and obstacles, in the real world, demonstrating an improvement of 23%, 17% and 79% motion planning success rate over state of the art sampling, optimization and learning based planning methods. Video results available at this http URL

Translated Abstract:
현재 모션 플래닝의 방식은 매번 새로운 문제를 해결하기 위해 처음부터 끝까지 모든 걸 새로 만들어야 해서 시간과 계산 자원을 많이 소모해. 복잡하고 어수선한 장면에서는 모션 플래닝이 솔루션을 내기까지 몇 분이 걸릴 때도 있는 반면, 사람은 이전 경험을 활용해서 몇 초 안에 목표를 안전하게 도달할 수 있어. 

우리는 이 문제를 해결하기 위해 데이터 기반 학습을 대규모로 적용하려고 해. 우리의 방법은 많은 복잡한 장면을 시뮬레이션에서 만들어내고, 모션 플래너에서 전문가 데이터를 수집한 다음, 이를 반응형 일반 정책으로 정제해. 그런 다음 가벼운 최적화 기법과 결합해서 실제 환경에서 안전한 경로를 찾아내. 

우리는 이 방법을 실제 세계의 네 가지 다양한 환경에서 랜덤한 자세, 장면, 장애물로 이루어진 64개의 모션 플래닝 작업에 대해 철저하게 평가했어. 그 결과, 최신 샘플링, 최적화 및 학습 기반 플래닝 방법에 비해 모션 플래닝 성공률이 각각 23%, 17%, 79% 향상되었어. 비디오 결과는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.05867
Title: Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering

Original Abstract:
State-of-the-art techniques for 3D reconstruction are largely based on volumetric scene representations, which require sampling multiple points to compute the color arriving along a ray. Using these representations for more general inverse rendering -- reconstructing geometry, materials, and lighting from observed images -- is challenging because recursively path-tracing such volumetric representations is expensive. Recent works alleviate this issue through the use of radiance caches: data structures that store the steady-state, infinite-bounce radiance arriving at any point from any direction. However, these solutions rely on approximations that introduce bias into the renderings and, more importantly, into the gradients used for optimization. We present a method that avoids these approximations while remaining computationally efficient. In particular, we leverage two techniques to reduce variance for unbiased estimators of the rendering equation: (1) an occlusion-aware importance sampler for incoming illumination and (2) a fast cache architecture that can be used as a control variate for the radiance from a high-quality, but more expensive, volumetric cache. We show that by removing these biases our approach improves the generality of radiance cache based inverse rendering, as well as increasing quality in the presence of challenging light transport effects such as specular reflections.

Translated Abstract:
최신 3D 재구성 기술은 주로 볼륨 장면 표현에 기반하고 있어. 이 방식은 레이를 따라 오는 색상을 계산하기 위해 여러 점을 샘플링해야 해. 하지만 이러한 표현을 사용해서 더 일반적인 역 렌더링, 즉 관찰된 이미지에서 기하학, 재료, 조명을 재구성하는 건 어려워. 왜냐하면 이런 볼륨 표현을 재귀적으로 경로 추적하는 게 비용이 많이 들거든.

최근 연구들은 이 문제를 해결하기 위해 방사선 캐시라는 데이터 구조를 사용하고 있어. 이 구조는 어떤 방향에서든지 특정 지점에 도착하는 안정 상태의 무한 반사 방사선을 저장해. 하지만 이런 방법들은 렌더링에 편향을 주는 근사값에 의존하고, 더 중요한 건 최적화에 사용되는 그래디언트에도 영향을 미쳐.

우리는 이런 근사값 없이도 계산 효율성을 유지하는 방법을 제안해. 특히 렌더링 방정식의 편향 없는 추정기를 위해 두 가지 기술을 활용했어: (1) 들어오는 조명에 대한 차폐 인식 중요 샘플러와 (2) 고품질이지만 비용이 더 드는 볼륨 캐시의 방사선에 대한 제어 변량으로 사용할 수 있는 빠른 캐시 아키텍처.

우리가 편향을 제거함으로써, 우리의 접근법은 방사선 캐시 기반 역 렌더링의 일반성을 향상시키고, 스페큘러 반사 같은 어려운 조명 전송 효과가 있을 때 품질도 높아진다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2106.15277
Title: EPMF: Efficient Perception-aware Multi-sensor Fusion for 3D Semantic Segmentation

Original Abstract:
We study multi-sensor fusion for 3D semantic segmentation that is important to scene understanding for many applications, such as autonomous driving and robotics. Existing fusion-based methods, however, may not achieve promising performance due to the vast difference between the two modalities. In this work, we investigate a collaborative fusion scheme called perception-aware multi-sensor fusion (PMF) to effectively exploit perceptual information from two modalities, namely, appearance information from RGB images and spatio-depth information from point clouds. To this end, we project point clouds to the camera coordinate using perspective projection, and process both inputs from LiDAR and cameras in 2D space while preventing the information loss of RGB images. Then, we propose a two-stream network to extract features from the two modalities, separately. The extracted features are fused by effective residual-based fusion modules. Moreover, we introduce additional perception-aware losses to measure the perceptual difference between the two modalities. Last, we propose an improved version of PMF, i.e., EPMF, which is more efficient and effective by optimizing data pre-processing and network architecture under perspective projection. Specifically, we propose cross-modal alignment and cropping to obtain tight inputs and reduce unnecessary computational costs. We then explore more efficient contextual modules under perspective projection and fuse the LiDAR features into the camera stream to boost the performance of the two-stream network. Extensive experiments on benchmark data sets show the superiority of our method. For example, on nuScenes test set, our EPMF outperforms the state-of-the-art method, i.e., RangeFormer, by 0.9% in mIoU. Our source code is available at this https URL.

Translated Abstract:
우리는 자율 주행이나 로봇 공학 같은 여러 응용 프로그램에서 중요한 장면 이해를 위해 3D 의미 분할을 위한 다중 센서 융합을 연구하고 있어. 기존의 융합 기반 방법들은 두 가지 모달리티 간의 큰 차이 때문에 성능이 좋지 않을 수 있어. 

이번 연구에서는 '인지 기반 다중 센서 융합(Perception-aware Multi-Sensor Fusion, PMF)'이라는 협업 융합 방식을 조사해서 RGB 이미지의 외관 정보와 포인트 클라우드에서의 공간-깊이 정보를 효과적으로 활용하려고 해. 이를 위해 포인트 클라우드를 카메라 좌표로 투영하고, LiDAR와 카메라의 두 입력을 2D 공간에서 처리하면서 RGB 이미지의 정보 손실을 방지해. 

그리고 두 가지 모달리티에서 특징을 각각 추출하기 위해 두 개의 스트림 네트워크를 제안해. 추출된 특징들은 효과적인 잔여 기반 융합 모듈을 통해 융합돼. 또한, 두 모달리티 간의 인지 차이를 측정하기 위해 추가적인 인지 기반 손실도 도입해. 

마지막으로, PMF의 개선된 버전인 EPMF를 제안하는데, 이는 원근 투영 하에서 데이터 전처리와 네트워크 구조를 최적화해서 더 효율적이고 효과적이야. 특히, 교차 모달 정렬과 크롭핑을 통해 입력을 조정하고 불필요한 계산 비용을 줄이는 방법을 사용해. 그런 다음 원근 투영 하에서 더 효율적인 컨텍스트 모듈을 탐색하고 LiDAR 특징을 카메라 스트림에 융합시켜 두 개의 스트림 네트워크 성능을 높이려고 해. 

다양한 벤치마크 데이터 세트에서 우리의 방법이 우수하다는 것을 보여줬어. 예를 들어, nuScenes 테스트 세트에서 우리의 EPMF는 최신 방법인 RangeFormer보다 0.9% 더 높은 mIoU를 기록했어. 우리의 소스 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2202.06198
Title: Data standardization for robust lip sync

Original Abstract:
Lip sync is a fundamental audio-visual task. However, existing lip sync methods fall short of being robust in the wild. One important cause could be distracting factors on the visual input side, making extracting lip motion information difficult. To address these issues, this paper proposes a data standardization pipeline to standardize the visual input for lip sync. Based on recent advances in 3D face reconstruction, we first create a model that can consistently disentangle lip motion information from the raw images. Then, standardized images are synthesized with disentangled lip motion information, with all other attributes related to distracting factors set to predefined values independent of the input, to reduce their effects. Using synthesized images, existing lip sync methods improve their data efficiency and robustness, and they achieve competitive performance for the active speaker detection task.

Translated Abstract:
립 싱크는 기본적인 오디오-비주얼 작업이야. 근데 기존의 립 싱크 방법들은 실제 상황에서 잘 작동하지 않아. 그 이유 중 하나는 시각 입력에서 방해 요소들이 있어서 입 모양 정보 추출이 어렵기 때문이야.

이 문제를 해결하기 위해, 이 논문에서는 립 싱크를 위한 시각 입력을 표준화하는 데이터 표준화 파이프라인을 제안해. 최근의 3D 얼굴 재구성 기술을 바탕으로, 우리는 먼저 원본 이미지에서 립 모션 정보를 일관되게 분리해낼 수 있는 모델을 만들어. 

그 다음, 분리된 립 모션 정보를 사용해서 표준화된 이미지를 생성해. 이때 방해 요소와 관련된 다른 속성들은 입력과는 관계없이 미리 정해진 값으로 설정해서 그 영향을 줄여. 

이렇게 생성된 이미지를 사용하면 기존의 립 싱크 방법들이 데이터 효율성과 강인성을 높일 수 있고, 활성화된 화자 감지 작업에서도 경쟁력 있는 성능을 달성할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2303.15225
Title: GP-PCS: One-shot Feature-Preserving Point Cloud Simplification with Gaussian Processes on Riemannian Manifolds

Original Abstract:
The processing, storage and transmission of large-scale point clouds is an ongoing challenge in the computer vision community which hinders progress in the application of 3D models to real-world settings, such as autonomous driving, virtual reality and remote sensing. We propose a novel, one-shot point cloud simplification method which preserves both the salient structural features and the overall shape of a point cloud without any prior surface reconstruction step. Our method employs Gaussian processes suitable for functions defined on Riemannian manifolds, allowing us to model the surface variation function across any given point cloud. A simplified version of the original cloud is obtained by sequentially selecting points using a greedy sparsification scheme. The selection criterion used for this scheme ensures that the simplified cloud best represents the surface variation of the original point cloud. We evaluate our method on several benchmark and self-acquired point clouds, compare it to a range of existing methods, demonstrate its application in downstream tasks of registration and surface reconstruction, and show that our method is competitive both in terms of empirical performance and computational efficiency. The code is available at \href{this https URL}{this https URL}.

Translated Abstract:
대규모 포인트 클라우드의 처리, 저장, 전송은 컴퓨터 비전 분야에서 계속해서 어려운 문제야. 이 문제는 자율주행, 가상 현실, 원거리 감지 같은 3D 모델의 실제 적용을 방해하고 있어. 

우리는 새로운 방식의 포인트 클라우드 간소화 방법을 제안해. 이 방법은 사전 표면 재구성이 필요 없이 포인트 클라우드의 중요한 구조적 특징과 전체 형태를 모두 유지해. 우리의 방법은 리만 다양체에서 정의된 함수에 적합한 가우시안 프로세스를 사용해서, 주어진 포인트 클라우드의 표면 변동 함수를 모델링할 수 있어.

원래 클라우드의 간소화 버전은 탐욕적 희소화 기법을 사용해 점들을 순차적으로 선택함으로써 얻어져. 이 선택 기준은 간소화된 클라우드가 원래 포인트 클라우드의 표면 변동을 잘 나타내도록 해.

우리는 여러 벤치마크와 자체 수집한 포인트 클라우드에서 우리의 방법을 평가하고, 기존의 여러 방법들과 비교했어. 또한, 등록 및 표면 재구성 같은 후속 작업에서의 적용도 보여주고, 우리의 방법이 실험적 성능과 계산 효율성 면에서 경쟁력이 있다는 것을 증명했어. 코드도 여기에 있어: \href{this https URL}{this https URL}.

================================================================================

URL: https://arxiv.org/abs/2303.16341
Title: Structured Video-Language Modeling with Temporal Grouping and Spatial Grounding

Original Abstract:
Existing video-language pre-training methods primarily focus on instance-level alignment between video clips and captions via global contrastive learning but neglect rich fine-grained local information in both videos and text, which is of importance to downstream tasks requiring temporal localization and semantic reasoning. A powerful model is expected to be capable of capturing region-object correspondences and recognizing scene changes in a video clip, reflecting spatial and temporal granularity, respectively. To strengthen model's understanding into such fine-grained details, we propose a simple yet effective video-language modeling framework, S-ViLM, by exploiting the intrinsic structures of these two modalities. It includes two novel designs, inter-clip spatial grounding and intra-clip temporal grouping, to promote learning region-object alignment and temporal-aware features, simultaneously. Comprehensive evaluations demonstrate that S-ViLM performs favorably against existing approaches in learning more expressive representations. Specifically, S-ViLM surpasses the state-of-the-art methods substantially on four representative downstream tasks, covering text-video retrieval, video question answering, video action recognition, and temporal action localization.

Translated Abstract:
기존의 비디오-언어 사전 학습 방법들은 주로 비디오 클립과 캡션 간의 인스턴스 수준 정렬에 집중하고, 글로벌 대조 학습을 통해 이루어져. 하지만 비디오와 텍스트에 있는 풍부한 세부 정보는 무시하고 있어. 이런 세부 정보는 시간적 위치 지정이나 의미적 추론 같은 다운스트림 작업에 중요해. 

강력한 모델은 비디오 클립에서 객체와 지역 간의 연관성을 잘 포착하고, 장면 변화도 인식할 수 있어야 해. 이렇게 공간적, 시간적 세부 정보를 반영하는 모델이 필요해. 그래서 우리는 S-ViLM이라는 간단하지만 효과적인 비디오-언어 모델링 프레임워크를 제안해. 이건 두 가지 모달리티의 내재적 구조를 활용해 세부 정보를 이해하는 데 도움을 줄 거야. 

S-ViLM은 두 가지 새로운 디자인을 포함하고 있어: 클립 간 공간 정렬(inter-clip spatial grounding)과 클립 내 시간 그룹화(intra-clip temporal grouping). 이 두 가지 디자인은 지역-객체 정렬과 시간 인식 기능을 동시에 학습하도록 도와줘. 

종합적인 평가 결과, S-ViLM은 기존 방법들과 비교했을 때 더 표현력이 뛰어난 표현을 학습하는 데 유리하다는 걸 보여줘. 특히, S-ViLM은 텍스트-비디오 검색, 비디오 질문 응답, 비디오 동작 인식, 시간적 동작 위치 지정 같은 네 가지 대표적인 다운스트림 작업에서 최신 방법들을 크게 초월했어.

================================================================================

URL: https://arxiv.org/abs/2304.02098
Title: Uncertainty estimation in Deep Learning for Panoptic segmentation

Original Abstract:
As deep learning-based computer vision algorithms continue to advance the state of the art, their robustness to real-world data continues to be an issue, making it difficult to bring an algorithm from the lab to the real world. Ensemble-based uncertainty estimation approaches such as Monte Carlo Dropout have been successfully used in many applications in an attempt to address this robustness issue. Unfortunately, it is not always clear if such ensemble-based approaches can be applied to a new problem domain. This is the case with panoptic segmentation, where the structure of the problem and architectures designed to solve it means that unlike image classification or even semantic segmentation, the typical solution of using a mean across samples cannot be directly applied. In this paper, we demonstrate how ensemble-based uncertainty estimation approaches such as Monte Carlo Dropout can be used in the panoptic segmentation domain with no changes to an existing network, providing both improved performance and more importantly a better measure of uncertainty for predictions made by the network. Results are demonstrated quantitatively and qualitatively on the COCO, KITTI-STEP and VIPER datasets.

Translated Abstract:
딥러닝 기반의 컴퓨터 비전 알고리즘이 계속 발전하고 있지만, 실제 데이터에 대한 강건성 문제는 여전히 해결되지 않고 있어, 연구실에서 개발한 알고리즘을 실제 환경에 적용하기가 어렵다. 몬테카를로 드롭아웃과 같은 앙상블 기반의 불확실성 추정 방법이 이 문제를 해결하기 위해 많은 응용 프로그램에서 성공적으로 사용되고 있다. 하지만 이런 앙상블 기반 방법이 새로운 문제 도메인에 적용될 수 있는지는 항상 확실하지 않다.

이런 문제가 팬옵틱 세그멘테이션에서도 발생하는데, 이 문제의 구조와 이를 해결하기 위해 설계된 아키텍처 때문에 이미지 분류나 의미론적 세그멘테이션과 달리 샘플 평균을 사용하는 전형적인 방법을 직접 적용할 수 없다. 이 논문에서는 몬테카를로 드롭아웃과 같은 앙상블 기반의 불확실성 추정 방법이 기존 네트워크를 변경하지 않고도 팬옵틱 세그멘테이션 분야에서 어떻게 사용될 수 있는지를 보여준다. 이를 통해 성능이 향상될 뿐만 아니라, 네트워크가 내는 예측의 불확실성에 대한 더 나은 측정도 제공한다.

결과는 COCO, KITTI-STEP, VIPER 데이터셋에서 정량적이고 정성적으로 시연되었다.

================================================================================

URL: https://arxiv.org/abs/2305.07825
Title: Student Classroom Behavior Detection based on YOLOv7-BRA and Multi-Model Fusion

Original Abstract:
Accurately detecting student behavior in classroom videos can aid in analyzing their classroom performance and improving teaching effectiveness. However, the current accuracy rate in behavior detection is low. To address this challenge, we propose the Student Classroom Behavior Detection system based on based on YOLOv7-BRA (YOLOv7 with Bi-level Routing Attention ). We identified eight different behavior patterns, including standing, sitting, speaking, listening, walking, raising hands, reading, and writing. We constructed a dataset, which contained 11,248 labels and 4,001 images, with an emphasis on the common behavior of raising hands in a classroom setting (Student Classroom Behavior dataset, SCB-Dataset). To improve detection accuracy, we added the biformer attention module to the YOLOv7 network. Finally, we fused the results from YOLOv7 CrowdHuman, SlowFast, and DeepSort models to obtain student classroom behavior data. We conducted experiments on the SCB-Dataset, and YOLOv7-BRA achieved an mAP@0.5 of 87.1%, resulting in a 2.2% improvement over previous results. Our SCB-dataset can be downloaded from: this https URL

Translated Abstract:
학생 행동을 교실 비디오에서 정확하게 감지하는 건 학생의 수업 성과를 분석하고 교육 효과를 높이는 데 도움이 돼. 하지만 현재 행동 감지의 정확도가 낮은 편이야. 이 문제를 해결하기 위해 YOLOv7-BRA(이중 라우팅 주의 메커니즘이 포함된 YOLOv7)를 기반으로 한 학생 교실 행동 감지 시스템을 제안해.

우리는 서 있기, 앉기, 말하기, 듣기, 걷기, 손들기, 읽기, 쓰기 등 총 8가지 행동 패턴을 찾았어. 이 패턴을 포함하는 데이터셋을 만들었고, 총 11,248개의 레이블과 4,001장의 이미지를 포함했어. 특히 교실에서 손드는 행동을 강조했어(학생 교실 행동 데이터셋, SCB-Dataset).

정확도를 높이기 위해 YOLOv7 네트워크에 바이포머 주의 모듈을 추가했어. 마지막으로 YOLOv7 CrowdHuman, SlowFast, DeepSort 모델의 결과를 결합해서 학생 교실 행동 데이터를 얻었어. SCB-Dataset에서 실험을 진행했더니, YOLOv7-BRA가 mAP@0.5에서 87.1%의 성과를 냈고, 이전 결과보다 2.2% 향상된 거야. 우리의 SCB 데이터셋은 이 링크에서 다운로드할 수 있어: this https URL

================================================================================

URL: https://arxiv.org/abs/2306.03318
Title: Student Classroom Behavior Detection based on Improved YOLOv7

Original Abstract:
Accurately detecting student behavior in classroom videos can aid in analyzing their classroom performance and improving teaching effectiveness. However, the current accuracy rate in behavior detection is low. To address this challenge, we propose the Student Classroom Behavior Detection method, based on improved YOLOv7. First, we created the Student Classroom Behavior dataset (SCB-Dataset), which includes 18.4k labels and 4.2k images, covering three behaviors: hand raising, reading, and writing. To improve detection accuracy in crowded scenes, we integrated the biformer attention module and Wise-IoU into the YOLOv7 network. Finally, experiments were conducted on the SCB-Dataset, and the model achieved an mAP@0.5 of 79%, resulting in a 1.8% improvement over previous results. The SCB-Dataset and code are available for download at: this https URL.

Translated Abstract:
교실 비디오에서 학생 행동을 정확하게 감지하는 건 그들의 수업 성과를 분석하고 교육 효과를 높이는 데 도움이 돼. 하지만 현재 행동 감지의 정확도는 낮아. 이 문제를 해결하기 위해 우리는 개선된 YOLOv7을 기반으로 한 학생 교실 행동 감지 방법을 제안해.

먼저, 우리는 학생 교실 행동 데이터셋(SCB-Dataset)을 만들었어. 이 데이터셋은 18,400개의 레이블과 4,200개의 이미지를 포함하고, 손 들기, 읽기, 쓰기 세 가지 행동을 다뤄. 혼잡한 장면에서 감지 정확도를 높이기 위해 biformer 주의 모듈과 Wise-IoU를 YOLOv7 네트워크에 통합했어.

마지막으로 SCB-Dataset에서 실험을 진행했는데, 모델의 mAP@0.5는 79%에 달했고, 이전 결과보다 1.8% 향상된 거야. SCB-Dataset과 코드는 이 URL에서 다운로드할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2307.13756
Title: PlaneRecTR++: Unified Query Learning for Joint 3D Planar Reconstruction and Pose Estimation

Original Abstract:
3D plane reconstruction from images can usually be divided into several sub-tasks of plane detection, segmentation, parameters regression and possibly depth prediction for per-frame, along with plane correspondence and relative camera pose estimation between frames. Previous works tend to divide and conquer these sub-tasks with distinct network modules, overall formulated by a two-stage paradigm. With an initial camera pose and per-frame plane predictions provided from the first stage, exclusively designed modules, potentially relying on extra plane correspondence labelling, are applied to merge multi-view plane entities and produce 6DoF camera pose. As none of existing works manage to integrate above closely related sub-tasks into a unified framework but treat them separately and sequentially, we suspect it potentially as a main source of performance limitation for existing approaches. Motivated by this finding and the success of query-based learning in enriching reasoning among semantic entities, in this paper, we propose PlaneRecTR++, a Transformer-based architecture, which for the first time unifies all sub-tasks related to multi-view reconstruction and pose estimation with a compact single-stage model, refraining from initial pose estimation and plane correspondence supervision. Extensive quantitative and qualitative experiments demonstrate that our proposed unified learning achieves mutual benefits across sub-tasks, obtaining a new state-of-the-art performance on public ScanNetv1, ScanNetv2, NYUv2-Plane, and MatterPort3D datasets.

Translated Abstract:
3D 평면 재구성을 위한 이미지 처리 과정은 보통 평면 탐지, 분할, 파라미터 회귀, 깊이 예측 같은 여러 하위 작업으로 나눌 수 있어. 이때 각 프레임마다 작업을 처리하고, 프레임 간 평면 대응 및 상대 카메라 자세 추정도 필요해. 

기존 연구들은 이런 하위 작업을 각각 다른 네트워크 모듈로 나눠서 해결하는 경향이 있었고, 보통 두 단계로 진행되는 방식이야. 첫 단계에서 초기 카메라 자세와 각 프레임의 평면 예측이 주어지면, 특별히 설계된 모듈이 여러 시점의 평면을 합쳐서 6DoF 카메라 자세를 만들어내. 하지만 기존 연구들은 서로 밀접하게 관련된 하위 작업들을 하나의 통합된 프레임워크로 묶지 않고, 따로따로 처리하는 경향이 있어서 성능의 한 원인으로 작용할 수 있다고 생각해.

이런 문제를 해결하기 위해, 그리고 의미 기반 학습이 세맨틱 엔티티 간의 추론을 풍부하게 하는 데 성공한 것을 바탕으로, 우리는 PlaneRecTR++라는 Transformer 기반 아키텍처를 제안해. 이 연구는 모든 하위 작업을 통합해서 단일 단계 모델로 처리하는 첫 번째 사례야. 초기 자세 추정이나 평면 대응 감독 없이도 가능해. 

광범위한 정량적 및 정성적 실험 결과, 우리가 제안한 통합 학습 방식이 하위 작업 간의 상호 이점을 얻어내며, 공공 데이터셋인 ScanNetv1, ScanNetv2, NYUv2-Plane, MatterPort3D에서 새로운 최첨단 성능을 달성했다는 걸 보여줘.

================================================================================

URL: https://arxiv.org/abs/2307.15588
Title: OAFuser: Towards Omni-Aperture Fusion for Light Field Semantic Segmentation

Original Abstract:
Light field cameras are capable of capturing intricate angular and spatial details. This allows for acquiring complex light patterns and details from multiple angles, significantly enhancing the precision of image semantic segmentation. However, two significant issues arise: (1) The extensive angular information of light field cameras contains a large amount of redundant data, which is overwhelming for the limited hardware resources of intelligent agents. (2) A relative displacement difference exists in the data collected by different micro-lenses. To address these issues, we propose an Omni-Aperture Fusion model (OAFuser) that leverages dense context from the central view and extracts the angular information from sub-aperture images to generate semantically consistent results. To simultaneously streamline the redundant information from the light field cameras and avoid feature loss during network propagation, we present a simple yet very effective Sub-Aperture Fusion Module (SAFM). This module efficiently embeds sub-aperture images in angular features, allowing the network to process each sub-aperture image with a minimal computational demand of only (around 1GFlops). Furthermore, to address the mismatched spatial information across viewpoints, we present a Center Angular Rectification Module (CARM) to realize feature resorting and prevent feature occlusion caused by misalignment. The proposed OAFuser achieves state-of-the-art performance on four UrbanLF datasets in terms of all evaluation metrics and sets a new record of 84.93% in mIoU on the UrbanLF-Real Extended dataset, with a gain of +3.69%. The source code for OAFuser is available at this https URL.

Translated Abstract:
라이트 필드 카메라는 복잡한 각도와 공간 세부정보를 캡처할 수 있어. 덕분에 여러 각도에서 복잡한 빛의 패턴과 세부정보를 얻을 수 있고, 이미지의 의미 분할 정확성을 크게 향상시킬 수 있어. 하지만 두 가지 큰 문제가 있어: 

첫째, 라이트 필드 카메라의 방대한 각도 정보에는 많은 중복 데이터가 포함되어 있어서, 지능형 장치의 제한된 하드웨어 자원에 부담을 줘. 

둘째, 서로 다른 마이크로 렌즈로 수집한 데이터 사이에 상대적인 이동 차이가 있어. 

이런 문제를 해결하기 위해, 우리는 Omni-Aperture Fusion 모델(OAFuser)을 제안해. 이 모델은 중앙 뷰에서 밀집된 컨텍스트를 활용하고, 서브 아퍼처 이미지에서 각도 정보를 추출해 의미적으로 일관된 결과를 생성해. 

또한, 라이트 필드 카메라의 중복 정보를 간소화하고 네트워크 전파 시 특징 손실을 피하기 위해, 간단하면서도 매우 효과적인 서브 아퍼처 융합 모듈(SAFM)을 소개해. 이 모듈은 서브 아퍼처 이미지를 각도 특징에 효율적으로 포함시켜서, 네트워크가 각 서브 아퍼처 이미지를 약 1GFlops의 최소 계산량으로 처리할 수 있게 해. 

그리고, 서로 다른 시점에서 공간 정보의 불일치를 해결하기 위해 중앙 각도 정정 모듈(CARM)을 제안해. 이 모듈은 특징 재정렬을 실현하고 정렬 불량으로 인한 특징 가림을 방지해. 

제안한 OAFuser는 네 가지 UrbanLF 데이터셋에서 모든 평가 지표에 대해 최첨단 성능을 달성했으며, UrbanLF-Real Extended 데이터셋에서 mIoU 84.93%의 새로운 기록을 세웠어. 이는 +3.69%의 향상된 결과야. OAFuser의 소스 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2308.09012
Title: FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings

Original Abstract:
Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.

Translated Abstract:
로고 임베딩 모델은 이미지 속 제품 로고를 벡터로 변환해서, 이를 전자상거래 플랫폼에서 로고 인식 및 감지에 활용할 수 있게 해. 이렇게 하면 지적 재산권을 강화하고 제품 검색 능력을 높일 수 있어. 하지만 현재의 방법들은 로고 임베딩을 순수한 시각적 문제로만 다뤄. 중요한 문제는 시각적 모델이 로고보다 더 많은 특징을 포착한다는 거야. 우리는 이를 다중 모달 작업으로 보고, 텍스트를 보조 정보로 활용해서 시각적 모델이 로고를 더 잘 이해하도록 돕는 방법을 생각했어.

최근에 등장한 다중 모달 대형 언어 모델(MLLMs)은 시각적 이해와 텍스트 이해 모두에서 뛰어난 성능을 보여줬어. 이런 영감을 받아서, 우리는 MLLMs가 제품 이미지에 적합한 텍스트를 생성하도록 유도하는 방법인 \textbf{FashionLOGO}를 제안해. 이 방법은 시각적 임베딩이 텍스트 임베딩으로부터 보조 지식을 자동으로 학습할 수 있게 해주는 교차 주의 변환기 블록을 사용해. 실제 데이터셋에서의 광범위한 실험을 통해, FashionLOGO가 일반적이고 강력한 로고 임베딩을 생성할 수 있음을 입증했고, 모든 벤치마크에서 최첨단 성능을 달성했어.

================================================================================

URL: https://arxiv.org/abs/2308.16911
Title: PointLLM: Empowering Large Language Models to Understand Point Clouds

Original Abstract:
The unprecedented advancements in Large Language Models (LLMs) have shown a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM understands colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate the perceptual and generalization capabilities of PointLLM, we establish two benchmarks: Generative 3D Object Classification and 3D Object Captioning, assessed through three different methods, including human evaluation, GPT-4/ChatGPT evaluation, and traditional metrics. Experimental results reveal PointLLM's superior performance over existing 2D and 3D baselines, with a notable achievement in human-evaluated object captioning tasks where it surpasses human annotators in over 50% of the samples. Codes, datasets, and benchmarks are available at this https URL .

Translated Abstract:
최근 대규모 언어 모델(LLM)의 발전이 자연어 처리에 큰 영향을 미쳤지만, 3D 이해 분야에서는 아직 많이 부족해. 이 논문에서는 PointLLM이라는 초기 시도를 소개하는데, 이건 LLM이 포인트 클라우드를 이해할 수 있게 해주고 2D 시각 데이터 이상의 새로운 가능성을 제공해.

PointLLM은 인간의 지시를 통해 색이 있는 물체의 포인트 클라우드를 이해하고, 그에 맞는 적절한 반응을 생성해. 이건 포인트 클라우드와 일반 상식에 대한 이해를 보여주는 거야. 구체적으로, PointLLM은 강력한 LLM과 함께 포인트 클라우드 인코더를 활용해서 기하학적, 외형적, 언어적 정보를 효과적으로 융합해.

우리는 660K개의 간단한 포인트-텍스트 지시 쌍과 70K개의 복잡한 포인트-텍스트 지시 쌍으로 이루어진 새로운 데이터셋을 수집했어. 이 데이터셋은 두 단계의 훈련 전략을 가능하게 해: 잠재 공간 정렬과 이후 통합 모델의 지시 튜닝. PointLLM의 인식과 일반화 능력을 엄격하게 평가하기 위해, 우리는 두 가지 벤치마크를 설정했어: 생성적 3D 객체 분류와 3D 객체 캡셔닝. 이건 인간 평가, GPT-4/ChatGPT 평가, 전통적인 메트릭을 포함한 세 가지 방법으로 평가해.

실험 결과는 PointLLM이 기존의 2D와 3D 기준보다 뛰어난 성능을 보였다는 걸 보여줘. 특히 인간이 평가한 객체 캡셔닝 작업에서는 50% 이상의 샘플에서 인간 주석자를 초과한 성과를 달성했어. 코드, 데이터셋, 벤치마크는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2309.06941
Title: DEFormer: DCT-driven Enhancement Transformer for Low-light Image and Dark Vision

Original Abstract:
Low-light image enhancement restores colors and details of single image and improves high-level visual tasks. However, restoring the lost details in the dark area is a challenge by only relying on the RGB domain. In this paper, we introduce frequency as a new clue into the network and propose a DCT-driven enhancement transformer (DEFormer) framework. First, we propose a learnable frequency branch (LFB) for frequency enhancement contains DCT processing and curvature-based frequency enhancement (CFE) to represent frequency features. In addition, we propose a cross domain fusion (CDF) for reducing the differences between the RGB domain and the frequency domain. Our DEFormer has achieved advanced results in both the LOL and MIT-Adobe FiveK datasets and improved the performance of dark detection.

Translated Abstract:
저조도 이미지 향상은 단일 이미지의 색상과 디테일을 복원하고 고급 시각 작업을 개선해. 하지만 어두운 영역에서 잃어버린 디테일을 복원하는 건 RGB 도메인만으로는 어려워. 

이 논문에서는 주파수를 새로운 단서로 사용해서 DCT 기반의 향상 변환기(DEFormer) 프레임워크를 제안해. 먼저, 주파수 향상을 위한 학습 가능한 주파수 분기(LFB)를 제안하는데, 여기에는 DCT 처리와 곡률 기반의 주파수 향상(CFE)이 포함되어 주파수 특징을 나타내. 

그리고 RGB 도메인과 주파수 도메인 간의 차이를 줄이기 위해 교차 도메인 융합(CDF)을 제안해. 우리의 DEFormer는 LOL 및 MIT-Adobe FiveK 데이터셋에서 뛰어난 결과를 달성했으며 어두운 영역 탐지 성능도 개선했어.

================================================================================

URL: https://arxiv.org/abs/2310.02522
Title: SCB-Dataset3: A Benchmark for Detecting Student Classroom Behavior

Original Abstract:
The use of deep learning methods to automatically detect students' classroom behavior is a promising approach for analyzing their class performance and improving teaching effectiveness. However, the lack of publicly available datasets on student behavior poses a challenge for researchers in this field. To address this issue, we propose the Student Classroom Behavior dataset (SCB-dataset3), which represents real-life scenarios. Our dataset comprises 5686 images with 45578 labels, focusing on six behaviors: hand-raising, reading, writing, using a phone, bowing the head, and leaning over the table. We evaluated the dataset using the YOLOv5, YOLOv7, and YOLOv8 algorithms, achieving a mean average precision (map) of up to 80.3$\%$. We believe that our dataset can serve as a robust foundation for future research in student behavior detection and contribute to advancements in this field. Our SCB-dataset3 is available for download at: this https URL

Translated Abstract:
학생들의 교실 행동을 자동으로 감지하기 위해 딥러닝 방법을 사용하는 건, 수업 성과를 분석하고 교육 효과를 높이는 데 좋은 방법이야. 하지만 학생 행동에 대한 공개 데이터셋이 부족해서 연구자들이 어려움을 겪고 있어. 

이 문제를 해결하기 위해, 우리는 실제 상황을 반영한 학생 교실 행동 데이터셋(SCB-dataset3)을 제안해. 이 데이터셋은 5686장의 이미지와 45578개의 레이블로 구성되어 있고, 여섯 가지 행동에 초점을 맞추고 있어: 손 들기, 읽기, 쓰기, 휴대폰 사용하기, 고개 숙이기, 그리고 책상에 기대기. 

우리는 YOLOv5, YOLOv7, 그리고 YOLOv8 알고리즘을 사용해 이 데이터셋을 평가했고, 평균 정밀도(map) 80.3%까지 달성했어. 우리의 데이터셋이 학생 행동 감지 연구의 튼튼한 기반이 되고, 이 분야의 발전에 기여할 수 있을 거라고 믿어. SCB-dataset3는 여기에서 다운로드할 수 있어: 이 링크.

================================================================================

URL: https://arxiv.org/abs/2310.02523
Title: A Spatio-Temporal Attention-Based Method for Detecting Student Classroom Behaviors

Original Abstract:
Accurately detecting student behavior from classroom videos is beneficial for analyzing their classroom status and improving teaching efficiency. However, low accuracy in student classroom behavior detection is a prevalent issue. To address this issue, we propose a Spatio-Temporal Attention-Based Method for Detecting Student Classroom Behaviors (BDSTA). Firstly, the SlowFast network is used to generate motion and environmental information feature maps from the video. Then, the spatio-temporal attention module is applied to the feature maps, including information aggregation, compression and stimulation processes. Subsequently, attention maps in the time, channel and space dimensions are obtained, and multi-label behavior classification is performed based on these attention maps. To solve the long-tail data problem that exists in student classroom behavior datasets, we use an improved focal loss function to assign more weight to the tail class data during training. Experimental results are conducted on a self-made student classroom behavior dataset named STSCB. Compared with the SlowFast model, the average accuracy of student behavior classification detection improves by 8.94\% using BDSTA.

Translated Abstract:
교실 비디오에서 학생 행동을 정확하게 감지하는 것은 학생의 수업 상태를 분석하고 교수 효율성을 높이는 데 유용해. 하지만 학생의 행동을 감지하는 정확도가 낮은 문제가 많이 발생해. 이 문제를 해결하기 위해 우리는 Spatio-Temporal Attention 기반의 학생 행동 감지 방법(BDSTA)을 제안해.

먼저, SlowFast 네트워크를 사용해서 비디오에서 움직임과 환경 정보의 특징 맵을 생성해. 그 다음에, 이 특징 맵에 공간-시간 주의 모듈을 적용해서 정보를 집계하고 압축하며 자극하는 과정을 거쳐. 이렇게 해서 시간, 채널, 공간 차원에서 주의 맵을 얻고, 이 주의 맵을 바탕으로 다중 레이블 행동 분류를 진행해.

학생 행동 데이터셋에서 발생하는 긴 꼬리 데이터 문제를 해결하기 위해, 우리는 개선된 포컬 손실 함수를 사용해서 훈련 중에 꼬리 클래스 데이터에 더 많은 가중치를 부여해. 실험은 STSCB라는 우리가 만든 학생 행동 데이터셋에서 진행했어. SlowFast 모델과 비교했을 때, BDSTA를 사용하면 학생 행동 분류 감지의 평균 정확도가 8.94% 향상됐어.

================================================================================

URL: https://arxiv.org/abs/2310.04900
Title: HowToCaption: Prompting LLMs to Transform Video Annotations at Scale

Original Abstract:
Instructional videos are a common source for learning text-video or even multimodal representations by leveraging subtitles extracted with automatic speech recognition systems (ASR) from the audio signal in the videos. However, in contrast to human-annotated captions, both speech and subtitles naturally differ from the visual content of the videos and thus provide only noisy supervision. As a result, large-scale annotation-free web video training data remains sub-optimal for training text-video models. In this work, we propose to leverage the capabilities of large language models (LLMs) to obtain high-quality video descriptions aligned with videos at scale. Specifically, we prompt an LLM to create plausible video captions based on ASR subtitles of instructional videos. To this end, we introduce a prompting method that is able to take into account a longer text of subtitles, allowing us to capture the contextual information beyond one single sentence. We further prompt the LLM to generate timestamps for each produced caption based on the timestamps of the subtitles and finally align the generated captions to the video temporally. In this way, we obtain human-style video captions at scale without human supervision. We apply our method to the subtitles of the HowTo100M dataset, creating a new large-scale dataset, HowToCaption. Our evaluation shows that the resulting captions not only significantly improve the performance over many different benchmark datasets for zero-shot text-video retrieval and video captioning, but also lead to a disentangling of textual narration from the audio, boosting the performance in text-video-audio tasks.

Translated Abstract:
교 instructional 동영상은 자동 음성 인식 시스템(ASR)으로부터 추출한 자막을 이용해 텍스트-비디오 또는 다중 모드 표현을 배우는 데 흔히 사용돼. 그런데 사람의 주석이 달린 캡션과는 달리, 음성과 자막은 비디오의 시각적 내용과 자연스럽게 다르게 되거든. 그래서 이들은 노이즈가 많은 감독만 제공해. 이런 이유로 대규모 주석 없는 웹 비디오 훈련 데이터는 텍스트-비디오 모델 훈련에 최적이 아니야.

우리는 이 문제를 해결하기 위해 대형 언어 모델(LLM)의 능력을 활용해서 비디오와 잘 맞는 고품질 비디오 설명을 대량으로 얻는 방법을 제안해. 구체적으로, 우리는 LLM에게 ASR 자막을 바탕으로 그럴듯한 비디오 캡션을 생성하도록 요청해. 이를 위해, 우리는 자막의 긴 텍스트를 고려할 수 있는 프롬프트 방법을 도입해, 한 문장을 넘어서는 맥락 정보를 캡처할 수 있게 해.

또한, LLM에게 생성된 각 캡션에 대해 자막의 타임스탬프를 바탕으로 타임스탬프를 생성하도록 요청하고, 최종적으로 생성된 캡션을 비디오와 시간적으로 정렬해. 이런 방식으로 우리는 인간 스타일의 비디오 캡션을 대규모로 만들 수 있어, 사람의 감독 없이 말이지. 

우리가 이 방법을 HowTo100M 데이터셋의 자막에 적용해서 새로운 대규모 데이터셋인 HowToCaption을 만들었어. 평가 결과, 생성된 캡션은 제로샷 텍스트-비디오 검색과 비디오 캡셔닝을 위한 다양한 벤치마크 데이터셋에서 성능을 크게 향상시킬 뿐만 아니라, 텍스트 내레이션과 오디오의 분리를 이끌어내어 텍스트-비디오-오디오 작업에서도 성능을 높이는 데 기여해.

================================================================================

URL: https://arxiv.org/abs/2310.16267
Title: Student Classroom Behavior Detection based on Spatio-Temporal Network and Multi-Model Fusion

Original Abstract:
Using deep learning methods to detect students' classroom behavior automatically is a promising approach for analyzing their class performance and improving teaching effectiveness. However, the lack of publicly available spatio-temporal datasets on student behavior, as well as the high cost of manually labeling such datasets, pose significant challenges for researchers in this field. To address this issue, we proposed a method for extending the spatio-temporal behavior dataset in Student Classroom Scenarios (SCB-ST-Dataset4) through image dataset. Our SCB-ST-Dataset4 comprises 757265 images with 25810 labels, focusing on 3 behaviors: hand-raising, reading, writing. Our proposed method can rapidly generate spatio-temporal behavior datasets without requiring extra manual labeling. Furthermore, we proposed a Behavior Similarity Index (BSI) to explore the similarity of behaviors. We evaluated the dataset using the YOLOv5, YOLOv7, YOLOv8, and SlowFast algorithms, achieving a mean average precision (map) of up to 82.3%. Last, we fused multiple models to generate student behavior-related data from various perspectives. The experiment further demonstrates the effectiveness of our method. And SCB-ST-Dataset4 provides a robust foundation for future research in student behavior detection, potentially contributing to advancements in this field. The SCB-ST-Dataset4 is available for download at: this https URL.

Translated Abstract:
학생들의 교실 행동을 자동으로 감지하는 딥러닝 방법은 수업 성과를 분석하고 교육 효과성을 높이는 데 유망한 접근 방식이야. 하지만 학생 행동에 대한 공개된 시공간 데이터셋이 부족하고, 이런 데이터셋을 수작업으로 라벨링하는 데 드는 비용이 너무 높아서 연구자들에게 큰 도전이 되고 있어.

이 문제를 해결하기 위해 우리는 이미지 데이터셋을 통해 학생 교실 상황에서의 시공간 행동 데이터셋(SCB-ST-Dataset4)을 확장하는 방법을 제안했어. 우리 SCB-ST-Dataset4는 총 757,265장의 이미지와 25,810개의 라벨로 구성되어 있고, 3가지 행동인 손들기, 읽기, 쓰기에 초점을 맞추고 있어. 우리가 제안한 방법은 추가적인 수작업 라벨링 없이도 시공간 행동 데이터셋을 빠르게 생성할 수 있어.

또한, 행동의 유사성을 탐색하기 위해 행동 유사성 지수(BSI)를 제안했어. YOLOv5, YOLOv7, YOLOv8, SlowFast 알고리즘을 사용해 데이터셋을 평가했더니 평균 정밀도(map)가 최대 82.3%에 도달했어. 마지막으로 여러 모델을 융합해서 다양한 관점에서 학생 행동 관련 데이터를 생성했어. 실험 결과는 우리의 방법의 효과성을 더 잘 보여주고 있어.

SCB-ST-Dataset4는 학생 행동 감지에 대한 미래 연구를 위한 튼튼한 기반을 제공하고, 이 분야의 발전에 기여할 가능성이 있어. SCB-ST-Dataset4는 이 링크에서 다운로드할 수 있어: this https URL.

================================================================================

URL: https://arxiv.org/abs/2310.18917
Title: TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural Radiance Fields

Original Abstract:
Previous attempts to integrate Neural Radiance Fields (NeRF) into the Simultaneous Localization and Mapping (SLAM) framework either rely on the assumption of static scenes or require the ground truth camera poses, which impedes their application in real-world scenarios. This paper proposes a time-varying representation to track and reconstruct the dynamic scenes. Firstly, two processes, a tracking process and a mapping process, are maintained simultaneously in our framework. In the tracking process, all input images are uniformly sampled and then progressively trained in a self-supervised paradigm. In the mapping process, we leverage motion masks to distinguish dynamic objects from the static background, and sample more pixels from dynamic areas. Secondly, the parameter optimization for both processes is comprised of two stages: the first stage associates time with 3D positions to convert the deformation field to the canonical field. The second stage associates time with the embeddings of the canonical field to obtain colors and a Signed Distance Function (SDF). Lastly, we propose a novel keyframe selection strategy based on the overlapping rate. Our approach is evaluated on two synthetic datasets and one real-world dataset, and the experiments validate that our method achieves competitive results in both tracking and mapping when compared to existing state-of-the-art NeRF-based dynamic SLAM systems.

Translated Abstract:
이전의 Neural Radiance Fields (NeRF)를 Simultaneous Localization and Mapping (SLAM) 프레임워크에 통합하려는 시도들은 보통 정적인 장면을 가정하거나 실제 카메라 위치 데이터를 필요로 했어. 이런 점 때문에 실제 상황에서 활용하기 어려웠지. 이 논문에서는 시간에 따라 변하는 장면을 추적하고 재구성하는 방법을 제안해.

첫째로, 우리의 프레임워크에서는 추적 과정과 매핑 과정을 동시에 진행해. 추적 과정에서는 모든 입력 이미지를 고르게 샘플링하고, 그 후에 자기 지도 학습 방식으로 점진적으로 훈련해. 매핑 과정에서는 움직이는 물체를 정적인 배경과 구분하기 위해 동작 마스크를 활용하고, 동적인 영역에서 더 많은 픽셀을 샘플링해.

둘째로, 두 과정의 파라미터 최적화는 두 단계로 구성되어 있어. 첫 번째 단계에서는 시간을 3D 위치와 연결해서 변형 필드를 기준 필드로 변환해. 두 번째 단계에서는 시간을 기준 필드의 임베딩과 연결해서 색상과 Signed Distance Function (SDF)을 얻어.

마지막으로, 우리는 겹치는 비율에 기반한 새로운 키프레임 선택 전략을 제안해. 우리의 방법은 두 개의 합성 데이터셋과 하나의 실제 데이터셋에서 평가되었고, 실험 결과 우리 방법이 기존의 최신 NeRF 기반 동적 SLAM 시스템과 비교했을 때 추적과 매핑 모두에서 경쟁력 있는 결과를 달성한다는 것을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2311.17366
Title: Generative Hierarchical Temporal Transformer for Hand Pose and Action Modeling

Original Abstract:
We present a novel unified framework that concurrently tackles recognition and future prediction for human hand pose and action modeling. Previous works generally provide isolated solutions for either recognition or prediction, which not only increases the complexity of integration in practical applications, but more importantly, cannot exploit the synergy of both sides and suffer suboptimal performances in their respective domains. To address this problem, we propose a generative Transformer VAE architecture to model hand pose and action, where the encoder and decoder capture recognition and prediction respectively, and their connection through the VAE bottleneck mandates the learning of consistent hand motion from the past to the future and vice versa. Furthermore, to faithfully model the semantic dependency and different temporal granularity of hand pose and action, we decompose the framework into two cascaded VAE blocks: the first and latter blocks respectively model the short-span poses and long-span action, and are connected by a mid-level feature representing a sub-second series of hand poses. This decomposition into block cascades facilitates capturing both short-term and long-term temporal regularity in pose and action modeling, and enables training two blocks separately to fully utilize datasets with annotations of different temporal granularities. We train and evaluate our framework across multiple datasets; results show that our joint modeling of recognition and prediction improves over isolated solutions, and that our semantic and temporal hierarchy facilitates long-term pose and action modeling.

Translated Abstract:
우리는 사람의 손 자세와 행동을 인식하고 미래 예측을 동시에 처리할 수 있는 새로운 통합 프레임워크를 제안해. 이전 연구들은 일반적으로 인식이나 예측 중 하나에만 집중하는 독립적인 해결책을 제공했어. 이런 방식은 실제 응용에서 통합이 복잡해질 뿐만 아니라, 두 가지를 함께 활용하지 못해 각 분야에서 성능이 최적이 아니야.

이 문제를 해결하기 위해, 우리는 손 자세와 행동을 모델링하기 위한 생성적 Transformer VAE 아키텍처를 제안해. 여기서 인코더는 인식을, 디코더는 예측을 담당하는데, VAE 병목을 통해 서로 연결되어 과거에서 미래로, 그리고 그 반대 방향으로 일관된 손 동작을 학습해야 해. 

또한, 손 자세와 행동의 의미적 의존성과 시간적 세분성을 제대로 모델링하기 위해, 프레임워크를 두 개의 연속된 VAE 블록으로 나눠. 첫 번째 블록은 짧은 기간의 자세를 모델링하고, 두 번째 블록은 긴 기간의 행동을 모델링해. 이 두 블록은 손 자세의 1초 미만의 시퀀스를 나타내는 중간 수준의 특징으로 연결돼. 이렇게 블록을 나누면 자세와 행동 모델링에서 단기와 장기 시간 규칙성을 모두 포착할 수 있고, 서로 다른 시간적 세분성의 주석이 있는 데이터셋을 완전히 활용하기 위해 두 블록을 따로 훈련할 수 있어.

우리는 여러 데이터셋에서 우리의 프레임워크를 훈련하고 평가했어. 결과는 인식과 예측의 결합 모델링이 독립된 해결책보다 개선된다는 것을 보여주고, 우리의 의미적 및 시간적 계층 구조가 장기적인 자세와 행동 모델링을 돕는다는 걸 나타내.

================================================================================

URL: https://arxiv.org/abs/2311.18799
Title: X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning

Original Abstract:
Recent research has achieved significant advancements in visual reasoning tasks through learning image-to-language projections and leveraging the impressive reasoning abilities of Large Language Models (LLMs). This paper introduces an efficient and effective framework that integrates multiple modalities (images, 3D, audio and video) to a frozen LLM and demonstrates an emergent ability for cross-modal reasoning (2+ modality inputs). Our approach explores two distinct projection mechanisms: Q-Formers and Linear Projections (LPs). Through extensive experimentation across all four modalities on 16 benchmarks, we explore both methods and assess their adaptability in integrated and separate cross-modal reasoning. The Q-Former projection demonstrates superior performance in single modality scenarios and adaptability in joint versus discriminative reasoning involving two or more modalities. However, it exhibits lower generalization capabilities than linear projection in contexts where task-modality data are limited. To enable this framework, we devise a scalable pipeline that automatically generates high-quality, instruction-tuning datasets from readily available captioning data across different modalities, and contribute 24K QA data for audio and 250K QA data for 3D. To facilitate further research in cross-modal reasoning, we introduce the DisCRn (Discriminative Cross-modal Reasoning) benchmark comprising 9K audio-video QA samples and 28K image-3D QA samples that require the model to reason discriminatively across disparate input modalities.

Translated Abstract:
최근 연구에서는 이미지와 언어를 연결하는 방법을 배우고 대형 언어 모델(LLM)의 뛰어난 추론 능력을 활용하여 시각적 추론 작업에서 큰 발전을 이뤘어. 이 논문에서는 여러 가지 매체(이미지, 3D, 오디오, 비디오)를 결합해서 멈춰 있는 LLM과 통합하는 효율적이고 효과적인 프레임워크를 소개하고, 교차 모달 추론(2개 이상의 모달 입력)에 대한 새로운 능력을 보여줘.

우리의 접근법에서는 두 가지 다른 프로젝션 방법인 Q-Former와 선형 프로젝션(LP)을 탐구해. 16개의 벤치마크를 통해 네 가지 모든 모달리티에서 extensive한 실험을 진행하면서 두 가지 방법을 살펴보고, 통합된 교차 모달 추론과 분리된 교차 모달 추론에서의 적응성을 평가해. Q-Former 프로젝션은 단일 모달 상황에서 더 나은 성능을 보이고, 두 개 이상의 모달을 포함한 공동 대 분별 추론에서의 적응성도 뛰어나. 하지만 작업-모달 데이터가 제한적인 상황에서는 선형 프로젝션보다 일반화 능력이 떨어져.

이 프레임워크를 가능하게 하기 위해, 우리는 서로 다른 모달리티에 있는 캡셔닝 데이터에서 고품질의 지침 조정 데이터셋을 자동으로 생성하는 확장 가능한 파이프라인을 개발했어. 그리고 오디오에 대해 24K QA 데이터, 3D에 대해 250K QA 데이터를 제공해. 교차 모달 추론에 대한 추가 연구를 돕기 위해, 우리는 9K 오디오-비디오 QA 샘플과 28K 이미지-3D QA 샘플로 구성된 DisCRn(구별적 교차 모달 추론) 벤치마크를 소개해, 이 모델이 서로 다른 입력 모달리티 간에 구별적으로 추론해야 해.

================================================================================

URL: https://arxiv.org/abs/2312.02647
Title: TPA3D: Triplane Attention for Fast Text-to-3D Generation

Original Abstract:
Due to the lack of large-scale text-3D correspondence data, recent text-to-3D generation works mainly rely on utilizing 2D diffusion models for synthesizing 3D data. Since diffusion-based methods typically require significant optimization time for both training and inference, the use of GAN-based models would still be desirable for fast 3D generation. In this work, we propose Triplane Attention for text-guided 3D generation (TPA3D), an end-to-end trainable GAN-based deep learning model for fast text-to-3D generation. With only 3D shape data and their rendered 2D images observed during training, our TPA3D is designed to retrieve detailed visual descriptions for synthesizing the corresponding 3D mesh data. This is achieved by the proposed attention mechanisms on the extracted sentence and word-level text features. In our experiments, we show that TPA3D generates high-quality 3D textured shapes aligned with fine-grained descriptions, while impressive computation efficiency can be observed.

Translated Abstract:
최근에 대규모 텍스트-3D 대응 데이터가 부족해서, 텍스트를 3D로 생성하는 작업들이 주로 2D 확산 모델을 이용해 3D 데이터를 합성하는 방식으로 진행되고 있어. 확산 기반 방법은 훈련과 추론에 많은 최적화 시간이 필요하기 때문에, 빠른 3D 생성을 위해 GAN 기반 모델을 사용하는 게 여전히 좋을 것 같아.

이 논문에서는 텍스트 기반 3D 생성을 위한 Triplane Attention (TPA3D)을 제안해. 이건 끝에서 끝까지 훈련 가능한 GAN 기반의 딥러닝 모델로, 빠른 텍스트-3D 생성을 목표로 하고 있어. TPA3D는 훈련 중에 관찰되는 3D 형태 데이터와 그에 대한 렌더링된 2D 이미지만 가지고, 해당 3D 메시 데이터를 합성하기 위해 상세한 시각적 설명을 찾아내도록 설계되었어. 이건 문장과 단어 수준의 텍스트 특징에서 제안된 주의(attention) 메커니즘을 통해 이루어져.

실험 결과, TPA3D는 세밀한 설명에 맞춰 고품질의 3D 텍스처 형태를 생성하고, 뛰어난 계산 효율성도 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2312.05447
Title: From Static to Dynamic: Adapting Landmark-Aware Image Models for Facial Expression Recognition in Videos

Original Abstract:
Dynamic facial expression recognition (DFER) in the wild is still hindered by data limitations, e.g., insufficient quantity and diversity of pose, occlusion and illumination, as well as the inherent ambiguity of facial expressions. In contrast, static facial expression recognition (SFER) currently shows much higher performance and can benefit from more abundant high-quality training data. Moreover, the appearance features and dynamic dependencies of DFER remain largely unexplored. To tackle these challenges, we introduce a novel Static-to-Dynamic model (S2D) that leverages existing SFER knowledge and dynamic information implicitly encoded in extracted facial landmark-aware features, thereby significantly improving DFER performance. Firstly, we build and train an image model for SFER, which incorporates a standard Vision Transformer (ViT) and Multi-View Complementary Prompters (MCPs) only. Then, we obtain our video model (i.e., S2D), for DFER, by inserting Temporal-Modeling Adapters (TMAs) into the image model. MCPs enhance facial expression features with landmark-aware features inferred by an off-the-shelf facial landmark detector. And the TMAs capture and model the relationships of dynamic changes in facial expressions, effectively extending the pre-trained image model for videos. Notably, MCPs and TMAs only increase a fraction of trainable parameters (less than +10\%) to the original image model. Moreover, we present a novel Emotion-Anchors (i.e., reference samples for each emotion category) based Self-Distillation Loss to reduce the detrimental influence of ambiguous emotion labels, further enhancing our S2D. Experiments conducted on popular SFER and DFER datasets show that we achieve the state of the art.

Translated Abstract:
야외에서의 동적 얼굴 표현 인식(DFER)은 데이터 부족 때문에 여전히 어려움을 겪고 있어. 예를 들어, 자세, 가림, 조명 등의 다양성이 부족하고 얼굴 표현 자체가 모호한 경우가 많아. 반면에 정적 얼굴 표현 인식(SFER)은 현재 훨씬 더 높은 성능을 보여주고, 더 많은 고품질 훈련 데이터의 이점을 누릴 수 있어. 게다가 DFER의 외형 특징과 동적 의존성은 아직 많이 연구되지 않았어.

이런 문제를 해결하기 위해, 우리는 기존의 SFER 지식과 추출된 얼굴 랜드마크 인식 특징에 암묵적으로 인코딩된 동적 정보를 활용하는 새로운 정적-동적 모델(S2D)을 소개해. 이 모델은 DFER 성능을 크게 향상시켜. 

먼저, 우리는 SFER을 위한 이미지 모델을 구축하고 훈련해. 이 모델은 표준 비전 트랜스포머(ViT)와 다중 뷰 보완 프로프트(MCP)만 포함해. 그리고 영상 모델(S2D)을 얻기 위해 이미지 모델에 시간 모델링 어댑터(TMA)를 추가해. MCP는 오프더셸프 얼굴 랜드마크 감지기를 통해 추론된 랜드마크 인식 특징으로 얼굴 표현 특징을 강화해. TMA는 얼굴 표현의 동적 변화 관계를 포착하고 모델링해, 기존의 이미지 모델을 비디오에 효과적으로 확장해.

특히, MCP와 TMA는 원래 이미지 모델에 비해 훈련할 수 있는 파라미터를 조금만 늘려(+10% 미만) 성능을 향상시켜. 그리고 우리는 모호한 감정 레이블의 부정적인 영향을 줄이기 위해 새로운 감정 앵커(각 감정 카테고리의 참조 샘플)를 기반으로 한 자기 증류 손실을 제안해, S2D의 성능을 더욱 높여. 

인기 있는 SFER 및 DFER 데이터셋에서 실시한 실험 결과, 우리는 최첨단 성능을 달성했어.

================================================================================

URL: https://arxiv.org/abs/2312.08873
Title: Diffusion Cocktail: Mixing Domain-Specific Diffusion Models for Diversified Image Generations

Original Abstract:
Diffusion models, capable of high-quality image generation, receive unparalleled popularity for their ease of extension. Active users have created a massive collection of domain-specific diffusion models by fine-tuning base models on self-collected datasets. Recent work has focused on improving a single diffusion model by uncovering semantic and visual information encoded in various architecture components. However, those methods overlook the vastly available set of fine-tuned diffusion models and, therefore, miss the opportunity to utilize their combined capacity for novel generation. In this work, we propose Diffusion Cocktail (Ditail), a training-free method that transfers style and content information between multiple diffusion models. This allows us to perform diversified generations using a set of diffusion models, resulting in novel images unobtainable by a single model. Ditail also offers fine-grained control of the generation process, which enables flexible manipulations of styles and contents. With these properties, Ditail excels in numerous applications, including style transfer guided by diffusion models, novel-style image generation, and image manipulation via prompts or collage inputs.

Translated Abstract:
확산 모델은 고품질 이미지를 생성할 수 있어서 인기가 많아. 이 모델들은 확장이 쉽고, 많은 사용자들이 스스로 수집한 데이터셋을 바탕으로 기본 모델을 미세 조정해서 도메인 특화된 확산 모델을 많이 만들어냈어.

최근 연구들은 다양한 아키텍처 구성 요소에 인코딩된 의미적 및 시각적 정보를 밝혀내면서 단일 확산 모델을 개선하는 데 집중했어. 하지만 이런 방법들은 이미 많이 만들어진 미세 조정된 확산 모델들을 간과하고, 그래서 새로운 이미지를 생성하기 위한 그들의 결합된 능력을 활용할 기회를 놓치고 있어.

이번 연구에서는 Diffusion Cocktail (Ditail)이라는 방법을 제안해. 이건 훈련이 필요 없는 방법으로, 여러 확산 모델 간에 스타일과 내용 정보를 전달할 수 있어. 덕분에 여러 확산 모델을 사용해서 다양한 이미지를 생성할 수 있게 되고, 단일 모델로는 얻을 수 없는 새로운 이미지를 만들어낼 수 있어.

Ditail은 생성 과정에 대한 세밀한 제어도 가능하게 해줘, 그래서 스타일과 내용을 유연하게 조작할 수 있어. 이런 특성 덕분에 Ditail은 확산 모델을 이용한 스타일 전이, 새로운 스타일 이미지 생성, 프롬프트나 콜라주 입력을 통한 이미지 조작 등 여러 응용 분야에서 뛰어난 성능을 보여.

================================================================================

URL: https://arxiv.org/abs/2312.13330
Title: SOVC: Subject-Oriented Video Captioning

Original Abstract:
Describing video content according to users' needs is a long-held goal. Although existing video captioning methods have made significant progress, the generated captions may not focus on the entity that users are particularly interested in. To address this problem, we propose a new video captioning task, Subject-Oriented Video Captioning (SOVC), which aims to allow users to specify the describing target via a bounding box. To support this task, we construct two subject-oriented video captioning datasets based on two widely used video captioning datasets: MSVD and MSRVTT, by annotating subjects in each video for each caption. These datasets pave the way for describing users' interested targets. To tackle this task, we introduce a method tailored to this task, named SOVCNet. It consists of two key components: a subject-oriented sampling module that samples frames related to the subject to minimize irrelevant information; and a subject-oriented encoding module that utilizes the subject areas as hard prompts and integrates learnable soft prompts, enhancing the model's focus on the subject's activities and facilitating adaptation to the downstream generation task. Extensive experimental results demonstrate the effectiveness of our method on this new task.

Translated Abstract:
사용자의 필요에 따라 비디오 내용을 설명하는 것은 오랫동안 목표로 해왔던 일이야. 기존의 비디오 캡셔닝 방법들이 많이 발전했지만, 생성된 자막이 사용자가 특히 관심 있는 대상에 초점을 맞추지 못하는 경우가 많아. 이 문제를 해결하기 위해 우리는 새로운 비디오 캡셔닝 작업인 '주제 지향 비디오 캡셔닝(Subject-Oriented Video Captioning, SOVC)'을 제안해. 이 작업은 사용자가 설명할 대상을 바운딩 박스를 통해 지정할 수 있도록 하는 거야.

이 작업을 지원하기 위해, 우리는 두 가지 널리 사용되는 비디오 캡셔닝 데이터셋인 MSVD와 MSRVTT를 기반으로 해서 각 비디오의 주제를 주석 처리한 두 개의 주제 지향 비디오 캡셔닝 데이터셋을 만들었어. 이러한 데이터셋은 사용자가 관심 있는 대상을 설명할 수 있는 길을 열어줘.

이 작업을 수행하기 위해, 우리는 SOVCNet이라는 방법을 도입했어. 이 방법은 두 가지 주요 구성 요소로 이루어져 있어. 첫 번째는 주제와 관련된 프레임을 샘플링하여 불필요한 정보를 최소화하는 주제 지향 샘플링 모듈이고, 두 번째는 주제 영역을 하드 프롬프트로 활용하고 학습 가능한 소프트 프롬프트를 통합하여 모델이 주제의 활동에 집중하도록 도와주는 주제 지향 인코딩 모듈이야. 

광범위한 실험 결과는 이 새로운 작업에서 우리의 방법이 효과적이라는 것을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2401.11204
Title: Towards Category Unification of 3D Single Object Tracking on Point Clouds

Original Abstract:
Category-specific models are provenly valuable methods in 3D single object tracking (SOT) regardless of Siamese or motion-centric paradigms. However, such over-specialized model designs incur redundant parameters, thus limiting the broader applicability of 3D SOT task. This paper first introduces unified models that can simultaneously track objects across all categories using a single network with shared model parameters. Specifically, we propose to explicitly encode distinct attributes associated to different object categories, enabling the model to adapt to cross-category data. We find that the attribute variances of point cloud objects primarily occur from the varying size and shape (e.g., large and square vehicles v.s. small and slender humans). Based on this observation, we design a novel point set representation learning network inheriting transformer architecture, termed AdaFormer, which adaptively encodes the dynamically varying shape and size information from cross-category data in a unified manner. We further incorporate the size and shape prior derived from the known template targets into the model's inputs and learning objective, facilitating the learning of unified representation. Equipped with such designs, we construct two category-unified models SiamCUT and MoCUT.Extensive experiments demonstrate that SiamCUT and MoCUT exhibit strong generalization and training stability. Furthermore, our category-unified models outperform the category-specific counterparts by a significant margin (e.g., on KITTI dataset, 12% and 3% performance gains on the Siamese and motion paradigms). Our code will be available.

Translated Abstract:
카테고리별 모델은 3D 단일 객체 추적(SOT)에서 매우 유용한 방법으로 입증되었어. 시암 쌍(Siamese) 방식이나 모션 중심 방식에 관계없이 말이야. 하지만 이렇게 너무 특화된 모델 디자인은 불필요한 매개변수를 발생시켜서 3D SOT 작업의 더 넓은 적용을 제한해. 

이 논문에서는 모든 카테고리의 객체를 동시에 추적할 수 있는 통합 모델을 소개할 거야. 이 모델은 동일한 네트워크에서 공유된 매개변수를 사용해. 특히, 서로 다른 객체 카테고리에 관련된 특징을 명시적으로 인코딩해서 모델이 카테고리 간 데이터에 적응할 수 있도록 해. 우리는 포인트 클라우드 객체의 특징이 주로 크기와 모양의 차이에서 발생한다는 걸 발견했어. 예를 들어, 큰 사각형 차량과 작은 날씬한 인간 같은 경우 말이야.

이 관찰을 바탕으로, 우리는 변형기(transformer) 구조를 계승한 새로운 포인트 세트 표현 학습 네트워크인 AdaFormer를 설계했어. 이 모델은 통합된 방식으로 카테고리 간 데이터에서 동적으로 변하는 모양과 크기 정보를 적응적으로 인코딩해. 그리고 알려진 템플릿 타겟에서 파생된 크기와 모양 정보를 모델의 입력과 학습 목표에 반영해서 통합 표현 학습을 도와줘. 

이런 디자인을 바탕으로 우리는 두 가지 카테고리 통합 모델인 SiamCUT과 MoCUT을 구축했어. 여러 실험을 통해 SiamCUT과 MoCUT이 강력한 일반화 능력과 학습 안정성을 보여준다는 걸 확인했어. 게다가 우리 카테고리 통합 모델이 카테고리별 모델보다 성능이 많이 높아(예를 들어, KITTI 데이터셋에서 시암 방식은 12%, 모션 방식은 3% 성능 향상을 보여). 우리의 코드는 곧 공개될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2401.13267
Title: Dynamic Traceback Learning for Medical Report Generation

Original Abstract:
Automated medical report generation has the potential to significantly reduce the workload associated with the time-consuming process of medical reporting. Recent generative representation learning methods have shown promise in integrating vision and language modalities for medical report generation. However, when trained end-to-end and applied directly to medical image-to-text generation, they face two significant challenges: i) difficulty in accurately capturing subtle yet crucial pathological details, and ii) reliance on both visual and textual inputs during inference, leading to performance degradation in zero-shot inference when only images are available. To address these challenges, this study proposes a novel multi-modal dynamic traceback learning framework (DTrace). Specifically, we introduce a traceback mechanism to supervise the semantic validity of generated content and a dynamic learning strategy to adapt to various proportions of image and text input, enabling text generation without strong reliance on the input from both modalities during inference. The learning of cross-modal knowledge is enhanced by supervising the model to recover masked semantic information from a complementary counterpart. Extensive experiments conducted on two benchmark datasets, IU-Xray and MIMIC-CXR, demonstrate that the proposed DTrace framework outperforms state-of-the-art methods for medical report generation.

Translated Abstract:
자동화된 의료 보고서 생성은 시간이 많이 걸리는 의료 보고 프로세스의 작업량을 크게 줄일 수 있는 잠재력이 있어. 최근에 나온 생성적 표현 학습 방법들이 의료 보고서 생성을 위해 시각과 언어 모드를 통합하는 데 좋은 결과를 보여줬어. 

하지만, 끝에서 끝으로 훈련하고 의료 이미지에서 텍스트로 직접 적용할 때 두 가지 큰 문제가 있어. 첫째, 미세하지만 중요한 병리학적 세부사항을 정확하게 잡아내는 게 어렵고, 둘째, 추론할 때 시각적 입력과 텍스트 입력 모두에 의존하게 되는데, 이러면 이미지만 있을 때 성능이 떨어져. 

이 문제를 해결하기 위해 이 연구에서는 새로운 다중 모달 동적 추적 학습 프레임워크인 DTrace를 제안해. 구체적으로, 생성된 내용의 의미적 유효성을 감독하는 추적 메커니즘과 다양한 이미지와 텍스트 입력 비율에 적응할 수 있는 동적 학습 전략을 도입했어. 이 덕분에 추론할 때 두 모달리티의 입력에 크게 의존하지 않고도 텍스트 생성을 할 수 있어. 

모델이 보완적인 정보에서 가려진 의미 정보를 복구하도록 감독함으로써 교차 모달 지식 학습이 강화돼. IU-Xray와 MIMIC-CXR 두 개의 벤치마크 데이터셋에서 진행한 다양한 실험 결과, 제안한 DTrace 프레임워크가 의료 보고서 생성에서 최신 기술보다 더 뛰어난 성능을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2401.15204
Title: LYT-NET: Lightweight YUV Transformer-based Network for Low-light Image Enhancement

Original Abstract:
This letter introduces LYT-Net, a novel lightweight transformer-based model for low-light image enhancement (LLIE). LYT-Net consists of several layers and detachable blocks, including our novel blocks--Channel-Wise Denoiser (CWD) and Multi-Stage Squeeze & Excite Fusion (MSEF)--along with the traditional Transformer block, Multi-Headed Self-Attention (MHSA). In our approach we adopt a dual-path approach, treating chrominance channels U and V and luminance channel Y as separate entities to help the model better handle illumination adjustment and corruption restoration. Our comprehensive evaluation on established LLIE datasets demonstrates that, despite its low complexity, our model outperforms recent LLIE methods. The source code and pre-trained models are available at this https URL

Translated Abstract:
이 논문에서는 LYT-Net이라는 새로운 경량 변환기 기반 모델을 소개해. 이 모델은 저조도 이미지 향상(LLIE)을 위해 만들어졌어. LYT-Net은 여러 층과 분리 가능한 블록으로 구성되어 있는데, 여기에는 우리가 새롭게 만든 블록인 채널별 노이즈 제거기(CWD)와 다단계 압축 및 자극 융합(MSEF)이 포함돼. 전통적인 변환기 블록인 다중 헤드 자기 주의(MHSA)도 있어.

우리의 접근 방식은 두 가지 경로를 사용하는 거야. 색채 채널인 U와 V, 그리고 밝기 채널 Y를 별개의 요소로 다뤄서 모델이 조명 조정과 손상 복원을 더 잘 처리할 수 있도록 돕는 거지. 우리가 검증한 여러 저조도 이미지 향상 데이터셋에서 이 모델이 복잡도가 낮음에도 불구하고 최근 LLIE 방법들보다 더 나은 성능을 보여줬어. 소스 코드와 사전 훈련된 모델은 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2402.02335
Title: Video Editing for Video Retrieval

Original Abstract:
Though pre-training vision-language models have demonstrated significant benefits in boosting video-text retrieval performance from large-scale web videos, fine-tuning still plays a critical role with manually annotated clips with start and end times, which requires considerable human effort. To address this issue, we explore an alternative cheaper source of annotations, single timestamps, for video-text retrieval. We initialise clips from timestamps in a heuristic way to warm up a retrieval model. Then a video clip editing method is proposed to refine the initial rough boundaries to improve retrieval performance. A student-teacher network is introduced for video clip editing. The teacher model is employed to edit the clips in the training set whereas the student model trains on the edited clips. The teacher weights are updated from the student's after the student's performance increases. Our method is model agnostic and applicable to any retrieval models. We conduct experiments based on three state-of-the-art retrieval models, COOT, VideoCLIP and CLIP4Clip. Experiments conducted on three video retrieval datasets, YouCook2, DiDeMo and ActivityNet-Captions show that our edited clips consistently improve retrieval performance over initial clips across all the three retrieval models.

Translated Abstract:
비전-언어 모델을 미리 훈련하면 대규모 웹 비디오에서 비디오-텍스트 검색 성능을 크게 향상시킬 수 있지만, 시작과 끝 시간이 수동으로 주석이 달린 클립으로 미세 조정하는 과정은 여전히 중요한 역할을 해. 이 과정은 많은 인력이 필요해. 그래서 우리는 비디오-텍스트 검색을 위한 대안으로 더 저렴한 주석 소스인 단일 타임스탬프를 탐색해봤어. 타임스탬프를 이용해 클립을 초기화해서 검색 모델을 준비시켜. 이후에는 비디오 클립 편집 방법을 제안해서 초기의 대략적인 경계를 다듬어 검색 성능을 향상시켜.

비디오 클립 편집을 위해 학생-교사 네트워크를 도입했어. 교사 모델은 훈련 세트의 클립을 편집하는 데 사용되고, 학생 모델은 편집된 클립으로 학습해. 학생의 성능이 향상되면 학생의 성과를 바탕으로 교사 모델의 가중치를 업데이트해. 우리의 방법은 모델에 구애받지 않아서 어떤 검색 모델에도 적용 가능해. 

세 가지 최첨단 검색 모델인 COOT, VideoCLIP, CLIP4Clip을 기반으로 실험을 진행했어. YouCook2, DiDeMo, ActivityNet-Captions이라는 세 가지 비디오 검색 데이터셋에서 실험한 결과, 우리의 편집된 클립이 초기 클립보다 모든 검색 모델에서 검색 성능을 일관되게 향상시켰어.

================================================================================

URL: https://arxiv.org/abs/2402.10093
Title: MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations

Original Abstract:
We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. MIM-Refiner is motivated by the insight that strong representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to different intermediate layers. In each head, a modified nearest neighbor objective constructs semantic clusters that capture semantic information which improves performance on downstream tasks, including off-the-shelf and fine-tuning settings.
The refinement process is short and simple - yet highly effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, sets a new state-of-the-art in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. MIM-Refiner efficiently combines the advantages of MIM and ID objectives and compares favorably against previous state-of-the-art SSL models on a variety of benchmarks such as low-shot classification, long-tailed classification, clustering and semantic segmentation.

Translated Abstract:
MIM(마스크드 이미지 모델링)-리파이너라는 걸 소개할게. 이건 사전 학습된 MIM 모델을 더 잘 활용하기 위한 대비 학습 기술이야. MIM-리파이너는 MIM 모델의 중간 레이어에서 강력한 표현이 많이 나타난다는 점에서 착안했어.

그래서 MIM-리파이너는 서로 다른 중간 레이어에 연결된 여러 대비 헤드를 사용해. 각 헤드에서는 수정된 최근접 이웃 목표를 통해 의미 있는 클러스터를 만들어내서, 이게 다운스트림 작업 성능을 더 좋게 만들어줘. 여기엔 기존 모델들을 활용하는 것과 미세 조정하는 것 모두 포함돼.

리파인먼트 과정은 짧고 간단하지만, 정말 효과적이야. 몇 번의 에포크 안에 MIM 모델의 특징을 평균 수준에서 최신 상태로 끌어올려. 예를 들어, 이미지넷-1K에서 data2vec 2.0으로 사전 학습된 ViT-H 모델을 리파인먼트하면, 선형 프로빙에서 84.7%라는 새로운 최신 기록을 세워. 

MIM-리파이너는 MIM과 ID 목표의 장점을 효율적으로 결합하고, 낮은 샷 분류, 긴 꼬리 분류, 클러스터링, 그리고 의미 분할 같은 다양한 벤치마크에서 이전의 최신 SSL 모델들과 비교했을 때도 좋은 성과를 보여줘.

================================================================================

URL: https://arxiv.org/abs/2402.15374
Title: Outlier detection by ensembling uncertainty with negative objectness

Original Abstract:
Outlier detection is an essential capability in safety-critical applications of supervised visual recognition. Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data. However, that approach conflates prediction uncertainty with recognition of the negative class. We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class. This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness. Now outliers can be independently detected due to i) high prediction uncertainty or ii) similarity with negative data. We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes. The training procedure encourages the novel K+2-th class to learn negative objectness at pasted negative instances. Our models outperform the current state-of-the art on standard benchmarks for image-wide and pixel-level outlier detection with and without training on real negative data.

Translated Abstract:
이 논문에서는 안전이 중요한 시각 인식에서 이상치 탐지가 어떻게 중요한지 설명해. 기존의 방법들은 대부분 부정적인 훈련 데이터에서 낮은 신뢰도를 가진 예측을 하도록 표준 폐쇄 집합 모델을 유도해서 좋은 결과를 내는데, 이 방식은 예측의 불확실성을 부정 클래스 인식과 혼동하게 만들어.

그래서 우리는 K개의 실제 클래스와 하나의 이상치 클래스를 나타내는 K+1개의 로짓을 직접 예측하는 방법을 다시 생각해봤어. 이 설정을 통해, 우리는 새로운 이상 점수(어노말리 스코어)를 만들어낼 수 있는데, 이건 분포 내 불확실성과 이상치 클래스의 후험을 조합한 거야. 우리는 이걸 '부정 객체성'이라고 부르기로 했어. 이제 이상치는 i) 높은 예측 불확실성이나 ii) 부정 데이터와의 유사성 덕분에 독립적으로 탐지될 수 있어.

우리 방법은 K+2 클래스에 대해 마스크 수준의 인식을 포함한 밀집 예측 아키텍처에 통합되어 있어. 훈련 과정에서는 새로운 K+2 번째 클래스가 부정 객체성을 학습하도록 유도해. 우리 모델은 실제 부정 데이터로 훈련을 하든 안 하든 이미지 전체와 픽셀 수준의 이상치 탐지에서 현재 최고 성능을 능가하고 있어.

================================================================================

URL: https://arxiv.org/abs/2402.19160
Title: Effective Message Hiding with Order-Preserving Mechanisms

Original Abstract:
Message hiding, a technique that conceals secret message bits within a cover image, aims to achieve an optimal balance among message capacity, recovery accuracy, and imperceptibility. While convolutional neural networks have notably improved message capacity and imperceptibility, achieving high recovery accuracy remains challenging. This challenge arises because convolutional operations struggle to preserve the sequential order of message bits and effectively address the discrepancy between these two modalities. To address this, we propose StegaFormer, an innovative MLP-based framework designed to preserve bit order and enable global fusion between modalities. Specifically, StegaFormer incorporates three crucial components: Order-Preserving Message Encoder (OPME), Decoder (OPMD) and Global Message-Image Fusion (GMIF). OPME and OPMD aim to preserve the order of message bits by segmenting the entire sequence into equal-length segments and incorporating sequential information during encoding and decoding. Meanwhile, GMIF employs a cross-modality fusion mechanism to effectively fuse the features from the two uncorrelated modalities. Experimental results on the COCO and DIV2K datasets demonstrate that StegaFormer surpasses existing state-of-the-art methods in terms of recovery accuracy, message capacity, and imperceptibility. We will make our code publicly available.

Translated Abstract:
메시지 숨기기 기술은 비밀 메시지 비트를 커버 이미지에 숨기는 방법으로, 메시지 용량, 복구 정확성, 그리고 인지 불가능성 사이의 최적 균형을 목표로 해. 합성곱 신경망이 메시지 용량과 인지 불가능성을 크게 개선했지만, 높은 복구 정확성을 달성하는 건 여전히 어려워. 이 문제는 합성곱 연산이 메시지 비트의 순서를 잘 유지하지 못하고, 두 가지 모달리티 간의 차이를 효과적으로 처리하지 못해서 생겨.

우리는 이 문제를 해결하기 위해 StegaFormer라는 혁신적인 MLP 기반 프레임워크를 제안해. 이 프레임워크는 비트 순서를 유지하고 모달리티 간의 글로벌 융합을 가능하게 해. StegaFormer는 세 가지 중요한 구성 요소를 포함하고 있어: 순서 보존 메시지 인코더 (OPME), 디코더 (OPMD), 그리고 글로벌 메시지-이미지 융합 (GMIF).

OPME와 OPMD는 전체 시퀀스를 같은 길이의 세그먼트로 나누고 인코딩과 디코딩 과정에서 순차적인 정보를 포함해 메시지 비트의 순서를 유지하려고 해. GMIF는 두 개의 비상관 모달리티에서 특징을 효과적으로 융합하기 위해 크로스 모달리티 융합 메커니즘을 사용해.

COCO와 DIV2K 데이터셋에서의 실험 결과, StegaFormer는 복구 정확성, 메시지 용량, 그리고 인지 불가능성 측면에서 기존의 최첨단 방법들을 능가한다는 걸 보여줬어. 우리는 우리의 코드를 공개할 예정이야.

================================================================================

URL: https://arxiv.org/abs/2403.09915
Title: Robust Light-Weight Facial Affective Behavior Recognition with CLIP

Original Abstract:
Human affective behavior analysis aims to delve into human expressions and behaviors to deepen our understanding of human emotions. Basic expression categories (EXPR) and Action Units (AUs) are two essential components in this analysis, which categorize emotions and break down facial movements into elemental units, respectively. Despite advancements, existing approaches in expression classification and AU detection often necessitate complex models and substantial computational resources, limiting their applicability in everyday settings. In this work, we introduce the first lightweight framework adept at efficiently tackling both expression classification and AU detection. This framework employs a frozen CLIP image encoder alongside a trainable multilayer perceptron (MLP), enhanced with Conditional Value at Risk (CVaR) for robustness and a loss landscape flattening strategy for improved generalization. Experimental results on the Aff-wild2 dataset demonstrate superior performance in comparison to the baseline while maintaining minimal computational demands, offering a practical solution for affective behavior analysis. The code is available at this https URL

Translated Abstract:
인간의 감정 행동 분석은 인간의 표현과 행동을 살펴보면서 감정을 더 잘 이해하려고 하는 거야. 기본 표현 범주(EXPR)와 행동 단위(AUs)는 이 분석에서 중요한 두 가지 요소야. EXPR은 감정을 분류하고, AUs는 얼굴 움직임을 기본 단위로 나누는 거지. 

하지만 기존의 표현 분류와 AU 탐지 방법은 복잡한 모델과 많은 계산 자원을 필요로 해서 일상적인 상황에서 사용하기 힘든 경우가 많아. 이 연구에서는 표현 분류와 AU 탐지를 효율적으로 처리할 수 있는 첫 번째 경량 프레임워크를 소개해. 이 프레임워크는 동결된 CLIP 이미지 인코더와 학습 가능한 다층 퍼셉트론(MLP)을 사용하고, 강건성을 위해 조건부 위험 가치(CVaR)를 추가했으며, 일반화를 개선하기 위해 손실 경량화 전략도 적용했어.

Aff-wild2 데이터셋에서 실험한 결과, 기존 기준보다 더 뛰어난 성능을 보여주면서도 최소한의 계산 요구를 유지했어. 그래서 감정 행동 분석을 위한 실용적인 해결책을 제공하는 거지. 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2403.14392
Title: A Bag of Tricks for Few-Shot Class-Incremental Learning

Original Abstract:
We present a bag of tricks framework for few-shot class-incremental learning (FSCIL), which is a challenging form of continual learning that involves continuous adaptation to new tasks with limited samples. FSCIL requires both stability and adaptability, i.e., preserving proficiency in previously learned tasks while learning new ones. Our proposed bag of tricks brings together six key and highly influential techniques that improve stability, adaptability, and overall performance under a unified framework for FSCIL. We organize these tricks into three categories: stability tricks, adaptability tricks, and training tricks. Stability tricks aim to mitigate the forgetting of previously learned classes by enhancing the separation between the embeddings of learned classes and minimizing interference when learning new ones. On the other hand, adaptability tricks focus on the effective learning of new classes. Finally, training tricks improve the overall performance without compromising stability or adaptability. We perform extensive experiments on three benchmark datasets, CIFAR-100, CUB-200, and miniIMageNet, to evaluate the impact of our proposed framework. Our detailed analysis shows that our approach substantially improves both stability and adaptability, establishing a new state-of-the-art by outperforming prior works in the area. We believe our method provides a go-to solution and establishes a robust baseline for future research in this area.

Translated Abstract:
우리는 몇 개의 샘플로 새로운 클래스를 배우는 'few-shot class-incremental learning' (FSCIL)을 위한 다양한 기술을 모은 프레임워크를 소개해. FSCIL은 제한된 샘플로 새로운 작업에 계속 적응해야 하는 어려운 학습 방식이야. 이 방식은 이전에 배운 작업에서의 능력을 유지하면서 새로운 작업도 배워야 하기 때문에 안정성과 적응력이 모두 필요해.

우리가 제안하는 이 프레임워크는 안정성, 적응성, 그리고 전반적인 성능을 향상시키는 여섯 가지 중요한 기술을 하나로 모아. 이 기술들을 안정성 관련 기술, 적응성 관련 기술, 그리고 훈련 기술의 세 가지 카테고리로 나눴어. 안정성 관련 기술은 이전에 배운 클래스의 기억을 잃지 않도록 도와주고, 새로운 클래스를 배울 때 방해를 최소화하는 데 초점을 맞춰. 반면에 적응성 관련 기술은 새로운 클래스를 효과적으로 배우는 데 중점을 두고 있어. 마지막으로 훈련 기술은 안정성이나 적응성을 해치지 않으면서 전체 성능을 향상시켜.

우리는 CIFAR-100, CUB-200, miniIMageNet의 세 가지 벤치마크 데이터셋에서 우리의 프레임워크가 어떤 영향을 미치는지 평가하기 위해 광범위한 실험을 진행했어. 자세한 분석 결과, 우리의 방법은 안정성과 적응성을 모두 크게 개선하며, 이전 연구들을 뛰어넘는 새로운 최첨단 성과를 세웠어. 우리는 우리의 방법이 이 분야의 미래 연구를 위한 튼튼한 기준을 마련해 줄 것이라고 생각해.

================================================================================

URL: https://arxiv.org/abs/2403.19586
Title: TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D DSA Rendering

Original Abstract:
Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical imaging technique that provides a series of 2D images captured at different stages and angles during the process of contrast agent filling blood vessels. It plays a significant role in the diagnosis of cerebrovascular diseases. Improving the rendering quality and speed under sparse sampling is important for observing the status and location of lesions. The current methods exhibit inadequate rendering quality in sparse views and suffer from slow rendering speed. To overcome these limitations, we propose TOGS, a Gaussian splatting method with opacity offset over time, which can effectively improve the rendering quality and speed of 4D DSA. We introduce an opacity offset table for each Gaussian to model the opacity offsets of the Gaussian, using these opacity-varying Gaussians to model the temporal variations in the radiance of the contrast agent. By interpolating the opacity offset table, the opacity variation of the Gaussian at different time points can be determined. This enables us to render the 2D DSA image at that specific moment. Additionally, we introduced a Smooth loss term in the loss function to mitigate overfitting issues that may arise in the model when dealing with sparse view scenarios. During the training phase, we randomly prune Gaussians, thereby reducing the storage overhead of the model. The experimental results demonstrate that compared to previous methods, this model achieves state-of-the-art render quality under the same number of training views. Additionally, it enables real-time rendering while maintaining low storage overhead. The code is available at this https URL.

Translated Abstract:
4D 디지털 감쇠 혈관조영술(4D DSA)은 조영제가 혈관에 채워지는 과정에서 여러 단계와 각도에서 촬영한 2D 이미지 시리즈를 제공하는 의료 이미징 기술이야. 이 기술은 뇌혈관 질환 진단에서 중요한 역할을 해. 

희소 샘플링 하에서 렌더링 품질과 속도를 개선하는 것이 병변의 상태와 위치를 관찰하는 데 중요해. 현재 방법들은 희소 뷰에서 렌더링 품질이 부족하고, 렌더링 속도가 느려. 이 문제를 해결하기 위해 TOGS라는 방법을 제안해. TOGS는 시간에 따라 불투명도 오프셋을 갖는 가우시안 스플래팅 방식으로, 4D DSA의 렌더링 품질과 속도를 효과적으로 개선할 수 있어.

각 가우시안에 대한 불투명도 오프셋 테이블을 도입해서, 이 불투명도가 변하는 가우시안을 사용해 조영제의 방사선 강도의 시간적 변화를 모델링해. 불투명도 오프셋 테이블을 보간함으로써, 서로 다른 시간 지점에서 가우시안의 불투명도 변화를 결정할 수 있어. 이렇게 하면 특정 순간의 2D DSA 이미지를 렌더링할 수 있어. 

또한, 희소 뷰 상황에서 모델이 과적합되는 문제를 완화하기 위해 손실 함수에 부드러움 손실 항을 도입했어. 훈련 단계에서는 가우시안을 무작위로 제거해서 모델의 저장 오버헤드를 줄이고 있어. 

실험 결과, 이전 방법들과 비교했을 때, 이 모델이 동일한 훈련 뷰 수에서 최첨단 렌더링 품질을 달성했어. 게다가, 낮은 저장 오버헤드를 유지하면서 실시간 렌더링도 가능해. 코드도 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2404.03507
Title: DQ-DETR: DETR with Dynamic Query for Tiny Object Detection

Original Abstract:
Despite previous DETR-like methods having performed successfully in generic object detection, tiny object detection is still a challenging task for them since the positional information of object queries is not customized for detecting tiny objects, whose scale is extraordinarily smaller than general objects. Also, DETR-like methods using a fixed number of queries make them unsuitable for aerial datasets, which only contain tiny objects, and the numbers of instances are imbalanced between different images. Thus, we present a simple yet effective model, named DQ-DETR, which consists of three different components: categorical counting module, counting-guided feature enhancement, and dynamic query selection to solve the above-mentioned problems. DQ-DETR uses the prediction and density maps from the categorical counting module to dynamically adjust the number of object queries and improve the positional information of queries. Our model DQ-DETR outperforms previous CNN-based and DETR-like methods, achieving state-of-the-art mAP 30.2% on the AI-TOD-V2 dataset, which mostly consists of tiny objects. Our code will be available at \url{this https URL}.

Translated Abstract:
기존의 DETR 유사 방법들이 일반 물체 탐지에서는 잘 작동했지만, 아주 작은 물체 탐지에서는 여전히 어려움이 많아. 그 이유는 물체 쿼리의 위치 정보가 아주 작은 물체를 탐지하는 데 맞춰져 있지 않기 때문이야. 아주 작은 물체는 일반 물체들보다 크기가 훨씬 작거든. 

또한, 고정된 수의 쿼리를 사용하는 DETR 유사 방법들은 오직 아주 작은 물체만 포함된 항공 데이터셋에선 적합하지 않아. 왜냐하면 서로 다른 이미지들 사이에서 인스턴스의 수가 불균형적이기 때문이야.

그래서 우리는 DQ-DETR이라는 간단하지만 효과적인 모델을 제안해. 이 모델은 세 가지 다른 구성 요소로 이루어져 있어: 카테고리 카운팅 모듈, 카운팅 기반 특징 강화, 그리고 동적 쿼리 선택이야. DQ-DETR은 카테고리 카운팅 모듈에서 얻은 예측과 밀도 지도를 사용해서 물체 쿼리의 수를 동적으로 조정하고 쿼리의 위치 정보를 개선해.

우리 모델인 DQ-DETR은 이전의 CNN 기반 및 DETR 유사 방법들을 능가하며, 대부분 아주 작은 물체로 이루어진 AI-TOD-V2 데이터셋에서 최첨단 mAP 30.2%를 달성했어. 우리 코드는 \url{this https URL}에서 사용할 수 있을 거야.

================================================================================

URL: https://arxiv.org/abs/2404.10267
Title: OneActor: Consistent Character Generation via Cluster-Conditioned Guidance

Original Abstract:
Text-to-image diffusion models benefit artists with high-quality image generation. Yet their stochastic nature hinders artists from creating consistent images of the same subject. Existing methods try to tackle this challenge and generate consistent content in various ways. However, they either depend on external restricted data or require expensive tuning of the diffusion model. For this issue, we propose a novel one-shot tuning paradigm, termed as OneActor. It efficiently performs consistent subject generation solely driven by prompts via a learned semantic guidance to bypass the laborious backbone tuning. We lead the way to formalize the objective of consistent subject generation from a clustering perspective, and thus design a cluster-conditioned model. To mitigate the overfitting challenge shared by one-shot tuning pipelines, we augment the tuning with auxiliary samples and devise two inference strategies: semantic interpolation and cluster guidance. These techniques are later verified to significantly enhance the generation quality. Comprehensive experiments show that our method outperforms a variety of baselines with satisfactory subject consistency, superior prompt conformity as well as high image quality. Our method is capable of multi-subject generation and compatible with popular diffusion extensions. Besides, we achieve a 4 times faster tuning speed than tuning-based baselines and, if desired, avoid increasing inference time. Furthermore, to our best knowledge, we are the first to prove that the semantic space of the diffusion model has the same interpolation property as the latent space does. This property can serve as another promising tool for fine generation control.

Translated Abstract:
텍스트-이미지 확산 모델은 예술가들에게 고품질 이미지를 생성하는 데 도움을 줘. 하지만 이 모델의 확률적 특성 때문에 같은 주제의 일관된 이미지를 만드는 데 어려움이 있어. 기존의 방법들은 이 문제를 해결하려고 다양한 방식으로 일관된 콘텐츠를 생성하려고 했지만, 대부분 외부의 제한된 데이터에 의존하거나 비싼 튜닝이 필요해.

그런 문제를 해결하기 위해 우리는 'OneActor'라는 새로운 원샷 튜닝 패러다임을 제안해. 이 방법은 번거로운 기본 튜닝 없이 학습된 의미적 가이드를 통해 프롬프트만으로 일관된 주제를 효율적으로 생성할 수 있어. 우리는 일관된 주제 생성을 클러스터 관점에서 정형화하고, 그래서 클러스터 조건 모델을 설계했어. 원샷 튜닝 파이프라인에서 발생하는 과적합 문제를 줄이기 위해 보조 샘플을 추가하고, 두 가지 추론 전략인 의미적 보간과 클러스터 가이드를 고안했어. 이 기술들은 생성 품질을 크게 향상시키는 데 효과적이라는 게 나중에 확인됐어.

포괄적인 실험 결과, 우리의 방법이 다양한 기준선보다 우수한 주제 일관성, 뛰어난 프롬프트 적합성, 그리고 높은 이미지 품질을 달성했어. 우리의 방법은 다중 주제 생성이 가능하고, 인기 있는 확산 확장과도 호환돼. 게다가 우리는 튜닝 기반 기준선보다 4배 빠른 튜닝 속도를 달성했고, 원한다면 추론 시간을 늘리지 않을 수도 있어. 더 나아가, 우리가 아는 한, 우리는 확산 모델의 의미 공간이 잠재 공간과 같은 보간 특성을 가진다는 것을 처음으로 증명했어. 이 특성은 세밀한 생성 제어를 위한 또 다른 유망한 도구가 될 수 있어.

================================================================================

URL: https://arxiv.org/abs/2404.12908
Title: Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images

Original Abstract:
Diffusion models (DMs) have revolutionized image generation, producing high-quality images with applications spanning various fields. However, their ability to create hyper-realistic images poses significant challenges in distinguishing between real and synthetic content, raising concerns about digital authenticity and potential misuse in creating deepfakes. This work introduces a robust detection framework that integrates image and text features extracted by CLIP model with a Multilayer Perceptron (MLP) classifier. We propose a novel loss that can improve the detector's robustness and handle imbalanced datasets. Additionally, we flatten the loss landscape during the model training to improve the detector's generalization capabilities. The effectiveness of our method, which outperforms traditional detection techniques, is demonstrated through extensive experiments, underscoring its potential to set a new state-of-the-art approach in DM-generated image detection. The code is available at this https URL.

Translated Abstract:
확산 모델(DM)은 이미지 생성에 혁신을 가져왔고, 다양한 분야에서 고품질 이미지를 만들어내고 있어. 하지만 이 모델이 만들어내는 하이퍼 리얼리틱 이미지 때문에 진짜와 합성된 내용을 구별하기 어려워져서, 디지털 진위와 딥페이크 생성에 대한 우려가 커지고 있어.

이 연구에서는 CLIP 모델로 추출한 이미지와 텍스트 특징을 Multilayer Perceptron(MLP) 분류기와 결합한 강력한 탐지 프레임워크를 소개해. 우리는 탐지기의 강인성을 높이고 불균형 데이터셋을 처리할 수 있는 새로운 손실 함수를 제안해. 또한, 모델 훈련 중 손실 경관을 평평하게 만들어서 탐지기의 일반화 능력을 향상시켜.

우리 방법은 전통적인 탐지 기술보다 성능이 뛰어난 것으로, 광범위한 실험을 통해 효과를 입증했어. 이 연구는 DM으로 생성된 이미지 탐지에서 새로운 최첨단 접근 방식을 제시할 가능성을 보여줘. 코드도 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2404.13043
Title: Data Alignment for Zero-Shot Concept Generation in Dermatology AI

Original Abstract:
AI in dermatology is evolving at a rapid pace but the major limitation to training trustworthy classifiers is the scarcity of data with ground-truth concept level labels, which are meta-labels semantically meaningful to humans. Foundation models like CLIP providing zero-shot capabilities can help alleviate this challenge by leveraging vast amounts of image-caption pairs available on the internet. CLIP can be fine-tuned using domain specific image-caption pairs to improve classification performance. However, CLIP's pre-training data is not well-aligned with the medical jargon that clinicians use to perform diagnoses. The development of large language models (LLMs) in recent years has led to the possibility of leveraging the expressive nature of these models to generate rich text. Our goal is to use these models to generate caption text that aligns well with both the clinical lexicon and with the natural human language used in CLIP's pre-training data. Starting with captions used for images in PubMed articles, we extend them by passing the raw captions through an LLM fine-tuned on the field's several textbooks. We find that using captions generated by an expressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept classification performance.

Translated Abstract:
피부과에서 AI 기술이 빠르게 발전하고 있는데, 신뢰할 수 있는 분류기를 훈련하는 데 가장 큰 제약은 실제 개념 수준의 라벨이 부족하다는 점이야. 이런 라벨은 인간에게 의미가 있는 메타 라벨이야. CLIP 같은 기본 모델은 인터넷에 있는 많은 이미지-캡션 쌍을 활용해서 이 문제를 해결할 수 있는 가능성이 있어. CLIP은 도메인에 맞는 이미지-캡션 쌍을 사용해 세부 조정을 할 수 있어서 분류 성능을 높일 수 있어.

하지만 CLIP의 사전 훈련 데이터는 의사들이 진단할 때 사용하는 의학 용어와 잘 맞지 않아. 최근 몇 년간 대규모 언어 모델(LLM)이 발전하면서, 이런 모델의 표현력을 활용해 풍부한 텍스트를 생성할 수 있는 가능성이 열렸어. 우리의 목표는 이런 모델을 사용해 임상 용어와 CLIP의 사전 훈련 데이터에서 사용되는 자연어에 잘 맞는 캡션 텍스트를 생성하는 거야.

PubMed 기사에 사용된 이미지 캡션을 시작으로, 우리는 원본 캡션을 해당 분야의 여러 교과서에 맞게 조정한 LLM을 통해 확장해 나가. 그 결과, GPT-3.5 같은 표현력이 뛰어난 LLM이 생성한 캡션을 사용하면 다운스트림 제로샷 개념 분류 성능이 향상된다는 것을 발견했어.

================================================================================

URL: https://arxiv.org/abs/2404.13306
Title: FakeBench: Probing Explainable Fake Image Detection via Large Multimodal Models

Original Abstract:
The ability to distinguish whether an image is generated by artificial intelligence (AI) is a crucial ingredient in human intelligence, usually accompanied by a complex and dialectical forensic and reasoning process. However, current fake image detection models and databases focus on binary classification without understandable explanations for the general populace. This weakens the credibility of authenticity judgment and may conceal potential model biases. Meanwhile, large multimodal models (LMMs) have exhibited immense visual-text capabilities on various tasks, bringing the potential for explainable fake image detection. Therefore, we pioneer the probe of LMMs for explainable fake image detection by presenting a multimodal database encompassing textual authenticity descriptions, the FakeBench. For construction, we first introduce a fine-grained taxonomy of generative visual forgery concerning human perception, based on which we collect forgery descriptions in human natural language with a human-in-the-loop strategy. FakeBench examines LMMs with four evaluation criteria: detection, reasoning, interpretation and fine-grained forgery analysis, to obtain deeper insights into image authenticity-relevant capabilities. Experiments on various LMMs confirm their merits and demerits in different aspects of fake image detection tasks. This research presents a paradigm shift towards transparency for the fake image detection area and reveals the need for greater emphasis on forensic elements in visual-language research and AI risk control. FakeBench will be available at this https URL.

Translated Abstract:
이미지가 인공지능(AI)으로 생성된 것인지 구별하는 능력은 인간 지능에 중요한 요소야. 이 과정은 보통 복잡하고 논리적인 법의학적 추론 과정을 동반해. 하지만 현재의 가짜 이미지 탐지 모델과 데이터베이스는 일반인들이 이해하기 어려운 이진 분류에만 집중하고 있어. 이건 진위 판단의 신뢰성을 약화시키고 잠재적인 모델 편향을 숨길 수도 있어.

한편, 대규모 멀티모달 모델(LMMs)은 다양한 작업에서 시각-텍스트 능력을 엄청나게 보여주고 있어. 그래서 우리는 설명 가능한 가짜 이미지 탐지를 위해 LMMs를 이용한 연구를 시작했어. 이를 위해 텍스트 진위 설명을 포함하는 멀티모달 데이터베이스인 FakeBench를 만들었어. 

FakeBench를 만드는 과정에서는 사람의 인식에 따라 세분화된 생성 시각 위조 분류법을 먼저 소개하고, 그에 따라 사람의 자연어로 위조 설명을 수집했어. FakeBench는 네 가지 평가 기준인 탐지, 추론, 해석, 세부적인 위조 분석을 통해 LMMs를 평가해, 이미지 진위와 관련된 능력에 대한 깊은 통찰을 얻고자 해. 

여러 LMMs에 대한 실험을 통해 이들의 장단점을 확인했어. 이 연구는 가짜 이미지 탐지 분야에서 투명성을 위한 패러다임 전환을 제시하고, 시각-언어 연구와 AI 위험 통제에서 법의학적 요소에 대한 더 큰 강조가 필요하다는 것을 드러내. FakeBench는 이 URL에서 사용할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2404.13579
Title: LTOS: Layout-controllable Text-Object Synthesis via Adaptive Cross-attention Fusions

Original Abstract:
Controllable text-to-image generation synthesizes visual text and objects in images with certain conditions, which are frequently applied to emoji and poster generation. Visual text rendering and layout-to-image generation tasks have been popular in controllable text-to-image generation. However, each of these tasks typically focuses on single modality generation or rendering, leaving yet-to-be-bridged gaps between the approaches correspondingly designed for each of the tasks. In this paper, we combine text rendering and layout-to-image generation tasks into a single task: layout-controllable text-object synthesis (LTOS) task, aiming at synthesizing images with object and visual text based on predefined object layout and text contents. As compliant datasets are not readily available for our LTOS task, we construct a layout-aware text-object synthesis dataset, containing elaborate well-aligned labels of visual text and object information. Based on the dataset, we propose a layout-controllable text-object adaptive fusion (TOF) framework, which generates images with clear, legible visual text and plausible objects. We construct a visual-text rendering module to synthesize text and employ an object-layout control module to generate objects while integrating the two modules to harmoniously generate and integrate text content and objects in images. To better the image-text integration, we propose a self-adaptive cross-attention fusion module that helps the image generation to attend more to important text information. Within such a fusion module, we use a self-adaptive learnable factor to learn to flexibly control the influence of cross-attention outputs on image generation. Experimental results show that our method outperforms the state-of-the-art in LTOS, text rendering, and layout-to-image tasks, enabling harmonious visual text rendering and object generation.

Translated Abstract:
제어 가능한 텍스트-이미지 생성은 특정 조건에 따라 이미지에서 시각적 텍스트와 객체를 합성하는 기술이야. 주로 이모지나 포스터 생성에 많이 쓰이지. 시각적 텍스트 렌더링과 레이아웃-이미지 생성 작업이 요즘 인기를 끌고 있는데, 이 두 작업은 보통 각각 하나의 방식에만 집중하다 보니, 서로 연결되지 않은 부분들이 남아 있어. 

이 논문에서는 텍스트 렌더링과 레이아웃-이미지 생성 작업을 하나의 작업으로 합쳤어. 이 작업을 '레이아웃 제어 텍스트-객체 합성(LTOS)'이라고 부르는데, 미리 정해진 객체 레이아웃과 텍스트 내용을 바탕으로 이미지를 합성하는 걸 목표로 해. 하지만 LTOS 작업을 위한 적절한 데이터셋이 쉽게 구할 수 없어서, 우리는 레이아웃 인식 텍스트-객체 합성 데이터셋을 만들었어. 이 데이터셋은 시각적 텍스트와 객체 정보를 잘 정렬한 자세한 레이블을 포함하고 있어.

이 데이터셋을 바탕으로, 우리는 명확하고 읽기 쉬운 시각적 텍스트와 그럴듯한 객체를 생성하는 '레이아웃 제어 텍스트-객체 적응형 융합(TOF)' 프레임워크를 제안했어. 텍스트를 합성하기 위한 시각 텍스트 렌더링 모듈을 만들고, 객체를 생성하기 위해 객체-레이아웃 제어 모듈을 사용해. 이 두 모듈을 통합해서 이미지에서 텍스트 내용과 객체를 조화롭게 생성하고 합성하는 거지.

이미지-텍스트 통합을 개선하기 위해, 우리는 이미지 생성이 중요한 텍스트 정보에 더 집중할 수 있도록 돕는 자기 적응형 크로스 어텐션 융합 모듈을 제안했어. 이 융합 모듈 안에서는 자기 적응형 학습 가능한 요소를 사용해 크로스 어텐션 출력이 이미지 생성에 미치는 영향을 유연하게 조절할 수 있도록 학습해. 실험 결과, 우리의 방법이 LTOS, 텍스트 렌더링, 레이아웃-이미지 작업에서 최신 기술보다 더 뛰어난 성능을 보여주었고, 시각적 텍스트 렌더링과 객체 생성을 조화롭게 할 수 있게 되었어.

================================================================================

URL: https://arxiv.org/abs/2404.17486
Title: TextGaze: Gaze-Controllable Face Generation with Natural Language

Original Abstract:
Generating face image with specific gaze information has attracted considerable attention. Existing approaches typically input gaze values directly for face generation, which is unnatural and requires annotated gaze datasets for training, thereby limiting its application. In this paper, we present a novel gaze-controllable face generation task. Our approach inputs textual descriptions that describe human gaze and head behavior and generates corresponding face images. Our work first introduces a text-of-gaze dataset containing over 90k text descriptions spanning a dense distribution of gaze and head poses. We further propose a gaze-controllable text-to-face method. Our method contains a sketch-conditioned face diffusion module and a model-based sketch diffusion module. We define a face sketch based on facial landmarks and eye segmentation map. The face diffusion module generates face images from the face sketch, and the sketch diffusion module employs a 3D face model to generate face sketch from text description. Experiments on the FFHQ dataset show the effectiveness of our method. We will release our dataset and code for future research.

Translated Abstract:
특정 시선 정보를 가진 얼굴 이미지를 생성하는 것은 많은 관심을 받고 있어. 기존의 방법들은 보통 시선 값을 직접 입력해서 얼굴을 생성하는데, 이게 자연스럽지 않고 훈련을 위해 주석이 달린 시선 데이터셋이 필요해서 적용에 한계가 있어. 

이 논문에서는 새로운 시선 조절 가능한 얼굴 생성 작업을 제안해. 우리의 방법은 사람의 시선과 머리 움직임을 설명하는 텍스트 설명을 입력하고, 그에 맞는 얼굴 이미지를 생성해. 이 연구에서는 시선과 머리 자세에 대한 9만 개 이상의 텍스트 설명을 포함한 '시선 텍스트 데이터셋'을 처음으로 소개해. 

또한, 우리는 시선 조절 가능한 텍스트-투-얼굴 방법을 제안해. 이 방법은 스케치 조건부 얼굴 확산 모듈과 모델 기반 스케치 확산 모듈을 포함해. 얼굴 랜드마크와 눈 분할 맵을 기반으로 얼굴 스케치를 정의해. 얼굴 확산 모듈은 얼굴 스케치에서 얼굴 이미지를 생성하고, 스케치 확산 모듈은 3D 얼굴 모델을 사용해 텍스트 설명으로부터 얼굴 스케치를 생성해. 

FFHQ 데이터셋에서 실험을 통해 우리의 방법의 효과성을 보여줬어. 앞으로 연구를 위해 데이터셋과 코드를 공개할 예정이야.

================================================================================

URL: https://arxiv.org/abs/2404.17793
Title: CLFT: Camera-LiDAR Fusion Transformer for Semantic Segmentation in Autonomous Driving

Original Abstract:
Critical research about camera-and-LiDAR-based semantic object segmentation for autonomous driving significantly benefited from the recent development of deep learning. Specifically, the vision transformer is the novel ground-breaker that successfully brought the multi-head-attention mechanism to computer vision applications. Therefore, we propose a vision-transformer-based network to carry out camera-LiDAR fusion for semantic segmentation applied to autonomous driving. Our proposal uses the novel progressive-assemble strategy of vision transformers on a double-direction network and then integrates the results in a cross-fusion strategy over the transformer decoder layers. Unlike other works in the literature, our camera-LiDAR fusion transformers have been evaluated in challenging conditions like rain and low illumination, showing robust performance. The paper reports the segmentation results over the vehicle and human classes in different modalities: camera-only, LiDAR-only, and camera-LiDAR fusion. We perform coherent controlled benchmark experiments of CLFT against other networks that are also designed for semantic segmentation. The experiments aim to evaluate the performance of CLFT independently from two perspectives: multimodal sensor fusion and backbone architectures. The quantitative assessments show our CLFT networks yield an improvement of up to 10% for challenging dark-wet conditions when comparing with Fully-Convolutional-Neural-Network-based (FCN) camera-LiDAR fusion neural network. Contrasting to the network with transformer backbone but using single modality input, the all-around improvement is 5-10%.

Translated Abstract:
카메라와 LiDAR를 이용한 자율주행을 위한 의미 객체 분할에 관한 중요한 연구가 최근 딥러닝의 발전 덕분에 큰 도움이 되었다. 특히, 비전 트랜스포머는 멀티 헤드 어텐션 메커니즘을 컴퓨터 비전 애플리케이션에 성공적으로 도입한 혁신적인 기술이다.

그래서 우리는 자율주행에 적용할 카메라-LiDAR 융합을 위해 비전 트랜스포머 기반 네트워크를 제안한다. 우리의 제안은 두 방향 네트워크에서 비전 트랜스포머의 새로운 점진적 조합 전략을 사용하고, 그 결과를 트랜스포머 디코더 레이어에서 크로스 융합 전략으로 통합한다.

다른 연구들과는 다르게, 우리의 카메라-LiDAR 융합 트랜스포머는 비 오는 날이나 저조도와 같은 어려운 조건에서도 평가되었고, 강력한 성능을 보여주었다. 이 논문은 차량과 사람 클래스에 대한 분할 결과를 카메라 전용, LiDAR 전용, 카메라-LiDAR 융합 세 가지 모드로 보고한다.

우리는 CLFT와 다른 의미 분할을 위해 설계된 네트워크들 간의 일관된 제어된 벤치마크 실험을 수행했다. 이 실험은 멀티모달 센서 융합과 백본 아키텍처 두 가지 관점에서 CLFT의 성능을 독립적으로 평가하는 것을 목표로 한다. 

정량적 평가 결과, 우리의 CLFT 네트워크는 FCN 기반 카메라-LiDAR 융합 신경망과 비교했을 때, 어려운 어두운 비 오는 조건에서 최대 10%의 성능 향상을 보였다. 트랜스포머 백본을 사용하지만 단일 모달리티 입력을 사용하는 네트워크와 비교했을 때, 전반적인 향상은 5-10%에 이른다.

================================================================================

URL: https://arxiv.org/abs/2404.17883
Title: Underwater Variable Zoom: Depth-Guided Perception Network for Underwater Image Enhancement

Original Abstract:
Underwater scenes intrinsically involve degradation problems owing to heterogeneous ocean elements. Prevailing underwater image enhancement (UIE) methods stick to straightforward feature modeling to learn the mapping function, which leads to limited vision gain as it lacks more explicit physical cues (e.g., depth). In this work, we investigate injecting the depth prior into the deep UIE model for more precise scene enhancement capability. To this end, we present a novel depth-guided perception UIE framework, dubbed underwater variable zoom (UVZ). Specifically, UVZ resorts to a two-stage pipeline. First, a depth estimation network is designed to generate critical depth maps, combined with an auxiliary supervision network introduced to suppress estimation differences during training. Second, UVZ parses near-far scenarios by harnessing the predicted depth maps, enabling local and non-local perceiving in different regions. Extensive experiments on five benchmark datasets demonstrate that UVZ achieves superior visual gain and delivers promising quantitative metrics. Besides, UVZ is confirmed to exhibit good generalization in some visual tasks, especially in unusual lighting conditions. The code, models and results are available at: this https URL.

Translated Abstract:
수중 장면은 다양한 해양 요소 때문에 본질적으로 품질 저하 문제가 있어. 기존의 수중 이미지 향상(UIE) 방법들은 간단한 특징 모델링에 의존해서 매핑 함수를 배우는데, 이 때문에 깊이 같은 더 명확한 물리적 단서가 부족해서 시각적 향상이 제한적이야.

이번 연구에서는 깊이 정보를 깊은 UIE 모델에 추가해서 더 정확한 장면 향상 능력을 얻는 방법을 조사했어. 이를 위해, 우리는 '수중 가변 줌(Underwater Variable Zoom, UVZ)'이라는 새로운 깊이 가이드 인식 UIE 프레임워크를 제안해. UVZ는 두 단계의 파이프라인으로 구성되어 있어. 첫 번째로, 깊이 추정 네트워크를 설계해서 중요한 깊이 맵을 생성하고, 훈련 과정에서 추정 차이를 줄이기 위해 보조 감독 네트워크를 도입했어.

두 번째로, UVZ는 예측된 깊이 맵을 활용해서 가까운 장면과 먼 장면을 분석해. 이를 통해 다른 지역에서 지역적 및 비지역적 인식을 가능하게 해. 다섯 개의 벤치마크 데이터셋에서의 실험 결과, UVZ는 뛰어난 시각적 향상을 달성하고 유망한 정량적 지표를 보여줘. 또한, UVZ는 특히 특이한 조명 조건에서 몇몇 시각적 작업에서 좋은 일반화 능력을 보여줬어. 코드, 모델 및 결과는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2404.19227
Title: Espresso: Robust Concept Filtering in Text-to-Image Models

Original Abstract:
Diffusion based text-to-image models are trained on large datasets scraped from the Internet, potentially containing unacceptable concepts (e.g., copyright infringing or unsafe). We need concept removal techniques (CRTs) which are effective in preventing the generation of images with unacceptable concepts, utility-preserving on acceptable concepts, and robust against evasion with adversarial prompts. None of the prior CRTs satisfy all these requirements simultaneously. We introduce Espresso, the first robust concept filter based on Contrastive Language-Image Pre-Training (CLIP). We configure CLIP to identify unacceptable concepts in generated images using the distance of their embeddings to the text embeddings of both unacceptable and acceptable concepts. This lets us fine-tune for robustness by separating the text embeddings of unacceptable and acceptable concepts while preserving their pairing with image embeddings for utility. We present a pipeline to evaluate various CRTs, attacks against them, and show that Espresso, is more effective and robust than prior CRTs, while retaining utility.

Translated Abstract:
확산 기반의 텍스트-이미지 모델은 인터넷에서 수집한 대규모 데이터셋으로 훈련되는데, 이 데이터셋에는 저작권 침해나 안전하지 않은 개념과 같은 문제가 있는 개념이 포함될 수 있어. 그래서 우리는 이런 문제 있는 개념을 막아주는 개념 제거 기술(CRTs)이 필요해. 이 기술은 문제 있는 개념은 막고, 허용 가능한 개념은 유지하며, 적대적인 프롬프트에 대해서도 강건해야 해. 하지만 이전의 CRT들은 이 모든 조건을 동시에 만족하지는 못해.

우리는 Espresso라는 첫 번째 강력한 개념 필터를 소개해. 이 필터는 Contrastive Language-Image Pre-Training (CLIP)을 기반으로 해. CLIP을 설정해서 생성된 이미지에서 문제 있는 개념을 식별할 수 있게 했는데, 이때 문제 있는 개념과 허용 가능한 개념의 텍스트 임베딩과의 거리로 판단해. 이렇게 하면 이미지 임베딩과의 연결성을 유지하면서 문제 있는 개념과 허용 가능한 개념의 텍스트 임베딩을 분리해 강건성을 높일 수 있어.

우리는 다양한 CRT와 그에 대한 공격을 평가할 수 있는 파이프라인을 제시하고, Espresso가 이전 CRT들보다 더 효과적이고 강력하면서도 유용성을 유지한다는 것을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2405.14977
Title: A Lost Opportunity for Vision-Language Models: A Comparative Study of Online Test-Time Adaptation for Vision-Language Models

Original Abstract:
In deep learning, maintaining model robustness against distribution shifts is critical. This work explores a broad range of possibilities to adapt vision-language foundation models at test-time, with a particular emphasis on CLIP and its variants. The study systematically examines prompt-based techniques and existing test-time adaptation methods, aiming to improve the robustness under distribution shift in diverse real-world scenarios. Specifically, the investigation covers various prompt engineering strategies, including handcrafted prompts, prompt ensembles, and prompt learning techniques. Additionally, we introduce a vision-text-space ensemble that substantially enhances average performance compared to text-space-only ensembles. Since online test-time adaptation has shown to be effective to mitigate performance drops under distribution shift, the study extends its scope to evaluate the effectiveness of existing test-time adaptation methods that were originally designed for vision-only classification models. Through extensive experimental evaluations conducted across multiple datasets and diverse model architectures, the research demonstrates the effectiveness of these adaptation strategies. Code is available at: this https URL

Translated Abstract:
딥러닝에서는 모델의 강건성을 유지하는 게 정말 중요해. 이 연구는 테스트할 때 비전-언어 기반 모델을 조정할 수 있는 다양한 방법을 살펴보는데, 특히 CLIP와 그 변형들에 중점을 두고 있어.

연구는 프롬프트 기반 기술과 기존의 테스트 시간 적응 방법을 체계적으로 분석해서, 다양한 실제 상황에서 분포 변화에 대한 강건성을 높이는 게 목표야. 구체적으로, 여러 가지 프롬프트 엔지니어링 전략을 다루는데, 핸드크래프트한 프롬프트, 프롬프트 앙상블, 그리고 프롬프트 학습 기술이 포함돼.

또한, 우리는 비전-텍스트 공간 앙상블을 소개하는데, 이 방법이 텍스트 공간만 사용한 앙상블에 비해 평균 성능을 상당히 개선해. 온라인 테스트 시간 적응이 분포 변화에 따른 성능 저하를 완화하는 데 효과적이라는 점이 밝혀졌으니, 연구는 원래 비전 전용 분류 모델을 위해 설계된 기존 테스트 시간 적응 방법의 효과도 평가해.

여러 데이터셋과 다양한 모델 아키텍처를 통해 광범위한 실험 평가를 진행하며, 이 적응 전략들이 효과적이라는 걸 보여줘. 코드도 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2405.15151
Title: NeB-SLAM: Neural Blocks-based Salable RGB-D SLAM for Unknown Scenes

Original Abstract:
Neural implicit representations have recently demonstrated considerable potential in the field of visual simultaneous localization and mapping (SLAM). This is due to their inherent advantages, including low storage overhead and representation continuity. However, these methods necessitate the size of the scene as input, which is impractical for unknown scenes. Consequently, we propose NeB-SLAM, a neural block-based scalable RGB-D SLAM for unknown scenes. Specifically, we first propose a divide-and-conquer mapping strategy that represents the entire unknown scene as a set of sub-maps. These sub-maps are a set of neural blocks of fixed size. Then, we introduce an adaptive map growth strategy to achieve adaptive allocation of neural blocks during camera tracking and gradually cover the whole unknown scene. Finally, extensive evaluations on various datasets demonstrate that our method is competitive in both mapping and tracking when targeting unknown environments.

Translated Abstract:
신경 임플리시트 표현(neural implicit representations)은 최근 시각적 동시 위치 추적 및 지도 작성(SLAM) 분야에서 큰 가능성을 보여줬어. 그 이유는 저장 공간이 적게 들고 표현이 연속적이라는 장점이 있기 때문이야. 하지만 이런 방법은 입력으로 장면의 크기를 요구하는데, 알지 못하는 장면에서는 이게 불편해. 그래서 우리는 NeB-SLAM이라는, 알 수 없는 장면을 위한 신경 블록 기반의 확장 가능한 RGB-D SLAM을 제안해.

구체적으로, 먼저 알 수 없는 전체 장면을 여러 개의 서브 맵으로 나누는 분할 정복(mapping) 전략을 제안해. 이 서브 맵들은 고정 크기의 신경 블록으로 구성돼. 그 다음에는 카메라 추적 중에 신경 블록을 적절히 배분하고 알 수 없는 장면 전체를 점진적으로 커버하기 위한 적응형 맵 성장 전략을 도입해.

마지막으로, 다양한 데이터셋에서의 평가를 통해 우리의 방법이 알 수 없는 환경에서 지도 작성과 추적 모두에서 경쟁력이 있다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2405.18880
Title: EventZoom: A Progressive Approach to Event-Based Data Augmentation for Enhanced Neuromorphic Vision

Original Abstract:
Dynamic Vision Sensors (DVS) capture event data with high temporal resolution and low power consumption, presenting a more efficient solution for visual processing in dynamic and real-time scenarios compared to conventional video capture methods. Event data augmentation serve as an essential method for overcoming the limitation of scale and diversity in event datasets. Our comparative experiments demonstrate that the two factors, spatial integrity and temporal continuity, can significantly affect the capacity of event data augmentation, which are guarantee for maintaining the sparsity and high dynamic range characteristics unique to event data. However, existing augmentation methods often neglect the preservation of spatial integrity and temporal continuity. To address this, we developed a novel event data augmentation strategy EventZoom, which employs a temporal progressive strategy, embedding transformed samples into the original samples through progressive scaling and shifting. The scaling process avoids the spatial information loss associated with cropping, while the progressive strategy prevents interruptions or abrupt changes in temporal information. We validated EventZoom across various supervised learning frameworks. The experimental results show that EventZoom consistently outperforms existing event data augmentation methods with SOTA performance. For the first time, we have concurrently employed Semi-supervised and Unsupervised learning to verify feasibility on event augmentation algorithms, demonstrating the applicability and effectiveness of EventZoom as a powerful event-based data augmentation tool in handling real-world scenes with high dynamics and variability environments.

Translated Abstract:
다이나믹 비전 센서(DVS)는 이벤트 데이터를 높은 시간 해상도와 낮은 전력 소비로 캡처해. 이 방식은 전통적인 비디오 캡처 방법보다 동적이고 실시간 시나리오에서 시각적 처리에 더 효율적이야. 이벤트 데이터 증강은 이벤트 데이터셋의 크기와 다양성 한계를 극복하는 데 중요한 방법이야.

우리의 비교 실험 결과, 공간적 무결성과 시간적 연속성 두 가지 요소가 이벤트 데이터 증강의 능력에 크게 영향을 미친다는 것을 보여줬어. 이 두 요소는 이벤트 데이터만의 희소성과 높은 동적 범위를 유지하는 데 중요해. 하지만 기존의 증강 방법은 종종 공간적 무결성과 시간적 연속성을 보존하는 걸 소홀히 해.

그래서 우리는 새로운 이벤트 데이터 증강 전략인 EventZoom을 개발했어. 이 방법은 시간적으로 점진적인 전략을 사용해서 변형된 샘플을 원래 샘플에 점진적으로 스케일링하고 이동시키면서 포함시켜. 이 스케일링 과정은 크롭핑으로 인한 공간 정보 손실을 피하고, 점진적인 전략은 시간 정보의 중단이나 갑작스러운 변화를 방지해.

우리는 다양한 감독 학습 프레임워크에서 EventZoom을 검증했어. 실험 결과, EventZoom이 기존의 이벤트 데이터 증강 방법보다 항상 더 나은 성능을 보였어. 이번에 처음으로 반감독 학습과 비감독 학습을 동시에 사용해서 이벤트 증강 알고리즘의 가능성을 검증했어. 이 연구는 EventZoom이 높은 동적이고 변동성이 큰 실제 환경에서 강력한 이벤트 기반 데이터 증강 도구로서의 적용 가능성과 효과성을 입증했어.

================================================================================

URL: https://arxiv.org/abs/2406.01594
Title: DiffUHaul: A Training-Free Method for Object Dragging in Images

Original Abstract:
Text-to-image diffusion models have proven effective for solving many image editing tasks. However, the seemingly straightforward task of seamlessly relocating objects within a scene remains surprisingly challenging. Existing methods addressing this problem often struggle to function reliably in real-world scenarios due to lacking spatial reasoning. In this work, we propose a training-free method, dubbed DiffUHaul, that harnesses the spatial understanding of a localized text-to-image model, for the object dragging task. Blindly manipulating layout inputs of the localized model tends to cause low editing performance due to the intrinsic entanglement of object representation in the model. To this end, we first apply attention masking in each denoising step to make the generation more disentangled across different objects and adopt the self-attention sharing mechanism to preserve the high-level object appearance. Furthermore, we propose a new diffusion anchoring technique: in the early denoising steps, we interpolate the attention features between source and target images to smoothly fuse new layouts with the original appearance; in the later denoising steps, we pass the localized features from the source images to the interpolated images to retain fine-grained object details. To adapt DiffUHaul to real-image editing, we apply a DDPM self-attention bucketing that can better reconstruct real images with the localized model. Finally, we introduce an automated evaluation pipeline for this task and showcase the efficacy of our method. Our results are reinforced through a user preference study.

Translated Abstract:
텍스트-이미지 확산 모델은 이미지 편집 작업을 해결하는 데 효과적이라는 것이 입증되었어. 하지만, 장면 안에서 물체를 부드럽게 옮기는 작업은 생각보다 꽤 어렵다는 걸 알 수 있어. 기존의 방법들은 이 문제를 해결하려고 하지만, 실제 상황에서는 공간적 추론이 부족해서 잘 작동하지 않아.

이번 연구에서는 DiffUHaul이라는 훈련이 필요 없는 방법을 제안해. 이 방법은 지역화된 텍스트-이미지 모델의 공간 이해력을 활용해서 물체를 끌어다 놓는 작업을 수행해. 지역화된 모델의 레이아웃 입력을 무작정 조작하면 물체 표현이 엉켜서 편집 성능이 떨어지게 돼. 그래서 우리는 먼저 각 노이즈 제거 단계에서 주의 마스킹을 적용해 서로 다른 물체들 간의 생성이 더 분리되도록 했고, 고수준의 물체 외관을 유지하기 위해 자기 주의 공유 메커니즘을 사용했어.

또한, 새로운 확산 앵커링 기법도 제안했어. 초기 노이즈 제거 단계에서는 소스 이미지와 타겟 이미지 간의 주의 특징을 보간해서 원래의 외관과 새로운 레이아웃을 부드럽게 융합하고, 나중의 노이즈 제거 단계에서는 소스 이미지에서 지역화된 특징을 보간된 이미지로 전달해서 세부적인 물체 정보를 유지해. DiffUHaul을 실제 이미지 편집에 맞추기 위해, 지역화된 모델로 실제 이미지를 더 잘 재구성할 수 있는 DDPM 자기 주의 버킷 방식을 적용했어.

마지막으로, 이 작업을 위한 자동 평가 파이프라인을 소개하고, 우리 방법의 효과를 보여줘. 사용자 선호 연구를 통해 결과를 강화했어.

================================================================================

URL: https://arxiv.org/abs/2406.02202
Title: No Captions, No Problem: Captionless 3D-CLIP Alignment with Hard Negatives via CLIP Knowledge and LLMs

Original Abstract:
In this study, we explore an alternative approach to enhance contrastive text-image-3D alignment in the absence of textual descriptions for 3D objects. We introduce two unsupervised methods, $I2I$ and $(I2L)^2$, which leverage CLIP knowledge about textual and 2D data to compute the neural perceived similarity between two 3D samples. We employ the proposed methods to mine 3D hard negatives, establishing a multimodal contrastive pipeline with hard negative weighting via a custom loss function. We train on different configurations of the proposed hard negative mining approach, and we evaluate the accuracy of our models in 3D classification and on the cross-modal retrieval benchmark, testing image-to-shape and shape-to-image retrieval. Results demonstrate that our approach, even without explicit text alignment, achieves comparable or superior performance on zero-shot and standard 3D classification, while significantly improving both image-to-shape and shape-to-image retrieval compared to previous methods.

Translated Abstract:
이번 연구에서는 3D 객체에 대한 텍스트 설명 없이 텍스트-이미지-3D 정렬의 대비를 높이는 새로운 접근 방식을 살펴봅니다. 우리는 CLIP의 텍스트와 2D 데이터 지식을 활용해 두 개의 3D 샘플 간의 신경적 유사성을 계산하는 두 가지 비지도 방법인 $I2I$와 $(I2L)^2$를 소개합니다.

제안한 방법을 사용해 3D의 어려운 부정 샘플을 찾고, 맞춤형 손실 함수를 통해 어려운 부정 가중치를 가진 다중 모달 대비 파이프라인을 구축합니다. 우리는 제안한 어려운 부정 샘플 마이닝 접근 방식의 다양한 설정에서 훈련을 진행하고, 3D 분류와 교차 모달 검색 벤치마크에서 모델의 정확성을 평가합니다. 여기서는 이미지-형상 검색과 형상-이미지 검색을 테스트합니다.

결과적으로, 우리의 접근 방식은 명시적인 텍스트 정렬 없이도 제로샷 및 표준 3D 분류에서 비슷하거나 더 나은 성능을 달성하며, 이전 방법들에 비해 이미지-형상 및 형상-이미지 검색에서 크게 개선된 성과를 보여줍니다.

================================================================================

URL: https://arxiv.org/abs/2406.07487
Title: GLAD: Towards Better Reconstruction with Global and Local Adaptive Diffusion Models for Unsupervised Anomaly Detection

Original Abstract:
Diffusion models have shown superior performance on unsupervised anomaly detection tasks. Since trained with normal data only, diffusion models tend to reconstruct normal counterparts of test images with certain noises added. However, these methods treat all potential anomalies equally, which may cause two main problems. From the global perspective, the difficulty of reconstructing images with different anomalies is uneven. Therefore, instead of utilizing the same setting for all samples, we propose to predict a particular denoising step for each sample by evaluating the difference between image contents and the priors extracted from diffusion models. From the local perspective, reconstructing abnormal regions differs from normal areas even in the same image. Theoretically, the diffusion model predicts a noise for each step, typically following a standard Gaussian distribution. However, due to the difference between the anomaly and its potential normal counterpart, the predicted noise in abnormal regions will inevitably deviate from the standard Gaussian distribution. To this end, we propose introducing synthetic abnormal samples in training to encourage the diffusion models to break through the limitation of standard Gaussian distribution, and a spatial-adaptive feature fusion scheme is utilized during inference. With the above modifications, we propose a global and local adaptive diffusion model (abbreviated to GLAD) for unsupervised anomaly detection, which introduces appealing flexibility and achieves anomaly-free reconstruction while retaining as much normal information as possible. Extensive experiments are conducted on three commonly used anomaly detection datasets (MVTec-AD, MPDD, and VisA) and a printed circuit board dataset (PCB-Bank) we integrated, showing the effectiveness of the proposed method.

Translated Abstract:
확산 모델은 비지도 이상 탐지 작업에서 뛰어난 성능을 보여줬어. 이 모델은 정상 데이터만으로 훈련되기 때문에, 테스트 이미지의 정상적인 부분을 재구성하면서 특정한 노이즈를 추가하는 경향이 있어. 하지만 이 방법은 모든 잠재적 이상을 동일하게 처리하니까, 두 가지 주요 문제가 생길 수 있어.

첫째, 전반적인 관점에서 보면, 다양한 이상을 가진 이미지를 재구성하는 난이도가 균일하지 않아. 그래서 모든 샘플에 대해 같은 설정을 사용하는 대신, 이미지 내용과 확산 모델에서 뽑아낸 사전 정보의 차이를 평가해서 각 샘플에 맞는 특정한 디노이징 단계를 예측하는 방안을 제안해. 

둘째, 같은 이미지 내에서도 비정상적인 영역과 정상적인 영역은 재구성하는 방식이 달라. 이론적으로 확산 모델은 각 단계마다 노이즈를 예측하는데, 보통은 표준 가우시안 분포를 따르지. 하지만 이상과 그에 맞는 정상 부분 간의 차이 때문에, 비정상적인 영역에서 예측된 노이즈는 필연적으로 표준 가우시안 분포와는 다를 수밖에 없어. 그래서 우리는 훈련 중에 합성 비정상 샘플을 도입해서 확산 모델이 표준 가우시안 분포의 제한을 극복하도록 유도하고, 추론할 때는 공간 적응형 특징 융합 방식을 사용해.

이런 수정 사항을 통해 우리는 비지도 이상 탐지를 위한 글로벌 및 로컬 적응형 확산 모델(GLAD)을 제안해. 이 모델은 유연성을 제공하고, 가능한 한 많은 정상 정보를 유지하면서 이상 없는 재구성을 달성해. 세 가지 일반적인 이상 탐지 데이터셋(MVTec-AD, MPDD, VisA)과 우리가 통합한 인쇄 회로 기판 데이터셋(PCB-Bank)에서 광범위한 실험을 진행하여 이 방법의 효과성을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2406.11802
Title: PhyBench: A Physical Commonsense Benchmark for Evaluating Text-to-Image Models

Original Abstract:
Text-to-image (T2I) models have made substantial progress in generating images from textual prompts. However, they frequently fail to produce images consistent with physical commonsense, a vital capability for applications in world simulation and everyday tasks. Current T2I evaluation benchmarks focus on metrics such as accuracy, bias, and safety, neglecting the evaluation of models' internal knowledge, particularly physical commonsense. To address this issue, we introduce PhyBench, a comprehensive T2I evaluation dataset comprising 700 prompts across 4 primary categories: mechanics, optics, thermodynamics, and material properties, encompassing 31 distinct physical scenarios. We assess 6 prominent T2I models, including proprietary models DALLE3 and Gemini, and demonstrate that incorporating physical principles into prompts enhances the models' ability to generate physically accurate images. Our findings reveal that: (1) even advanced models frequently err in various physical scenarios, except for optics; (2) GPT-4o, with item-specific scoring instructions, effectively evaluates the models' understanding of physical commonsense, closely aligning with human assessments; and (3) current T2I models are primarily focused on text-to-image translation, lacking profound reasoning regarding physical commonsense. We advocate for increased attention to the inherent knowledge within T2I models, beyond their utility as mere image generation tools. The code and data are available at this https URL.

Translated Abstract:
텍스트-이미지(T2I) 모델은 텍스트 프롬프트로부터 이미지를 생성하는 데 많은 발전을 이루었어. 하지만 이 모델들이 현실 세계의 상식에 맞는 이미지를 잘 만들어내지 못하는 경우가 많아. 이건 세계 시뮬레이션이나 일상적인 작업에서 매우 중요한 능력이야. 현재 T2I 평가 기준은 정확성, 편향, 안전성 같은 지표에 초점을 맞추고 있지만, 모델의 내부 지식, 특히 물리적 상식을 평가하는 건 놓치고 있어.

이 문제를 해결하기 위해 우리는 PhyBench라는 T2I 평가 데이터셋을 만들었어. 이 데이터셋은 4가지 주요 카테고리(역학, 광학, 열역학, 물질 특성)에서 700개의 프롬프트로 구성되어 있고, 31개의 다양한 물리적 시나리오를 포함하고 있어. 우리는 DALLE3와 Gemini 같은 6개의 주요 T2I 모델을 평가했고, 프롬프트에 물리적 원리를 포함하면 모델이 물리적으로 정확한 이미지를 생성하는 능력이 향상된다는 걸 보여줬어.

우리의 연구 결과는 다음과 같아: (1) 고급 모델들도 다양한 물리적 시나리오에서 자주 실수해, 광학을 제외하고는 말이야; (2) GPT-4o는 특정 아이템에 대한 점수 매기기 지침을 통해 모델들이 물리적 상식을 이해하는 정도를 효과적으로 평가할 수 있고, 인간 평가와 매우 유사하게 나와; (3) 현재 T2I 모델들은 주로 텍스트-이미지 변환에 집중하고 있어서, 물리적 상식에 대한 깊은 추론이 부족해. 우리는 T2I 모델들이 단순한 이미지 생성 도구 이상의 내재된 지식에 더 많은 주의를 기울여야 한다고 주장해. 코드와 데이터는 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2406.17815
Title: SUM: Saliency Unification through Mamba for Visual Attention Modeling

Original Abstract:
Visual attention modeling, important for interpreting and prioritizing visual stimuli, plays a significant role in applications such as marketing, multimedia, and robotics. Traditional saliency prediction models, especially those based on Convolutional Neural Networks (CNNs) or Transformers, achieve notable success by leveraging large-scale annotated datasets. However, the current state-of-the-art (SOTA) models that use Transformers are computationally expensive. Additionally, separate models are often required for each image type, lacking a unified approach. In this paper, we propose Saliency Unification through Mamba (SUM), a novel approach that integrates the efficient long-range dependency modeling of Mamba with U-Net to provide a unified model for diverse image types. Using a novel Conditional Visual State Space (C-VSS) block, SUM dynamically adapts to various image types, including natural scenes, web pages, and commercial imagery, ensuring universal applicability across different data types. Our comprehensive evaluations across five benchmarks demonstrate that SUM seamlessly adapts to different visual characteristics and consistently outperforms existing models. These results position SUM as a versatile and powerful tool for advancing visual attention modeling, offering a robust solution universally applicable across different types of visual content.

Translated Abstract:
시각 주의 모델링은 시각 자극을 해석하고 우선순위를 정하는 데 중요한 역할을 해. 이건 마케팅, 멀티미디어, 로봇공학 같은 분야에서 많이 사용돼. 전통적인 주목도 예측 모델, 특히 CNN이나 트랜스포머 기반의 모델은 대규모 주석 데이터셋을 활용해서 좋은 성과를 내고 있어. 하지만 현재의 최첨단 모델은 트랜스포머를 사용하니 계산 비용이 많이 들어. 게다가 각 이미지 타입마다 별도의 모델이 필요해서 통합된 접근 방식이 부족해.

이 논문에서는 Mamba를 통한 주목도 통합(SUM)이라는 새로운 접근 방식을 제안해. 이 방법은 Mamba의 효율적인 장거리 의존성 모델링을 U-Net과 결합해서 다양한 이미지 타입에 대한 통합 모델을 제공해. 새로운 조건부 시각 상태 공간(C-VSS) 블록을 사용해서 SUM은 자연 장면, 웹 페이지, 상업 이미지 등 다양한 이미지 타입에 맞춰 동적으로 조정돼. 그래서 여러 데이터 타입에 걸쳐서 보편적으로 적용할 수 있어.

다섯 개의 벤치마크에서 종합적으로 평가한 결과, SUM은 다양한 시각 특성에 잘 적응하고 기존 모델보다 꾸준히 더 나은 성능을 보여줬어. 이 결과들은 SUM이 시각 주의 모델링을 발전시키는 데 유용하고, 다양한 시각 콘텐츠에 보편적으로 적용할 수 있는 강력한 도구로 자리 잡을 수 있게 해.

================================================================================

URL: https://arxiv.org/abs/2406.20024
Title: eMoE-Tracker: Environmental MoE-based Transformer for Robust Event-guided Object Tracking

Original Abstract:
The unique complementarity of frame-based and event cameras for high frame rate object tracking has recently inspired some research attempts to develop multi-modal fusion approaches. However, these methods directly fuse both modalities and thus ignore the environmental attributes, e.g., motion blur, illumination variance, occlusion, scale variation, etc. Meanwhile, no interaction between search and template features makes distinguishing target objects and backgrounds difficult. As a result, performance degradation is induced especially in challenging conditions. This paper proposes a novel and effective Transformer-based event-guided tracking framework, called eMoE-Tracker, which achieves new SOTA performance under various conditions. Our key idea is to disentangle the environment into several learnable attributes to dynamically learn the attribute-specific features for better interaction and discriminability between the target information and background. To achieve the goal, we first propose an environmental Mix-of-Experts (eMoE) module that is built upon the environmental Attributes Disentanglement to learn attribute-specific features and environmental Attributes Gating to assemble the attribute-specific features by the learnable attribute scores dynamically. The eMoE module is a subtle router that fine-tunes the transformer backbone more efficiently. We then introduce a contrastive relation modeling (CRM) module to improve interaction and discriminability between the target information and background. Extensive experiments on diverse event-based benchmark datasets showcase the superior performance of our eMoE-Tracker compared to the prior arts.

Translated Abstract:
프레임 기반 카메라와 이벤트 카메라의 독특한 보완성 덕분에 고속 객체 추적을 위한 여러 연구가 진행되고 있어. 최근에는 멀티모달 융합 방법을 개발하려는 시도가 있었는데, 이 방법들은 두 가지 모드를 직접 융합해서 환경의 특성, 예를 들어 모션 블러, 조명 변화, 가림, 스케일 변화 같은 것들을 무시하고 있어. 게다가, 검색과 템플릿 피처 간의 상호작용이 없어서 목표 객체와 배경을 구분하기가 어려워. 이로 인해 특히 어려운 조건에서는 성능이 떨어지게 돼.

이 논문에서는 eMoE-Tracker라는 새로운 Transformer 기반의 이벤트 유도 추적 프레임워크를 제안해. 이 방법은 다양한 조건에서 새로운 최첨단 성능을 달성해. 우리의 핵심 아이디어는 환경을 여러 개의 학습 가능한 특성으로 분리해서, 목표 정보와 배경 간의 상호작용과 구분력을 높이기 위해 특성별로 적절한 피처를 동적으로 학습하는 거야.

이를 위해 먼저 환경 특성 분리(Attributes Disentanglement)를 기반으로 한 환경 믹스 오브 엑스퍼트(eMoE) 모듈을 제안해. 이 모듈은 특성별 피처를 학습하고, 학습 가능한 특성 점수를 통해 동적으로 특성별 피처를 조합하는 환경 특성 게이팅(Attributes Gating)을 포함하고 있어. eMoE 모듈은 Transformer 백본을 더 효율적으로 미세 조정하는 미세한 라우터 역할을 해.

그리고 나서 목표 정보와 배경 간의 상호작용과 구분력을 개선하기 위해 대조적 관계 모델링(CRM) 모듈을 도입해. 다양한 이벤트 기반 벤치마크 데이터셋에서 실시한 광범위한 실험 결과, 우리의 eMoE-Tracker가 기존 방법들에 비해 뛰어난 성능을 보여준다는 것을 확인했어.

================================================================================

URL: https://arxiv.org/abs/2407.05266
Title: CLAMP-ViT: Contrastive Data-Free Learning for Adaptive Post-Training Quantization of ViTs

Original Abstract:
We present CLAMP-ViT, a data-free post-training quantization method for vision transformers (ViTs). We identify the limitations of recent techniques, notably their inability to leverage meaningful inter-patch relationships, leading to the generation of simplistic and semantically vague data, impacting quantization accuracy. CLAMP-ViT employs a two-stage approach, cyclically adapting between data generation and model quantization. Specifically, we incorporate a patch-level contrastive learning scheme to generate richer, semantically meaningful data. Furthermore, we leverage contrastive learning in layer-wise evolutionary search for fixed- and mixed-precision quantization to identify optimal quantization parameters while mitigating the effects of a non-smooth loss landscape. Extensive evaluations across various vision tasks demonstrate the superiority of CLAMP-ViT, with performance improvements of up to 3% in top-1 accuracy for classification, 0.6 mAP for object detection, and 1.5 mIoU for segmentation at similar or better compression ratio over existing alternatives. Code is available at this https URL

Translated Abstract:
우리는 CLAMP-ViT라는 데이터 없이도 사용할 수 있는 포스트 트레이닝 양자화 방법을 소개해. 이건 비전 트랜스포머(ViTs)를 위한 거야. 최근 기술들의 한계를 발견했는데, 그중 하나는 패치 간의 의미 있는 관계를 잘 활용하지 못한다는 거야. 그 때문에 만들어지는 데이터가 너무 단순하고 의미가 불분명해져서 양자화 정확도에 영향을 미쳐.

CLAMP-ViT는 두 단계로 진행돼. 데이터 생성과 모델 양자화 사이를 주기적으로 조정하는 방식이야. 특히, 우리는 패치 수준의 대조 학습 방식을 도입해서 더 풍부하고 의미 있는 데이터를 생성해. 그리고 계층별 진화 탐색에서 대조 학습을 활용해 고정 및 혼합 정밀도 양자화의 최적 양자화 파라미터를 찾아내고, 비부드러운 손실 경관의 영향을 줄여.

다양한 비전 작업에서 광범위한 평가를 진행해봤고, CLAMP-ViT의 우수성을 보여줬어. 분류에서는 최대 3%의 top-1 정확도 향상, 객체 탐지에서는 0.6 mAP, 세분화에서는 1.5 mIoU 향상이 있었어. 기존 대안들과 비슷하거나 더 나은 압축 비율을 유지하면서 말이야. 코드도 이 URL에서 사용할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2407.05965
Title: T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models

Original Abstract:
The recent development of Sora leads to a new era in text-to-video (T2V) generation. Along with this comes the rising concern about its security risks. The generated videos may contain illegal or unethical content, and there is a lack of comprehensive quantitative understanding of their safety, posing a challenge to their reliability and practical deployment. Previous evaluations primarily focus on the quality of video generation. While some evaluations of text-to-image models have considered safety, they cover fewer aspects and do not address the unique temporal risk inherent in video generation. To bridge this research gap, we introduce T2VSafetyBench, a new benchmark designed for conducting safety-critical assessments of text-to-video models. We define 12 critical aspects of video generation safety and construct a malicious prompt dataset including real-world prompts, LLM-generated prompts and jailbreak attack-based prompts. Based on our evaluation results, we draw several important findings, including: 1) no single model excels in all aspects, with different models showing various strengths; 2) the correlation between GPT-4 assessments and manual reviews is generally high; 3) there is a trade-off between the usability and safety of text-to-video generative models. This indicates that as the field of video generation rapidly advances, safety risks are set to surge, highlighting the urgency of prioritizing video safety. We hope that T2VSafetyBench can provide insights for better understanding the safety of video generation in the era of generative AI.

Translated Abstract:
최근 Sora의 발전은 텍스트-비디오(T2V) 생성의 새로운 시대를 열고 있어. 그런데 이와 함께 보안 위험에 대한 우려도 커지고 있어. 생성된 비디오에는 불법적이거나 비윤리적인 내용이 포함될 수 있고, 안전성에 대한 포괄적인 정량적 이해가 부족해서 신뢰성과 실제 배포에 도전이 되고 있어. 

이전 평가들은 주로 비디오 생성 품질에 초점을 맞췄어. 텍스트-이미지 모델의 안전성 평가도 있었지만, 다루는 측면이 적고 비디오 생성에 특유의 시간적 위험을 다루지 않았어. 이 연구 격차를 메우기 위해, 우리는 T2VSafetyBench라는 새로운 기준을 소개해. 이 기준은 텍스트-비디오 모델의 안전-critical 평가를 위한 거야. 우리는 비디오 생성 안전의 12가지 중요한 측면을 정의하고, 실제 프롬프트, LLM 생성 프롬프트, 그리고 탈옥 공격 기반 프롬프트를 포함한 악의적인 프롬프트 데이터셋을 구축했어. 

우리의 평가 결과를 바탕으로 몇 가지 중요한 발견을 했어. 첫째, 어떤 모델도 모든 측면에서 뛰어나지 않고, 각 모델이 다양한 강점을 보여줘. 둘째, GPT-4 평가와 수동 리뷰 간의 상관관계는 일반적으로 높아. 셋째, 텍스트-비디오 생성 모델의 사용성과 안전성 간에는 trade-off가 있어. 이건 비디오 생성 분야가 빠르게 발전함에 따라 안전 위험이 증가할 것임을 나타내고, 비디오 안전을 우선시할 필요성이 시급하다는 걸 강조해. 우리는 T2VSafetyBench가 생성 AI 시대의 비디오 생성 안전성을 더 잘 이해하는 데 도움이 되길 바래.

================================================================================

URL: https://arxiv.org/abs/2407.11398
Title: Animate3D: Animating Any 3D Model with Multi-view Video Diffusion

Original Abstract:
Recent advances in 4D generation mainly focus on generating 4D content by distilling pre-trained text or single-view image-conditioned models. It is inconvenient for them to take advantage of various off-the-shelf 3D assets with multi-view attributes, and their results suffer from spatiotemporal inconsistency owing to the inherent ambiguity in the supervision signals. In this work, we present Animate3D, a novel framework for animating any static 3D model. The core idea is two-fold: 1) We propose a novel multi-view video diffusion model (MV-VDM) conditioned on multi-view renderings of the static 3D object, which is trained on our presented large-scale multi-view video dataset (MV-Video). 2) Based on MV-VDM, we introduce a framework combining reconstruction and 4D Score Distillation Sampling (4D-SDS) to leverage the multi-view video diffusion priors for animating 3D objects. Specifically, for MV-VDM, we design a new spatiotemporal attention module to enhance spatial and temporal consistency by integrating 3D and video diffusion models. Additionally, we leverage the static 3D model's multi-view renderings as conditions to preserve its identity. For animating 3D models, an effective two-stage pipeline is proposed: we first reconstruct motions directly from generated multi-view videos, followed by the introduced 4D-SDS to refine both appearance and motion. Benefiting from accurate motion learning, we could achieve straightforward mesh animation. Qualitative and quantitative experiments demonstrate that Animate3D significantly outperforms previous approaches. Data, code, and models will be open-released.

Translated Abstract:
최근 4D 생성 기술은 주로 사전 훈련된 텍스트나 단일 뷰 이미지에 기반한 모델을 사용해 4D 콘텐츠를 만드는 데 집중하고 있어. 이런 방법은 다양한 다중 뷰 속성을 가진 3D 자산을 활용하기에 불편하고, 결과물은 감독 신호의 모호성 때문에 시공간 일관성이 떨어지는 문제가 있어. 

이번 연구에서는 Animate3D라는 새로운 프레임워크를 소개할 거야. 이 프레임워크는 정적인 3D 모델을 애니메이션으로 만드는 방법이야. 핵심 아이디어는 두 가지야: 1) 우리는 정적인 3D 객체의 다중 뷰 렌더링을 기반으로 하는 새로운 다중 뷰 비디오 확산 모델(MV-VDM)을 제안해. 이 모델은 우리가 만든 대규모 다중 뷰 비디오 데이터셋(MV-Video)에서 훈련됐어. 2) MV-VDM을 기반으로, 우리는 3D 객체 애니메이션을 위해 재구성과 4D 점수 증류 샘플링(4D-SDS)을 결합하는 프레임워크를 도입해. 

특히, MV-VDM을 위해 3D와 비디오 확산 모델을 통합해 공간적 및 시간적 일관성을 높이는 새로운 시공간 주의 모듈을 설계했어. 또한 정적인 3D 모델의 다중 뷰 렌더링을 조건으로 사용해 그 모델의 정체성을 유지해. 3D 모델을 애니메이션화하기 위해, 우리는 효과적인 두 단계 파이프라인을 제안해. 먼저 생성된 다중 뷰 비디오에서 직접 모션을 재구성하고, 그 다음 4D-SDS를 사용해 외형과 모션을 다듬어. 정확한 모션 학습 덕분에 간단한 메쉬 애니메이션을 만들 수 있었어. 

질적 및 양적 실험 결과에 따르면, Animate3D는 이전 방법들보다 훨씬 뛰어난 성능을 보여줘. 데이터, 코드, 모델은 공개될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2407.11633
Title: Scaling Diffusion Transformers to 16 Billion Parameters

Original Abstract:
In this paper, we present DiT-MoE, a sparse version of the diffusion Transformer, that is scalable and competitive with dense networks while exhibiting highly optimized inference. The DiT-MoE includes two simple designs: shared expert routing and expert-level balance loss, thereby capturing common knowledge and reducing redundancy among the different routed experts. When applied to conditional image generation, a deep analysis of experts specialization gains some interesting observations: (i) Expert selection shows preference with spatial position and denoising time step, while insensitive with different class-conditional information; (ii) As the MoE layers go deeper, the selection of experts gradually shifts from specific spacial position to dispersion and balance. (iii) Expert specialization tends to be more concentrated at the early time step and then gradually uniform after half. We attribute it to the diffusion process that first models the low-frequency spatial information and then high-frequency complex information. Based on the above guidance, a series of DiT-MoE experimentally achieves performance on par with dense networks yet requires much less computational load during inference. More encouragingly, we demonstrate the potential of DiT-MoE with synthesized image data, scaling diffusion model at a 16.5B parameter that attains a new SoTA FID-50K score of 1.80 in 512$\times$512 resolution settings. The project page: this https URL.

Translated Abstract:
이 논문에서는 DiT-MoE라는 희소한 확산 변환기를 소개해. 이건 밀집 네트워크와 경쟁하면서도 스케일이 가능하고, 추론이 매우 최적화되어 있어. DiT-MoE는 두 가지 간단한 설계를 포함하고 있어: 공유 전문가 라우팅과 전문가 수준의 균형 손실이야. 이를 통해 서로 다른 라우팅된 전문가들 사이의 공통 지식을 잡고 중복을 줄일 수 있어.

조건부 이미지 생성에 적용했을 때, 전문가의 전문화에 대한 깊은 분석에서 몇 가지 흥미로운 관찰이 있었어: (i) 전문가 선택은 공간 위치와 노이즈 제거 시간 단계에 따라 선호도가 다르지만, 서로 다른 클래스 조건 정보에는 둔감해; (ii) MoE 레이어가 깊어질수록 전문가 선택이 특정 공간 위치에서 분산과 균형으로 점차 이동해; (iii) 전문가의 전문화는 초기 시간 단계에서 더 집중되는 경향이 있고, 그 후 절반이 지나면서 점점 균일해져. 우리는 이것이 확산 과정 때문이라고 생각해. 처음에는 저주파 공간 정보를 모델링하고, 이후에 고주파 복잡한 정보를 모델링하기 때문이야.

위의 가이드를 바탕으로, DiT-MoE는 실험적으로 밀집 네트워크와 비슷한 성능을 달성하면서도 추론 시 훨씬 적은 계산량을 요구해. 더 고무적인 건, 합성 이미지 데이터로 DiT-MoE의 잠재력을 보여주었다는 거야. 16.5B 파라미터로 확산 모델을 스케일해서 512$\times$512 해상도 설정에서 새로운 SoTA FID-50K 점수인 1.80을 달성했어. 프로젝트 페이지는 이 URL이야.

================================================================================

URL: https://arxiv.org/abs/2407.11802
Title: Invariant Causal Knowledge Distillation in Neural Networks

Original Abstract:
Knowledge distillation (KD) involves transferring the knowledge from one neural network to another, often from a larger, well-trained model (teacher) to a smaller, more efficient model (student). Traditional KD methods minimize the Kullback-Leibler (KL) divergence between the probabilistic outputs of the teacher and student networks. However, this approach often overlooks crucial structural knowledge embedded within the teacher's network. In this paper, we introduce Invariant Consistency Distillation (ICD), a novel methodology designed to enhance KD by ensuring that the student model's representations are both discriminative and invariant with respect to the teacher's outputs. Our approach is based on causal inference principles and combines contrastive learning with an explicit invariance penalty, capturing significantly more information from the teacher's representation. ICD uses an efficient, parameter-free approach for flexible teacher-student alignment. We provide a theoretical foundation for ICD and demonstrate its effectiveness through extensive experiments. Our results on CIFAR-100 and ImageNet ILSVRC-2012 show that ICD outperforms traditional KD techniques and surpasses state-of-the-art methods. In some cases, the student model even exceeds the teacher model in terms of accuracy. Furthermore, we successfully apply our method to other datasets, such as Tiny ImageNet and STL-10, demonstrating superior cross-dataset generalization. Code is available at this https URL.

Translated Abstract:
지식 증류(KD)는 한 신경망에서 다른 신경망으로 지식을 전달하는 과정인데, 보통 큰 모델(선생님)에서 작고 효율적인 모델(학생)로 이뤄져. 전통적인 KD 방식은 선생님과 학생 네트워크의 확률적 출력 간의 쿨백-라이블러(KL) 발산을 최소화하는데, 이 방법은 종종 선생님의 네트워크에 내재된 중요한 구조적 지식을 놓치게 돼.

이 논문에서는 불변 일관성 증류(ICD)라는 새로운 방법론을 소개해. 이 방법은 KD를 개선하기 위해 학생 모델의 표현이 선생님 출력에 대해 분별력이 있으면서도 불변하도록 보장해. 우리의 접근 방식은 인과 추론 원리에 기반하며, 대조 학습과 명시적인 불변 페널티를 결합해 선생님의 표현에서 훨씬 더 많은 정보를 포착해. ICD는 유연한 선생님-학생 정렬을 위해 효율적이고 파라미터가 없는 방식을 사용해.

우리는 ICD의 이론적 기초를 제공하고, 광범위한 실험을 통해 그 효과를 입증했어. CIFAR-100과 ImageNet ILSVRC-2012에서의 결과는 ICD가 전통적인 KD 기법을 능가하고 최신 방법들보다 더 나은 성능을 보인다는 것을 보여줘. 어떤 경우에는 학생 모델이 정확도 면에서 선생님 모델을 초과하기도 해. 게다가, 우리는 Tiny ImageNet과 STL-10 같은 다른 데이터셋에도 우리의 방법을 성공적으로 적용해, 뛰어난 교차 데이터셋 일반화를 보여줬어. 코드도 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2407.11820
Title: Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation

Original Abstract:
Audio-Visual Segmentation (AVS) aims to achieve pixel-level localization of sound sources in videos, while Audio-Visual Semantic Segmentation (AVSS), as an extension of AVS, further pursues semantic understanding of audio-visual scenes. However, since the AVSS task requires the establishment of audio-visual correspondence and semantic understanding simultaneously, we observe that previous methods have struggled to handle this mashup of objectives in end-to-end training, resulting in insufficient learning and sub-optimization. Therefore, we propose a two-stage training strategy called \textit{Stepping Stones}, which decomposes the AVSS task into two simple subtasks from localization to semantic understanding, which are fully optimized in each stage to achieve step-by-step global optimization. This training strategy has also proved its generalization and effectiveness on existing methods. To further improve the performance of AVS tasks, we propose a novel framework Adaptive Audio Visual Segmentation, in which we incorporate an adaptive audio query generator and integrate masked attention into the transformer decoder, facilitating the adaptive fusion of visual and audio features. Extensive experiments demonstrate that our methods achieve state-of-the-art results on all three AVS benchmarks. The project homepage can be accessed at this https URL.

Translated Abstract:
오디오-비주얼 세분화(Audio-Visual Segmentation, AVS)는 비디오에서 소리의 위치를 픽셀 단위로 찾는 걸 목표로 해. 오디오-비주얼 의미 세분화(Audio-Visual Semantic Segmentation, AVSS)는 AVS의 확장으로, 오디오와 비주얼 장면에 대한 의미적 이해를 더 추구해. 

하지만 AVSS 작업은 오디오와 비주얼의 일치를 찾고 의미를 이해하는 걸 동시에 해야 해서, 이전 방법들은 이 두 가지 목표를 동시에 처리하는 데 어려움을 겪었어. 그래서 학습이 충분히 이루어지지 않거나 최적화가 잘 안 되는 문제가 생겼지. 

그래서 우리는 \textit{Stepping Stones}라는 두 단계 훈련 전략을 제안해. 이 전략은 AVSS 작업을 위치 파악에서 의미 이해로 나누어서 각 단계에서 완전히 최적화할 수 있도록 해. 이렇게 하면 단계별로 전반적인 최적화를 이룰 수 있어. 이 훈련 전략은 기존 방법에서도 일반화와 효과성을 입증했어. 

AVS 작업의 성능을 더 향상시키기 위해, 우리는 적응형 오디오 쿼리 생성기를 포함하고 마스크드 어텐션을 트랜스포머 디코더에 통합한 새로운 프레임워크인 적응형 오디오 비주얼 세분화(Adaptive Audio Visual Segmentation)를 제안해. 이를 통해 시각적 특성과 오디오 특성을 유연하게 융합할 수 있어. 

광범위한 실험 결과, 우리의 방법이 모든 AVS 벤치마크에서 최첨단 성과를 달성했다는 걸 보여줘. 프로젝트 홈페이지는 이 https URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2407.12073
Title: Relational Representation Distillation

Original Abstract:
Knowledge distillation (KD) is an effective method for transferring knowledge from a large, well-trained teacher model to a smaller, more efficient student model. Despite its success, one of the main challenges in KD is ensuring the efficient transfer of complex knowledge while maintaining the student's computational efficiency. Unlike previous works that applied contrastive objectives promoting explicit negative instances with little attention to the relationships between them, we introduce Relational Representation Distillation (RRD). Our approach leverages pairwise similarities to explore and reinforce the relationships between the teacher and student models. Inspired by self-supervised learning principles, it uses a relaxed contrastive loss that focuses on similarity rather than exact replication. This method aligns the output distributions of teacher samples in a large memory buffer, improving the robustness and performance of the student model without the need for strict negative instance differentiation. Our approach demonstrates superior performance on CIFAR-100 and ImageNet ILSVRC-2012, outperforming traditional KD and sometimes even outperforms the teacher network when combined with KD. It also transfers successfully to other datasets like Tiny ImageNet and STL-10. Code is available at this https URL.

Translated Abstract:
지식 증류(KD)는 큰 규모의 잘 훈련된 교사 모델에서 작고 효율적인 학생 모델로 지식을 전달하는 효과적인 방법이야. 하지만 KD의 주요 도전 중 하나는 복잡한 지식을 효율적으로 전이하면서도 학생 모델의 계산 효율성을 유지하는 거야.

이전 연구들은 명시적인 부정 인스턴스를 촉진하는 대조적 목표를 적용했는데, 이들은 서로의 관계를 잘 고려하지 않았어. 우리는 관계 표현 증류(Relational Representation Distillation, RRD)를 도입해. 이 방법은 쌍별 유사성을 활용해서 교사 모델과 학생 모델 간의 관계를 탐색하고 강화해.

자기 감독 학습 원칙에서 영감을 받아서, 정확한 복제보다는 유사성에 초점을 맞춘 완화된 대조 손실을 사용해. 이 방법은 큰 메모리 버퍼에서 교사 샘플의 출력 분포를 정렬해서, 부정 인스턴스의 엄격한 구분 없이 학생 모델의 강건성과 성능을 향상시켜.

우리의 접근법은 CIFAR-100과 ImageNet ILSVRC-2012에서 뛰어난 성능을 보여주고, 전통적인 KD보다 더 나은 성과를 내며, 가끔은 KD와 결합했을 때 교사 네트워크보다도 더 좋은 성능을 보여줘. 또한 Tiny ImageNet과 STL-10 같은 다른 데이터셋에서도 성공적으로 전이할 수 있어. 코드도 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2407.14066
Title: 360VFI: A Dataset and Benchmark for Omnidirectional Video Frame Interpolation

Original Abstract:
Head-mounted 360° displays and portable 360° cameras have significantly progressed, providing viewers a realistic and immersive experience. However, many omnidirectional videos have low frame rates that can lead to visual fatigue, and the prevailing plane frame interpolation methodologies are unsuitable for omnidirectional video interpolation because they are designed solely for traditional videos. This paper introduces the benchmark dataset, 360VFI, for Omnidirectional Video Frame Interpolation. We present a practical implementation that introduces a distortion prior from omnidirectional video into the network to modulate distortions. Specifically, we propose a pyramid distortion-sensitive feature extractor that uses the unique characteristics of equirectangular projection (ERP) format as prior information. Moreover, we devise a decoder that uses an affine transformation to further facilitate the synthesis of intermediate frames. 360VFI is the first dataset and benchmark that explores the challenge of Omnidirectional Video Frame Interpolation. Through our benchmark analysis, we present four different distortion condition scenes in the proposed 360VFI dataset to evaluate the challenges triggered by distortion during interpolation. Besides, experimental results demonstrate that Omnidirectional Video Interpolation can be effectively improved by modeling for omnidirectional distortion.

Translated Abstract:
헤드 마운트형 360° 디스플레이와 휴대용 360° 카메라가 많이 발전해서, 시청자에게 현실적이고 몰입감 있는 경험을 제공하고 있어. 하지만 많은 전방향 비디오가 프레임 속도가 낮아서 시각적으로 피로감을 줄 수 있어. 기존의 평면 프레임 보간 방법은 전통적인 비디오만을 위해 설계되었기 때문에 전방향 비디오 보간에는 적합하지 않아. 

이 논문에서는 전방향 비디오 프레임 보간을 위한 벤치마크 데이터셋인 360VFI를 소개해. 우리는 전방향 비디오의 왜곡 정보를 네트워크에 도입해서 왜곡을 조절하는 실용적인 구현 방법을 제시해. 특히, 우리는 구면 직각 투영(ERP) 형식의 독특한 특성을 사전 정보로 활용하는 피라미드 왜곡 민감 특징 추출기를 제안해. 게다가, 우리는 중간 프레임 합성을 더 쉽게 하기 위해 아핀 변환을 사용하는 디코더도 개발했어. 

360VFI는 전방향 비디오 프레임 보간의 도전을 탐구하는 최초의 데이터셋이자 벤치마크야. 우리의 벤치마크 분석을 통해, 우리는 제안된 360VFI 데이터셋에서 왜곡으로 인한 도전 과제를 평가하기 위해 네 가지 다른 왜곡 조건 장면을 보여줘. 또한 실험 결과는 전방향 왜곡을 모델링함으로써 전방향 비디오 보간이 효과적으로 개선될 수 있음을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2407.18611
Title: IOVS4NeRF:Incremental Optimal View Selection for Large-Scale NeRFs

Original Abstract:
Neural Radiance Fields (NeRF) have recently demonstrated significant efficiency in the reconstruction of three-dimensional scenes and the synthesis of novel perspectives from a limited set of two-dimensional images. However, large-scale reconstruction using NeRF requires a substantial amount of aerial imagery for training, making it impractical in resource-constrained environments. This paper introduces an innovative incremental optimal view selection framework, IOVS4NeRF, designed to model a 3D scene within a restricted input budget. Specifically, our approach involves adding the existing training set with newly acquired samples, guided by a computed novel hybrid uncertainty of candidate views, which integrates rendering uncertainty and positional uncertainty. By selecting views that offer the highest information gain, the quality of novel view synthesis can be enhanced with minimal additional resources. Comprehensive experiments substantiate the efficiency of our model in realistic scenes, outperforming baselines and similar prior works, particularly under conditions of sparse training data.

Translated Abstract:
Neural Radiance Fields (NeRF)는 최근에 제한된 2D 이미지 세트로부터 3D 장면을 재구성하고 새로운 관점을 합성하는 데 상당한 효율성을 보여줬어. 하지만 NeRF를 사용한 대규모 재구성은 훈련을 위해 많은 항공 이미지를 필요로 해서 자원이 제한된 환경에서는 실용적이지 않아.

이 논문에서는 IOVS4NeRF라는 새로운 점진적 최적 뷰 선택 프레임워크를 소개해. 이 프레임워크는 제한된 입력 예산 안에서 3D 장면을 모델링할 수 있도록 설계됐어. 구체적으로, 기존 훈련 세트에 새로 얻은 샘플을 추가하는데, 이때 후보 뷰의 혼합 불확실성을 계산해서 안내해. 이 불확실성은 렌더링 불확실성과 위치 불확실성을 통합한 거야.

정보 이득이 가장 큰 뷰를 선택함으로써 추가 자원이 최소화되면서도 새로운 뷰 합성의 품질을 높일 수 있어. 다양한 실험을 통해 우리의 모델이 현실적인 장면에서 효율적이라는 것을 입증했고, 특히 훈련 데이터가 부족한 상황에서도 기존 방법들과 비슷한 이전 연구들보다 더 나은 성능을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2407.19001
Title: PromptCCD: Learning Gaussian Mixture Prompt Pool for Continual Category Discovery

Original Abstract:
We tackle the problem of Continual Category Discovery (CCD), which aims to automatically discover novel categories in a continuous stream of unlabeled data while mitigating the challenge of catastrophic forgetting -- an open problem that persists even in conventional, fully supervised continual learning. To address this challenge, we propose PromptCCD, a simple yet effective framework that utilizes a Gaussian Mixture Model (GMM) as a prompting method for CCD. At the core of PromptCCD lies the Gaussian Mixture Prompting (GMP) module, which acts as a dynamic pool that updates over time to facilitate representation learning and prevent forgetting during category discovery. Moreover, GMP enables on-the-fly estimation of category numbers, allowing PromptCCD to discover categories in unlabeled data without prior knowledge of the category numbers. We extend the standard evaluation metric for Generalized Category Discovery (GCD) to CCD and benchmark state-of-the-art methods on diverse public datasets. PromptCCD significantly outperforms existing methods, demonstrating its effectiveness. Project page: this https URL .

Translated Abstract:
우리는 연속 카테고리 발견(Continual Category Discovery, CCD) 문제를 다루고 있어. 이건 레이블이 없는 데이터의 연속적인 흐름에서 새로운 카테고리를 자동으로 발견하는 걸 목표로 하는데, 기존의 완전 감독 학습에서도 여전히 존재하는 재앙적 망각(catastrophic forgetting) 문제를 줄이는 데 초점을 맞추고 있어.

이 문제를 해결하기 위해, 우리는 PromptCCD라는 간단하지만 효과적인 프레임워크를 제안해. 이건 가우시안 혼합 모델(Gaussian Mixture Model, GMM)을 사용해서 CCD를 위한 프롬프트 방법으로 활용해. PromptCCD의 핵심에는 가우시안 혼합 프롬프트(Gaussian Mixture Prompting, GMP) 모듈이 있는데, 이건 시간이 지남에 따라 업데이트되는 동적 풀 역할을 해서 카테고리 발견 중에 표현 학습을 도와주고 잊어버리지 않게 해줘.

게다가, GMP는 카테고리 수를 실시간으로 추정할 수 있게 해줘. 그래서 PromptCCD는 카테고리 수에 대한 사전 지식 없이 레이블이 없는 데이터에서 카테고리를 발견할 수 있어. 우리는 일반화된 카테고리 발견(Generalized Category Discovery, GCD)을 위한 표준 평가 지표를 CCD에 맞게 확장하고, 다양한 공개 데이터셋에서 최신 방법들과 벤치마킹을 했어. 

PromptCCD는 기존 방법들보다 훨씬 뛰어난 성능을 보여주면서 그 효과를 입증하고 있어.

================================================================================

URL: https://arxiv.org/abs/2408.01688
Title: SiamMo: Siamese Motion-Centric 3D Object Tracking

Original Abstract:
Current 3D single object tracking methods primarily rely on the Siamese matching-based paradigm, which struggles with textureless and incomplete LiDAR point clouds. Conversely, the motion-centric paradigm avoids appearance matching, thus overcoming these issues. However, its complex multi-stage pipeline and the limited temporal modeling capability of a single-stream architecture constrain its potential. In this paper, we introduce SiamMo, a novel and simple Siamese motion-centric tracking approach. Unlike the traditional single-stream architecture, we employ Siamese feature extraction for motion-centric tracking. This decouples feature extraction from temporal fusion, significantly enhancing tracking performance. Additionally, we design a Spatio-Temporal Feature Aggregation module to integrate Siamese features at multiple scales, capturing motion information effectively. We also introduce a Box-aware Feature Encoding module to encode object size priors into motion estimation. SiamMo is a purely motion-centric tracker that eliminates the need for additional processes like segmentation and box refinement. Without whistles and bells, SiamMo not only surpasses state-of-the-art methods across multiple benchmarks but also demonstrates exceptional robustness in challenging scenarios. SiamMo sets a new record on the KITTI tracking benchmark with 90.1\% precision while maintaining a high inference speed of 108 FPS. The code will be released at this https URL.

Translated Abstract:
현재 3D 단일 객체 추적 방법은 주로 시암ese 매칭 기반으로 작동하는데, 이 방식은 텍스처가 없는 LiDAR 포인트 클라우드나 불완전한 데이터에서 어려움을 겪어. 반면에, 모션 중심 접근 방식은 외관 매칭을 피해서 이런 문제를 해결할 수 있어. 하지만 이 방식은 복잡한 다단계 파이프라인과 단일 스트림 구조의 제한된 시간 모델링 능력 때문에 한계가 있어.

이 논문에서는 시암모(SiamMo)라는 새로운 간단한 시암ese 모션 중심 추적 방법을 소개해. 전통적인 단일 스트림 구조와는 다르게, 우리는 모션 중심 추적을 위해 시암ese 피처 추출을 사용해. 이렇게 하면 피처 추출과 시간 융합이 분리되어 추적 성능이 크게 향상돼. 그리고 우리는 여러 스케일에서 시암ese 피처를 통합하는 시공간 피처 집계 모듈도 설계했어. 이 모듈은 모션 정보를 효과적으로 캡처해.

또한, 우리는 객체 크기 사전 정보를 모션 추정에 인코딩하는 박스 인식 피처 인코딩 모듈도 도입했어. 시암모는 순수한 모션 중심 추적기로, 세분화나 박스 정제 같은 추가적인 과정이 필요 없어. 화려한 장치 없이도, 시암모는 여러 벤치마크에서 최신 기술을 능가할 뿐만 아니라 어려운 상황에서도 뛰어난 강인성을 보여줘. 시암모는 KITTI 추적 벤치마크에서 90.1%의 정밀도로 새로운 기록을 세우면서도, 108 FPS의 높은 추론 속도를 유지해. 코드는 이 https URL에서 공개될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2408.02761
Title: Dimensionality Reduction and Nearest Neighbors for Improving Out-of-Distribution Detection in Medical Image Segmentation

Original Abstract:
Clinically deployed deep learning-based segmentation models are known to fail on data outside of their training distributions. While clinicians review the segmentations, these models tend to perform well in most instances, which could exacerbate automation bias. Therefore, detecting out-of-distribution images at inference is critical to warn the clinicians that the model likely failed. This work applied the Mahalanobis distance (MD) post hoc to the bottleneck features of four Swin UNETR and nnU-net models that segmented the liver on T1-weighted magnetic resonance imaging and computed tomography. By reducing the dimensions of the bottleneck features with either principal component analysis or uniform manifold approximation and projection, images the models failed on were detected with high performance and minimal computational load. In addition, this work explored a non-parametric alternative to the MD, a k-th nearest neighbors distance (KNN). KNN drastically improved scalability and performance over MD when both were applied to raw and average-pooled bottleneck features.

Translated Abstract:
임상에서 사용되는 딥러닝 기반의 세분화 모델은 훈련 데이터와 다른 데이터에서는 잘 작동하지 않는 것으로 알려져 있어. 의사들이 세분화 결과를 검토할 때, 이런 모델들이 대체로 잘하는 것처럼 보이는데, 이게 자동화 편향을 더 악화시킬 수 있어. 그래서 추론할 때 훈련 데이터와 다른 이미지를 감지하는 게 중요해, 모델이 실패했을 가능성이 있으니까. 

이 연구에서는 Mahalanobis 거리(MD)를 활용해서 네 가지 Swin UNETR와 nnU-net 모델의 병목 특성에서 간을 T1 가중 자기공명영상과 컴퓨터 단층촬영으로 세분화하는 데 적용했어. 병목 특성의 차원을 주성분 분석(PCA)이나 균일 매니폴드 근사 및 투영(UMAP)으로 줄여서, 모델이 실패한 이미지를 높은 성능과 적은 계산 부담으로 잘 찾아냈어. 

또한, 이 연구에서는 MD의 비모수적 대안인 k-최근접 이웃 거리(KNN)를 탐색했어. KNN은 원본과 평균 풀링된 병목 특성에 모두 적용했을 때 MD보다 확장성과 성능을 크게 향상시켰어.

================================================================================

URL: https://arxiv.org/abs/2408.03632
Title: Concept Conductor: Orchestrating Multiple Personalized Concepts in Text-to-Image Synthesis

Original Abstract:
The customization of text-to-image models has seen significant advancements, yet generating multiple personalized concepts remains a challenging task. Current methods struggle with attribute leakage and layout confusion when handling multiple concepts, leading to reduced concept fidelity and semantic consistency. In this work, we introduce a novel training-free framework, Concept Conductor, designed to ensure visual fidelity and correct layout in multi-concept customization. Concept Conductor isolates the sampling processes of multiple custom models to prevent attribute leakage between different concepts and corrects erroneous layouts through self-attention-based spatial guidance. Additionally, we present a concept injection technique that employs shape-aware masks to specify the generation area for each concept. This technique injects the structure and appearance of personalized concepts through feature fusion in the attention layers, ensuring harmony in the final image. Extensive qualitative and quantitative experiments demonstrate that Concept Conductor can consistently generate composite images with accurate layouts while preserving the visual details of each concept. Compared to existing baselines, Concept Conductor shows significant performance improvements. Our method supports the combination of any number of concepts and maintains high fidelity even when dealing with visually similar concepts. The code and models are available at this https URL.

Translated Abstract:
텍스트-이미지 모델의 맞춤화가 많이 발전했지만, 여러 개인화된 개념을 생성하는 건 여전히 어려운 일이다. 현재 방법들은 여러 개념을 처리할 때 속성 누수나 레이아웃 혼란 문제로 인해 개념의 충실도와 의미 일관성이 떨어진다. 

이 연구에서는 여러 개념 맞춤화에서 시각적 충실도와 올바른 레이아웃을 보장하기 위해 새로운 훈련 없는 프레임워크인 Concept Conductor를 소개한다. Concept Conductor는 여러 맞춤 모델의 샘플링 과정을 분리해서 서로 다른 개념 간의 속성 누수를 방지하고, 자기 주의 기반의 공간 가이드를 통해 잘못된 레이아웃을 수정한다.

또한, 각 개념의 생성 영역을 지정하는 형태 인식 마스크를 사용하는 개념 주입 기법도 제안한다. 이 기법은 주의 레이어에서 기능 융합을 통해 개인화된 개념의 구조와 외관을 주입하여 최종 이미지의 조화를 이룬다. 

광범위한 정성적 및 정량적 실험 결과, Concept Conductor는 정확한 레이아웃을 유지하면서 각 개념의 시각적 세부 사항을 잘 보존한 합성 이미지를 일관되게 생성할 수 있음을 보여준다. 기존 기준과 비교했을 때 Concept Conductor는 성능이 크게 향상되었다. 우리의 방법은 어떤 수의 개념을 결합할 수 있으며, 시각적으로 유사한 개념을 다룰 때도 높은 충실도를 유지한다. 코드와 모델은 이 URL에서 확인할 수 있다.

================================================================================

URL: https://arxiv.org/abs/2408.05743
Title: Neural Architecture Search based Global-local Vision Mamba for Palm-Vein Recognition

Original Abstract:
Due to the advantages such as high security, high privacy, and liveness recognition, vein recognition has been received more and more attention in past years. Recently, deep learning models, e.g., Mamba has shown robust feature representation with linear computational complexity and successfully applied for visual tasks. However, vision Manba can capture long-distance feature dependencies but unfortunately deteriorate local feature details. Besides, manually designing a Mamba architecture based on human priori knowledge is very time-consuming and error-prone. In this paper, first, we propose a hybrid network structure named Global-local Vision Mamba (GLVM), to learn the local correlations in images explicitly and global dependencies among tokens for vein feature representation. Secondly, we design a Multi-head Mamba to learn the dependencies along different directions, so as to improve the feature representation ability of vision Mamba. Thirdly, to learn the complementary features, we propose a ConvMamba block consisting of three branches, named Multi-head Mamba branch (MHMamba), Feature Iteration Unit branch (FIU), and Convolutional Neural Network (CNN) branch, where the Feature Iteration Unit branch aims to fuse convolutional local features with Mamba-based global representations. Finally, a Globallocal Alternate Neural Architecture Search (GLNAS) method is proposed to search the optimal architecture of GLVM alternately with the evolutionary algorithm, thereby improving the recognition performance for vein recognition tasks. We conduct rigorous experiments on three public palm-vein databases to estimate the performance. The experimental results demonstrate that the proposed method outperforms the representative approaches and achieves state-of-the-art recognition accuracy.

Translated Abstract:
정맥 인식은 높은 보안성, 프라이버시, 생체 인식의 장점 덕분에 최근 몇 년 동안 많은 주목을 받고 있어. 최근에 딥러닝 모델인 Mamba가 선형 계산 복잡도로 강력한 특징 표현을 보여주고 시각적 작업에 성공적으로 적용되었어. 하지만 비전 Mamba는 장거리 특징 의존성을 잡을 수 있지만, 안타깝게도 로컬 특징 세부정보는 나빠져. 게다가 사람의 선험적 지식을 바탕으로 Mamba 아키텍처를 수동으로 설계하는 건 매우 시간이 많이 들고 오류가 발생하기 쉬워.

이 논문에서는 먼저, 이미지에서 로컬 상관관계를 명시적으로 배우고, 정맥 특징 표현을 위해 토큰 간의 글로벌 의존성을 배우는 하이브리드 네트워크 구조인 Global-local Vision Mamba (GLVM)를 제안해. 둘째로, 다양한 방향으로 의존성을 배우기 위해 Multi-head Mamba를 설계해서 비전 Mamba의 특징 표현 능력을 향상시켜. 셋째로, 보완적 특징을 배우기 위해 세 가지 브랜치로 구성된 ConvMamba 블록을 제안하는데, 이 블록에는 Multi-head Mamba 브랜치 (MHMamba), Feature Iteration Unit 브랜치 (FIU), 그리고 Convolutional Neural Network (CNN) 브랜치가 있어. Feature Iteration Unit 브랜치는 컨볼루션 로컬 특징과 Mamba 기반 글로벌 표현을 융합하는 걸 목표로 해.

마지막으로, GLVM의 최적 아키텍처를 진화 알고리즘과 함께 번갈아 검색하는 Globallocal Alternate Neural Architecture Search (GLNAS) 방법을 제안해서 정맥 인식 작업의 성능을 향상시켜. 우리는 세 개의 공개 손바닥 정맥 데이터베이스에서 철저한 실험을 수행해서 성능을 평가했어. 실험 결과, 제안한 방법이 대표적인 접근법보다 더 나은 성능을 보여주고, 최첨단 인식 정확도를 달성했다는 걸 확인했어.

================================================================================

URL: https://arxiv.org/abs/2408.08381
Title: Pre-processing and Compression: Understanding Hidden Representation Refinement Across Imaging Domains via Intrinsic Dimension

Original Abstract:
In recent years, there has been interest in how geometric properties such as intrinsic dimension (ID) of a neural network's hidden representations change through its layers, and how such properties are predictive of important model behavior such as generalization ability. However, evidence has begun to emerge that such behavior can change significantly depending on the domain of the network's training data, such as natural versus medical images. Here, we further this inquiry by exploring how the ID of a network's learned representations changes through its layers, in essence, characterizing how the network successively refines the information content of input data to be used for predictions. Analyzing eleven natural and medical image datasets across six network architectures, we find that how ID changes through the network differs noticeably between natural and medical image models. Specifically, medical image models peak in representation ID earlier in the network, implying a difference in the image features and their abstractness that are typically used for downstream tasks in these domains. Additionally, we discover a strong correlation of this peak representation ID with the ID of the data in its input space, implying that the intrinsic information content of a model's learned representations is guided by that of the data it was trained on. Overall, our findings emphasize notable discrepancies in network behavior between natural and non-natural imaging domains regarding hidden representation information content, and provide further insights into how a network's learned features are shaped by its training data.

Translated Abstract:
최근 몇 년 동안, 신경망의 숨겨진 표현에서 내재 차원(ID) 같은 기하학적 특성이 레이어를 통해 어떻게 변하는지, 그리고 이런 특성이 일반화 능력 같은 중요한 모델 행동을 어떻게 예측하는지에 대한 관심이 높아졌어. 그런데, 이런 행동이 신경망이 학습한 데이터의 도메인에 따라 크게 달라질 수 있다는 증거도 나타나기 시작했어. 예를 들면, 자연 이미지와 의료 이미지가 그렇지.

이번 연구에서는 신경망의 학습된 표현에서 ID가 레이어를 통해 어떻게 변하는지를 탐구했어. 쉽게 말해, 네트워크가 입력 데이터의 정보를 예측에 사용할 수 있도록 점차적으로 어떻게 다듬는지를 살펴본 거야. 6개의 네트워크 아키텍처를 이용해 11개의 자연 이미지와 의료 이미지 데이터셋을 분석해본 결과, ID가 네트워크를 통해 변화하는 방식이 자연 이미지 모델과 의료 이미지 모델 사이에서 눈에 띄게 다르다는 걸 발견했어.

특히, 의료 이미지 모델은 네트워크의 초기 단계에서 더 높은 표현 ID를 보였어. 이는 이 도메인에서 일반적으로 사용되는 이미지 특징과 추상성의 차이를 의미해. 게다가, 이 최대 표현 ID와 입력 공간에서 데이터의 ID 간에 강한 상관관계가 있다는 것도 발견했어. 즉, 모델이 학습한 표현의 내재 정보 내용이 학습한 데이터의 정보 내용에 의해 영향을 받는다는 거지.

전반적으로, 우리의 발견은 자연 이미지와 비자연 이미지 도메인 간의 네트워크 행동에서 숨겨진 표현 정보 내용의 뚜렷한 차이를 강조하고, 네트워크의 학습된 특징이 어떻게 학습 데이터에 의해 형성되는지에 대한 추가적인 통찰을 제공해.

================================================================================

URL: https://arxiv.org/abs/2408.09460
Title: Fine-Grained Building Function Recognition from Street-View Images via Geometry-Aware Semi-Supervised Learning

Original Abstract:
In this work, we propose a geometry-aware semi-supervised framework for fine-grained building function recognition, utilizing geometric relationships among multi-source data to enhance pseudo-label accuracy in semi-supervised learning, broadening its applicability to various building function categorization systems. Firstly, we design an online semi-supervised pre-training stage, which facilitates the precise acquisition of building facade location information in street-view images. In the second stage, we propose a geometry-aware coarse annotation generation module. This module effectively combines GIS data and street-view data based on the geometric relationships, improving the accuracy of pseudo annotations. In the third stage, we combine the newly generated coarse annotations with the existing labeled dataset to achieve fine-grained functional recognition of buildings across multiple cities at a large scale. Extensive experiments demonstrate that our proposed framework exhibits superior performance in fine-grained functional recognition of buildings. Within the same categorization system, it achieves improvements of 7.6\% and 4.8\% compared to fully-supervised methods and state-of-the-art semi-supervised methods, respectively. Additionally, our method also performs well in cross-city scenarios, i.e., extending the model trained on OmniCity (New York) to new cities (i.e., Los Angeles and Boston) with different building function categorization systems. This study offers a new solution for large-scale multi-city applications with minimal annotation requirements, facilitating more efficient data updates and resource allocation in urban management.

Translated Abstract:
이번 연구에서는 건물 기능을 세밀하게 인식하기 위한 지오메트리 인식 반지도 학습 프레임워크를 제안해. 여러 출처의 데이터 간의 기하학적 관계를 활용해서 반지도 학습에서 의사 라벨의 정확성을 높이는 거야. 이 방법은 다양한 건물 기능 분류 시스템에 적용할 수 있어.

먼저, 온라인 반지도 학습을 위한 사전 훈련 단계를 설계했어. 이 단계에서는 거리 뷰 이미지에서 건물 외관의 위치 정보를 정확하게 얻을 수 있도록 도와줘. 

두 번째 단계에서는 기하학적 관계를 기반으로 GIS 데이터와 거리 뷰 데이터를 효과적으로 결합하는 거칠은 주석 생성 모듈을 제안해. 이 모듈은 의사 주석의 정확성을 높여줘. 

세 번째 단계에서는 새로 생성된 거칠은 주석을 기존에 라벨이 붙은 데이터셋과 결합해서 여러 도시에서 건물의 세밀한 기능 인식을 달성해. 다양한 실험 결과, 우리 프레임워크가 건물의 세밀한 기능 인식에서 뛰어난 성능을 보이는 걸 확인했어. 같은 분류 시스템 내에서, 완전 지도 학습 방법과 최신 반지도 학습 방법에 비해 각각 7.6%와 4.8%의 성능 향상을 이뤘어.

또한, 우리의 방법은 도시 간 시나리오에서도 잘 작동해. 예를 들어, OmniCity(뉴욕)에서 훈련된 모델을 라스베가스와 보스턴 같은 새로운 도시로 확장할 수 있어. 이 연구는 최소한의 주석 요구사항으로 대규모 다도시 응용을 위한 새로운 솔루션을 제공하며, 도시 관리에서 데이터 업데이트와 자원 할당을 더 효율적으로 만들어 줘.

================================================================================

URL: https://arxiv.org/abs/2408.11001
Title: MegaFusion: Extend Diffusion Models towards Higher-resolution Image Generation without Further Tuning

Original Abstract:
Diffusion models have emerged as frontrunners in text-to-image generation, however, their fixed image resolution during training often leads to challenges in high-resolution image generation, such as semantic deviations and object replication. This paper introduces MegaFusion, a novel approach that extends existing diffusion-based text-to-image generation models towards efficient higher-resolution generation without additional fine-tuning or extra adaptation. Specifically, we employ an innovative truncate and relay strategy to bridge the denoising processes across different resolutions, allowing for high-resolution image generation in a coarse-to-fine manner. Moreover, by integrating dilated convolutions and noise re-scheduling, we further adapt the model's priors for higher resolution. The versatility and efficacy of MegaFusion make it universally applicable to both latent-space and pixel-space diffusion models, along with other derivative models. Extensive experiments confirm that MegaFusion significantly boosts the capability of existing models to produce images of megapixels and various aspect ratios, while only requiring about 40% of the original computational cost.

Translated Abstract:
확산 모델은 텍스트에서 이미지로 생성하는 데 주도적인 역할을 하고 있어. 하지만 훈련 중에 이미지 해상도가 고정되어 있어서, 고해상도 이미지를 만들 때 의미가 어긋나거나 물체가 중복되는 문제들이 생겨. 이 논문에서는 MegaFusion이라는 새로운 접근 방식을 소개하는데, 이건 기존의 확산 기반 텍스트-이미지 생성 모델을 더 효율적으로 고해상도 이미지를 생성할 수 있게 해줘. 추가적인 미세 조정이나 적응 없이 말이야.

특히, 우리는 '트렁케이트 앤드 릴레이'라는 혁신적인 전략을 사용해서 서로 다른 해상도에서의 노이즈 제거 과정을 연결해. 이렇게 하면 고해상도 이미지를 점진적으로 생성할 수 있어. 더불어, 팽창된 합성과 노이즈 재조정 기법을 통합해서 모델의 사전 정보를 고해상도에 맞게 조정해. MegaFusion의 다재다능함과 효율성 덕분에, 이건 잠재 공간(latent space)과 픽셀 공간(pixel space) 확산 모델은 물론 다른 파생 모델에도 보편적으로 적용할 수 있어.

광범위한 실험 결과, MegaFusion이 기존 모델들이 메가픽셀 이미지와 다양한 비율의 이미지를 생성하는 능력을 크게 향상시킨다는 것을 확인했어. 게다가 원래의 계산 비용의 약 40%만으로도 가능하다는 점이 큰 장점이야.

================================================================================

URL: https://arxiv.org/abs/2408.13152
Title: Long-term Pre-training for Temporal Action Detection with Transformers

Original Abstract:
Temporal action detection (TAD) is challenging, yet fundamental for real-world video applications. Recently, DETR-based models for TAD have been prevailing thanks to their unique benefits. However, transformers demand a huge dataset, and unfortunately data scarcity in TAD causes a severe degeneration. In this paper, we identify two crucial problems from data scarcity: attention collapse and imbalanced performance. To this end, we propose a new pre-training strategy, Long-Term Pre-training (LTP), tailored for transformers. LTP has two main components: 1) class-wise synthesis, 2) long-term pretext tasks. Firstly, we synthesize long-form video features by merging video snippets of a target class and non-target classes. They are analogous to untrimmed data used in TAD, despite being created from trimmed data. In addition, we devise two types of long-term pretext tasks to learn long-term dependency. They impose long-term conditions such as finding second-to-fourth or short-duration actions. Our extensive experiments show state-of-the-art performances in DETR-based methods on ActivityNet-v1.3 and THUMOS14 by a large margin. Moreover, we demonstrate that LTP significantly relieves the data scarcity issues in TAD.

Translated Abstract:
시간 기반 행동 탐지(TAD)는 어렵지만 실제 비디오 응용 프로그램에선 매우 중요해. 최근에 DETR 기반 모델들이 TAD에서 인기를 끌고 있는데, 그 이유는 그들의 독특한 장점 덕분이야. 하지만 트랜스포머는 큰 데이터셋이 필요하고, TAD에서는 데이터 부족이 심각한 문제를 일으켜.

이 논문에서는 데이터 부족에서 발생하는 두 가지 중요한 문제를 확인했어: 주의력 붕괴와 불균형 성능이야. 그래서 우리는 트랜스포머를 위해 맞춤형 새로운 사전 훈련 전략인 '장기 사전 훈련(Long-Term Pre-training, LTP)'을 제안해. LTP는 두 가지 주요 요소로 구성돼: 1) 클래스별 합성, 2) 장기 전제 작업이야.

먼저, 우리는 특정 클래스와 비특정 클래스의 비디오 조각을 합쳐서 긴 형식의 비디오 특징을 합성해. 이 데이터는 TAD에서 사용하는 비편집 데이터와 유사하지만, 편집된 데이터로부터 만들어졌어. 또, 우리는 장기 의존성을 배우기 위해 두 가지 유형의 장기 전제 작업을 고안했어. 이 작업들은 두 번째에서 네 번째 행동이나 짧은 지속 시간의 행동을 찾는 것처럼 장기 조건을 부과해.

우리의 광범위한 실험 결과는 ActivityNet-v1.3과 THUMOS14에서 DETR 기반 방법들이 최신 기술 수준을 훨씬 초과하는 성능을 보였어. 게다가, LTP가 TAD의 데이터 부족 문제를 상당히 완화한다는 걸 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2408.14846
Title: Diffusion-Occ: 3D Point Cloud Completion via Occupancy Diffusion

Original Abstract:
Point clouds are crucial for capturing three-dimensional data but often suffer from incompleteness due to limitations such as resolution and occlusion. Traditional methods typically rely on point-based approaches within discriminative frameworks for point cloud completion. In this paper, we introduce \textbf{Diffusion-Occ}, a novel framework for Diffusion Point Cloud Completion. Diffusion-Occ utilizes a two-stage coarse-to-fine approach. In the first stage, the Coarse Density Voxel Prediction Network (CDNet) processes partial points to predict coarse density voxels, streamlining global feature extraction through voxel classification, as opposed to previous regression-based methods. In the second stage, we introduce the Occupancy Generation Network (OccGen), a conditional occupancy diffusion model based on a transformer architecture and enhanced by our Point-Voxel Fuse (PVF) block. This block integrates coarse density voxels with partial points to leverage both global and local features for comprehensive completion. By thresholding the occupancy field, we convert it into a complete point cloud. Additionally, our method employs diverse training mixtures and efficient diffusion parameterization to enable effective one-step sampling during both training and inference. Experimental results demonstrate that Diffusion-Occ outperforms existing discriminative and generative methods.

Translated Abstract:
포인트 클라우드는 3D 데이터를 캡처하는 데 중요하지만, 해상도나 가림 현상 같은 한계로 인해 종종 불완전해져. 전통적인 방법들은 보통 포인트 기반 접근법을 사용해서 포인트 클라우드를 완성하는데, 이 논문에서는 새로운 프레임워크인 **Diffusion-Occ**를 소개할 거야. 

Diffusion-Occ는 두 단계의 코스에서 파인으로 진행되는 방식을 사용해. 첫 번째 단계에서는 Coarse Density Voxel Prediction Network (CDNet)가 부분 포인트를 처리해서 대략적인 밀도 복셀을 예측해. 이 과정은 이전의 회귀 기반 방법 대신 복셀 분류를 통해 전역 특징을 추출하는 데 도움을 줘. 

두 번째 단계에서는 Occupancy Generation Network (OccGen)라는 조건부 점유 확산 모델을 도입해. 이 모델은 트랜스포머 구조를 기반으로 하고, 우리의 Point-Voxel Fuse (PVF) 블록으로 강화돼. 이 블록은 대략적인 밀도 복셀과 부분 포인트를 결합해서 전역적이고 로컬한 특징을 모두 활용할 수 있게 해줘. 점유 필드를 임계값 처리해서 완전한 포인트 클라우드로 변환해. 

또한, 우리 방법은 다양한 훈련 혼합물과 효율적인 확산 매개변수를 사용해서 훈련과 추론 시 효과적인 일회성 샘플링을 가능하게 해. 실험 결과에 따르면, Diffusion-Occ는 기존의 판별적 및 생성적 방법들보다 더 뛰어난 성능을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2408.15915
Title: Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models

Original Abstract:
The cultivation of expertise for large language models (LLMs) to solve tasks of specific areas often requires special-purpose tuning with calibrated behaviors on the expected stable outputs. To avoid huge cost brought by manual preparation of instruction datasets and training resources up to hundreds of hours, the exploitation of open knowledge including a wealth of low rank adaptation (LoRA) models and instruction datasets serves as a good starting point. However, existing methods on model and data selection focus on the performance of general-purpose capabilities while neglecting the knowledge gap exposed in domain-specific deployment. In the present study, we propose to bridge such gap by introducing few human-annotated samples (i.e., K-shot) for advancing task expertise of LLMs with open knowledge. Specifically, we develop an efficient and scalable pipeline to cost-efficiently produce task experts where K-shot data intervene in selecting the most promising expert candidates and the task-relevant instructions. A mixture-of-expert (MoE) system is built to make the best use of individual-yet-complementary knowledge between multiple experts. We unveil the two keys to the success of a MoE system, 1) the abidance by K-shot, and 2) the insistence on diversity. For the former, we ensure that models that truly possess problem-solving abilities on K-shot are selected rather than those blind guessers. Besides, during data selection, instructions that share task-relevant contexts with K-shot are prioritized. For the latter, we highlight the diversity of constituting experts and that of the fine-tuning instructions throughout the model and data selection process. Extensive experimental results confirm the superiority of our approach over existing methods on utilization of open knowledge across various tasks. Our codes will be available at this https URL.

Translated Abstract:
대규모 언어 모델(LLM)이 특정 분야의 작업을 수행하기 위해 전문 지식을 기르는 건 보통 특정 용도로 조정이 필요해. 이때 원하는 안정적인 결과를 내는 행동을 조정하는 게 중요해. 그런데 수백 시간에 달하는 수작업 교육 데이터셋과 훈련 자원을 준비하는 데 드는 막대한 비용을 피하려면, 낮은 순위 적응(LoRA) 모델이나 교육 데이터셋 같은 공개 지식을 활용하는 게 좋은 출발점이야.

하지만 기존의 모델과 데이터 선택 방법은 일반적인 능력의 성능에만 집중하고, 특정 분야에 배치했을 때 드러나는 지식 격차는 간과하고 있어. 그래서 이번 연구에서는 몇 개의 인간 주석 샘플(즉, K-shot)을 도입해서 LLM의 작업 전문성을 높이는 방법을 제안해. 구체적으로, K-shot 데이터가 가장 유망한 전문가 후보와 작업 관련 지침을 선택하는 데 개입하는 효율적이고 확장 가능한 파이프라인을 개발했어.

여기서 혼합 전문가(MoE) 시스템을 만들어 여러 전문가 간의 개별적이면서도 보완적인 지식을 최대한 활용할 수 있게 했어. MoE 시스템의 성공을 위한 두 가지 핵심 요소는 1) K-shot 준수, 2) 다양성 강조야. 첫 번째로, K-shot에 대한 문제 해결 능력을 정말 갖춘 모델을 선택하고, 단순히 추측만 하는 모델은 배제해. 또한 데이터 선택 중에는 K-shot과 작업 관련 맥락이 공유되는 지침을 우선적으로 고려해.

두 번째로, 전문가 구성의 다양성과 모델 및 데이터 선택 과정에서 미세 조정 지침의 다양성을 강조했어. 다양한 작업에서 공개 지식 활용 측면에서 우리의 접근 방식이 기존 방법보다 우수하다는 걸 많은 실험 결과가 확인해 줬어. 우리의 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2408.16729
Title: Prediction-Feedback DETR for Temporal Action Detection

Original Abstract:
Temporal Action Detection (TAD) is fundamental yet challenging for real-world video applications. Leveraging the unique benefits of transformers, various DETR-based approaches have been adopted in TAD. However, it has recently been identified that the attention collapse in self-attention causes the performance degradation of DETR for TAD. Building upon previous research, this paper newly addresses the attention collapse problem in cross-attention within DETR-based TAD methods. Moreover, our findings reveal that cross-attention exhibits patterns distinct from predictions, indicating a short-cut phenomenon. To resolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR), which utilizes predictions to restore the collapse and align the cross- and self-attention with predictions. Specifically, we devise novel prediction-feedback objectives using guidance from the relations of the predictions. As a result, Pred-DETR significantly alleviates the collapse and achieves state-of-the-art performance among DETR-based methods on various challenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and FineAction.

Translated Abstract:
시간 행동 탐지(TAD)는 실제 비디오 애플리케이션에서 기본적이면서도 어려운 문제야. 트랜스포머의 독특한 장점을 활용해서, 여러 DETR 기반의 접근 방식이 TAD에 적용되고 있어. 하지만 최근에 자기 주의(self-attention)에서 발생하는 주의 붕괴(attention collapse)가 DETR의 성능 저하를 초래한다는 사실이 밝혀졌어.

이 논문은 이전 연구를 바탕으로 DETR 기반 TAD 방법에서 교차 주의(cross-attention)에서의 주의 붕괴 문제를 새롭게 다루고 있어. 그리고 우리의 발견은 교차 주의가 예측과는 다른 패턴을 보인다는 것을 보여주는데, 이는 일종의 단축키 현상(short-cut phenomenon)이라고 할 수 있어. 

이 문제를 해결하기 위해, 우리는 예측을 활용해 붕괴를 복원하고 교차 주의와 자기 주의를 예측에 맞추는 새로운 프레임워크인 Prediction-Feedback DETR(Pred-DETR)을 제안해. 구체적으로, 우리는 예측의 관계에서 얻은 가이드를 사용해 새로운 예측-피드백 목표를 고안했어. 

그 결과, Pred-DETR은 주의 붕괴를 크게 완화시키고, THUMOS14, ActivityNet-v1.3, HACS, FineAction 등 다양한 도전적인 벤치마크에서 DETR 기반 방법들 중 최첨단 성능을 달성했어.

================================================================================

URL: https://arxiv.org/abs/2408.17422
Title: Open-vocabulary Temporal Action Localization using VLMs

Original Abstract:
Video action localization aims to find timings of a specific action from a long video. Although existing learning-based approaches have been successful, those require annotating videos that come with a considerable labor cost. This paper proposes a learning-free, open-vocabulary approach based on emerging off-the-shelf vision-language models (VLM). The challenge stems from the fact that VLMs are neither designed to process long videos nor tailored for finding actions. We overcome these problems by extending an iterative visual prompting technique. Specifically, we sample video frames into a concatenated image with frame index labels, making a VLM guess a frame that is considered to be closest to the start/end of the action. Iterating this process by narrowing a sampling time window results in finding a specific frame of start and end of an action. We demonstrate that this sampling technique yields reasonable results, illustrating a practical extension of VLMs for understanding videos. A sample code is available at this https URL.

Translated Abstract:
비디오 액션 로컬라이제이션은 긴 비디오에서 특정 행동의 타이밍을 찾는 거야. 기존의 학습 기반 방법들은 성공적이긴 했지만, 비디오에 주석을 달아야 해서 많은 노동력이 필요해. 이 논문에서는 새로운 학습 없이 사용할 수 있는 오픈 어휘 접근 방식을 제안해. 이건 최신 비전-언어 모델(VLM)을 기반으로 하고 있어.

문제는 VLM이 긴 비디오를 처리하도록 설계되지 않았고, 행동을 찾는 데 맞춰져 있지 않다는 거야. 우리는 반복적인 비주얼 프롬프팅 기법을 확장해서 이 문제를 해결했어. 구체적으로, 비디오 프레임을 프레임 인덱스 라벨과 함께 연결된 이미지로 샘플링해서, VLM이 행동의 시작이나 끝에 가장 가까운 프레임을 추측하게 만들어. 이 과정을 샘플링 시간 창을 좁히면서 반복하면 특정 행동의 시작과 끝 프레임을 찾을 수 있어.

우리는 이 샘플링 기법이 괜찮은 결과를 준다는 걸 보여줬어. 이게 VLM을 비디오 이해에 실용적으로 확장하는 방법이라는 걸 설명해. 샘플 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.01199
Title: OD-VAE: An Omni-dimensional Video Compressor for Improving Latent Video Diffusion Model

Original Abstract:
Variational Autoencoder (VAE), compressing videos into latent representations, is a crucial preceding component of Latent Video Diffusion Models (LVDMs). With the same reconstruction quality, the more sufficient the VAE's compression for videos is, the more efficient the LVDMs are. However, most LVDMs utilize 2D image VAE, whose compression for videos is only in the spatial dimension and often ignored in the temporal dimension. How to conduct temporal compression for videos in a VAE to obtain more concise latent representations while promising accurate reconstruction is seldom explored. To fill this gap, we propose an omni-dimension compression VAE, named OD-VAE, which can temporally and spatially compress videos. Although OD-VAE's more sufficient compression brings a great challenge to video reconstruction, it can still achieve high reconstructed accuracy by our fine design. To obtain a better trade-off between video reconstruction quality and compression speed, four variants of OD-VAE are introduced and analyzed. In addition, a novel tail initialization is designed to train OD-VAE more efficiently, and a novel inference strategy is proposed to enable OD-VAE to handle videos of arbitrary length with limited GPU memory. Comprehensive experiments on video reconstruction and LVDM-based video generation demonstrate the effectiveness and efficiency of our proposed methods.

Translated Abstract:
변분 오토인코더(Variational Autoencoder, VAE)는 비디오를 잠재 표현으로 압축하는 중요한 과정으로, 잠재 비디오 확산 모델(Latent Video Diffusion Models, LVDMs)의 핵심 요소야. 같은 복원 품질을 유지하면서, VAE의 비디오 압축이 더 잘 될수록 LVDM의 효율성도 높아져. 그런데 대부분의 LVDM은 2D 이미지 VAE를 사용하는데, 이건 비디오의 압축이 공간 차원에서만 이루어지고 시간 차원은 종종 무시돼.

그래서 비디오의 시간 압축을 어떻게 VAE에서 수행할지, 더 간결한 잠재 표현을 얻으면서 정확한 복원을 보장하는 방법에 대해서는 잘 연구되지 않았어. 이를 해결하기 위해, 우리는 OD-VAE라는 전방향 압축 VAE를 제안해. 이건 비디오를 시간적 및 공간적으로 압축할 수 있어. OD-VAE가 더 나은 압축을 제공하지만, 비디오 복원에 큰 도전이 될 수 있어. 그럼에도 불구하고, 우리는 세심하게 설계해서 높은 복원 정확도를 달성할 수 있었어.

비디오 복원 품질과 압축 속도 사이의 더 좋은 균형을 찾기 위해 OD-VAE의 네 가지 변형을 도입하고 분석했어. 또한, OD-VAE를 더 효율적으로 훈련하기 위한 새로운 꼬리 초기화 방법을 설계했고, 제한된 GPU 메모리로 임의 길이의 비디오를 처리할 수 있는 새로운 추론 전략도 제안했어. 비디오 복원과 LVDM 기반 비디오 생성에 대한 실험 결과, 우리의 방법이 효과적이고 효율적임을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.01322
Title: Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free Real Image Editing

Original Abstract:
Despite recent advances in large-scale text-to-image generative models, manipulating real images with these models remains a challenging problem. The main limitations of existing editing methods are that they either fail to perform with consistent quality on a wide range of image edits or require time-consuming hyperparameter tuning or fine-tuning of the diffusion model to preserve the image-specific appearance of the input image. We propose a novel approach that is built upon a modified diffusion sampling process via the guidance mechanism. In this work, we explore the self-guidance technique to preserve the overall structure of the input image and its local regions appearance that should not be edited. In particular, we explicitly introduce layout-preserving energy functions that are aimed to save local and global structures of the source image. Additionally, we propose a noise rescaling mechanism that allows to preserve noise distribution by balancing the norms of classifier-free guidance and our proposed guiders during generation. Such a guiding approach does not require fine-tuning the diffusion model and exact inversion process. As a result, the proposed method provides a fast and high-quality editing mechanism. In our experiments, we show through human evaluation and quantitative analysis that the proposed method allows to produce desired editing which is more preferable by humans and also achieves a better trade-off between editing quality and preservation of the original image. Our code is available at this https URL.

Translated Abstract:
최근 대규모 텍스트-이미지 생성 모델이 발전했지만, 실제 이미지를 이 모델로 조작하는 건 여전히 어려운 문제야. 기존의 편집 방법들은 다양한 이미지 편집에서 일관된 품질을 유지하지 못하거나, 입력 이미지의 특정한 모습을 유지하기 위해 시간이 많이 걸리는 하이퍼파라미터 조정이나 확산 모델의 미세 조정이 필요해.

우리는 수정된 확산 샘플링 프로세스를 기반으로 한 새로운 접근 방식을 제안해. 이 작업에서는 입력 이미지의 전체 구조와 편집하지 말아야 할 지역의 모습을 유지하기 위해 자기 안내 기술을 탐구해. 특히, 원본 이미지의 지역적 및 전역적 구조를 보존하기 위해 레이아웃 보존 에너지 함수를 도입했어.

또한, 생성 과정에서 분류기 없는 안내와 우리가 제안한 안내자의 노름을 조화롭게 맞춰서 노이즈 분포를 유지할 수 있는 노이즈 재조정 메커니즘도 제안해. 이런 안내 방식은 확산 모델의 미세 조정이나 정확한 역전 과정이 필요하지 않아. 결과적으로, 제안한 방법은 빠르고 고품질의 편집 메커니즘을 제공해.

우리의 실험에서는 사람 평가와 정량적 분석을 통해, 제안한 방법이 사람들에게 더 선호되는 원하는 편집을 생성할 수 있고, 편집 품질과 원본 이미지 보존 사이에서 더 나은 균형을 이룬다는 걸 보여줬어. 우리의 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.01672
Title: Enhancing Fine-Grained Visual Recognition in the Low-Data Regime Through Feature Magnitude Regularization

Original Abstract:
Training a fine-grained image recognition model with limited data presents a significant challenge, as the subtle differences between categories may not be easily discernible amidst distracting noise patterns. One commonly employed strategy is to leverage pretrained neural networks, which can generate effective feature representations for constructing an image classification model with a restricted dataset. However, these pretrained neural networks are typically trained for different tasks than the fine-grained visual recognition (FGVR) task at hand, which can lead to the extraction of less relevant features. Moreover, in the context of building FGVR models with limited data, these irrelevant features can dominate the training process, overshadowing more useful, generalizable discriminative features. Our research has identified a surprisingly simple solution to this challenge: we introduce a regularization technique to ensure that the magnitudes of the extracted features are evenly distributed. This regularization is achieved by maximizing the uniformity of feature magnitude distribution, measured through the entropy of the normalized features. The motivation behind this regularization is to remove bias in feature magnitudes from pretrained models, where some features may be more prominent and, consequently, more likely to be used for classification. Additionally, we have developed a dynamic weighting mechanism to adjust the strength of this regularization throughout the learning process. Despite its apparent simplicity, our approach has demonstrated significant performance improvements across various fine-grained visual recognition datasets.

Translated Abstract:
제한된 데이터로 세부 이미지 인식 모델을 훈련하는 건 큰 도전이야. 카테고리 간의 미세한 차이가 방해가 되는 노이즈 속에서 쉽게 구분되지 않을 수 있거든. 일반적으로 사용하는 방법 중 하나는 사전 훈련된 신경망을 활용하는 건데, 이걸로 제한된 데이터셋으로 이미지 분류 모델을 만드는 데 필요한 효과적인 특징 표현을 생성할 수 있어.

하지만 이 사전 훈련된 신경망은 세부 시각 인식(FGVR) 작업과는 다른 작업을 위해 훈련된 경우가 많아서, 덜 관련 있는 특징을 추출할 수 있어. 게다가 제한된 데이터로 FGVR 모델을 만들 때, 이런 관련 없는 특징들이 훈련 과정에서 지배적이 되어 더 유용하고 일반화 가능한 구별 특징들을 가릴 수 있어.

우리 연구에서는 이 문제에 대해 놀랍도록 간단한 해결책을 찾았어: 추출된 특징의 크기가 고르게 분포되도록 하는 정규화 기법을 도입했어. 이 정규화는 정규화된 특징의 엔트로피를 통해 특징 크기 분포의 균일성을 극대화함으로써 이루어져. 이 정규화의 목적은 사전 훈련된 모델에서 특징 크기에 있는 편향을 제거하는 거야. 어떤 특징은 더 두드러지게 나타나고, 그래서 분류에 더 많이 사용될 가능성이 높거든.

또한, 우리는 학습 과정에서 이 정규화의 강도를 조정할 수 있는 동적 가중치 메커니즘도 개발했어. 이 방법은 겉보기에는 간단해 보이지만, 여러 세부 시각 인식 데이터셋에서 성능이 크게 개선되는 결과를 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2409.01835
Title: Towards Generative Class Prompt Learning for Fine-grained Visual Recognition

Original Abstract:
Although foundational vision-language models (VLMs) have proven to be very successful for various semantic discrimination tasks, they still struggle to perform faithfully for fine-grained categorization. Moreover, foundational models trained on one domain do not generalize well on a different domain without fine-tuning. We attribute these to the limitations of the VLM's semantic representations and attempt to improve their fine-grained visual awareness using generative modeling. Specifically, we propose two novel methods: Generative Class Prompt Learning (GCPL) and Contrastive Multi-class Prompt Learning (CoMPLe). Utilizing text-to-image diffusion models, GCPL significantly improves the visio-linguistic synergy in class embeddings by conditioning on few-shot exemplars with learnable class prompts. CoMPLe builds on this foundation by introducing a contrastive learning component that encourages inter-class separation during the generative optimization process. Our empirical results demonstrate that such a generative class prompt learning approach substantially outperform existing methods, offering a better alternative to few shot image recognition challenges. The source code will be made available at: this https URL.

Translated Abstract:
기초적인 비전-언어 모델(VLM)은 여러 가지 의미 구분 작업에서 매우 성공적이긴 하지만, 세밀한 범주화 작업에서는 여전히 어려움을 겪고 있어. 게다가, 한 도메인에서 훈련된 기초 모델은 다른 도메인에선 잘 일반화되지 않고, 세부 조정이 필요해. 우리는 이런 문제를 VLM의 의미 표현의 한계 때문이라고 생각하고, 생성 모델링을 이용해 세밀한 시각 인식을 개선하려고 해.

특히, 우리는 두 가지 새로운 방법을 제안해: 생성 클래스 프롬프트 학습(GCPL)과 대조 다중 클래스 프롬프트 학습(CoMPLe)야. GCPL은 텍스트-이미지 확산 모델을 활용해서, 학습 가능한 클래스 프롬프트로 몇 개의 샘플에 맞춰 클래스 임베딩에서 시각-언어적 시너지를 크게 개선해. CoMPLe는 생성 최적화 과정에서 클래스 간 분리를 촉진하는 대조 학습 요소를 도입해서 이 기반 위에 구축돼.

우리의 실험 결과는 이런 생성 클래스 프롬프트 학습 방식이 기존 방법들보다 훨씬 더 뛰어난 성능을 보이며, 몇 개의 샷으로 이미지 인식 문제를 해결하는 데 더 나은 대안이 된다는 걸 보여줘. 소스 코드는 이 URL에서 사용할 수 있을 거야.

================================================================================

URL: https://arxiv.org/abs/2409.02371
Title: Unfolding Videos Dynamics via Taylor Expansion

Original Abstract:
Taking inspiration from physical motion, we present a new self-supervised dynamics learning strategy for videos: Video Time-Differentiation for Instance Discrimination (ViDiDi). ViDiDi is a simple and data-efficient strategy, readily applicable to existing self-supervised video representation learning frameworks based on instance discrimination. At its core, ViDiDi observes different aspects of a video through various orders of temporal derivatives of its frame sequence. These derivatives, along with the original frames, support the Taylor series expansion of the underlying continuous dynamics at discrete times, where higher-order derivatives emphasize higher-order motion features. ViDiDi learns a single neural network that encodes a video and its temporal derivatives into consistent embeddings following a balanced alternating learning algorithm. By learning consistent representations for original frames and derivatives, the encoder is steered to emphasize motion features over static backgrounds and uncover the hidden dynamics in original frames. Hence, video representations are better separated by dynamic features. We integrate ViDiDi into existing instance discrimination frameworks (VICReg, BYOL, and SimCLR) for pretraining on UCF101 or Kinetics and test on standard benchmarks including video retrieval, action recognition, and action detection. The performances are enhanced by a significant margin without the need for large models or extensive datasets.

Translated Abstract:
물리적 움직임에서 영감을 받아, 우리는 비디오를 위한 새로운 자기 지도 동역학 학습 전략인 '비디오 시간 미분(instance discrimination을 위한 ViDiDi)'를 제안해. ViDiDi는 간단하고 데이터 효율적인 전략으로, 기존의 자기 지도 비디오 표현 학습 프레임워크에 쉽게 적용할 수 있어. 

ViDiDi의 핵심은 비디오의 프레임 시퀀스에서 다양한 시간 미분의 순서를 통해 비디오의 여러 측면을 관찰하는 거야. 이 미분 값들과 원래의 프레임들은 이산 시간에서의 연속 동역학의 테일러 급수 전개를 지원해. 높은 차수의 미분은 더 높은 차수의 움직임 특징을 강조해. 

ViDiDi는 비디오와 그 시간 미분을 일관된 임베딩으로 인코딩하는 단일 신경망을 학습해. 균형 잡힌 교대 학습 알고리즘을 따라 원래 프레임과 미분에 대한 일관된 표현을 학습함으로써, 인코더는 정적 배경보다 움직임 특징을 강조하도록 유도되고, 원래 프레임의 숨겨진 동역학을 드러내. 이렇게 해서 비디오 표현은 동적 특징에 의해 더 잘 구분되게 돼. 

우리는 ViDiDi를 기존의 인스턴스 차별화 프레임워크(VICReg, BYOL, SimCLR)에 통합해서 UCF101이나 Kinetics에서 사전 학습을 하고, 비디오 검색, 행동 인식, 행동 탐지 같은 표준 벤치마크에서 테스트했어. 그 결과, 큰 모델이나 방대한 데이터셋 없이도 성능이 크게 향상됐어.

================================================================================

URL: https://arxiv.org/abs/2409.02919
Title: HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts

Original Abstract:
The potential for higher-resolution image generation using pretrained diffusion models is immense, yet these models often struggle with issues of object repetition and structural artifacts especially when scaling to 4K resolution and higher. We figure out that the problem is caused by that, a single prompt for the generation of multiple scales provides insufficient efficacy. In response, we propose HiPrompt, a new tuning-free solution that tackles the above problems by introducing hierarchical prompts. The hierarchical prompts offer both global and local guidance. Specifically, the global guidance comes from the user input that describes the overall content, while the local guidance utilizes patch-wise descriptions from MLLMs to elaborately guide the regional structure and texture generation. Furthermore, during the inverse denoising process, the generated noise is decomposed into low- and high-frequency spatial components. These components are conditioned on multiple prompt levels, including detailed patch-wise descriptions and broader image-level prompts, facilitating prompt-guided denoising under hierarchical semantic guidance. It further allows the generation to focus more on local spatial regions and ensures the generated images maintain coherent local and global semantics, structures, and textures with high definition. Extensive experiments demonstrate that HiPrompt outperforms state-of-the-art works in higher-resolution image generation, significantly reducing object repetition and enhancing structural quality.

Translated Abstract:
미리 훈련된 확산 모델을 사용한 고해상도 이미지 생성의 가능성은 엄청나지만, 이런 모델들은 4K 해상도 이상으로 확대할 때 물체 반복과 구조적 아티팩트 문제를 자주 겪어. 우리는 여러 크기에서 이미지를 생성할 때 하나의 프롬프트만 사용하는 게 효과적이지 않다는 걸 발견했어.

그래서 우리는 HiPrompt라는 새로운 해결책을 제안해. 이건 위에서 언급한 문제들을 해결하기 위해 계층적 프롬프트를 도입한 거야. 계층적 프롬프트는 전반적인 안내와 지역적인 안내를 모두 제공해. 구체적으로, 전반적인 안내는 사용자가 입력한 전체 내용에 대한 설명에서 오고, 지역적인 안내는 MLLMs의 패치별 설명을 활용해서 지역 구조와 텍스처 생성을 세밀하게 안내해.

게다가, 역 노이즈 제거 과정에서 생성된 노이즈는 저주파 및 고주파 공간 구성 요소로 분해돼. 이 구성 요소들은 상세한 패치별 설명과 넓은 이미지 수준의 프롬프트를 포함한 여러 프롬프트 수준에 따라 조정돼, 계층적 의미 안내 아래에서 프롬프트 기반 노이즈 제거를 가능하게 해. 이를 통해 생성 과정이 지역 공간 영역에 더 집중할 수 있게 되고, 생성된 이미지가 고해상도로 일관된 지역 및 전반적인 의미, 구조, 텍스처를 유지하도록 해.

많은 실험 결과, HiPrompt는 고해상도 이미지 생성에서 최첨단 기술보다 더 뛰어난 성능을 보여주고, 물체 반복을 크게 줄이며 구조적 품질을 향상시켜.

================================================================================

URL: https://arxiv.org/abs/2409.03420
Title: mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding

Original Abstract:
Multimodel Large Language Models(MLLMs) have achieved promising OCR-free Document Understanding performance by increasing the supported resolution of document images. However, this comes at the cost of generating thousands of visual tokens for a single document image, leading to excessive GPU memory and slower inference times, particularly in multi-page document comprehension. In this work, to address these challenges, we propose a High-resolution DocCompressor module to compress each high-resolution document image into 324 tokens, guided by low-resolution global visual features. With this compression module, to strengthen multi-page document comprehension ability and balance both token efficiency and question-answering performance, we develop the DocOwl2 under a three-stage training framework: Single-image Pretraining, Multi-image Continue-pretraining, and Multi-task Finetuning. DocOwl2 sets a new state-of-the-art across multi-page document understanding benchmarks and reduces first token latency by more than 50%, demonstrating advanced capabilities in multi-page questioning answering, explanation with evidence pages, and cross-page structure understanding. Additionally, compared to single-image MLLMs trained on similar data, our DocOwl2 achieves comparable single-page understanding performance with less than 20% of the visual tokens. Our codes, models, and data are publicly available at this https URL.

Translated Abstract:
다중모델 대형 언어 모델(MLLMs)은 문서 이미지의 해상도를 높여 OCR 없이도 문서를 이해하는 성능이 좋다는 것을 보여줬어. 하지만 이 방법은 한 문서 이미지에 수천 개의 시각적 토큰을 생성하게 되어서, GPU 메모리가 너무 많이 소모되고 여러 페이지 문서를 이해할 때 속도가 느려지는 단점이 있어.

이 문제를 해결하기 위해, 우리는 고해상도 문서 이미지를 324개의 토큰으로 압축하는 'High-resolution DocCompressor' 모듈을 제안했어. 이 모듈은 저해상도 전역 시각적 특징을 기반으로 압축을 진행해. 이 압축 모듈을 사용해서 다중 페이지 문서 이해 능력을 강화하고, 토큰 효율성과 질문-답변 성능 간의 균형을 맞추기 위해 'DocOwl2'를 개발했어. 이 모델은 세 단계의 훈련 프레임워크를 따르는데, 각각 단일 이미지 사전 훈련, 다중 이미지 계속 사전 훈련, 다중 작업 미세 조정이야.

DocOwl2는 다중 페이지 문서 이해 벤치마크에서 새로운 최첨단 성능을 기록했고, 첫 번째 토큰 지연 시간을 50% 이상 줄였어. 이로써 다중 페이지 질문 답변, 증거 페이지를 통한 설명, 페이지 간 구조 이해에서 뛰어난 능력을 보여줬어. 게다가, 비슷한 데이터로 훈련된 단일 이미지 MLLMs와 비교했을 때, DocOwl2는 20%도 안 되는 시각적 토큰으로도 유사한 단일 페이지 이해 성능을 달성했어. 우리 코드, 모델, 데이터는 이 URL에서 공개되어 있어.

================================================================================

URL: https://arxiv.org/abs/2409.03431
Title: UV-Mamba: A DCN-Enhanced State Space Model for Urban Village Boundary Identification in High-Resolution Remote Sensing Images

Original Abstract:
Due to the diverse geographical environments, intricate landscapes, and high-density settlements, the automatic identification of urban village boundaries using remote sensing images remains a highly challenging task. This paper proposes a novel and efficient neural network model called UV-Mamba for accurate boundary detection in high-resolution remote sensing images. UV-Mamba mitigates the memory loss problem in lengthy sequence modeling, which arises in state space models with increasing image size, by incorporating deformable convolutions. Its architecture utilizes an encoder-decoder framework and includes an encoder with four deformable state space augmentation blocks for efficient multi-level semantic extraction and a decoder to integrate the extracted semantic information. We conducted experiments on two large datasets showing that UV-Mamba achieves state-of-the-art performance. Specifically, our model achieves 73.3% and 78.1% IoU on the Beijing and Xi'an datasets, respectively, representing improvements of 1.2% and 3.4% IoU over the previous best model while also being 6x faster in inference speed and 40x smaller in parameter count. Source code and pre-trained models are available at this https URL.

Translated Abstract:
도시 마을 경계를 자동으로 식별하는 건 원격 감지 이미지 덕분에 진짜 어려운 일이야. 다양한 지리적 환경, 복잡한 경관, 인구 밀도가 높은 지역 때문에 더더욱 그렇지. 이 논문에서는 UV-Mamba라는 새로운 신경망 모델을 제안해. 이 모델은 고해상도 원격 감지 이미지에서 경계를 정확하게 찾을 수 있게 도와줘.

UV-Mamba는 긴 시퀀스를 모델링할 때 생기는 메모리 손실 문제를 해결해. 이 문제는 이미지 크기가 커지면서 발생하는데, 변형 가능한 합성곱을 사용해서 해결해. 구조는 인코더-디코더 프레임워크를 사용하고, 효율적인 다단계 의미 추출을 위해 네 개의 변형 가능한 상태 공간 증강 블록이 있는 인코더와 추출한 의미 정보를 통합하는 디코더가 포함돼 있어.

우리는 두 개의 큰 데이터셋에서 실험을 했고, UV-Mamba가 최신 기술 중 가장 좋은 성능을 보인다는 걸 확인했어. 구체적으로, 우리 모델은 베이징 데이터셋에서 73.3%의 IoU, 시안 데이터셋에서 78.1%의 IoU를 달성했어. 이건 이전 모델보다 각각 1.2%와 3.4% 더 높은 수치야. 게다가 추론 속도는 6배 빠르고, 파라미터 수는 40배 적어. 소스 코드와 미리 학습된 모델은 이 https URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.03777
Title: A Greedy Hierarchical Approach to Whole-Network Filter-Pruning in CNNs

Original Abstract:
Deep convolutional neural networks (CNNs) have achieved impressive performance in many computer vision tasks. However, their large model sizes require heavy computational resources, making pruning redundant filters from existing pre-trained CNNs an essential task in developing efficient models for resource-constrained devices. Whole-network filter pruning algorithms prune varying fractions of filters from each layer, hence providing greater flexibility. Current whole-network pruning methods are either computationally expensive due to the need to calculate the loss for each pruned filter using a training dataset, or use various heuristic / learned criteria for determining the pruning fractions for each layer. This paper proposes a two-level hierarchical approach for whole-network filter pruning which is efficient and uses the classification loss as the final criterion. The lower-level algorithm (called filter-pruning) uses a sparse-approximation formulation based on linear approximation of filter weights. We explore two algorithms: orthogonal matching pursuit-based greedy selection and a greedy backward pruning approach. The backward pruning algorithm uses a novel closed-form error criterion for efficiently selecting the optimal filter at each stage, thus making the whole algorithm much faster. The higher-level algorithm (called layer-selection) greedily selects the best-pruned layer (pruning using the filter-selection algorithm) using a global pruning criterion. We propose algorithms for two different global-pruning criteria: (1) layer-wise relative error (HBGS), and (2) final classification error (HBGTS). Our suite of algorithms outperforms state-of-the-art pruning methods on ResNet18, ResNet32, ResNet56, VGG16, and ResNext101. Our method reduces the RAM requirement for ResNext101 from 7.6 GB to 1.5 GB and achieves a 94% reduction in FLOPS without losing accuracy on CIFAR-10.

Translated Abstract:
딥 컨볼루션 신경망(CNN)은 여러 컴퓨터 비전 작업에서 뛰어난 성능을 보여줬어. 하지만 모델 크기가 크기 때문에 많은 계산 자원이 필요해. 그래서 기존에 훈련된 CNN에서 불필요한 필터를 잘라내는 게 자원이 제한된 장치에서 효율적인 모델을 개발하는 데 꼭 필요해.

전체 네트워크 필터 프루닝 알고리즘은 각 레이어에서 다양한 비율로 필터를 잘라내서 더 유연함을 제공해. 현재의 전체 네트워크 프루닝 방법은 훈련 데이터셋을 이용해 잘라낸 필터마다 손실을 계산해야 해서 계산 비용이 많이 들거나, 각 레이어의 프루닝 비율을 정하기 위해 여러 가지 휴리스틱 또는 학습된 기준을 사용해.

이 논문에서는 효율적이고 분류 손실을 최종 기준으로 사용하는 두 단계 계층적 접근 방식을 제안해. 하위 알고리즘(필터 프루닝이라고 불림)은 필터 가중치의 선형 근사를 기반으로 한 희소 근사 형식을 사용해. 우리는 두 가지 알고리즘을 탐구했어: 직교 매칭 추구 기반 탐욕적 선택과 탐욕적 역방향 프루닝 접근법. 역방향 프루닝 알고리즘은 각 단계에서 최적의 필터를 효율적으로 선택하기 위해 새로운 닫힌 형태의 오류 기준을 사용해서 알고리즘이 훨씬 빨라졌어.

상위 알고리즘(레이어 선택이라고 불림)은 글로벌 프루닝 기준을 이용해 가장 잘 잘린 레이어를 탐욕적으로 선택해. 우리는 두 가지 다른 글로벌 프루닝 기준에 대한 알고리즘을 제안해: (1) 레이어별 상대 오류(HBGS), (2) 최종 분류 오류(HBGTS). 우리의 알고리즘 모음은 ResNet18, ResNet32, ResNet56, VGG16, ResNext101에서 최신 프루닝 방법을 능가해. 우리의 방법은 ResNext101의 RAM 요구량을 7.6GB에서 1.5GB로 줄이고, CIFAR-10에서 정확도를 잃지 않으면서 FLOPS를 94% 줄여.

================================================================================

URL: https://arxiv.org/abs/2409.03887
Title: The Influence of Faulty Labels in Data Sets on Human Pose Estimation

Original Abstract:
In this study we provide empirical evidence demonstrating that the quality of training data impacts model performance in Human Pose Estimation (HPE). Inaccurate labels in widely used data sets, ranging from minor errors to severe mislabeling, can negatively influence learning and distort performance metrics. We perform an in-depth analysis of popular HPE data sets to show the extent and nature of label inaccuracies. Our findings suggest that accounting for the impact of faulty labels will facilitate the development of more robust and accurate HPE models for a variety of real-world applications. We show improved performance with cleansed data.

Translated Abstract:
이번 연구에서는 훈련 데이터의 질이 인간 자세 추정(HPE) 모델 성능에 미치는 영향을 실증적으로 보여줍니다. 많이 사용되는 데이터 세트에서 부정확한 레이블이 있는 경우, 사소한 오류부터 심각한 잘못된 레이블까지, 학습에 부정적인 영향을 주고 성능 지표를 왜곡할 수 있습니다.

우리는 인기 있는 HPE 데이터 세트를 깊이 분석해서 레이블 부정확성의 정도와 성격을 보여줍니다. 우리의 연구 결과에 따르면, 잘못된 레이블의 영향을 고려하면 다양한 실제 응용을 위한 더 강력하고 정확한 HPE 모델 개발에 도움이 될 것입니다. 

우리는 정제된 데이터를 사용했을 때 성능이 향상되는 것을 보여줍니다.

================================================================================

URL: https://arxiv.org/abs/2409.04214
Title: Diagram Formalization Enhanced Multi-Modal Geometry Problem Solver

Original Abstract:
Mathematical reasoning remains an ongoing challenge for AI models, especially for geometry problems that require both linguistic and visual signals. As the vision encoders of most MLLMs are trained on natural scenes, they often struggle to understand geometric diagrams, performing no better in geometry problem solving than LLMs that only process text. This limitation is amplified by the lack of effective methods for representing geometric relationships. To address these issues, we introduce the Diagram Formalization Enhanced Geometry Problem Solver (DFE-GPS), a new framework that integrates visual features, geometric formal language, and natural language representations. We propose a novel synthetic data approach and create a large-scale geometric dataset, SynthGeo228K, annotated with both formal and natural language captions, designed to enhance the vision encoder for a better understanding of geometric structures. Our framework improves MLLMs' ability to process geometric diagrams and extends their application to open-ended tasks on the formalgeo7k dataset.

Translated Abstract:
수학적 추론은 AI 모델에게 여전히 힘든 문제야. 특히 기하학 문제는 언어적 신호와 시각적 신호를 모두 필요로 해서 더 어렵지. 대부분의 다중 모달 언어 모델(MLLMs)의 비전 인코더는 자연 장면을 기준으로 훈련되기 때문에, 기하학 도형을 이해하는 데 어려움을 겪어. 그래서 텍스트만 처리하는 대형 언어 모델(LLMs)과 비슷한 성능밖에 내지 못해.

이런 한계를 극복하기 위해 우리는 Diagram Formalization Enhanced Geometry Problem Solver (DFE-GPS)라는 새로운 프레임워크를 소개해. 이 프레임워크는 시각적 특징, 기하학적 형식 언어, 자연어 표현을 통합해. 우리는 새로운 합성 데이터 접근 방식을 제안하고, 기하학 구조를 더 잘 이해할 수 있도록 돕기 위해 공식 언어와 자연어 캡션이 주석 처리된 대규모 기하학 데이터셋인 SynthGeo228K를 만들었어. 

이 프레임워크는 MLLMs가 기하학 도형을 처리하는 능력을 향상시키고, formalgeo7k 데이터셋에서 개방형 작업으로의 적용을 확장할 수 있도록 해.

================================================================================

URL: https://arxiv.org/abs/2409.04388
Title: Question-Answering Dense Video Events

Original Abstract:
Multimodal Large Language Models (MLLMs) have shown excellent performance in question-answering of single-event videos. In this paper, we present question-answering dense video events, a novel task that requires answering and grounding the dense-event questions in long videos, thus challenging MLLMs to faithfully comprehend and reason about multiple events occurring over extended time periods. To facilitate the study, we construct DeVE-QA - a dataset featuring 78K questions about 26K events on 10.6K long videos. We then benchmark and show that existing MLLMs excelling at single-event QA struggle to perform well in DeVE-QA. For improvement, we propose DeVi, a novel training-free MLLM approach that highlights a hierarchical captioning module, a temporal event memory module, and a self-consistency checking module to respectively detect, contextualize and memorize, and ground dense-events in long videos for question answering. Extensive experiments show that DeVi is superior at answering dense-event questions and grounding relevant video moments. Compared with existing MLLMs, it achieves a remarkable increase of 4.1 percent and 3.7 percent for G(round)QA accuracy on DeVE-QA and NExT-GQA respectively.

Translated Abstract:
다중 모달 대형 언어 모델(MLLMs)은 단일 이벤트 비디오에서 질문에 답하는 데 아주 뛰어난 성능을 보였어. 이 논문에서는 긴 비디오에서 복잡한 이벤트에 대한 질문에 답하고 그 질문을 실제 영상에 맞추는 새로운 작업인 질문-답변 밀집 비디오 이벤트를 제안해. 이 작업은 MLLMs가 긴 시간 동안 발생하는 여러 이벤트를 제대로 이해하고 추론하는 것을 도전하게 해.

연구를 돕기 위해, 우리는 DeVE-QA라는 데이터셋을 만들었어. 이 데이터셋은 10.6K의 긴 비디오에서 26K의 이벤트에 대한 78K 질문을 포함하고 있어. 그런 다음, 기존의 MLLM들이 단일 이벤트 질문에서는 잘 성능을 내지만, DeVE-QA에서는 잘 못하는 걸 보여줬어.

개선을 위해 우리는 DeVi라는 새로운 훈련 없는 MLLM 접근 방식을 제안해. DeVi는 계층적 캡셔닝 모듈, 시간 이벤트 메모리 모듈, 자기 일관성 확인 모듈을 강조해. 이 모듈들은 각각 긴 비디오에서 질문에 답하기 위해 밀집 이벤트를 감지하고, 맥락을 제공하고, 기억하며, 실제 영상에 맞추는 역할을 해.

광범위한 실험 결과, DeVi는 밀집 이벤트 질문에 답하고 관련 비디오 순간을 잘 맞추는 데 뛰어난 성능을 보여줬어. 기존 MLLM들과 비교했을 때, DeVE-QA와 NExT-GQA에서 각각 4.1%와 3.7%의 정확도 향상을 기록했어.

================================================================================

URL: https://arxiv.org/abs/2409.04398
Title: HiSC4D: Human-centered interaction and 4D Scene Capture in Large-scale Space Using Wearable IMUs and LiDAR

Original Abstract:
We introduce HiSC4D, a novel Human-centered interaction and 4D Scene Capture method, aimed at accurately and efficiently creating a dynamic digital world, containing large-scale indoor-outdoor scenes, diverse human motions, rich human-human interactions, and human-environment interactions. By utilizing body-mounted IMUs and a head-mounted LiDAR, HiSC4D can capture egocentric human motions in unconstrained space without the need for external devices and pre-built maps. This affords great flexibility and accessibility for human-centered interaction and 4D scene capturing in various environments. Taking into account that IMUs can capture human spatially unrestricted poses but are prone to drifting for long-period using, and while LiDAR is stable for global localization but rough for local positions and orientations, HiSC4D employs a joint optimization method, harmonizing all sensors and utilizing environment cues, yielding promising results for long-term capture in large scenes. To promote research of egocentric human interaction in large scenes and facilitate downstream tasks, we also present a dataset, containing 8 sequences in 4 large scenes (200 to 5,000 $m^2$), providing 36k frames of accurate 4D human motions with SMPL annotations and dynamic scenes, 31k frames of cropped human point clouds, and scene mesh of the environment. A variety of scenarios, such as the basketball gym and commercial street, alongside challenging human motions, such as daily greeting, one-on-one basketball playing, and tour guiding, demonstrate the effectiveness and the generalization ability of HiSC4D. The dataset and code will be publicated on this http URL available for research purposes.

Translated Abstract:
HiSC4D라는 새로운 방법을 소개해. 이건 사람 중심의 상호작용과 4D 장면 캡처를 위한 방법이야. 목표는 대규모 실내외 장면, 다양한 인간 동작, 풍부한 인간 간 상호작용, 그리고 인간과 환경 간의 상호작용을 정확하고 효율적으로 만들어내는 거야.

HiSC4D는 몸에 부착하는 IMU와 머리에 착용하는 LiDAR를 활용해서, 외부 장치나 미리 만들어진 지도가 없이도 자유로운 공간에서 인간의 동작을 캡처할 수 있어. 이 덕분에 다양한 환경에서 인간 중심의 상호작용과 4D 장면 캡처를 쉽게 할 수 있어.

IMU는 공간적으로 제약 없는 인간의 자세를 캡처할 수 있지만, 오랜 시간 사용할 때 드리프트가 발생하기 쉬워. 반면에 LiDAR는 전역 위치 파악에는 안정적이지만, 지역 위치와 방향을 잡는 데는 좀 부정확해. 그래서 HiSC4D는 센서들을 조화롭게 최적화하는 방법을 써서 환경 정보를 활용하고, 큰 장면에서 장기간 캡처를 위한 좋은 결과를 얻고 있어.

또한, 대규모 장면에서의 인간 상호작용 연구를 촉진하고 후속 작업을 지원하기 위해, 4개의 큰 장면(200에서 5,000 $m^2$)에서 8개의 시퀀스를 포함하는 데이터셋도 제공해. 이 데이터셋은 SMPL 주석이 달린 36,000 프레임의 정확한 4D 인간 동작과 동적인 장면, 잘라낸 인간 포인트 클라우드의 31,000 프레임, 그리고 환경의 장면 메쉬를 포함하고 있어.

농구 체육관이나 상업 거리 같은 다양한 시나리오와 일상적인 인사, 1대1 농구 경기, 투어 가이드 같은 도전적인 인간 동작들이 HiSC4D의 효과성과 일반화 능력을 보여줘. 데이터셋과 코드는 연구 목적으로 사용할 수 있도록 공개될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2010.15775
Title: Understanding the Failure Modes of Out-of-Distribution Generalization

Original Abstract:
Empirical studies suggest that machine learning models often rely on features, such as the background, that may be spuriously correlated with the label only during training time, resulting in poor accuracy during test-time. In this work, we identify the fundamental factors that give rise to this behavior, by explaining why models fail this way {\em even} in easy-to-learn tasks where one would expect these models to succeed. In particular, through a theoretical study of gradient-descent-trained linear classifiers on some easy-to-learn tasks, we uncover two complementary failure modes. These modes arise from how spurious correlations induce two kinds of skews in the data: one geometric in nature, and another, statistical in nature. Finally, we construct natural modifications of image classification datasets to understand when these failure modes can arise in practice. We also design experiments to isolate the two failure modes when training modern neural networks on these datasets.

Translated Abstract:
경험적인 연구에 따르면, 머신러닝 모델은 종종 배경 같은 특징에 의존하는데, 이 특징은 학습할 때만 레이블과 잘못 연결되어 있는 경우가 많아. 그래서 테스트할 때 정확도가 낮아지는 거야. 

이 연구에서는 그런 행동이 발생하는 근본적인 이유를 알아봤어. 왜 모델이 쉽게 배울 수 있는 작업에서도 실패하는지를 설명하는 거지. 특히, 쉽게 배울 수 있는 몇 가지 작업에서 그래디언트 하강법으로 훈련된 선형 분류기를 이론적으로 연구하면서 두 가지 보완적인 실패 모드를 발견했어. 

이 두 모드는 잘못된 상관관계가 데이터에서 두 가지 왜곡을 유발하는 방식에서 비롯돼. 하나는 기하학적인 왜곡이고, 다른 하나는 통계적인 왜곡이야. 마지막으로, 이미지 분류 데이터셋의 자연스러운 수정을 만들어서 이러한 실패 모드가 실제로 언제 발생할 수 있는지를 이해하려고 했어. 또한, 현대 신경망을 이 데이터셋으로 훈련할 때 두 가지 실패 모드를 따로 분리할 수 있는 실험도 설계했어.

================================================================================

URL: https://arxiv.org/abs/2304.10985
Title: INK: Inheritable Natural Backdoor Attack Against Model Distillation

Original Abstract:
Deep learning models are vulnerable to backdoor attacks, where attackers inject malicious behavior through data poisoning and later exploit triggers to manipulate deployed models. To improve the stealth and effectiveness of backdoors, prior studies have introduced various imperceptible attack methods targeting both defense mechanisms and manual inspection. However, all poisoning-based attacks still rely on privileged access to the training dataset. Consequently, model distillation using a trusted dataset has emerged as an effective defense against these attacks. To bridge this gap, we introduce INK, an inheritable natural backdoor attack that targets model distillation. The key insight behind INK is the use of naturally occurring statistical features in all datasets, allowing attackers to leverage them as backdoor triggers without direct access to the training data. Specifically, INK employs image variance as a backdoor trigger and enables both clean-image and clean-label attacks by manipulating the labels and image variance in an unauthenticated dataset. Once the backdoor is embedded, it transfers from the teacher model to the student model, even when defenders use a trusted dataset for distillation. Theoretical analysis and experimental results demonstrate the robustness of INK against transformation-based, search-based, and distillation-based defenses. For instance, INK maintains an attack success rate of over 98\% post-distillation, compared to an average success rate of 1.4\% for existing methods.

Translated Abstract:
딥러닝 모델은 백도어 공격에 취약해. 이 공격은 공격자가 데이터에 악성 행동을 주입하고, 나중에 그 트리거를 이용해 배포된 모델을 조작하는 방식이야. 백도어의 은밀함과 효과를 높이기 위해, 이전 연구들은 방어 메커니즘과 수동 검사를 겨냥한 다양한 눈에 띄지 않는 공격 방법들을 소개했어. 하지만 모든 데이터 오염 기반 공격은 여전히 훈련 데이터셋에 대한 특권 접근이 필요해.

그래서 신뢰할 수 있는 데이터셋을 이용한 모델 증류가 이런 공격에 대한 효과적인 방어 방법으로 떠올랐어. 이 간극을 메우기 위해, 우리는 INK라는 상속 가능한 자연 백도어 공격을 소개해. INK의 핵심 아이디어는 모든 데이터셋에 자연적으로 발생하는 통계적 특성을 사용하는 거야. 이렇게 해서 공격자는 훈련 데이터에 직접 접근하지 않고도 백도어 트리거를 활용할 수 있어.

구체적으로 INK는 이미지 분산을 백도어 트리거로 사용하고, 인증되지 않은 데이터셋에서 레이블과 이미지 분산을 조작해 깨끗한 이미지와 깨끗한 레이블 공격을 가능하게 해. 백도어가 삽입되면, 교사 모델에서 학생 모델로 전이돼. 심지어 방어자가 신뢰할 수 있는 데이터셋을 사용하더라도 말이야. 이론적 분석과 실험 결과는 INK가 변환 기반, 탐색 기반, 증류 기반 방어에 대해 강력하다는 걸 보여줘. 예를 들어, INK는 증류 후 공격 성공률이 98%를 넘는데, 기존 방법들은 평균 1.4%의 성공률을 보였어.

================================================================================

URL: https://arxiv.org/abs/2305.09868
Title: The Principle of Uncertain Maximum Entropy

Original Abstract:
The principle of maximum entropy is a well-established technique for choosing a distribution that matches available information while minimizing bias. It finds broad use across scientific disciplines and in machine learning. However, the principle as defined by is susceptible to noise and error in observations. This forces real-world practitioners to use relaxed versions of the principle in an ad hoc way, negatively impacting interpretation. To address this situation, we present a new principle we call uncertain maximum entropy that generalizes the classic principle and provides interpretable solutions irrespective of the observational methods in use. We introduce a convex approximation and expectation-maximization based algorithm for finding solutions to our new principle. Finally, we contrast this new technique with two simpler generally applicable solutions theoretically and experimentally show our technique provides superior accuracy.

Translated Abstract:
최대 엔트로피 원리는 주어진 정보를 바탕으로 편향을 최소화하면서 분포를 선택하는 잘 알려진 방법이야. 이건 과학 분야와 머신 러닝에서 널리 사용되고 있어. 하지만 이 원리는 관측의 노이즈나 오류에 취약해. 그래서 실제로 이걸 사용하는 사람들은 상황에 맞게 원리를 완화해서 사용해야 하고, 이건 해석에도 부정적인 영향을 미쳐.

이런 문제를 해결하기 위해 우리는 불확실한 최대 엔트로피라는 새로운 원리를 제안해. 이 원리는 고전적인 원리를 일반화하고, 사용되는 관측 방법과 관계없이 해석할 수 있는 솔루션을 제공해. 우리는 이 새로운 원리의 해결책을 찾기 위해 볼록 근사와 기대-극대화 기반 알고리즘을 소개할 거야.

마지막으로, 이 새로운 기법을 두 가지 간단한 일반적 솔루션과 비교해 볼 거고, 이론적으로나 실험적으로 우리 기법이 더 높은 정확성을 제공한다는 걸 보여줄 거야.

================================================================================

URL: https://arxiv.org/abs/2309.11500
Title: Auto-ACD: A Large-scale Dataset for Audio-Language Representation Learning

Original Abstract:
Recently, the AI community has made significant strides in developing powerful foundation models, driven by large-scale multimodal datasets. However, for audio representation learning, existing datasets suffer from limitations in the following aspects: insufficient volume, simplistic content, and arduous collection procedures. To establish an audio dataset with high-quality captions, we propose an innovative, automatic approach leveraging multimodal inputs, such as video frames, audio streams. Specifically, we construct a large-scale, high-quality, audio-language dataset, named as Auto-ACD, comprising over 1.5M audio-text pairs. We exploit a series of pre-trained models or APIs, to determine audio-visual synchronisation, generate image captions, object detection, or audio tags for specific videos. Subsequently, we employ LLM to paraphrase a congruent caption for each audio, guided by the extracted multi-modality clues. To demonstrate the effectiveness of the proposed dataset, we train widely used models on our dataset and show performance improvement on various downstream tasks, for example, audio-language retrieval, audio captioning, zero-shot classification. In addition, we establish a novel benchmark with environmental information and provide a benchmark for audio-text tasks.

Translated Abstract:
최근 AI 커뮤니티는 대규모 멀티모달 데이터셋 덕분에 강력한 기초 모델을 개발하는 데 큰 발전을 이루었어. 하지만 오디오 표현 학습을 위한 기존 데이터셋은 다음과 같은 여러 한계가 있어: 데이터 양이 부족하고, 내용이 단순하며, 수집 과정이 어렵다는 거야. 

그래서 우리는 고품질 캡션이 달린 오디오 데이터셋을 만들기 위해, 비디오 프레임이나 오디오 스트림 같은 멀티모달 입력을 활용한 혁신적인 자동화 접근 방식을 제안해. 구체적으로, 우리는 Auto-ACD라는 이름의 대규모 고품질 오디오-언어 데이터셋을 구축했어. 이 데이터셋은 150만 개 이상의 오디오-텍스트 쌍으로 이루어져 있어. 

우리는 일련의 사전 훈련된 모델이나 API를 이용해 오디오와 비디오의 동기화를 확인하고, 이미지 캡션을 생성하거나 특정 비디오에 대한 객체 탐지와 오디오 태그를 생성해. 그 다음, 추출한 멀티모달 단서를 바탕으로 LLM을 사용해 각 오디오에 맞는 재구성된 캡션을 만들어. 

제안한 데이터셋의 효과를 보여주기 위해, 우리는 널리 사용되는 모델들을 우리의 데이터셋으로 훈련시켜서 오디오-언어 검색, 오디오 캡셔닝, 제로샷 분류 같은 다양한 하위 작업에서 성능 향상을 보여줬어. 게다가, 환경 정보를 포함한 새로운 벤치마크를 설정하고 오디오-텍스트 작업을 위한 기준도 제공했어.

================================================================================

URL: https://arxiv.org/abs/2311.02573
Title: Group Testing for Accurate and Efficient Range-Based Near Neighbor Search for Plagiarism Detection

Original Abstract:
This work presents an adaptive group testing framework for the range-based high dimensional near neighbor search problem. Our method efficiently marks each item in a database as neighbor or non-neighbor of a query point, based on a cosine distance threshold without exhaustive search. Like other methods for large scale retrieval, our approach exploits the assumption that most of the items in the database are unrelated to the query. However, it does not assume a large difference between the cosine similarity of the query vector with the least related neighbor and that with the least unrelated non-neighbor. Following a multi-stage adaptive group testing algorithm based on binary splitting, we divide the set of items to be searched into half at each step, and perform dot product tests on smaller and smaller subsets, many of which we are able to prune away. We show that, using softmax-based features, our method achieves a more than ten-fold speed-up over exhaustive search with no loss of accuracy, on a variety of large datasets. Based on empirically verified models for the distribution of cosine distances, we present a theoretical analysis of the expected number of distance computations per query and the probability that a pool will be pruned. Our method has the following features: (i) It implicitly exploits useful distributional properties of cosine distances unlike other methods; (ii) All required data structures are created purely offline; (iii) It does not impose any strong assumptions on the number of true near neighbors; (iv) It is adaptable to streaming settings where new vectors are dynamically added to the database; and (v) It does not require any parameter tuning. The high recall of our technique makes it particularly suited to plagiarism detection scenarios where it is important to report every database item that is sufficiently similar item to the query.

Translated Abstract:
이 연구는 범위 기반의 고차원 근접 이웃 검색 문제를 위한 적응형 그룹 테스트 프레임워크를 소개해. 우리의 방법은 데이터베이스에서 각 항목을 쿼리 포인트의 이웃인지 아닌지 효율적으로 표시해. 이 과정은 코사인 거리 임계값을 기반으로 하며, exhaustive search(모든 검색)를 하지 않아. 

다른 대규모 검색 방법들과 마찬가지로, 우리의 접근 방식은 데이터베이스의 대부분 항목이 쿼리와 관련이 없다는 가정을 활용해. 하지만, 가장 관련이 적은 이웃과 가장 무관한 비이웃의 코사인 유사성 간에 큰 차이가 있다고 가정하진 않아. 

우리는 이진 분할에 기반한 다단계 적응형 그룹 테스트 알고리즘을 따르며, 검색할 항목 집합을 매 단계마다 반으로 나누고, 점점 더 작은 부분집합에 대해 내적 테스트를 수행해. 이 과정에서 많은 부분을 잘라낼 수 있어. 소프트맥스 기반의 특성을 사용했을 때, 우리의 방법은 다양한 대형 데이터셋에서 exhaustive search에 비해 열 배 이상의 속도 향상을 이루면서 정확도는 유지해. 

코사인 거리 분포에 대한 경험적으로 검증된 모델을 바탕으로, 우리는 쿼리당 예상 거리 계산 수와 풀을 잘라낼 확률에 대한 이론적인 분석을 제시해. 우리의 방법은 다음과 같은 특징이 있어: 

(i) 다른 방법들과 달리 코사인 거리의 유용한 분포 특성을 암묵적으로 활용해;  
(ii) 필요한 모든 데이터 구조는 오프라인에서만 생성돼;  
(iii) 진짜 근접 이웃의 수에 대해 강한 가정을 하지 않아;  
(iv) 데이터베이스에 새로운 벡터가 동적으로 추가되는 스트리밍 환경에 적응할 수 있어;  
(v) 어떤 매개변수 조정도 필요하지 않아. 

우리 기술의 높은 재현율은 쿼리와 충분히 유사한 데이터베이스 항목을 모두 보고하는 것이 중요한 표절 탐지 상황에 특히 적합해.

================================================================================

URL: https://arxiv.org/abs/2401.11944
Title: CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark

Original Abstract:
As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU. CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. CMMMU focuses on complex perception and reasoning with domain-specific knowledge in the Chinese context. We evaluate 11 open-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%, indicating a large space for improvement. CMMMU will boost the community to build the next-generation LMMs towards expert artificial intelligence and promote the democratization of LMMs by providing diverse language contexts.

Translated Abstract:
대형 다중 모달 모델(LMMs)의 능력이 계속 발전함에 따라, LMMs의 성능을 평가하는 필요성이 커지고 있어. 특히, 중국어 같은 비영어 환경에서 LMMs의 고급 지식과 추론 능력을 평가하는 데는 더 큰 격차가 있어. 

우리는 CMMMU라는 새로운 중국어 대규모 다학제 다중 모달 이해 기준을 소개해. 이 기준은 대학 수준의 주제 지식과 중국어 환경에서의 신중한 추론이 필요한 작업을 평가하기 위해 설계됐어. CMMMU는 MMMU의 주석 및 분석 패턴에서 영감을 받아서 엄격하게 따르고 있어.

CMMMU에는 12,000개의 수동으로 수집된 다중 모달 질문이 포함돼, 대학 시험, 퀴즈, 교과서에서 나왔고, 여섯 가지 핵심 분야인 예술 및 디자인, 비즈니스, 과학, 건강 및 의학, 인문학 및 사회과학, 기술 및 공학을 다뤄. 이 질문들은 30개 과목에 걸쳐 있으며, 차트, 다이어그램, 지도, 표, 악보, 화학 구조와 같은 39종의 다양한 이미지 유형으로 구성돼.

CMMMU는 중국어 환경에서의 도메인 특정 지식을 바탕으로 복잡한 인식과 추론에 초점을 맞추고 있어. 우리는 11개의 오픈 소스 LLM과 하나의 독점 GPT-4V(ision)을 평가했어. 심지어 GPT-4V도 42%의 정확도만 달성했는데, 이는 개선할 여지가 크다는 것을 의미해. CMMMU는 커뮤니티가 차세대 LMMs를 전문가 인공지능으로 발전시키고, 다양한 언어 환경을 제공함으로써 LMMs의 민주화를 촉진할 수 있도록 도울 거야.

================================================================================

URL: https://arxiv.org/abs/2402.02263
Title: MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers

Original Abstract:
Adversarial robustness often comes at the cost of degraded accuracy, impeding real-life applications of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet datasets, experimental results with custom strong adaptive attacks demonstrate MixedNUTS's vastly improved accuracy and near-SOTA robustness -- it boosts CIFAR-100 clean accuracy by 7.86 points, sacrificing merely 0.87 points in robust accuracy.

Translated Abstract:
적대적 강인성은 보통 정확도를 떨어뜨리는 대가를 치르게 되어, 실제로 강인한 분류 모델을 사용하는 데 방해가 된다. 훈련 기반의 해결책은 이미 훈련된 고성능 대형 모델과의 호환성 문제로 한계가 있어서, 훈련 없이도 사용할 수 있는 앙상블 접근법을 탐색할 필요가 있다. 

강인한 모델이 깨끗한 데이터와 적대적 데이터 모두에서 올바른 예측에 대해 더 자신감을 보인다는 점을 관찰하면서, 우리는 이 "온화한 자신감 특성"을 증대시키면 앙상블 환경에서 정확도와 강인성을 조화롭게 할 수 있을 것이라고 추측했다. 

이를 위해 우리는 "MixedNUTS"라는 훈련 없이 사용할 수 있는 방법을 제안한다. 이 방법은 강인한 분류기와 일반 비강인 분류기의 출력 로짓을 세 개의 파라미터로 비선형 변환을 통해 처리하는데, 이 파라미터들은 효율적인 알고리즘을 통해 최적화된다. MixedNUTS는 변환된 로짓을 확률로 바꾸고, 이를 전체 출력으로 혼합한다. 

CIFAR-10, CIFAR-100, 그리고 ImageNet 데이터셋에서, 사용자 정의 강력한 적응 공격을 이용한 실험 결과는 MixedNUTS가 정확도를 크게 향상시키고 거의 최신 수준의 강인성을 가지고 있다는 것을 보여준다. 예를 들어, CIFAR-100의 깨끗한 정확도를 7.86 포인트 높이면서, 강인한 정확도는 단 0.87 포인트만 희생했다.

================================================================================

URL: https://arxiv.org/abs/2402.02349
Title: 3D Lymphoma Segmentation on PET/CT Images via Multi-Scale Information Fusion with Cross-Attention

Original Abstract:
Background: Accurate segmentation of diffuse large B-cell lymphoma (DLBCL) lesions is challenging due to their complex patterns in medical imaging.
Objective: This study aims to develop a precise segmentation method for DLBCL using 18F-Fluorodeoxyglucose (FDG) positron emission tomography (PET) and computed tomography (CT) images.
Methods: We propose a 3D dual-branch encoder segmentation method using shifted window transformers and a Multi-Scale Information Fusion (MSIF) module. To enhance feature integration, the MSIF module performs multi-scale feature fusion using cross-attention mechanisms with a shifted window framework. A gated neural network within the MSIF module dynamically balances the contributions from each modality. The model was optimized using the Dice Similarity Coefficient (DSC) loss function. Additionally, we computed the total metabolic tumor volume (TMTV) and performed statistical analyses.
Results: The model was trained and validated on a dataset of 165 DLBCL patients using 5-fold cross-validation, achieving a DSC of 0.7512. Statistical analysis showed a significant improvement over comparative methods (p < 0.05). Additionally, a Pearson correlation coefficient of 0.91 and an R^2 of 0.89 were observed when comparing manual annotations to segmentation results for TMTV measurement.
Conclusion: This study presents an effective automatic segmentation method for DLBCL that leverages the complementary strengths of PET and CT imaging. Our method has the potential to improve diagnostic interpretations and assist in treatment planning for DLBCL patients.

Translated Abstract:
배경: 확산 대세포 B 림프종(DLBCL) 병변을 정확하게 분할하는 것은 의료 이미징에서 복잡한 패턴 때문에 어려워요.

목적: 이 연구는 18F-플루오로데옥시글루코스(FDG) 양전자 방출 단층촬영(PET)과 컴퓨터 단층촬영(CT) 이미지를 사용해 DLBCL을 정확하게 분할하는 방법을 개발하는 거예요.

방법: 우리는 이동 창 변환기와 다중 스케일 정보 융합(MSIF) 모듈을 사용하는 3D 이중 분기 인코더 분할 방법을 제안해요. MSIF 모듈은 이동 창 프레임워크를 사용해 교차 주의 메커니즘으로 다중 스케일 특징 융합을 수행해서 특징 통합을 강화해요. MSIF 모듈 내의 게이트 신경망은 각 모달리티의 기여도를 동적으로 균형을 맞춰줘요. 모델은 Dice 유사도 계수(DSC) 손실 함수를 사용해 최적화했어요. 또한, 총 대사 종양 볼륨(TMTV)을 계산하고 통계 분석을 수행했어요.

결과: 모델은 165명의 DLBCL 환자의 데이터셋을 사용해 5배 교차 검증으로 훈련하고 검증했으며, DSC는 0.7512를 달성했어요. 통계 분석 결과, 비교 방법에 비해 유의한 개선이 있음을 보여줬어요(p < 0.05). 또한, TMTV 측정을 위한 수동 주석과 분할 결과를 비교했을 때 피어슨 상관 계수 0.91과 R^2 0.89가 관찰되었어요.

결론: 이 연구는 PET과 CT 이미징의 상호 보완적인 장점을 활용한 DLBCL에 대한 효과적인 자동 분할 방법을 제시해요. 우리의 방법은 진단 해석을 개선하고 DLBCL 환자의 치료 계획에 도움을 줄 가능성이 있어요.

================================================================================

URL: https://arxiv.org/abs/2402.13629
Title: Adversarial Purification and Fine-tuning for Robust UDC Image Restoration

Original Abstract:
This study delves into the enhancement of Under-Display Camera (UDC) image restoration models, focusing on their robustness against adversarial attacks. Despite its innovative approach to seamless display integration, UDC technology faces unique image degradation challenges exacerbated by the susceptibility to adversarial perturbations. Our research initially conducts an in-depth robustness evaluation of deep-learning-based UDC image restoration models by employing several white-box and black-box attacking methods. This evaluation is pivotal in understanding the vulnerabilities of current UDC image restoration techniques. Following the assessment, we introduce a defense framework integrating adversarial purification with subsequent fine-tuning processes. First, our approach employs diffusion-based adversarial purification, effectively neutralizing adversarial perturbations. Then, we apply the fine-tuning methodologies to refine the image restoration models further, ensuring that the quality and fidelity of the restored images are maintained. The effectiveness of our proposed approach is validated through extensive experiments, showing marked improvements in resilience against typical adversarial attacks.

Translated Abstract:
이 연구는 언더디스플레이 카메라(UDC) 이미지 복원 모델의 성능을 높이는 방법을 다뤄. 특히 적대적 공격에 대한 강인성에 초점을 맞추고 있어. UDC 기술은 매끄러운 디스플레이 통합에 혁신적이지만, 적대적 방해로 인해 이미지 품질 저하 문제가 생길 수 있어.

우리 연구는 먼저 여러 가지 화이트박스와 블랙박스 공격 방법을 사용해서 딥러닝 기반 UDC 이미지 복원 모델의 강인성을 깊게 평가해. 이 평가는 현재 UDC 이미지 복원 기술의 취약점을 이해하는 데 중요해. 평가 후에는 적대적 정화를 통합한 방어 프레임워크를 소개해.

먼저, 우리 방법은 확산 기반의 적대적 정화를 사용해서 적대적 방해를 효과적으로 없애. 그 다음에 이미지 복원 모델을 더 다듬기 위해 파인튜닝 방법론을 적용해, 복원된 이미지의 품질과 충실도를 유지할 수 있도록 해. 우리의 접근 방식은 광범위한 실험을 통해 효과가 입증되었고, 일반적인 적대적 공격에 대한 저항력이 크게 향상된 것을 보여줘.

================================================================================

URL: https://arxiv.org/abs/2403.05847
Title: iBA: Backdoor Attack on 3D Point Cloud via Reconstructing Itself

Original Abstract:
The widespread deployment of Deep Neural Networks (DNNs) for 3D point cloud processing starkly contrasts with their susceptibility to security breaches, notably backdoor attacks. These attacks hijack DNNs during training, embedding triggers in the data that, once activated, cause the network to make predetermined errors while maintaining normal performance on unaltered data. This vulnerability poses significant risks, especially given the insufficient research on robust defense mechanisms for 3D point cloud networks against such sophisticated threats. Existing attacks either struggle to resist basic point cloud pre-processing methods, or rely on delicate manual design. Exploring simple, effective, imperceptible, and difficult-to-defend triggers in 3D point clouds is still this http URL address these challenges, we introduce MirrorAttack, a novel effective 3D backdoor attack method, which implants the trigger by simply reconstructing a clean point cloud with an auto-encoder. The data-driven nature of the MirrorAttack obviates the need for complex manual design. Minimizing the reconstruction loss automatically improves imperceptibility. Simultaneously, the reconstruction network endows the trigger with pronounced nonlinearity and sample specificity, rendering traditional preprocessing techniques ineffective in eliminating it. A trigger smoothing module based on spherical harmonic transformation is also attached to regulate the intensity of the attack.Both quantitive and qualitative results verify the effectiveness of our method. We achieve state-of-the-art ASR on different types of victim models with the intervention of defensive techniques. Moreover, the minimal perturbation introduced by our trigger, as assessed by various metrics, attests to the method's stealth, ensuring its imperceptibility.

Translated Abstract:
딥 뉴럴 네트워크(DNNs)가 3D 포인트 클라우드 처리를 위해 널리 사용되고 있지만, 보안에 취약하다는 문제도 있어. 특히 백도어 공격이 문제인데, 이 공격은 DNN 훈련 중에 DNN을 조작해서 데이터에 트리거를 숨기고, 이 트리거가 활성화되면 네트워크가 미리 정해진 오류를 발생하게 해. 동시에 원래 데이터에서는 정상적으로 작동하는 것처럼 보여. 이런 취약점은 큰 위험을 초래하는데, 특히 3D 포인트 클라우드 네트워크를 위한 강력한 방어 기법에 대한 연구가 부족해서 더욱 문제야.

기존 공격들은 기본적인 포인트 클라우드 전처리 방법에 잘 저항하지 못하거나, 복잡한 수동 설계에 의존해. 그래서 3D 포인트 클라우드에서 간단하고 효과적이며 감지하기 어려운 트리거를 탐구하는 게 필요해. 이를 해결하기 위해 우리는 MirrorAttack이라는 새로운 3D 백도어 공격 방법을 소개해. 이 방법은 오토인코더를 사용해 깨끗한 포인트 클라우드를 재구성함으로써 트리거를 심어.

MirrorAttack은 데이터 기반으로 설계가 복잡할 필요가 없어. 재구성 손실을 최소화하면 자연스럽게 감지되지 않게 돼. 동시에, 재구성 네트워크는 트리거에 뚜렷한 비선형성과 샘플 특수성을 부여해서, 전통적인 전처리 기술로는 이를 없애기 어려워. 또한, 구형 조화 변환에 기반한 트리거 스무딩 모듈도 추가해서 공격의 강도를 조절해.

정량적 및 정성적 결과 모두 우리 방법의 효과를 입증해. 우리는 다양한 피해 모델에서 방어 기술 개입에도 불구하고 최첨단 ASR을 달성했어. 게다가, 우리 트리거로 인해 발생하는 최소한의 교란은 여러 지표로 평가했을 때 이 방법의 은밀함을 보여줬어.

================================================================================

URL: https://arxiv.org/abs/2403.08947
Title: Robust COVID-19 Detection in CT Images with CLIP

Original Abstract:
In the realm of medical imaging, particularly for COVID-19 detection, deep learning models face substantial challenges such as the necessity for extensive computational resources, the paucity of well-annotated datasets, and a significant amount of unlabeled data. In this work, we introduce the first lightweight detector designed to overcome these obstacles, leveraging a frozen CLIP image encoder and a trainable multilayer perception (MLP). Enhanced with Conditional Value at Risk (CVaR) for robustness and a loss landscape flattening strategy for improved generalization, our model is tailored for high efficacy in COVID-19 detection. Furthermore, we integrate a teacher-student framework to capitalize on the vast amounts of unlabeled data, enabling our model to achieve superior performance despite the inherent data limitations. Experimental results on the COV19-CT-DB dataset demonstrate the effectiveness of our approach, surpassing baseline by up to 10.6% in `macro' F1 score in supervised learning. The code is available at this https URL.

Translated Abstract:
의료 이미징, 특히 COVID-19 탐지 분야에서는 딥러닝 모델이 여러 가지 어려움에 직면해 있어. 예를 들면, 많은 계산 자원이 필요하고, 잘 주석이 달린 데이터셋이 부족하며, 라벨이 없는 데이터가 많다는 거야. 

이번 연구에서는 이런 문제들을 해결하기 위해 설계된 첫 번째 경량 탐지기를 소개해. 이 탐지기는 동결된 CLIP 이미지 인코더와 훈련 가능한 다층 퍼셉트론(MLP)을 활용해. 그리고 강건성을 위해 Conditional Value at Risk (CVaR)을 사용하고, 일반화를 개선하기 위해 손실 경량화 전략을 적용했어. 이 모델은 COVID-19 탐지에서 높은 효능을 낼 수 있도록 맞춰져 있어.

또한, 우리는 교사-학생 프레임워크를 통합해서 라벨이 없는 데이터를 잘 활용할 수 있게 했어. 덕분에 데이터의 한계에도 불구하고 모델 성능이 더 좋아졌어. COV19-CT-DB 데이터셋에서 실험한 결과, 감독 학습에서 베이스라인보다 최대 10.6% 더 높은 '매크로' F1 점수를 기록했어. 코드는 이 https URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2403.09964
Title: Boundary Constraint-free Biomechanical Model-Based Surface Matching for Intraoperative Liver Deformation Correction

Original Abstract:
In image-guided liver surgery, 3D-3D non-rigid registration methods play a crucial role in estimating the mapping between the preoperative model and the intraoperative surface represented as point clouds, addressing the challenge of tissue deformation. Typically, these methods incorporate a biomechanical model, represented as a finite element model (FEM), used to regularize a surface matching term. This paper introduces a novel 3D-3D non-rigid registration method. In contrast to the preceding techniques, our method uniquely incorporates the FEM within the surface matching term itself, ensuring that the estimated deformation maintains geometric consistency throughout the registration process. Additionally, we eliminate the need to determine zero-boundary conditions and applied force locations in the FEM. We achieve this by integrating soft springs into the stiffness matrix and allowing forces to be distributed across the entire liver surface. To further improve robustness, we introduce a regularization technique focused on the gradient of the force magnitudes. This regularization imposes spatial smoothness and helps prevent the overfitting of irregular noise in intraoperative data. Optimization is achieved through an accelerated proximal gradient algorithm, further enhanced by our proposed method for determining the optimal step size. Our method is evaluated and compared to both a learning-based method and a traditional method that features FEM regularization using data collected on our custom-developed phantom, as well as two publicly available datasets. Our method consistently outperforms or is comparable to the baseline techniques. Both the code and dataset will be made publicly available.

Translated Abstract:
이미지 유도 간간의 수술에서 3D-3D 비강체 등록 방법은 수술 전 모델과 수술 중 표면을 점군으로 표현한 것 사이의 매핑을 추정하는 데 중요한 역할을 해. 이 과정에서 조직 변형 문제를 다루는 거야. 보통 이런 방법들은 유한 요소 모델(FEM) 같은 생체역학 모델을 포함해서 표면 일치 항을 규칙화해.

이 논문에서는 새로운 3D-3D 비강체 등록 방법을 소개해. 기존 기술들과는 다르게, 우리 방법은 FEM을 표면 일치 항 안에 직접 포함시켜서, 추정된 변형이 등록 과정 동안 기하학적 일관성을 유지하도록 해. 게다가 FEM에서 제로 경계 조건이나 적용할 힘의 위치를 정할 필요도 없애. 이건 부드러운 스프링을 강성 행렬에 통합하고 힘이 간 전체 간 표면에 분포하도록 해서 가능해.

또한, 강도 크기의 그래디언트에 집중한 규칙화 기법을 도입해 강건성을 더 높였어. 이 규칙화는 공간적 부드러움을 부여하고 수술 중 데이터에서 불규칙한 노이즈의 과적합을 방지하는 데 도움을 줘. 최적화는 가속된 근접 그래디언트 알고리즘을 통해 이루어지고, 최적의 스텝 크기를 결정하기 위한 우리 방법으로 더욱 개선됐어.

우리 방법은 커스텀 개발한 팬텀에서 수집한 데이터와 두 개의 공개 데이터셋을 사용해 학습 기반 방법 및 전통적인 FEM 규칙화 방법과 비교 평가했어. 우리 방법은 일관되게 기준 기술보다 나은 성능을 보이거나 비슷한 성능을 보여. 코드와 데이터셋은 공개될 예정이야.

================================================================================

URL: https://arxiv.org/abs/2403.18103
Title: Tutorial on Diffusion Models for Imaging and Vision

Original Abstract:
The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on diffusion models or applying these models to solve other problems.

Translated Abstract:
최근 몇 년 동안 생성 도구들이 엄청나게 발전하면서 텍스트를 이미지로 변환하거나 텍스트를 비디오로 변환하는 흥미로운 많은 애플리케이션이 생겼어. 이 생성 도구들의 기본 원리는 '확산'이라는 개념인데, 이건 이전 접근 방식에서 해결하기 어려웠던 문제들을 극복한 특별한 샘플링 기법이야. 

이 튜토리얼의 목표는 확산 모델의 기본 아이디어를 설명하는 거야. 이 튜토리얼은 확산 모델에 대한 연구에 관심이 있는 대학생과 대학원생들을 대상으로 하고 있어. 다른 문제를 해결하기 위해 이 모델들을 적용하는 것도 포함돼.

================================================================================

URL: https://arxiv.org/abs/2404.02999
Title: MeshBrush: Painting the Anatomical Mesh with Neural Stylization for Endoscopy

Original Abstract:
Style transfer is a promising approach to close the sim-to-real gap in medical endoscopy. Rendering synthetic endoscopic videos by traversing pre-operative scans (such as MRI or CT) can generate structurally accurate simulations as well as ground truth camera poses and depth maps. Although image-to-image (I2I) translation models such as CycleGAN can imitate realistic endoscopic images from these simulations, they are unsuitable for video-to-video synthesis due to the lack of temporal consistency, resulting in artifacts between frames. We propose MeshBrush, a neural mesh stylization method to synthesize temporally consistent videos with differentiable rendering. MeshBrush uses the underlying geometry of patient imaging data while leveraging existing I2I methods. With learned per-vertex textures, the stylized mesh guarantees consistency while producing high-fidelity outputs. We demonstrate that mesh stylization is a promising approach for creating realistic simulations for downstream tasks such as training networks and preoperative planning. Although our method is tested and designed for ureteroscopy, its components are transferable to general endoscopic and laparoscopic procedures. The code will be made public on GitHub.

Translated Abstract:
스타일 전이는 의료 내시경에서 시뮬레이션과 실제 간극을 좁히는 유망한 방법이야. 수술 전 스캔(예: MRI나 CT)을 사용해 합성 내시경 비디오를 만들면 구조적으로 정확한 시뮬레이션과 실제 카메라 위치, 깊이 맵을 생성할 수 있어. 

CycleGAN 같은 이미지 간 변환 모델은 이런 시뮬레이션에서 현실적인 내시경 이미지를 모방할 수 있지만, 비디오에서 비디오로 만드는 데는 적합하지 않아. 왜냐하면 시간 일관성이 부족해서 프레임 간에 아티팩트가 생기거든. 그래서 우리는 MeshBrush라는 신경망 메쉬 스타일화 방법을 제안해. 이건 차별화 가능한 렌더링을 통해 시간적으로 일관된 비디오를 합성할 수 있어. 

MeshBrush는 환자 이미징 데이터의 기본 기하학을 사용하면서 기존 I2I 방법도 활용해. 각 정점에 학습된 텍스처를 통해 스타일화된 메쉬는 일관성을 보장하면서 높은 품질의 출력을 만들어. 우리는 메쉬 스타일화가 네트워크 훈련이나 수술 전 계획 같은 후속 작업을 위한 현실적인 시뮬레이션을 만드는 데 유망한 방법이라는 걸 보여줬어. 

비록 우리의 방법이 요관경 검사에 대해 테스트되고 설계되었지만, 그 요소들은 일반적인 내시경과 복강경 절차에도 적용할 수 있어. 이 코드는 GitHub에 공개할 예정이야.

================================================================================

URL: https://arxiv.org/abs/2405.09990
Title: A Comprehensive Evaluation of Histopathology Foundation Models for Ovarian Cancer Subtype Classification

Original Abstract:
Large pretrained transformers are increasingly being developed as generalised foundation models which can underpin powerful task-specific artificial intelligence models. Histopathology foundation models show great promise across many tasks, but analyses have typically been limited by arbitrary hyperparameters that were not tuned to the specific task. We report the most rigorous single-task validation of histopathology foundation models to date, specifically in ovarian cancer morphological subtyping. Attention-based multiple instance learning classifiers were compared using three ImageNet-pretrained feature extractors and fourteen histopathology foundation models. The training set consisted of 1864 whole slide images from 434 ovarian carcinoma cases at Leeds Teaching Hospitals NHS Trust. Five-class classification performance was evaluated through five-fold cross-validation, and these cross-validation models were ensembled for hold-out testing and external validation on the Transcanadian Study and OCEAN Challenge datasets. The best-performing model used the H-optimus-0 foundation model, with five-class balanced accuracies of 89%, 97%, and 74% in the test sets. Normalisations and augmentations aided the performance of the ImageNet-pretrained ResNets, but these were still outperformed by 13 of the 14 foundation models. Hyperparameter tuning the downstream classifiers improved performance by a median 1.9% balanced accuracy, with many improvements being statistically significant. Histopathology foundation models offer a clear benefit to ovarian cancer subtyping, improving classification performance to a degree where clinical utility is tangible, albeit with an increased computational burden. Such models could provide a second opinion to histopathologists diagnosing challenging cases and may improve the accuracy, objectivity, and efficiency of pathological diagnoses overall.

Translated Abstract:
대규모로 미리 훈련된 트랜스포머 모델들이 강력한 특정 작업 인공지능 모델을 지원할 수 있는 일반화된 기초 모델로 점점 더 많이 개발되고 있어. 특히, 조직병리학 기초 모델은 여러 작업에서 큰 가능성을 보여주지만, 분석은 보통 특정 작업에 맞게 조정되지 않은 임의의 하이퍼파라미터로 제한되어 왔어.

이번 연구에서는 난소암 형태학적 하위 유형 분류에 대해 가장 철저한 단일 작업 검증을 진행했어. 주의 기반의 다중 인스턴스 학습 분류기를 세 가지 ImageNet 미리 훈련된 특징 추출기와 14개의 조직병리학 기초 모델을 사용해 비교했어. 훈련 세트는 리즈 교육 병원 NHS Trust에서 수집한 434개의 난소암 사례로부터 1864개의 전체 슬라이드 이미지로 구성되었어.

다섯 가지 분류 성능은 5배 교차 검증을 통해 평가되었고, 이 교차 검증 모델들은 홀드 아웃 테스트와 Transcanadian Study 및 OCEAN Challenge 데이터셋의 외부 검증을 위해 앙상블 되었어. 가장 성능이 좋았던 모델은 H-optimus-0 기초 모델을 사용했으며, 테스트 세트에서 다섯 가지 클래스의 균형 정확도가 각각 89%, 97%, 74%였어.

정규화와 증강이 ImageNet 미리 훈련된 ResNet의 성능을 도왔지만, 여전히 14개의 기초 모델 중 13개 모델에 비해 성능이 떨어졌어. 다운스트림 분류기를 하이퍼파라미터 조정했더니 균형 정확도가 중간에 1.9% 향상되었고, 많은 개선이 통계적으로 유의미했어.

조직병리학 기초 모델은 난소암 하위 유형 분류에 명확한 이점을 제공해, 임상에서 유용할 정도로 분류 성능을 향상시켜. 물론 계산 부담이 늘어나긴 하지만, 이런 모델들이 조직병리학자들이 어려운 사례를 진단할 때 제2의 의견을 제공할 수 있고, 전체적으로 병리학적 진단의 정확성, 객관성, 효율성을 개선할 수 있을 거야.

================================================================================

URL: https://arxiv.org/abs/2407.06064
Title: Pan-denoising: Guided Hyperspectral Image Denoising via Weighted Represent Coefficient Total Variation

Original Abstract:
This paper introduces a novel paradigm for hyperspectral image (HSI) denoising, which is termed \textit{pan-denoising}. In a given scene, panchromatic (PAN) images capture similar structures and textures to HSIs but with less noise. This enables the utilization of PAN images to guide the HSI denoising process. Consequently, pan-denoising, which incorporates an additional prior, has the potential to uncover underlying structures and details beyond the internal information modeling of traditional HSI denoising methods. However, the proper modeling of this additional prior poses a significant challenge. To alleviate this issue, the paper proposes a novel regularization term, Panchromatic Weighted Representation Coefficient Total Variation (PWRCTV). It employs the gradient maps of PAN images to automatically assign different weights of TV regularization for each pixel, resulting in larger weights for smooth areas and smaller weights for edges. This regularization forms the basis of a pan-denoising model, which is solved using the Alternating Direction Method of Multipliers. Extensive experiments on synthetic and real-world datasets demonstrate that PWRCTV outperforms several state-of-the-art methods in terms of metrics and visual quality. Furthermore, an HSI classification experiment confirms that PWRCTV, as a preprocessing method, can enhance the performance of downstream classification tasks. The code and data are available at this https URL.

Translated Abstract:
이 논문은 새로운 하이퍼스펙트럼 이미지(HSI) 노이즈 제거 방식인 '판 노이즈 제거(pan-denoising)'를 소개해. 주어진 장면에서 파노라마(panchromatic, PAN) 이미지는 HSI와 비슷한 구조와 질감을 가지면서 노이즈가 적어. 그래서 PAN 이미지를 활용해서 HSI 노이즈 제거 과정을 도울 수 있어. 판 노이즈 제거는 추가적인 사전 정보를 포함하기 때문에, 전통적인 HSI 노이즈 제거 방법보다 더 많은 구조와 세부 사항을 찾아낼 가능성이 있어. 

하지만 이 추가적인 사전 정보 모델링이 큰 도전 과제가 돼. 이 문제를 해결하기 위해, 논문에서는 새로운 정규화 항인 '파노라마 가중치 표현 계수 총 변동(PWRCTV)'을 제안해. 이 방법은 PAN 이미지의 기울기 맵을 사용해서 각 픽셀에 대해 TV 정규화의 가중치를 자동으로 다르게 부여해. 그 결과 매끄러운 영역은 더 큰 가중치를 받고, 엣지 부분은 더 작은 가중치를 받아. 이 정규화는 판 노이즈 제거 모델의 기초가 되고, 교대 방향 제곱 방법(Alternating Direction Method of Multipliers)을 사용해 해결돼.

합성 데이터와 실제 데이터셋에서의 실험 결과, PWRCTV가 여러 최신 방법들보다 성능과 시각적 품질 면에서 더 뛰어난 것으로 나타났어. 또한 HSI 분류 실험에서도 PWRCTV가 전처리 방법으로서 하위 분류 작업의 성능을 향상시킬 수 있다는 걸 확인했어. 코드와 데이터는 이 링크에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2407.07868
Title: Green Screen Augmentation Enables Scene Generalisation in Robotic Manipulation

Original Abstract:
Generalising vision-based manipulation policies to novel environments remains a challenging area with limited exploration. Current practices involve collecting data in one location, training imitation learning or reinforcement learning policies with this data, and deploying the policy in the same location. However, this approach lacks scalability as it necessitates data collection in multiple locations for each task. This paper proposes a novel approach where data is collected in a location predominantly featuring green screens. We introduce Green-screen Augmentation (GreenAug), employing a chroma key algorithm to overlay background textures onto a green screen. Through extensive real-world empirical studies with over 850 training demonstrations and 8.2k evaluation episodes, we demonstrate that GreenAug surpasses no augmentation, standard computer vision augmentation, and prior generative augmentation methods in performance. While no algorithmic novelties are claimed, our paper advocates for a fundamental shift in data collection practices. We propose that real-world demonstrations in future research should utilise green screens, followed by the application of GreenAug. We believe GreenAug unlocks policy generalisation to visually distinct novel locations, addressing the current scene generalisation limitations in robot learning.

Translated Abstract:
새로운 환경에서 비전 기반 조작 정책을 일반화하는 것은 여전히 어려운 분야이고, 탐구가 제한적이에요. 현재의 방법은 한 장소에서 데이터를 수집하고, 그 데이터를 가지고 모방 학습이나 강화 학습 정책을 훈련한 다음, 같은 장소에서 그 정책을 적용하는 방식이에요. 하지만 이 방법은 각 작업마다 여러 장소에서 데이터 수집이 필요하므로 확장성이 떨어져요.

이 논문에서는 주로 초록색 스크린이 있는 장소에서 데이터를 수집하는 새로운 접근 방식을 제안해요. 우리는 크로마 키 알고리즘을 사용해서 초록색 스크린 위에 배경 텍스처를 덧붙이는 '그린 스크린 증강(GreenAug)'을 소개해요. 850개 이상의 실제 훈련 시연과 8,200개의 평가 에피소드에 대한 광범위한 실험을 통해, GreenAug가 증강이 없는 경우, 일반적인 컴퓨터 비전 증강, 그리고 이전의 생성적 증강 방법보다 성능이 뛰어나다는 것을 보여줘요.

새로운 알고리즘을 제안하는 건 아니지만, 우리의 논문은 데이터 수집 방식의 근본적인 변화를 주장해요. 앞으로의 연구에서는 실제 시연에 초록색 스크린을 활용하고, 그 다음에 GreenAug를 적용해야 한다고 제안해요. 우리는 GreenAug가 시각적으로 뚜렷하게 다른 새로운 장소로의 정책 일반화를 가능하게 한다고 믿어요. 이는 로봇 학습에서 현재의 장면 일반화 한계를 해결하는 데 도움이 될 거예요.

================================================================================

URL: https://arxiv.org/abs/2408.05892
Title: Polyp SAM 2: Advancing Zero shot Polyp Segmentation in Colorectal Cancer Detection

Original Abstract:
Polyp segmentation plays a crucial role in the early detection and diagnosis of colorectal cancer. However, obtaining accurate segmentations often requires labor-intensive annotations and specialized models. Recently, Meta AI Research released a general Segment Anything Model 2 (SAM 2), which has demonstrated promising performance in several segmentation tasks. In this manuscript, we evaluate the performance of SAM 2 in segmenting polyps under various prompted settings. We hope this report will provide insights to advance the field of polyp segmentation and promote more interesting work in the future. This project is publicly available at this https URL sajjad-sh33/Polyp-SAM-2.

Translated Abstract:
폴립 분할은 대장암을 초기 단계에서 발견하고 진단하는 데 아주 중요한 역할을 해. 근데 정확한 분할을 얻으려면 많은 시간과 노력이 드는 주석 작업이 필요하고, 전문적인 모델도 필요해. 

최근에 메타 AI 연구팀이 일반적인 Segment Anything Model 2 (SAM 2)를 발표했는데, 이 모델이 여러 분할 작업에서 좋은 성능을 보였어. 

이번 논문에서는 다양한 설정에서 SAM 2가 폴립을 분할하는 성능을 평가해봤어. 이 보고서가 폴립 분할 분야를 발전시키고, 앞으로 더 흥미로운 연구들이 진행되는 데 도움이 되길 바래. 이 프로젝트는 여기서 공개되어 있어: https URL sajjad-sh33/Polyp-SAM-2.

================================================================================

URL: https://arxiv.org/abs/2408.11077
Title: Solving Oscillator Ordinary Differential Equations via Soft-constrained Physics-informed Neural Network with Small Data

Original Abstract:
This paper compared physics-informed neural network (PINN), conventional neural network (NN) and traditional numerical discretization methods on solving differential equations (DEs) through literature investigation and experimental validation. We focused on the soft-constrained PINN approach and formalized its mathematical framework and computational flow for solving Ordinary DEs and Partial DEs (ODEs/PDEs). The working mechanism and its accuracy and efficiency were experimentally verified by solving typical linear and non-linear oscillator ODEs. We demonstrate that the DeepXDE-based implementation of PINN is not only light code and efficient in training, but also flexible across CPU/GPU platforms. PINN greatly reduces the need for labeled data: when the nonlinearity of the ODE is weak, a very small amount of supervised training data plus a few unsupervised collocation points are sufficient to predict the solution; in the minimalist case, only one or two training points (with initial values) are needed for first- or second-order ODEs, respectively. We also find that, with the aid of collocation points and the use of physical information, PINN has the ability to extrapolate data outside the time domain of the training set, and especially is robust to noisy data, thus with enhanced generalization capabilities. Training is accelerated when the gains obtained along with the reduction in the amount of data outweigh the delay caused by the increase in the loss function terms. The soft-constrained PINN can easily impose a physical law (e.g., conservation of energy) constraint by adding a regularization term to the total loss function, thus improving the solution performance to ODEs that obey this physical law. Furthermore, PINN can also be used for stiff ODEs, PDEs, and other types of DEs, and is becoming a favorable catalyst for the era of Digital Twins.

Translated Abstract:
이 논문은 물리 정보 기반 신경망(PINN), 전통적인 신경망(NN), 그리고 전통적인 수치 이산화 방법을 비교했어. 주로 미분 방정식(DEs)을 푸는 데 중점을 두었고, 문헌 조사와 실험 검증을 통해 내용을 정리했어. 우리는 소프트 제약이 있는 PINN 접근법에 집중하고, 일반 미분 방정식(ODEs)과 편미분 방정식(PDEs)을 푸는 수학적 구조와 계산 흐름을 정립했어.

PINN의 작동 방식과 정확성, 효율성은 일반적인 선형 및 비선형 진동기 ODE를 풀면서 실험적으로 검증했어. DeepXDE 기반의 PINN 구현은 코드가 가볍고 훈련이 효율적일 뿐만 아니라 CPU/GPU 플랫폼에서도 유연하게 작동하는 걸 보여줬어. PINN은 레이블이 있는 데이터의 필요성을 크게 줄여줘. ODE의 비선형성이 약할 땐, 아주 적은 양의 감독된 훈련 데이터와 몇 개의 비감독 집합점만으로도 해결책을 예측할 수 있어. 최소한의 경우에는, 첫 번째 또는 두 번째 차수 ODE를 위해 각각 한두 개의 훈련 포인트(초기 값 포함)만 필요해.

우리는 또한, 집합점과 물리적 정보를 활용하면 PINN이 훈련 세트의 시간 영역 밖에서도 데이터를 외삽할 수 있는 능력이 있고, 특히 노이즈가 있는 데이터에도 강하다는 걸 발견했어. 이렇게 해서 일반화 능력이 향상돼. 훈련은 데이터 양의 감소로 얻는 이득이 손실 함수 항의 증가로 인한 지연보다 클 때 가속화돼. 소프트 제약 PINN은 물리 법칙(예: 에너지 보존 법칙)을 총 손실 함수에 정규화 항을 추가함으로써 쉽게 적용할 수 있어, 그래서 이런 물리 법칙을 따르는 ODE의 성능을 개선할 수 있어.

더 나아가, PINN은 강한 ODE, PDE, 그리고 다른 유형의 DE에도 사용할 수 있고, 디지털 트윈 시대의 유망한 촉매제로 자리 잡고 있어.

================================================================================

URL: https://arxiv.org/abs/2408.11289
Title: HMT-UNet: A hybird Mamba-Transformer Vision UNet for Medical Image Segmentation

Original Abstract:
In the field of medical image segmentation, models based on both CNN and Transformer have been thoroughly investigated. However, CNNs have limited modeling capabilities for long-range dependencies, making it challenging to exploit the semantic information within images fully. On the other hand, the quadratic computational complexity poses a challenge for Transformers. State Space Models (SSMs), such as Mamba, have been recognized as a promising method. They not only demonstrate superior performance in modeling long-range interactions, but also preserve a linear computational complexity. The hybrid mechanism of SSM (State Space Model) and Transformer, after meticulous design, can enhance its capability for efficient modeling of visual features. Extensive experiments have demonstrated that integrating the self-attention mechanism into the hybrid part behind the layers of Mamba's architecture can greatly improve the modeling capacity to capture long-range spatial dependencies. In this paper, leveraging the hybrid mechanism of SSM, we propose a U-shape architecture model for medical image segmentation, named Hybird Transformer vision Mamba UNet (HTM-UNet). We conduct comprehensive experiments on the ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir, CVC-ColonDB, ETIS-Larib PolypDB public datasets and ZD-LCI-GIM private dataset. The results indicate that HTM-UNet exhibits competitive performance in medical image segmentation tasks. Our code is available at this https URL.

Translated Abstract:
의료 이미지 분할 분야에서는 CNN과 Transformer를 기반으로 한 모델들이 많이 연구되어 왔어. 그런데 CNN은 긴 거리의 의존성을 잘 모델링하지 못해서 이미지 내의 의미 정보를 제대로 활용하기가 힘들어. 반면에 Transformer는 계산 복잡도가 제곱으로 늘어나서 문제야. 그래서 Mamba 같은 상태 공간 모델(SSM)이 주목받고 있어. 이 모델들은 긴 거리 상호작용을 잘 모델링할 뿐만 아니라 계산 복잡도도 선형으로 유지할 수 있어.

SSM과 Transformer의 하이브리드 메커니즘을 신중하게 설계하면 시각적 특징을 효율적으로 모델링하는 능력을 높일 수 있어. 많은 실험을 통해 Mamba 아키텍처의 레이어 뒤쪽에 자기 주의 메커니즘을 통합하면 긴 거리 공간 의존성을 잡는 모델링 능력이 크게 향상된다는 걸 보여줬어.

이 논문에서는 SSM의 하이브리드 메커니즘을 활용해서 의료 이미지 분할을 위한 U자 형태 아키텍처 모델, 즉 Hybird Transformer vision Mamba UNet (HTM-UNet)을 제안해. ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir, CVC-ColonDB, ETIS-Larib PolypDB와 같은 공공 데이터셋과 ZD-LCI-GIM이라는 개인 데이터셋에서 종합적인 실험을 진행했어. 결과적으로 HTM-UNet이 의료 이미지 분할 작업에서 경쟁력 있는 성능을 보인다는 걸 알 수 있었어. 코드도 이 URL에서 확인할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.00442
Title: Separation of Body and Background in Radiological Images. A Practical Python Code

Original Abstract:
Radiological images, such as magnetic resonance imaging (MRI) and computed tomography (CT) images, typically consist of a body part and a dark background. For many analyses, it is necessary to separate the body part from the background. In this article, we present a Python code designed to separate body and background regions in 2D and 3D radiological images. We tested the algorithm on various MRI and CT images of different body parts, including the brain, neck, and abdominal regions. Additionally, we introduced a method for intensity normalization and outlier restriction, adjusted for data conversion into 8-bit unsigned integer (UINT8) format, and examined its effects on body-background separation. Our Python code is available for use with proper citation.

Translated Abstract:
방사선 이미지는 일반적으로 자기 공명 영상(MRI)이나 컴퓨터 단층 촬영(CT) 이미지처럼 신체 부분과 어두운 배경으로 구성돼 있어. 많은 분석을 위해서는 신체 부분과 배경을 분리하는 게 필요해.

이 논문에서는 2D와 3D 방사선 이미지에서 신체와 배경 영역을 분리하기 위해 설계된 파이썬 코드를 소개해. 우리는 다양한 신체 부분, 예를 들어 뇌, 목, 복부 지역의 여러 MRI와 CT 이미지에서 이 알고리즘을 테스트했어.

또한, 강도 정규화와 이상치 제한 방법도 도입했고, 8비트 부호 없는 정수(UINT8) 형식으로 데이터 변환할 때의 조정도 했어. 이 조정이 신체-배경 분리에 미치는 영향도 살펴봤어. 우리 파이썬 코드는 적절한 인용과 함께 사용할 수 있어.

================================================================================

URL: https://arxiv.org/abs/2409.01421
Title: DiffCSG: Differentiable CSG via Rasterization

Original Abstract:
Differentiable rendering is a key ingredient for inverse rendering and machine learning, as it allows to optimize scene parameters (shape, materials, lighting) to best fit target images. Differentiable rendering requires that each scene parameter relates to pixel values through differentiable operations. While 3D mesh rendering algorithms have been implemented in a differentiable way, these algorithms do not directly extend to Constructive-Solid-Geometry (CSG), a popular parametric representation of shapes, because the underlying boolean operations are typically performed with complex black-box mesh-processing libraries. We present an algorithm, DiffCSG, to render CSG models in a differentiable manner. Our algorithm builds upon CSG rasterization, which displays the result of boolean operations between primitives without explicitly computing the resulting mesh and, as such, bypasses black-box mesh processing. We describe how to implement CSG rasterization within a differentiable rendering pipeline, taking special care to apply antialiasing along primitive intersections to obtain gradients in such critical areas. Our algorithm is simple and fast, can be easily incorporated into modern machine learning setups, and enables a range of applications for computer-aided design, including direct and image-based editing of CSG primitives. Code and data: this https URL.

Translated Abstract:
차별화된 렌더링은 역 렌더링과 머신 러닝에 중요한 요소야. 이걸 통해 장면의 매개변수(형상, 재료, 조명)를 최적화해서 목표 이미지에 잘 맞출 수 있어. 차별화된 렌더링은 각 장면 매개변수가 픽셀 값과 차별화 가능한 연산으로 연결되어야 해.

3D 메쉬 렌더링 알고리즘은 차별화된 방식으로 구현되긴 했지만, 이런 알고리즘은 CSG(Constructive-Solid-Geometry) 같은 인기 있는 매개변수 형태로 직접 확장되기 어려워. 그 이유는 기본적인 불리언 연산이 복잡한 블랙 박스 메쉬 처리 라이브러리로 수행되기 때문이야.

우리는 DiffCSG라는 알고리즘을 제안해. 이 알고리즘은 CSG 모델을 차별화된 방식으로 렌더링할 수 있게 해줘. DiffCSG는 기본 도형 간의 불리언 연산 결과를 직접적으로 메쉬를 계산하지 않고 보여주는 CSG 래스터화에 기반하고 있어. 그래서 블랙 박스 메쉬 처리를 피할 수 있어.

우리는 차별화된 렌더링 파이프라인에서 CSG 래스터화를 구현하는 방법을 설명할 거야. 특히, 기본 도형의 교차점에서 안티앨리어싱을 적용해 중요한 영역에서 기울기를 얻는 데 신경 썼어. 이 알고리즘은 간단하고 빠르며, 현대의 머신 러닝 설정에 쉽게 통합될 수 있어. 그리고 CSG 기본 도형의 직접적인 편집이나 이미지 기반 편집 같은 다양한 컴퓨터 지원 설계 애플리케이션에 활용할 수 있어. 

코드와 데이터는 이 링크에서 확인해.

================================================================================

URL: https://arxiv.org/abs/2409.03336
Title: Estimating Indoor Scene Depth Maps from Ultrasonic Echoes

Original Abstract:
Measuring 3D geometric structures of indoor scenes requires dedicated depth sensors, which are not always available. Echo-based depth estimation has recently been studied as a promising alternative solution. All previous studies have assumed the use of echoes in the audible range. However, one major problem is that audible echoes cannot be used in quiet spaces or other situations where producing audible sounds is prohibited. In this paper, we consider echo-based depth estimation using inaudible ultrasonic echoes. While ultrasonic waves provide high measurement accuracy in theory, the actual depth estimation accuracy when ultrasonic echoes are used has remained unclear, due to its disadvantage of being sensitive to noise and susceptible to attenuation. We first investigate the depth estimation accuracy when the frequency of the sound source is restricted to the high-frequency band, and found that the accuracy decreased when the frequency was limited to ultrasonic ranges. Based on this observation, we propose a novel deep learning method to improve the accuracy of ultrasonic echo-based depth estimation by using audible echoes as auxiliary data only during training. Experimental results with a public dataset demonstrate that our method improves the estimation accuracy.

Translated Abstract:
실내 장면의 3D 기하학 구조를 측정하려면 전용 깊이 센서가 필요한데, 이런 센서가 항상 있는 건 아냐. 최근에는 에코를 이용한 깊이 추정이 유망한 대안으로 연구되고 있어. 그런데 기존 연구들은 모두 들리는 주파수 범위의 에코만 사용한다고 가정했어. 

하지만 문제는 조용한 공간이나 소리를 내는 게 금지된 상황에서는 들리는 에코를 사용할 수 없다는 거야. 그래서 이 논문에서는 들리지 않는 초음파 에코를 이용한 깊이 추정을 고려해봤어. 이론적으로 초음파는 높은 측정 정확도를 제공하지만, 실제로 초음파 에코를 사용할 때 깊이 추정의 정확도가 얼마나 되는지는 잘 몰랐어. 왜냐면 초음파는 소음에 민감하고 감쇠에 취약한 단점이 있기 때문이야. 

우리는 먼저 소리의 주파수를 높은 주파수 대역으로 제한했을 때 깊이 추정의 정확성을 조사해봤고, 주파수를 초음파 범위로 제한하니 정확도가 떨어진다는 걸 발견했어. 이 관찰을 바탕으로, 우리는 훈련할 때만 들리는 에코를 보조 데이터로 사용해서 초음파 에코 기반 깊이 추정의 정확성을 높이는 새로운 딥러닝 방법을 제안했어. 공개 데이터셋을 활용한 실험 결과, 우리의 방법이 깊이 추정의 정확성을 개선한다는 걸 보여줬어.

================================================================================

