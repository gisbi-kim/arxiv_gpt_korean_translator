URL:
https://arxiv.org/pdf/2409.06807.pdf

Title: Kino-PAX: Highly Parallel Kinodynamic Sampling-based Planner

Original Abstract:
Sampling-based motion planners (SBMPs) are effective for planning with complex kinodynamic constraints in high-dimensional spaces, but they still struggle to achieve real-time performance, which is mainly due to their serial computation design. We present Kinodynamic Parallel Accelerated eXpansion (Kino-PAX), a novel highly parallel kinodynamic SBMP designed for parallel devices such as GPUs. Kino-PAX grows a tree of trajectory segments directly in parallel. Our key insight is how to decompose the iterative tree growth process into three massively parallel subroutines. Kino-PAX is designed to align with the parallel device execution hierarchies, through ensuring that threads are largely independent, share equal workloads, and take advantage of low-latency resources while minimizing high-latency data transfers and process synchronization. This design results in a very efficient GPU implementation. We prove that Kino-PAX is probabilistically complete and analyze its scalability with compute hardware improvements. Empirical evaluations demonstrate solutions in the order of 10 ms on a desktop GPU and in the order of 100 ms on an embedded GPU, representing up to 1000 times improvement compared to coarse-grained CPU parallelization of state-of-the-art sequential algorithms over a range of complex environments and systems.

Translated Abstract:
샘플링 기반 모션 플래너(SBMP)는 복잡한 운동 동역학 제약을 가진 고차원 공간에서 계획을 세우는 데 효과적이지만, 실시간 성능을 달성하는 데 어려움을 겪고 있어. 주된 이유는 직렬 계산 설계 때문이야. 

우리는 Kinodynamic Parallel Accelerated eXpansion (Kino-PAX)라는 새로운 고도로 병렬화된 운동 동역학 SBMP를 소개해. 이건 GPU 같은 병렬 장치에 맞춰 설계되었어. Kino-PAX는 경로 조각의 트리를 직접 병렬로 확장해. 우리의 핵심 아이디어는 반복적인 트리 성장 과정을 세 가지 대규모 병렬 서브루틴으로 분해하는 거야. 

Kino-PAX는 스레드가 독립적이고, 동일한 작업량을 공유하며, 낮은 지연 자원을 활용할 수 있도록 병렬 장치 실행 계층에 맞춰 설계됐어. 덕분에 고급 GPU 구현이 매우 효율적이야. 우리는 Kino-PAX가 확률적으로 완전하다는 걸 증명하고, 컴퓨트 하드웨어 개선에 따른 확장성을 분석했어. 

실험 결과, 데스크탑 GPU에서 약 10ms, 임베디드 GPU에서 약 100ms의 솔루션을 보여줬어. 이는 최신 순차 알고리즘의 조잡한 CPU 병렬화와 비교했을 때 최대 1000배의 개선을 나타내. 복잡한 환경과 시스템에서도 효과적으로 작동해.

================================================================================

URL:
https://arxiv.org/pdf/2409.06817.pdf

Title: Bifurcation Identification for Ultrasound-driven Robotic Cannulation

Original Abstract:
In trauma and critical care settings, rapid and precise intravascular access is key to patients' survival. Our research aims at ensuring this access, even when skilled medical personnel are not readily available. Vessel bifurcations are anatomical landmarks that can guide the safe placement of catheters or needles during medical procedures. Although ultrasound is advantageous in navigating anatomical landmarks in emergency scenarios due to its portability and safety, to our knowledge no existing algorithm can autonomously extract vessel bifurcations using ultrasound images. This is primarily due to the limited availability of ground truth data, in particular, data from live subjects, needed for training and validating reliable models. Researchers often resort to using data from anatomical phantoms or simulations. We introduce BIFURC, Bifurcation Identification for Ultrasound-driven Robot Cannulation, a novel algorithm that identifies vessel bifurcations and provides optimal needle insertion sites for an autonomous robotic cannulation system. BIFURC integrates expert knowledge with deep learning techniques to efficiently detect vessel bifurcations within the femoral region and can be trained on a limited amount of in-vivo data. We evaluated our algorithm using a medical phantom as well as real-world experiments involving live pigs. In all cases, BIFURC consistently identified bifurcation points and needle insertion locations in alignment with those identified by expert clinicians.

Translated Abstract:
외상 및 중환자 치료 환경에서는 신속하고 정확한 혈관 접근이 환자의 생존에 매우 중요해. 우리 연구는 숙련된 의료 인력이 항상 이용 가능하지 않을 때도 이 접근을 보장하는 걸 목표로 하고 있어. 혈관 분기점은 의료 절차 중 카테터나 바늘을 안전하게 배치하는 데 도움을 주는 해부학적 기준점이야. 초음파는 휴대성이 좋고 안전해서 응급 상황에서 해부학적 기준점을 찾는 데 유리하지만, 우리가 아는 한 현재 초음파 이미지를 사용해 혈관 분기점을 자동으로 추출할 수 있는 알고리즘은 없어. 이건 주로 신뢰할 수 있는 모델을 교육하고 검증하는 데 필요한 실제 데이터가 부족하기 때문이야. 연구자들은 종종 해부학적 팬텀이나 시뮬레이션 데이터를 사용하곤 해.

우리는 BIFURC, 즉 초음파 기반 로봇 카누레이션을 위한 분기점 식별 알고리즘을 소개해. 이 알고리즘은 혈관 분기점을 식별하고 자율 로봇 카누레이션 시스템을 위한 최적의 바늘 삽입 위치를 제공해. BIFURC는 전문가의 지식을 딥러닝 기법과 결합해 대퇴부 지역 내에서 혈관 분기점을 효율적으로 감지할 수 있어. 그리고 제한된 양의 생체 데이터를 사용해서도 훈련할 수 있어.

우리는 이 알고리즘을 의료 팬텀과 실제 생돼지를 대상으로 한 실험을 통해 평가했어. 모든 경우에서 BIFURC는 전문가들이 식별한 것과 일치하는 분기점과 바늘 삽입 위치를 일관되게 찾아냈어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06864.pdf

Title: PRO-MIND: Proximity and Reactivity Optimisation of robot Motion to tune safety limits, human stress, and productivity in INDustrial settings

Original Abstract:
Despite impressive advancements of industrial collaborative robots, their potential remains largely untapped due to the difficulty in balancing human safety and comfort with fast production constraints. To help address this challenge, we present PRO-MIND, a novel human-in-the-loop framework that leverages valuable data about the human co-worker to optimise robot trajectories. By estimating human attention and mental effort, our method dynamically adjusts safety zones and enables on-the-fly alterations of the robot path to enhance human comfort and optimal stopping conditions. Moreover, we formulate a multi-objective optimisation to adapt the robot's trajectory execution time and smoothness based on the current human psycho-physical stress, estimated from heart rate variability and frantic movements. These adaptations exploit the properties of B-spline curves to preserve continuity and smoothness, which are crucial factors in improving motion predictability and comfort. Evaluation in two realistic case studies showcases the framework's ability to restrain the operators' workload and stress and to ensure their safety while enhancing human-robot productivity. Further strengths of PRO-MIND include its adaptability to each individual's specific needs and sensitivity to variations in attention, mental effort, and stress during task execution.

Translated Abstract:
산업 협동 로봇은 많은 발전을 이루었지만, 인간의 안전과 편안함을 빠른 생산 요구사항과 잘 맞추는 것이 어렵기 때문에 그 잠재력이 크게 활용되지 않고 있어. 이 문제를 해결하기 위해, 우리는 PRO-MIND라는 새로운 인간-로봇 협력 프레임워크를 제안해. 이 시스템은 인간 동료에 대한 유용한 데이터를 활용해서 로봇의 경로를 최적화해.

우리 방법은 인간의 주의력과 정신적 노력을 추정해서, 안전 구역을 동적으로 조정하고 로봇의 경로를 즉석에서 변경할 수 있도록 해. 이렇게 해서 인간의 편안함과 최적의 정지 조건을 향상시킬 수 있어. 더불어, 우리는 심박수 변동성과 불안정한 움직임을 바탕으로 현재 인간의 심리적 스트레스에 맞춰 로봇의 경로 실행 시간과 부드러움을 조정하는 다중 목표 최적화를 만들어냈어.

이러한 조정은 B-스플라인 곡선의 특성을 활용해서 연속성과 부드러움을 유지하는데, 이는 동작 예측 가능성과 편안함을 개선하는 데 중요한 요소야. 두 가지 현실적인 사례 연구에서 평가해본 결과, 이 프레임워크가 작업자의 업무량과 스트레스를 줄이고, 안전을 보장하면서 인간-로봇 생산성을 높일 수 있는 능력을 보여줬어.

PRO-MIND의 또 다른 강점은 각 개인의 특정 요구에 적응할 수 있고, 작업 수행 중 주의력, 정신적 노력, 스트레스의 변화를 민감하게 반영할 수 있다는 점이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06912.pdf

Title: A Bayesian framework for active object recognition, pose estimation and shape transfer learning through touch

Original Abstract:
As humans can explore and understand the world through the sense of touch, tactile sensing is also an important aspect of robotic perception. In unstructured environments, robots can encounter both known and novel objects, this calls for a method to address both known and novel objects. In this study, we combine a particle filter (PF) and Gaussian process implicit surface (GPIS) in a unified Bayesian framework. The framework can differentiate between known and novel objects, perform object recognition, estimate pose for known objects, and reconstruct shapes for unknown objects, in an active learning fashion. By grounding the selection of the GPIS prior with the maximum-likelihood-estimation (MLE) shape from the PF, the knowledge about known objects' shapes can be transferred to learn novel shapes. An exploration procedure with global shape estimation is proposed to guide active data acquisition and conclude the exploration when sufficient information is obtained. The performance of the proposed Bayesian framework is evaluated through simulations on known and novel objects, initialized with random poses and is compared with a rapidly explore random tree (RRT).The results show that the proposed exploration procedure, utilizing global shape estimation, achieves faster exploration than the RRT-based local exploration procedure. Overall, results indicate that the proposed framework is effective and efficient in object recognition, pose estimation and shape reconstruction. Moreover, we show that a learned shape can be included as a new prior and used effectively for future object recognition and pose estimation of novel objects.

Translated Abstract:
사람들이 촉각을 통해 세상을 탐험하고 이해할 수 있듯이, 로봇의 인식에서도 촉각 센싱이 중요한 부분이에요. 비구조적인 환경에서는 로봇이 이미 알고 있는 물체와 새로운 물체 모두를 만날 수 있는데, 이를 처리할 방법이 필요해요.

이 연구에서는 입자 필터(PF)와 가우시안 프로세스 암묵적 표면(GPIS)을 통합한 베이지안 프레임워크를 결합했어요. 이 프레임워크는 이미 아는 물체와 새로운 물체를 구분하고, 물체 인식을 수행하며, 아는 물체의 자세를 추정하고, 모르는 물체의 형태를 재구성할 수 있어요. 이 모든 과정은 능동적인 학습 방식으로 진행돼요.

GPIS의 선택을 PF에서 최대 우도 추정(MLE) 형태로 고정함으로써, 아는 물체의 형태에 대한 지식을 새로운 형태를 배우는 데 사용할 수 있어요. 글로벌 형태 추정과 함께 탐색 절차를 제안해서, 능동적으로 데이터를 수집하고 충분한 정보가 얻어지면 탐색을 종료해요.

제안된 베이지안 프레임워크의 성능은 랜덤 자세로 시작한 아는 물체와 새로운 물체에 대한 시뮬레이션을 통해 평가되었고, 빠른 탐색 랜덤 트리(RRT)와 비교했어요. 결과적으로, 글로벌 형태 추정을 활용한 탐색 절차가 RRT 기반의 지역 탐색 절차보다 더 빠른 탐색을 달성했어요. 

전체적으로 결과는 제안된 프레임워크가 물체 인식, 자세 추정, 형태 재구성에서 효과적이고 효율적임을 나타내요. 게다가 학습한 형태를 새로운 우선사항으로 포함시켜서, 새로운 물체의 인식과 자세 추정에 효과적으로 사용할 수 있다는 것도 보여주고 있어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.06948.pdf

Title: Equivariant Filter for Tightly Coupled LiDAR-Inertial Odometry

Original Abstract:
Pose estimation is a crucial problem in simultaneous localization and mapping (SLAM). However, developing a robust and consistent state estimator remains a significant challenge, as the traditional extended Kalman filter (EKF) struggles to handle the model nonlinearity, especially for inertial measurement unit (IMU) and light detection and ranging (LiDAR). To provide a consistent and efficient solution of pose estimation, we propose Eq-LIO, a robust state estimator for tightly coupled LIO systems based on an equivariant filter (EqF). Compared with the invariant Kalman filter based on the $\SE_2(3)$ group structure, the EqF uses the symmetry of the semi-direct product group to couple the system state including IMU bias, navigation state and LiDAR extrinsic calibration state, thereby suppressing linearization error and improving the behavior of the estimator in the event of unexpected state changes. The proposed Eq-LIO owns natural consistency and higher robustness, which is theoretically proven with mathematical derivation and experimentally verified through a series of tests on both public and private datasets.

Translated Abstract:
포즈 추정은 동시에 위치 추정과 맵핑(SLAM)에서 중요한 문제야. 하지만, 강력하고 일관된 상태 추정기를 개발하는 건 여전히 큰 도전이야. 전통적인 확장 칼만 필터(EKF)는 모델 비선형성 처리에 어려움을 겪는데, 특히 관성 측정 장치(IMU)와 라이다(LiDAR)에 대해 그렇지. 

우리는 Eq-LIO라는 새로운 방법을 제안해. 이건 동등 불변 필터(EqF)를 기반으로 한 강력한 상태 추정기로, 밀접하게 연결된 LIO 시스템을 위한 거야. $\SE_2(3)$ 그룹 구조에 기반한 불변 칼만 필터와 비교했을 때, EqF는 반직접 곱 그룹의 대칭성을 이용해 IMU 편향, 내비게이션 상태, 라이다 외부 보정 상태를 포함한 시스템 상태를 연결해. 이렇게 해서 선형화 오류를 줄이고, 예상치 못한 상태 변화가 발생했을 때 추정기의 성능을 개선해.

제안한 Eq-LIO는 자연스러운 일관성과 더 높은 강인성을 가지고 있어. 이건 수학적 유도와 실험을 통해 이론적으로 증명되었고, 공공 데이터셋과 개인 데이터셋에서 여러 테스트를 통해 실험적으로 검증되었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06952.pdf

Title: Flow-Inspired Lightweight Multi-Robot Real-Time Scheduling Planner

Original Abstract:
Collision avoidance and trajectory planning are crucial in multi-robot systems, particularly in environments with numerous obstacles. Although extensive research has been conducted in this field, the challenge of rapid traversal through such environments has not been fully addressed. This paper addresses this problem by proposing a novel real-time scheduling scheme designed to optimize the passage of multi-robot systems through complex, obstacle-rich maps. Inspired from network flow optimization, our scheme decomposes the environment into a network structure, enabling the efficient allocation of robots to paths based on real-time congestion data. The proposed scheduling planner operates on top of existing collision avoidance algorithms, focusing on minimizing traversal time by balancing robot detours and waiting times. Our simulation results demonstrate the efficiency of the proposed scheme. Additionally, we validated its effectiveness through real world flight tests using ten quadrotors. This work contributes a lightweight, effective scheduling planner capable of meeting the real-time demands of multi-robot systems in obstacle-rich environments.

Translated Abstract:
다중 로봇 시스템에서 충돌 회피와 경로 계획은 정말 중요해. 특히 장애물이 많은 환경에서는 더 그렇고. 이 분야에 대한 연구가 많이 진행됐지만, 복잡한 환경을 빠르게 통과하는 문제는 아직 완전히 해결되지 않았어. 

이 논문에서는 이 문제를 해결하기 위해 새로운 실시간 스케줄링 방안을 제안해. 이 방법은 다중 로봇 시스템이 복잡하고 장애물이 많은 맵을 더 잘 통과할 수 있도록 최적화하는 거야. 네트워크 흐름 최적화에서 영감을 받아서, 환경을 네트워크 구조로 나누고, 로봇이 경로에 할당될 때 실시간 혼잡 데이터를 기반으로 효율적으로 배분할 수 있게 해. 

제안한 스케줄링 플래너는 기존의 충돌 회피 알고리즘 위에서 작동해, 로봇의 우회와 대기 시간을 조절하면서 통과 시간을 최소화하는 데 집중해. 시뮬레이션 결과로 이 방법의 효율성을 보여줬고, 실제로 10대의 쿼드로터를 이용한 비행 테스트에서도 효과를 검증했어. 

이 연구는 장애물이 많은 환경에서 다중 로봇 시스템의 실시간 요구를 충족할 수 있는 가볍고 효과적인 스케줄링 플래너를 제안하는 데 기여하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06959.pdf

Title: Pyramid-Monozone Synergistic Grasping Policy in Dense Clutter

Original Abstract:
Grasping a diverse range of novel objects from dense clutter poses a great challenge to robots because of the occlusion among these objects. In this work, we propose the Pyramid-Monozone Synergistic Grasping Policy (PMSGP) that enables robots to cleverly avoid most occlusions during grasping. Specifically, we initially construct the Pyramid Se quencing Policy (PSP) to sequence each object in the scene into a pyramid structure. By isolating objects layer-by-layer, the grasp candidates will focus on a single layer during each grasp. Then, we devise the Monozone Sampling Policy (MSP) to sample the grasp candidates in the top layer. Through this manner, each grasp will target the topmost object, thereby effectively avoiding most occlusions. We perform more than 7000 real world grasping among 300 novel objects in dense clutter scenes, demonstrating that PMSGP significantly outperforms seven competitive grasping methods. All grasping videos are available at: this https URL.

Translated Abstract:
밀집된 물체들 사이에서 다양한 새로운 물체를 잡는 건 로봇에게 큰 도전이에요. 왜냐하면 물체들 사이에 가려지는 부분이 많거든요. 이번 연구에서는 로봇이 잡을 때 대부분의 가림을 똑똑하게 피할 수 있는 '피라미드-모노존 시너제틱 그랩 정책(PMSGP)'을 제안해요.

우선, '피라미드 순서 정책(PSP)'을 만들어서 장면 속의 각 물체를 피라미드 구조로 정리해요. 각 층마다 물체를 분리하면서, 그랩 후보들은 각 그랩마다 한 층에만 집중할 수 있게 돼요. 그런 다음, '모노존 샘플링 정책(MSP)'을 통해 가장 위층의 그랩 후보들을 샘플링해요. 이렇게 하면 각 그랩은 제일 위에 있는 물체를 목표로 하게 돼서 대부분의 가림을 효과적으로 피할 수 있어요.

우리는 밀집된 장면에서 300개의 새로운 물체를 대상으로 7000회 이상의 실제 그랩 실험을 진행했어요. 그 결과, PMSGP가 7개의 경쟁적인 그랩 방법보다 훨씬 뛰어난 성능을 보였어요. 모든 그랩 영상은 이 URL에서 확인할 수 있어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.06961.pdf

Title: Control Pneumatic Soft Bending Actuator with Feedforward Hysteresis Compensation by Pneumatic Physical Reservoir Computing

Original Abstract:
The nonlinearities of soft robots bring control challenges like hysteresis but also provide them with computational capacities. This paper introduces a fuzzy pneumatic physical reservoir computing (FPRC) model for feedforward hysteresis compensation in motion tracking control of soft actuators. Our method utilizes a pneumatic bending actuator as a physical reservoir with nonlinear computing capacities to control another pneumatic bending actuator. The FPRC model employs a Takagi-Sugeno (T-S) fuzzy model to process outputs from the physical reservoir. In comparative evaluations, the FPRC model shows equivalent training performance to an Echo State Network (ESN) model, whereas it exhibits better test accuracies with significantly reduced execution time. Experiments validate the proposed FPRC model's effectiveness in controlling the bending motion of the pneumatic soft actuator with open and closed-loop control systems. The proposed FPRC model's robustness against environmental disturbances has also been experimentally verified. To the authors' knowledge, this is the first implementation of a physical system in the feedforward hysteresis compensation model for controlling soft actuators. This study is expected to advance physical reservoir computing in nonlinear control applications and extend the feedforward hysteresis compensation methods for controlling soft actuators.

Translated Abstract:
부드러운 로봇의 비선형 특성은 제어에 어려움을 주는 히스테리시스를 발생시키지만, 동시에 계산 능력을 제공합니다. 이 논문에서는 부드러운 액추에이터의 동작 추적 제어에서 피드포워드 히스테리시스 보상을 위한 퍼지 공압 물리적 저장소 컴퓨팅(FPRC) 모델을 소개합니다.

우리의 방법은 비선형 계산 능력을 가진 물리적 저장소로 공압 구부림 액추에이터를 사용하여 다른 공압 구부림 액추에이터를 제어합니다. FPRC 모델은 물리적 저장소의 출력을 처리하기 위해 타카기-수게노(T-S) 퍼지 모델을 활용합니다. 비교 평가에서 FPRC 모델은 에코 상태 네트워크(ESN) 모델과 비슷한 훈련 성능을 보였지만, 테스트 정확도는 더 높고 실행 시간은 크게 줄어드는 결과를 나타냈습니다.

실험을 통해 FPRC 모델이 열린 루프와 닫힌 루프 제어 시스템에서 공압 소프트 액추에이터의 구부림 동작을 제어하는 데 효과적임을 입증했습니다. 또한, FPRC 모델이 환경 방해에 대해 강건하다는 것도 실험적으로 확인되었습니다. 저자들의 지식으로는, 이것이 부드러운 액추에이터를 제어하기 위한 피드포워드 히스테리시스 보상 모델에서 물리적 시스템을 처음으로 구현한 사례입니다. 이 연구는 비선형 제어 응용 분야에서 물리적 저장소 컴퓨팅을 발전시키고, 부드러운 액추에이터 제어를 위한 피드포워드 히스테리시스 보상 방법을 확장할 것으로 기대됩니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.06990.pdf

Title: SIS: Seam-Informed Strategy for T-shirt Unfolding

Original Abstract:
Seams are information-rich components of garments. The presence of different types of seams and their combinations helps to select grasping points for garment handling. In this paper, we propose a new Seam-Informed Strategy (SIS) for finding actions for handling a garment, such as grasping and unfolding a T-shirt. Candidates for a pair of grasping points for a dual-arm manipulator system are extracted using the proposed Seam Feature Extraction Method (SFEM). A pair of grasping points for the robot system is selected by the proposed Decision Matrix Iteration Method (DMIM). The decision matrix is first computed by multiple human demonstrations and updated by the robot execution results to improve the grasping and unfolding performance of the robot. Note that the proposed scheme is trained on real data without relying on simulation. Experimental results demonstrate the effectiveness of the proposed strategy. The project video is available at this https URL.

Translated Abstract:
옷의 솔기는 정보가 풍부한 부분이야. 다양한 종류의 솔기와 그 조합이 옷을 다룰 때 잡는 지점을 선택하는 데 도움을 줘. 이 논문에서는 티셔츠 같은 옷을 잡고 펼치는 동작을 찾기 위한 새로운 솔기 정보 기반 전략(SIS)을 제안해.

먼저, 제안된 솔기 특징 추출 방법(SFEM)을 사용해 이중 팔 조작기 시스템을 위한 잡는 지점 후보를 뽑아. 그런 다음, 제안된 결정 행렬 반복 방법(DMIM)을 통해 로봇 시스템을 위한 잡는 지점을 선택해. 결정 행렬은 여러 사람의 시연을 바탕으로 처음 계산되고, 로봇의 실행 결과에 따라 업데이트돼서 잡기와 펼치기 성능을 개선해.

중요한 건 이 방법이 시뮬레이션에 의존하지 않고 실제 데이터를 기반으로 훈련된다는 거야. 실험 결과는 이 전략의 효과를 보여줘. 프로젝트 비디오는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07013.pdf

Title: Enabling Shared-Control for A Riding Ballbot System

Original Abstract:
This study introduces a shared-control approach for collision avoidance in a self-balancing riding ballbot, called PURE, marked by its dynamic stability, omnidirectional movement, and hands-free interface. Integrated with a sensor array and a novel Passive Artificial Potential Field (PAPF) method, PURE provides intuitive navigation with deceleration assistance and haptic/audio feedback, effectively mitigating collision risks. This approach addresses the limitations of traditional APF methods, such as control oscillations and unnecessary speed reduction in challenging scenarios. A human-robot interaction experiment, with 20 manual wheelchair users and able-bodied individuals, was conducted to evaluate the performance of indoor navigation and obstacle avoidance with the proposed shared-control algorithm. Results indicated that shared-control significantly reduced collisions and cognitive load without affecting travel speed, offering intuitive and safe operation. These findings highlight the shared-control system's suitability for enhancing collision avoidance in self-balancing mobility devices, a relatively unexplored area in assistive mobility research.

Translated Abstract:
이 연구는 PURE라는 자가 균형 유지가 가능한 롤링 볼봇에서 충돌을 피하기 위한 공유 제어 방법을 소개해. 이 볼봇은 동적인 안정성, 전방위 이동, 그리고 손이 필요 없는 인터페이스가 특징이야. 

여기엔 센서 배열과 새로운 수동 인공 잠재력 장(field) 방법인 PAPF가 통합되어 있어서, 직관적인 내비게이션을 제공하고 감속 지원, 촉각 및 음향 피드백을 통해 충돌 위험을 효과적으로 줄일 수 있어. 이 방법은 전통적인 APF 방법의 한계인 제어 진동과 힘든 상황에서의 불필요한 속도 감소 문제를 해결해. 

연구에서는 20명의 수동 휠체어 사용자와 비장애인을 대상으로 실내 내비게이션과 장애물 회피 성능을 평가하기 위한 인간-로봇 상호작용 실험을 진행했어. 결과적으로 공유 제어 방식이 충돌을 크게 줄이고 인지 부담을 낮추면서도 이동 속도에는 영향을 미치지 않는다는 걸 보여줬어. 

이 발견은 자가 균형 유지 이동 장치에서 충돌 회피를 개선하는 데 공유 제어 시스템이 적합하다는 걸 강조해. 이 부분은 보조 이동 연구에서 상대적으로 탐구되지 않은 분야야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07050.pdf

Title: Invariant filtering for wheeled vehicle localization with unknown wheel radius and unknown GNSS lever arm

Original Abstract:
We consider the problem of observer design for a nonholonomic car (more generally a wheeled robot) equipped with wheel speeds with unknown wheel radius, and whose position is measured via a GNSS antenna placed at an unknown position in the car. In a tutorial and unified exposition, we recall the recent theory of two-frame systems within the field of invariant Kalman filtering. We then show how to adapt it geometrically to address the considered problem, although it seems at first sight out of its scope. This yields an invariant extended Kalman filter having autonomous error equations, and state-independent Jacobians, which is shown to work remarkably well in simulations. The proposed novel construction thus extends the application scope of invariant filtering.

Translated Abstract:
우리는 바퀴가 있는 로봇, 특히 비홀로노믹 자동차를 위한 관측자 설계 문제를 고려해. 이 자동차는 바퀴 속도를 측정하는데 바퀴 반지름이 알려져 있지 않고, 위치는 자동차 안에 있는 GNSS 안테나를 통해 측정돼. 

이 논문에서는 불변 칼만 필터링 분야에서 최근에 개발된 두 프레임 시스템 이론을 간단하게 설명할 거야. 그런 다음 이 이론을 기하학적으로 어떻게 조정해서 우리가 다루고 있는 문제를 해결할 수 있는지 보여줄 거야. 처음에는 이 문제에 맞지 않는 것처럼 보이지만, 실제로는 잘 작동해.

이 결과로, 자율적인 오차 방정식과 상태에 독립적인 자코비안을 가진 불변 확장 칼만 필터를 얻었어. 이 필터는 시뮬레이션에서 정말 잘 작동하는 걸 보여줬어. 제안한 새로운 구조는 불변 필터링의 적용 범위를 확장하는 데 기여해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07080.pdf

Title: Scenario Execution for Robotics: A generic, backend-agnostic library for running reproducible robotics experiments and tests

Original Abstract:
Testing and evaluation of robotics systems is a difficult and oftentimes tedious task due to the systems' complexity and a lack of tools to conduct reproducible robotics experiments. Additionally, almost all available tools are either tailored towards a specific application domain, simulator or middleware. Particularly scenario-based testing, a common practice in the domain of automated driving, is not sufficiently covered in the robotics domain. In this paper, we propose a novel backend- and middleware-agnostic approach for conducting systematic, reproducible and automatable robotics experiments called Scenario Execution for Robotics. Our approach is implemented as a Python library built on top of the generic scenario description language OpenSCENARIO 2 and Behavior Trees and is made publicly available on GitHub. In extensive experiments, we demonstrate that our approach supports multiple simulators as backend and can be used as a standalone Python-library or as part of the ROS2 ecosystem. Furthermore, we demonstrate how our approach enables testing over ranges of varying values. Finally, we show how Scenario Execution for Robotics allows to move from simulation-based to real-world experiments with minimal adaptations to the scenario description file.

Translated Abstract:
로봇 시스템을 테스트하고 평가하는 건 복잡해서 힘들고 지루한 일인 경우가 많아. 게다가 reproducible한 로봇 실험을 할 수 있는 도구가 부족해. 현재 있는 도구들은 특정한 응용 분야나 시뮬레이터, 미들웨어에 맞춰져 있는 경우가 대부분이야. 특히, 자율주행 분야에서 많이 쓰이는 시나리오 기반 테스트는 로봇 분야에서는 충분히 다뤄지지 않고 있어.

이 논문에서는 "Scenario Execution for Robotics"라고 하는 새로운 접근 방식을 제안해. 이 방법은 시스템과 미들웨어에 구애받지 않고 체계적이고 재현 가능하며 자동화된 로봇 실험을 할 수 있게 해줘. 우리의 접근 방식은 OpenSCENARIO 2라는 일반적인 시나리오 설명 언어와 Behavior Trees를 기반으로 한 파이썬 라이브러리로 구현됐고, GitHub에서 공개돼 있어.

우리는 광범위한 실험을 통해 이 방법이 여러 시뮬레이터를 백엔드로 지원하고, 독립적인 파이썬 라이브러리로 사용되거나 ROS2 생태계의 일부로 활용될 수 있음을 보여줬어. 그리고 이 방법이 다양한 값의 범위에 걸쳐 테스트를 가능하게 해주는 방법도 설명했어. 마지막으로, "Scenario Execution for Robotics"가 시뮬레이션 기반 실험에서 실제 세계의 실험으로 최소한의 수정으로 넘어갈 수 있게 해준다는 점도 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07091.pdf

Title: Learning Task Specifications from Demonstrations as Probabilistic Automata

Original Abstract:
Specifying tasks for robotic systems traditionally requires coding expertise, deep domain knowledge, and significant time investment. While learning from demonstration offers a promising alternative, existing methods often struggle with tasks of longer horizons. To address this limitation, we introduce a computationally efficient approach for learning probabilistic deterministic finite automata (PDFA) that capture task structures and expert preferences directly from demonstrations. Our approach infers sub-goals and their temporal dependencies, producing an interpretable task specification that domain experts can easily understand and adjust. We validate our method through experiments involving object manipulation tasks, showcasing how our method enables a robot arm to effectively replicate diverse expert strategies while adapting to changing conditions.

Translated Abstract:
로봇 시스템에 작업을 지정하는 데는 보통 코딩 기술, 깊은 전문 지식, 그리고 많은 시간이 필요해. 하지만 시연을 통해 배우는 방법이 괜찮은 대안이 될 수 있어. 다만 기존 방법들은 긴 시간의 작업을 다루는 데 어려움이 있어.

이런 한계를 해결하기 위해, 우리는 시연에서 작업 구조와 전문가의 선호를 직접 잡아내는 확률적 결정론적 유한 자동기계(PDFA)를 배우는 효율적인 방법을 소개해. 이 방법은 서브 목표와 그 시간적 의존성을 추론해서, 전문가들이 쉽게 이해하고 조정할 수 있는 해석 가능한 작업 명세를 만들어.

우리는 물체 조작 작업을 포함한 실험을 통해 이 방법을 검증했어. 이 실험에서는 로봇 팔이 다양한 전문가 전략을 효과적으로 복제하면서 변화하는 조건에 적응하는 모습을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07107.pdf

Title: End-to-End and Highly-Efficient Differentiable Simulation for Robotics

Original Abstract:
Over the past few years, robotics simulators have largely improved in efficiency and scalability, enabling them to generate years of simulated data in a few hours. Yet, efficiently and accurately computing the simulation derivatives remains an open challenge, with potentially high gains on the convergence speed of reinforcement learning and trajectory optimization algorithms, especially for problems involving physical contact interactions. This paper contributes to this objective by introducing a unified and efficient algorithmic solution for computing the analytical derivatives of robotic simulators. The approach considers both the collision and frictional stages, accounting for their intrinsic nonsmoothness and also exploiting the sparsity induced by the underlying multibody systems. These derivatives have been implemented in C++, and the code will be open-sourced in the Simple simulator. They depict state-of-the-art timings ranging from 5 microseconds for a 7-dof manipulator up to 95 microseconds for 36-dof humanoid, outperforming alternative solutions by a factor of at least 100.

Translated Abstract:
최근 몇 년 동안 로봇 시뮬레이터의 효율성과 확장성이 크게 향상되었어요. 덕분에 몇 시간 만에 수년 분의 시뮬레이션 데이터를 생성할 수 있게 되었죠. 하지만 시뮬레이션의 파생값을 효율적이고 정확하게 계산하는 건 여전히 해결해야 할 문제예요. 이 부분이 강화 학습과 경로 최적화 알고리즘의 수렴 속도에 큰 영향을 줄 수 있어요. 특히 물리적 접촉이 포함된 문제에서는 더 그렇죠.

이 논문은 로봇 시뮬레이터의 분석적 파생값을 계산하기 위한 통합적이고 효율적인 알고리즘 솔루션을 제안해요. 이 방법은 충돌과 마찰 단계 모두를 고려하고, 이들 고유의 비부드러움도 반영해요. 또, 다물체 시스템에서 발생하는 희소성도 활용해요. 

이 파생값들은 C++로 구현되었고, Simple 시뮬레이터에서 오픈소스로 공개될 예정이에요. 이 코드의 성능은 7자유도 조작기에서 5마이크로초에서 36자유도 휴머노이드에서 95마이크로초까지 다양하고, 다른 대안 솔루션보다 최소 100배 더 뛰어난 성능을 보여줘요.

================================================================================

URL:
https://arxiv.org/pdf/2409.07116.pdf

Title: iKalibr-RGBD: Partially-Specialized Target-Free Visual-Inertial Spatiotemporal Calibration For RGBDs via Continuous-Time Velocity Estimation

Original Abstract:
Visual-inertial systems have been widely studied and applied in the last two decades, mainly due to their low cost and power consumption, small footprint, and high availability. Such a trend simultaneously leads to a large amount of visual-inertial calibration methods being presented, as accurate spatiotemporal parameters between sensors are a prerequisite for visual-inertial fusion. In our previous work, i.e., iKalibr, a continuous-time-based visual-inertial calibration method was proposed as a part of one-shot multi-sensor resilient spatiotemporal calibration. While requiring no artificial target brings considerable convenience, computationally expensive pose estimation is demanded in initialization and batch optimization, limiting its availability. Fortunately, this could be vastly improved for the RGBDs with additional depth information, by employing mapping-free ego-velocity estimation instead of mapping-based pose estimation. In this paper, we present the continuous-time ego-velocity estimation-based RGBD-inertial spatiotemporal calibration, termed as iKalibr-RGBD, which is also targetless but computationally efficient. The general pipeline of iKalibr-RGBD is inherited from iKalibr, composed of a rigorous initialization procedure and several continuous-time batch optimizations. The implementation of iKalibr-RGBD is open-sourced at (this https URL) to benefit the research community.

Translated Abstract:
비주얼-관성 시스템은 지난 20년 동안 널리 연구되고 사용되어 왔어. 그 이유는 저렴한 비용, 낮은 전력 소비, 작은 크기, 그리고 높은 가용성 때문이야. 이런 트렌드는 비주얼-관성 융합을 위한 센서 간의 정확한 시공간 파라미터가 필요하다는 점에서 많은 비주얼-관성 캘리브레이션 방법이 제시되게 만들었어.

우리가 이전에 발표한 연구인 iKalibr에서는 연속 시간 기반의 비주얼-관성 캘리브레이션 방법을 제안했어. 이건 한 번에 여러 센서를 잘 맞추는 데 도움을 줘. 인공 목표물이 필요 없다는 점은 엄청 편리하지만, 초기화와 배치 최적화 과정에서 계산 비용이 많이 드는 포즈 추정이 필요해. 이 때문에 사용에 제한이 있었어. 하지만, RGBD 센서에 깊이 정보를 추가하면 매핑 없는 자가 속도 추정을 사용해 계산 효율을 크게 개선할 수 있어.

이번 논문에서는 연속 시간 자가 속도 추정 기반의 RGBD-관성 시공간 캘리브레이션 방법인 iKalibr-RGBD를 소개해. 이 방법도 목표물이 필요 없고, 계산적으로 효율적이야. iKalibr-RGBD의 일반적인 파이프라인은 iKalibr에서 이어져 오며, 정밀한 초기화 절차와 여러 연속 시간 배치 최적화로 구성되어 있어. iKalibr-RGBD의 구현은 연구 커뮤니티에 도움이 되도록 오픈 소스로 제공돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.07145.pdf

Title: The Critical Role of Effective Communication in Human-Robot Collaborative Assembly

Original Abstract:
In the rapidly evolving landscape of Human-Robot Collaboration (HRC), effective communication between humans and robots is crucial for complex task execution. Traditional request-response systems often lack naturalness and may hinder efficiency. This study emphasizes the importance of adopting human-like communication interactions to enable fluent vocal communication between human operators and robots simulating a collaborative human-robot industrial assembly. We propose a novel approach that employs human-like interactions through natural dialogue, enabling human operators to engage in vocal conversations with robots. Through a comparative experiment, we demonstrate the efficacy of our approach in enhancing task performance and collaboration efficiency. The robot's ability to engage in meaningful vocal conversations enables it to seek clarification, provide status updates, and ask for assistance when required, leading to improved coordination and a smoother workflow. The results indicate that the adoption of human-like conversational interactions positively influences the human-robot collaborative dynamic. Human operators find it easier to convey complex instructions and preferences, resulting in a more productive and satisfying collaboration experience.

Translated Abstract:
인간-로봇 협업(HRC)이 빠르게 발전하는 요즘, 인간과 로봇 간의 효과적인 소통이 복잡한 작업을 수행하는 데 정말 중요해. 전통적인 요청-응답 시스템은 자연스럽지 않은 경우가 많아서 효율을 떨어뜨릴 수 있어. 이 연구에서는 인간 같은 소통 방식을 채택하는 것이 얼마나 중요한지를 강조해. 이를 통해 인간 조작자와 로봇이 협력하는 산업 조립 작업에서 자연스러운 음성 소통을 가능하게 하려고 해.

우리는 자연스러운 대화를 통해 인간 같은 상호작용을 사용하는 새로운 방법을 제안해. 이 방법 덕분에 인간 조작자가 로봇과 음성 대화를 할 수 있게 돼. 비교 실험을 통해 우리의 접근 방식이 작업 성과와 협업 효율성을 높이는 데 효과적임을 보여줬어.

로봇이 의미 있는 음성 대화에 참여할 수 있게 되면, 로봇이 명확한 설명을 요청하거나 상태 업데이트를 제공하고, 필요할 때 도움을 요청할 수 있어. 이로 인해 조정이 잘 되고 작업 흐름이 매끄러워져. 결과적으로 인간 같은 대화 상호작용을 채택하면 인간-로봇 협업의 역동성에 긍정적인 영향을 미친다는 걸 알 수 있었어. 인간 조작자들은 복잡한 지시사항이나 선호도를 더 쉽게 전달할 수 있어서, 결과적으로 더 생산적이고 만족스러운 협업 경험을 하게 돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.07155.pdf

Title: Compliant Blind Handover Control for Human-Robot Collaboration

Original Abstract:
This paper presents a Human-Robot Blind Handover architecture within the context of Human-Robot Collaboration (HRC). The focus lies on a blind handover scenario where the operator is intentionally faced away, focused in a task, and requires an object from the robot. In this context, it is imperative for the robot to autonomously manage the entire handover process. Key considerations include ensuring safety while handing the object to the operator's hand, and detect the proper timing to release the object. The article explores strategies to navigate these challenges, emphasizing the need for a robot to operate safely and independently in facilitating blind handovers, thereby contributing to the advancement of HRC protocols and fostering a natural and efficient collaboration between humans and robots.

Translated Abstract:
이 논문은 인간-로봇 협력(HRC)에서의 인간-로봇 블라인드 핸드오버 아키텍처를 소개해. 여기서 블라인드 핸드오버는 조작자가 의도적으로 고개를 돌리고 작업에 집중하고 있을 때 로봇이 물체를 전달하는 상황을 말해.

이런 상황에서는 로봇이 전체 핸드오버 과정을 자율적으로 관리하는 게 중요해. 여기서 중요한 포인트는 조작자의 손에 물체를 안전하게 전달하고, 물체를 놓는 적절한 타이밍을 감지하는 거야.

이 논문은 이런 도전과제를 해결하기 위한 전략을 탐구하고, 로봇이 안전하고 독립적으로 블라인드 핸드오버를 지원할 필요성을 강조해. 결국, 이 연구는 HRC 프로토콜의 발전에 기여하고 인간과 로봇 간의 자연스럽고 효율적인 협력을 촉진할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07158.pdf

Title: Collaborative Conversation in Safe Multimodal Human-Robot Collaboration

Original Abstract:
In the context of Human-Robot Collaboration (HRC), it is crucial that the two actors are able to communicate with each other in a natural and efficient manner. The absence of a communication interface is often a cause of undesired slowdowns. On one hand, this is because unforeseen events may occur, leading to errors. On the other hand, due to the close contact between humans and robots, the speed must be reduced significantly to comply with safety standard ISO/TS 15066. In this paper, we propose a novel architecture that enables operators and robots to communicate efficiently, emulating human-to-human dialogue, while addressing safety concerns. This approach aims to establish a communication framework that not only facilitates collaboration but also reduces undesired speed reduction. Through the use of a predictive simulator, we can anticipate safety-related limitations, ensuring smoother workflows, minimizing risks, and optimizing efficiency. The overall architecture has been validated with a UR10e and compared with a state of the art technique. The results show a significant improvement in user experience, with a corresponding 23% reduction in execution times and a 50% decrease in robot downtime.

Translated Abstract:
인간-로봇 협업(HRC)에서 두 주체가 자연스럽고 효율적으로 소통하는 게 정말 중요해. 소통할 방법이 없으면 원하는 대로 진행되지 않고 느려지는 경우가 많아. 예를 들어, 예기치 않은 일이 발생하면 오류가 생길 수 있고, 사람과 로봇이 가까이 있으니 안전 기준인 ISO/TS 15066을 맞추기 위해 속도를 많이 줄여야 해.

이 논문에서는 작업자와 로봇이 효과적으로 대화할 수 있도록 하는 새로운 구조를 제안해. 사람끼리 대화하는 방식처럼 소통하면서도 안전 문제를 해결하려고 해. 이 방식은 협업을 쉽게 해줄 뿐만 아니라 불필요한 속도 저하도 줄여줄 거야.

예측 시뮬레이터를 활용해서 안전과 관련된 한계를 미리 예측할 수 있어서, 더 매끄러운 작업 흐름을 만들고 위험을 최소화하며 효율성을 높일 수 있어. 전체 구조는 UR10e 로봇으로 검증되었고 최신 기술과 비교했어. 결과적으로 사용자 경험이 크게 개선되었고, 실행 시간은 23% 줄어들었고 로봇의 가동 중지 시간은 50% 감소했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07160.pdf

Title: Distance Measurement for UAVs in Deep Hazardous Tunnels

Original Abstract:
The localization of Unmanned aerial vehicles (UAVs) in deep tunnels is extremely challenging due to their inaccessibility and hazardous environment. Conventional outdoor localization techniques (such as using GPS) and indoor localization techniques (such as those based on WiFi, Infrared (IR), Ultra-Wideband, etc.) do not work in deep tunnels. We are developing a UAV-based system for the inspection of defects in the Deep Tunnel Sewerage System (DTSS) in Singapore. To enable the UAV localization in the DTSS, we have developed a distance measurement module based on the optical flow technique. However, the standard optical flow technique does not work well in tunnels with poor lighting and a lack of features. Thus, we have developed an enhanced optical flow algorithm with prediction, to improve the distance measurement for UAVs in deep hazardous tunnels.

Translated Abstract:
심층 터널에서 무인 항공기(UAV)의 위치를 파악하는 건 정말 어려워. 터널이 접근하기 힘들고 위험한 환경이라서 그렇지. 일반적인 야외 위치 확인 방법인 GPS나 실내 위치 확인 방법인 WiFi, 적외선(IR), 초광대역 같은 건 깊은 터널에서는 잘 작동하지 않아.

우리는 싱가포르의 심층 터널 하수 시스템(DTSS)에서 결함을 검사하기 위해 UAV 기반 시스템을 개발하고 있어. 이 시스템이 DTSS에서 UAV의 위치를 파악할 수 있도록, 우리는 광학 흐름 기술을 기반으로 한 거리 측정 모듈을 만들었어. 

하지만 표준 광학 흐름 기술은 조명이 나쁘고 특징이 부족한 터널에서는 잘 작동하지 않아. 그래서 우리는 예측 기능을 추가한 향상된 광학 흐름 알고리즘을 개발했어, 이걸로 깊고 위험한 터널에서 UAV의 거리 측정을 개선하려고 해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07163.pdf

Title: Mamba Policy: Towards Efficient 3D Diffusion Policy with Hybrid Selective State Models

Original Abstract:
Diffusion models have been widely employed in the field of 3D manipulation due to their efficient capability to learn distributions, allowing for precise prediction of action trajectories. However, diffusion models typically rely on large parameter UNet backbones as policy networks, which can be challenging to deploy on resource-constrained devices. Recently, the Mamba model has emerged as a promising solution for efficient modeling, offering low computational complexity and strong performance in sequence modeling. In this work, we propose the Mamba Policy, a lighter but stronger policy that reduces the parameter count by over 80% compared to the original policy network while achieving superior performance. Specifically, we introduce the XMamba Block, which effectively integrates input information with conditional features and leverages a combination of Mamba and Attention mechanisms for deep feature extraction. Extensive experiments demonstrate that the Mamba Policy excels on the Adroit, Dexart, and MetaWorld datasets, requiring significantly fewer computational resources. Additionally, we highlight the Mamba Policy's enhanced robustness in long-horizon scenarios compared to baseline methods and explore the performance of various Mamba variants within the Mamba Policy framework. Our project page is in this https URL.

Translated Abstract:
확산 모델은 3D 조작 분야에서 널리 사용되고 있어. 이 모델은 분포를 잘 학습해서 행동 경로를 정확하게 예측할 수 있는 능력이 뛰어나거든. 하지만 보통 확산 모델은 큰 파라미터를 가진 UNet 백본을 정책 네트워크로 사용하는데, 이게 자원이 제한된 기기에서 운영하기엔 어려울 수 있어.

최근에 Mamba 모델이 효율적인 모델링을 위한 유망한 솔루션으로 떠올랐어. 이 모델은 계산 복잡성이 낮고, 시퀀스 모델링에서 강력한 성능을 보여줘. 이번 연구에서는 Mamba Policy를 제안하는데, 이건 원래 정책 네트워크에 비해 파라미터 수를 80% 이상 줄이면서도 성능은 더 뛰어나.

특히, XMamba Block을 소개하는데, 이 블록은 입력 정보와 조건적 특징을 효과적으로 통합하고, Mamba와 Attention 메커니즘을 조합해서 깊은 특징을 추출해. 다양한 실험 결과에서 Mamba Policy가 Adroit, Dexart, MetaWorld 데이터셋에서 우수한 성능을 보여주고, 필요한 계산 자원도 훨씬 적게 드는 걸 확인했어.

또한 Mamba Policy는 장기 시나리오에서 기존 방법들보다 더 강한 견고성을 보여준다는 점도 강조하고 싶어. 마지막으로 Mamba Policy 프레임워크 내에서 다양한 Mamba 변형의 성능도 살펴봤어. 프로젝트 페이지는 이 https URL에 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07195.pdf

Title: Perceptive Pedipulation with Local Obstacle Avoidance

Original Abstract:
Pedipulation leverages the feet of legged robots for mobile manipulation, eliminating the need for dedicated robotic arms. While previous works have showcased blind and task-specific pedipulation skills, they fail to account for static and dynamic obstacles in the environment. To address this limitation, we introduce a reinforcement learning-based approach to train a whole-body obstacle-aware policy that tracks foot position commands while simultaneously avoiding obstacles. Despite training the policy in only five different static scenarios in simulation, we show that it generalizes to unknown environments with different numbers and types of obstacles. We analyze the performance of our method through a set of simulation experiments and successfully deploy the learned policy on the ANYmal quadruped, demonstrating its capability to follow foot commands while navigating around static and dynamic obstacles.

Translated Abstract:
페디퓰레이션은 다리 로봇의 발을 이용해서 이동하면서 물체를 다루는 방법인데, 전용 로봇 팔이 필요 없게 해. 이전 연구들은 장애물에 대한 고려 없이 특정 작업을 위한 페디퓰레이션 기술을 보여줬지만, 정적인 장애물이나 동적인 장애물에 대해서는 신경 쓰지 않았어.

이 문제를 해결하기 위해 우리는 강화 학습 기반의 접근 방식을 도입했어. 이 방법은 발 위치 명령을 추적하면서 동시에 장애물을 피하는 전체 몸 장애물 인식 정책을 훈련하는 거야. 시뮬레이션에서 다섯 가지 정적 시나리오만으로 훈련했는데도, 다양한 장애물의 수와 종류가 있는 새로운 환경에서도 잘 작동하는 걸 보여줬어.

우리는 시뮬레이션 실험을 통해 이 방법의 성능을 분석했고, 학습한 정책을 ANYmal 네 발 로봇에 성공적으로 배포했어. 이로써 정적이고 동적인 장애물을 피하면서 발 명령을 따르는 능력을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07218.pdf

Title: Behavioral Cloning Models Reality Check for Autonomous Driving

Original Abstract:
How effective are recent advancements in autonomous vehicle perception systems when applied to real-world autonomous vehicle control? While numerous vision-based autonomous vehicle systems have been trained and evaluated in simulated environments, there is a notable lack of real-world validation for these systems. This paper addresses this gap by presenting the real-world validation of state-of-the-art perception systems that utilize Behavior Cloning (BC) for lateral control, processing raw image data to predict steering commands. The dataset was collected using a scaled research vehicle and tested on various track setups. Experimental results demonstrate that these methods predict steering angles with low error margins in real-time, indicating promising potential for real-world applications.

Translated Abstract:
최근 자율주행차 인식 시스템의 발전이 실제 자율주행차 제어에 얼마나 효과적인지 알아보려는 연구야. 많은 비전 기반 자율주행차 시스템이 시뮬레이션 환경에서 훈련되고 평가되었지만, 실제 세계에서의 검증이 부족하다는 문제가 있어. 

이 논문은 이 빈틈을 메우기 위해 최첨단 인식 시스템의 실제 세계 검증을 제시해. 이 시스템은 행동 복제(Behavior Cloning, BC)를 이용해서 측면 제어를 하고, 원본 이미지 데이터를 처리해 조향 명령을 예측해. 데이터셋은 축소된 연구 차량을 사용해서 수집되었고, 여러 트랙 세팅에서 테스트했어. 

실험 결과는 이 방법들이 실시간으로 조향 각도를 낮은 오차 범위로 예측한다는 걸 보여줘. 이는 실제 세계에서 활용 가능성이 높다는 것을 의미해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07228.pdf

Title: Reusability and Modifiability in Robotics Software (Extended Version)

Original Abstract:
We show the design of the software of the microcontroller unit of a weeding robot based on the Process Control architectural style and design patterns. The design consists of 133 modules resulting from using 8 design patterns for a total of 30 problems. As a result the design yields more reusable components and an easily modifiable and extensible program. Design documentation is also presented. Finally, the implementation (12 KLOC of C++ code) is empirically evaluated to prove that the design does not produce an inefficient implementation.

Translated Abstract:
우리는 잡초 제거 로봇의 마이크로컨트롤러 유닛 소프트웨어 디자인을 보여줘. 이 디자인은 프로세스 제어 아키텍처 스타일과 디자인 패턴을 기반으로 하고 있어. 

디자인은 30개의 문제를 해결하기 위해 8개의 디자인 패턴을 사용해서 나온 133개의 모듈로 구성돼. 이렇게 설계하면 더 재사용 가능한 컴포넌트와 쉽게 수정하고 확장할 수 있는 프로그램이 만들어져.

디자인 문서도 제공해. 마지막으로, 구현한 코드(12 KLOC의 C++ 코드)를 실제로 평가해서 이 디자인이 비효율적인 구현을 만들지 않는다는 걸 증명해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07288.pdf

Title: General Methods for Evaluating Collision Probability of Different Types of Theta-phi Positioners

Original Abstract:
In many modern astronomical facilities, multi-object telescopes are crucial instruments. Most of these telescopes have thousands of robotic fiber positioners(RFPs) installed on their focal plane, sharing an overlapping workspace. Collisions between RFPs during their movement can result in some targets becoming unreachable and cause structural damage. Therefore, it is necessary to reasonably assess and evaluate the collision probability of the RFPs. In this study, we propose a mathematical models of collision probability and validate its results using Monte Carlo simulations. In addition, a new collision calculation method is proposed for faster calculation(nearly 0.15% of original time). Simulation experiments have verified that our method can evaluate the collision probability between RFPs with both equal and unequal arm lengths. Additionally, we found that adopting a target distribution based on a Poisson distribution can reduce the collision probability by approximately 2.6% on average.

Translated Abstract:
현대 천문관측 시설에서는 다중 물체 망원경이 중요한 기기야. 대부분의 망원경에는 초점면에 수천 개의 로봇 섬유 위치기(RFPs)가 설치되어 있어서, 서로 겹치는 작업 공간을 공유해. RFP들이 움직일 때 충돌이 발생하면 일부 목표물에 도달할 수 없게 되고, 구조적인 피해도 생길 수 있어. 그래서 RFP들의 충돌 확률을 합리적으로 평가하는 게 필요해.

이번 연구에서는 충돌 확률에 대한 수학적 모델을 제안하고, 몬테카를로 시뮬레이션을 이용해 그 결과를 검증했어. 그리고 계산 속도를 높이기 위한 새로운 충돌 계산 방법도 제안했어(원래 시간의 거의 0.15% 정도 걸려). 시뮬레이션 실험 결과, 우리 방법이 RFP들 사이의 충돌 확률을 팔 길이가 같은 경우와 다른 경우 모두 평가할 수 있다는 걸 확인했어.

또한, 포아송 분포를 기반으로 한 목표물 분포를 채택하면 평균적으로 충돌 확률이 약 2.6% 감소한다는 것도 발견했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07293.pdf

Title: Electrokinetic Propulsion for Electronically Integrated Microscopic Robots

Original Abstract:
Robots too small to see by eye have rapidly evolved in recent years thanks to the incorporation of on-board microelectronics. Semiconductor circuits have been used in microrobots capable of executing controlled wireless steering, prescribed legged gait patterns, and user-triggered transitions between digital states. Yet these promising new capabilities have come at the steep price of complicated fabrication. Even though circuit components can be reliably built by semiconductor foundries, currently available actuators for electronically integrated microrobots are built with intricate multi-step cleanroom protocols and use mechanisms like articulated legs or bubble generators that are hard to design and control. Here, we present a propulsion system for electronically integrated microrobots that can be built with a single step of lithographic processing, readily integrates with microelectronics thanks to low current/low voltage operation (1V, 10nA), and yields robots that swim at speeds over one body length per second. Inspired by work on micromotors, these robots generate electric fields in a surrounding fluid, and by extension propulsive electrokinetic flows. The underlying physics is captured by a model in which robot speed is proportional to applied current, making design and control straightforward. As proof, we build basic robots that use on-board circuits and a closed-loop optical control scheme to navigate waypoints and move in coordinated swarms. Broadly, solid-state propulsion clears the way for robust, easy to manufacture, electronically controlled microrobots that operate reliably over months to years.

Translated Abstract:
눈에 잘 보이지 않을 정도로 작은 로봇들이 최근 몇 년 동안 빠르게 발전했어. 이건 온보드 마이크로 전자기술이 도입됐기 때문인데, 반도체 회로를 이용해서 조정된 무선 조향, 정해진 보행 패턴, 사용자가 트리거할 수 있는 디지털 상태 전환 같은 기능을 가진 마이크로 로봇들이 생겼어.

하지만 이런 새로운 가능성들은 복잡한 제작 방식 때문에 큰 대가를 치렀어. 반도체 공장에서 회로 부품을 안정적으로 만들 수는 있지만, 현재 사용 가능한 전자 통합 마이크로 로봇의 작동기는 복잡한 다단계 클린룸 프로토콜로 만들어져야 하고, 설계와 제어가 어려운 관절이 있는 다리나 기포 생성기 같은 메커니즘을 사용해.

여기서는 전자 통합 마이크로 로봇을 위해 한 번의 리소그래픽 처리로 만들 수 있는 추진 시스템을 소개할 거야. 이 시스템은 낮은 전류/저전압(1V, 10nA) 작동 덕분에 마이크로 전자기술과 쉽게 통합되고, 로봇이 초당 한 신체 길이 이상으로 수영할 수 있게 해.

이 로봇들은 주변 액체에서 전기장을 생성하고, 이를 통해 추진 전기유동을 만들어. 로봇 속도가 적용된 전류에 비례한다는 모델로 물리학을 설명할 수 있어서 설계와 제어가 간단해. 증거로 우리는 온보드 회로와 닫힌 루프 광 제어 방식을 사용해 웨이포인트를 탐색하고 조정된 떼를 지어 이동하는 기본 로봇들을 만들어봤어.

전반적으로, 고체 상태 추진 방식은 신뢰성 있게 작동하며, 몇 달에서 몇 년 동안 사용할 수 있는 견고하고 쉽게 제조할 수 있는 전자 제어 마이크로 로봇의 길을 열어줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.07343.pdf

Title: Learning Robotic Manipulation Policies from Point Clouds with Conditional Flow Matching

Original Abstract:
Learning from expert demonstrations is a promising approach for training robotic manipulation policies from limited data. However, imitation learning algorithms require a number of design choices ranging from the input modality, training objective, and 6-DoF end-effector pose representation. Diffusion-based methods have gained popularity as they enable predicting long-horizon trajectories and handle multimodal action distributions. Recently, Conditional Flow Matching (CFM) (or Rectified Flow) has been proposed as a more flexible generalization of diffusion models. In this paper, we investigate the application of CFM in the context of robotic policy learning and specifically study the interplay with the other design choices required to build an imitation learning algorithm. We show that CFM gives the best performance when combined with point cloud input observations. Additionally, we study the feasibility of a CFM formulation on the SO(3) manifold and evaluate its suitability with a simplified example. We perform extensive experiments on RLBench which demonstrate that our proposed PointFlowMatch approach achieves a state-of-the-art average success rate of 67.8% over eight tasks, double the performance of the next best method.

Translated Abstract:
전문가의 시연을 통해 배우는 것은 제한된 데이터로 로봇 조작 정책을 훈련하는 유망한 방법이야. 하지만 모방 학습 알고리즘은 입력 방식, 훈련 목표, 6자유도 종단 효과기 자세 표현 등 여러 디자인 선택이 필요해. 확산 기반 방법은 긴 시간 동안의 경로를 예측하고 다중 모드 행동 분포를 처리할 수 있어서 인기가 높아졌어. 최근에는 Conditional Flow Matching (CFM) 또는 Rectified Flow가 확산 모델의 더 유연한 일반화로 제안됐어.

이 논문에서는 로봇 정책 학습의 맥락에서 CFM을 적용하는 방법을 살펴보고, 모방 학습 알고리즘을 만들기 위해 필요한 다른 디자인 선택들과의 상호작용을 구체적으로 연구해. CFM은 포인트 클라우드 입력 관찰과 결합할 때 최고의 성능을 보여준다는 걸 보여줘. 또한, SO(3) 매니폴드에서 CFM 공식을 적용할 수 있는지 살펴보고, 간단한 예제로 그 적합성을 평가해.

우리는 RLBench에서 광범위한 실험을 진행했어. 그 결과, 우리가 제안한 PointFlowMatch 접근 방식이 여덟 가지 작업에서 67.8%의 최첨단 평균 성공률을 달성했어. 이는 다음으로 좋은 방법의 성능을 두 배로 높인 수치야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07409.pdf

Title: Robust Robot Walker: Learning Agile Locomotion over Tiny Traps

Original Abstract:
Quadruped robots must exhibit robust walking capabilities in practical applications. In this work, we propose a novel approach that enables quadruped robots to pass various small obstacles, or "tiny traps". Existing methods often rely on exteroceptive sensors, which can be unreliable for detecting such tiny traps. To overcome this limitation, our approach focuses solely on proprioceptive inputs. We introduce a two-stage training framework incorporating a contact encoder and a classification head to learn implicit representations of different traps. Additionally, we design a set of tailored reward functions to improve both the stability of training and the ease of deployment for goal-tracking tasks. To benefit further research, we design a new benchmark for tiny trap task. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness and robustness of our method. Project Page: this https URL

Translated Abstract:
사족 로봇은 실제 상황에서 강력한 걷기 능력을 보여야 해. 이번 연구에서는 사족 로봇이 다양한 작은 장애물, 즉 "작은 함정"을 넘을 수 있게 하는 새로운 방법을 제안해. 기존 방법들은 외부 감지 센서에 의존하는 경우가 많은데, 이런 센서가 작은 함정을 탐지하는 데는 신뢰성이 떨어질 수 있어.

그래서 우리는 proprioceptive 입력만을 사용해 이 문제를 해결하려고 했어. 우리는 접촉 인코더와 분류 헤드를 포함한 2단계 훈련 프레임워크를 도입해 다양한 함정에 대한 암묵적인 표현을 배우도록 했어. 또, 훈련의 안정성을 높이고 목표 추적 작업을 더 쉽게 배치할 수 있도록 맞춤형 보상 함수를 설계했어. 

추가 연구를 위해 작은 함정 작업에 대한 새로운 벤치마크도 만들었어. 시뮬레이션과 실제 환경에서 광범위한 실험을 통해 우리의 방법이 효과적이고 견고하다는 것을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07449.pdf

Title: Autonomous loading of ore piles with Load-Haul-Dump machines using Deep Reinforcement Learning

Original Abstract:
This work presents a deep reinforcement learning-based approach to train controllers for the autonomous loading of ore piles with a Load-Haul-Dump (LHD) machine. These controllers must perform a complete loading maneuver, filling the LHD's bucket with material while avoiding wheel drift, dumping material, or getting stuck in the pile. The training process is conducted entirely in simulation, using a simple environment that leverages the Fundamental Equation of Earth-Moving Mechanics so as to achieve a low computational cost. Two different types of policies are trained: one with a hybrid action space and another with a continuous action space. The RL-based policies are evaluated both in simulation and in the real world using a scaled LHD and a scaled muck pile, and their performance is compared to that of a heuristics-based controller and human teleoperation. Additional real-world experiments are performed to assess the robustness of the RL-based policies to measurement errors in the characterization of the piles. Overall, the RL-based controllers show good performance in the real world, achieving fill factors between 71-94%, and less wheel drift than the other baselines during the loading maneuvers. A video showing the training environment and the learned behavior in simulation, as well as some of the performed experiments in the real world, can be found in this https URL.

Translated Abstract:
이 연구는 심층 강화 학습을 기반으로 한 접근 방식을 소개해. 이 방식은 Load-Haul-Dump (LHD) 기계로 광석 더미를 자동으로 적재하는 제어기를 훈련시키는 거야. 이 제어기는 LHD의 버킷을 물질로 가득 채우면서 바퀴가 미끄러지지 않게 하고, 물질을 버리거나 더미에 갇히지 않도록 완전한 적재 동작을 수행해야 해.

훈련 과정은 시뮬레이션에서만 진행되는데, 여기서는 지구 이동 역학의 기본 방정식을 활용한 간단한 환경을 사용해서 계산 비용을 낮췄어. 두 가지 다른 정책을 훈련했는데, 하나는 하이브리드 액션 공간을 사용하고, 다른 하나는 연속 액션 공간을 사용해.

강화 학습 기반 정책은 시뮬레이션과 실제 환경에서 평가되었고, 스케일된 LHD와 스케일된 더미를 사용해서 성능을 비교했어. 이때 휴리스틱 기반 제어기와 인간 원격 조작과도 비교했지. 추가로 실제 환경에서 RL 기반 정책이 더미 특성 측정의 오차에 얼마나 강한지를 평가하는 실험도 했어.

전반적으로 RL 기반 제어기는 실제 환경에서 좋은 성능을 보였고, 적재 동작 중 다른 기준치보다 바퀴 미끄러짐이 적었으며, 충전 비율이 71%에서 94% 사이에 이르렀어. 훈련 환경과 시뮬레이션에서 학습된 행동, 그리고 실제 환경에서 수행된 실험을 보여주는 비디오는 이 https URL에서 찾아볼 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06764.pdf

Title: Modeling Image Tone Dichotomy with the Power Function

Original Abstract:
The primary purpose of this paper is to present the concept of dichotomy in image illumination modeling based on the power function. In particular, we review several mathematical properties of the power function to identify the limitations and propose a new mathematical model capable of abstracting illumination dichotomy. The simplicity of the equation opens new avenues for classical and modern image analysis and processing. The article provides practical and illustrative image examples to explain how the new model manages dichotomy in image perception. The article shows dichotomy image space as a viable way to extract rich information from images despite poor contrast linked to tone, lightness, and color perception. Moreover, a comparison with state-of-the-art methods in image enhancement provides evidence of the method's value.

Translated Abstract:
이 논문의 주된 목적은 파워 함수에 기반한 이미지 조명 모델링에서의 이분법 개념을 소개하는 거야. 특히, 우리는 파워 함수의 여러 수학적 특성을 검토해서 한계를 파악하고, 조명 이분법을 추상화할 수 있는 새로운 수학 모델을 제안해.

이 방정식의 간단함 덕분에 고전 및 현대 이미지 분석과 처리에 새로운 가능성이 열려. 논문에서는 새로운 모델이 이미지 인식에서 이분법을 어떻게 다루는지 설명하기 위해 실제 이미지 예시를 제공해. 

또한, 이분법 이미지 공간이 톤, 밝기, 색상 인식과 관련된 낮은 대비에도 불구하고 이미지에서 풍부한 정보를 추출하는 유용한 방법임을 보여줘. 마지막으로, 최신 이미지 향상 방법들과의 비교를 통해 이 방법의 가치도 입증하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07003.pdf

Title: ODYSSEE: Oyster Detection Yielded by Sensor Systems on Edge Electronics

Original Abstract:
Oysters are a keystone species in coastal ecosystems, offering significant economic, environmental, and cultural benefits. However, current monitoring systems are often destructive, typically involving dredging to physically collect and count oysters. A nondestructive alternative is manual identification from video footage collected by divers, which is time-consuming and labor-intensive with expert input.
An alternative to human monitoring is the deployment of a system with trained object detection models that performs real-time, on edge oyster detection in the field. One such platform is the Aqua2 robot. Effective training of these models requires extensive high-quality data, which is difficult to obtain in marine settings. To address these complications, we introduce a novel method that leverages stable diffusion to generate high-quality synthetic data for the marine domain. We exploit diffusion models to create photorealistic marine imagery, using ControlNet inputs to ensure consistency with the segmentation ground-truth mask, the geometry of the scene, and the target domain of real underwater images for oysters. The resulting dataset is used to train a YOLOv10-based vision model, achieving a state-of-the-art 0.657 mAP@50 for oyster detection on the Aqua2 platform. The system we introduce not only improves oyster habitat monitoring, but also paves the way to autonomous surveillance for various tasks in marine contexts, improving aquaculture and conservation efforts.

Translated Abstract:
굴은 해안 생태계에서 중요한 역할을 하는 종으로, 경제적, 환경적, 문화적으로 많은 혜택을 제공합니다. 하지만 현재의 굴 모니터링 시스템은 종종 파괴적이에요. 보통 굴을 물리적으로 잡아내고 세기 위해 준설 작업을 하거든요. 

대안으로, 잠수부가 촬영한 비디오에서 손으로 굴을 식별하는 방법이 있지만, 이건 시간이 많이 걸리고 전문가의 도움이 필요해요. 

그래서 사람 대신에 훈련된 객체 감지 모델을 이용한 시스템을 현장에 배치하는 방법이 있어요. 이 시스템은 실시간으로 굴을 감지할 수 있습니다. 그 중 하나가 Aqua2 로봇이에요. 하지만 이 모델을 효과적으로 훈련시키려면 고품질의 데이터가 필요한데, 해양 환경에서는 이 데이터를 얻기가 어렵습니다. 

이 문제를 해결하기 위해 우리는 안정적인 확산 기술을 활용해 해양 분야의 고품질 합성 데이터를 생성하는 새로운 방법을 소개합니다. 우리는 확산 모델을 이용해 사실적인 해양 이미지를 만들고, ControlNet 입력을 사용해 세분화된 진실 마스크와 장면의 기하학, 실제 해양 이미지와 일치하도록 했어요. 

이렇게 만들어진 데이터셋을 사용해 YOLOv10 기반의 비전 모델을 훈련시켰고, Aqua2 플랫폼에서 굴 감지 성능이 0.657 mAP@50에 도달했어요. 우리가 소개하는 시스템은 굴 서식지 모니터링을 개선할 뿐만 아니라, 해양 환경에서 다양한 작업을 위한 자율 감시의 길을 열어주고, 양식업과 보존 노력을 향상시킵니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.07011.pdf

Title: Physical synchronization of soft self-oscillating limbs for fast and autonomous locomotion

Original Abstract:
Animals achieve robust locomotion by offloading regulation from the brain to physical couplings within the body. Contrarily, locomotion in artificial systems often depends on centralized processors. Here, we introduce a rapid and autonomous locomotion strategy with synchronized gaits emerging through physical interactions between self-oscillating limbs and the environment, without control signals. Each limb is a single soft tube that only requires constant flow of air to perform cyclic stepping motions at frequencies reaching 300 hertz. By combining several of these self-oscillating limbs, their physical synchronization enables tethered and untethered locomotion speeds that are orders of magnitude faster than comparable state-of-the-art. We demonstrate that these seemingly simple devices exhibit autonomy, including obstacle avoidance and phototaxis, opening up avenues for robust and functional robots at all scales.

Translated Abstract:
동물들은 뇌에서 신체의 물리적 연결로 운동 조절을 넘겨줌으로써 강력한 이동성을 이룹니다. 반면, 인공지능 시스템의 이동성은 중앙 처리 장치에 의존하는 경우가 많습니다. 여기서는 제어 신호 없이 자가 진동하는 팔다리와 환경 간의 물리적 상호작용을 통해 동기화된 보행이 발생하는 빠르고 자율적인 이동 전략을 소개합니다. 

각 팔다리는 단일 부드러운 튜브로, 주기적인 스텝 동작을 수행하기 위해 일정한 공기 흐름만 있으면 됩니다. 주파수는 300 헬츠에 이를 수 있습니다. 이러한 자가 진동 팔다리 여러 개를 조합하면, 물리적 동기화 덕분에 연결된 상태와 아닌 상태 모두에서 매우 빠른 이동 속도를 얻을 수 있습니다. 

이 간단해 보이는 장치들이 자율성을 가지고 장애물을 피하거나 빛을 따라가는 능력을 보여준다는 것을 입증했습니다. 이는 모든 규모에서 강력하고 기능적인 로봇을 위한 새로운 가능성을 열어줍니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.07245.pdf

Title: Single-View 3D Reconstruction via SO(2)-Equivariant Gaussian Sculpting Networks

Original Abstract:
This paper introduces SO(2)-Equivariant Gaussian Sculpting Networks (GSNs) as an approach for SO(2)-Equivariant 3D object reconstruction from single-view image observations.
GSNs take a single observation as input to generate a Gaussian splat representation describing the observed object's geometry and texture. By using a shared feature extractor before decoding Gaussian colors, covariances, positions, and opacities, GSNs achieve extremely high throughput (>150FPS). Experiments demonstrate that GSNs can be trained efficiently using a multi-view rendering loss and are competitive, in quality, with expensive diffusion-based reconstruction algorithms. The GSN model is validated on multiple benchmark experiments. Moreover, we demonstrate the potential for GSNs to be used within a robotic manipulation pipeline for object-centric grasping.

Translated Abstract:
이 논문에서는 SO(2)-Equivariant Gaussian Sculpting Networks (GSNs)를 소개하는데, 이건 단일 시점 이미지 관찰로부터 SO(2)-Equivariant 3D 객체 재구성을 위한 방법이야. 

GSNs는 하나의 관찰 데이터를 입력으로 받아서, 관찰된 객체의 기하학과 질감을 설명하는 가우시안 스플랫 표현을 생성해. 가우시안 색상, 공분산, 위치, 불투명도를 디코딩하기 전에 공통 피쳐 추출기를 사용함으로써, GSNs는 매우 높은 처리량(150FPS 이상)을 달성해. 

실험 결과, GSNs는 다중 시점 렌더링 손실을 이용해 효율적으로 학습할 수 있고, 비싸고 복잡한 확산 기반 재구성 알고리즘과 품질 면에서 경쟁력이 있다는 걸 보여줬어. GSN 모델은 여러 벤치마크 실험에서 검증되었고, 게다가 GSNs가 로봇 조작 파이프라인에서 객체 중심의 그랩핑에 사용될 가능성도 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07341.pdf

Title: Online Decision MetaMorphFormer: A Casual Transformer-Based Reinforcement Learning Framework of Universal Embodied Intelligence

Original Abstract:
Interactive artificial intelligence in the motion control field is an interesting topic, especially when universal knowledge is adaptive to multiple tasks and universal environments. Despite there being increasing efforts in the field of Reinforcement Learning (RL) with the aid of transformers, most of them might be limited by the offline training pipeline, which prohibits exploration and generalization abilities. To address this limitation, we propose the framework of Online Decision MetaMorphFormer (ODM) which aims to achieve self-awareness, environment recognition, and action planning through a unified model architecture. Motivated by cognitive and behavioral psychology, an ODM agent is able to learn from others, recognize the world, and practice itself based on its own experience. ODM can also be applied to any arbitrary agent with a multi-joint body, located in different environments, and trained with different types of tasks using large-scale pre-trained datasets. Through the use of pre-trained datasets, ODM can quickly warm up and learn the necessary knowledge to perform the desired task, while the target environment continues to reinforce the universal policy. Extensive online experiments as well as few-shot and zero-shot environmental tests are used to verify ODM's performance and generalization ability. The results of our study contribute to the study of general artificial intelligence in embodied and cognitive fields. Code, results, and video examples can be found on the website \url{this https URL}.

Translated Abstract:
모션 제어 분야에서의 인터랙티브 인공지능은 재미있는 주제야. 특히 보편적인 지식이 여러 작업과 환경에 적응할 수 있을 때 더욱 그렇지. 요즘 강화 학습(RL) 분야에서 트랜스포머를 활용한 연구가 늘어나고 있지만, 대부분의 연구는 오프라인 훈련 방식에 제한을 받아서 탐색과 일반화 능력이 떨어져. 

이 문제를 해결하기 위해 우리는 온라인 결정 메타모프 포머(ODM)라는 프레임워크를 제안해. 이 모델은 자기 인식, 환경 인식, 행동 계획을 통합된 구조로 수행할 수 있도록 만들어졌어. 인지 및 행동 심리학에 영감을 받아서, ODM 에이전트는 다른 사람에게서 배우고, 세상을 인식하며, 자신의 경험을 바탕으로 스스로 연습할 수 있어. 

ODM은 다관절 몸체를 가진 어떤 에이전트에도 적용할 수 있고, 다양한 환경에서 여러 작업 유형으로 훈련이 가능해. 대규모로 사전 훈련된 데이터셋을 사용하면 ODM은 빠르게 준비하고 원하는 작업을 수행하는 데 필요한 지식을 배울 수 있어. 동시에 목표 환경은 보편적인 정책을 강화해줘. 

우리는 ODM의 성능과 일반화 능력을 검증하기 위해 광범위한 온라인 실험과 몇 번의 샷, 제로 샷 환경 테스트를 진행했어. 이 연구 결과는 구현된 인공지능과 인지 분야의 일반 인공지능 연구에 기여해. 코드, 결과, 비디오 예시는 웹사이트에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07365.pdf

Title: Event-based Mosaicing Bundle Adjustment

Original Abstract:
We tackle the problem of mosaicing bundle adjustment (i.e., simultaneous refinement of camera orientations and scene map) for a purely rotating event camera. We formulate the problem as a regularized non-linear least squares optimization. The objective function is defined using the linearized event generation model in the camera orientations and the panoramic gradient map of the scene. We show that this BA optimization has an exploitable block-diagonal sparsity structure, so that the problem can be solved efficiently. To the best of our knowledge, this is the first work to leverage such sparsity to speed up the optimization in the context of event-based cameras, without the need to convert events into image-like representations. We evaluate our method, called EMBA, on both synthetic and real-world datasets to show its effectiveness (50% photometric error decrease), yielding results of unprecedented quality. In addition, we demonstrate EMBA using high spatial resolution event cameras, yielding delicate panoramas in the wild, even without an initial map. Project page: this https URL

Translated Abstract:
우리는 순수하게 회전하는 이벤트 카메라에 대한 모자이크 번들 조정 문제를 다룹니다. 여기서 번들 조정은 카메라 방향과 장면 지도를 동시에 조정하는 걸 말해요. 이 문제를 정규화된 비선형 최소 제곱 최적화로 정식화했습니다.

목표 함수는 카메라 방향에서의 선형화된 이벤트 생성 모델과 장면의 파노라마 그래디언트 맵을 사용해 정의됩니다. 이 번들 조정 최적화는 이용할 수 있는 블록 대각선 희소 구조를 가지고 있어서, 문제를 효율적으로 해결할 수 있어요. 우리가 아는 한, 이 연구는 이벤트 기반 카메라의 최적화를 빠르게 하기 위해 이런 희소성을 활용한 첫 번째 작업입니다. 이벤트를 이미지 같은 형태로 변환할 필요가 없어요.

우리는 EMBA라는 방법을 합성 데이터셋과 실제 데이터셋에서 평가하여 효과성을 보여주었습니다. 그 결과, 사진 측정 오류가 50% 감소했고, 품질이 전례 없는 수준으로 향상되었습니다. 또한 초기 지도 없이도 고해상도 이벤트 카메라를 사용해 EMBA를 시연하여 야외에서 섬세한 파노라마를 얻었습니다.

================================================================================

URL:
https://arxiv.org/pdf/2203.13653.pdf

Title: An introduction to using dual quaternions to study kinematics

Original Abstract:
We explain the use of dual quaternions to represent poses, twists, and wrenches.

Translated Abstract:
이 연구에서는 이중 쿼터니언을 사용해서 자세, 비틀림, 그리고 힘을 표현하는 방법에 대해 설명해. 

이중 쿼터니언은 3D 공간에서 물체의 위치와 방향을 쉽게 나타내는 데 도움을 줘. 비틀림과 힘을 표현하는 데도 유용해.

================================================================================

URL:
https://arxiv.org/pdf/2305.12644.pdf

Title: PO-VINS: An Efficient and Robust Pose-Only Visual-Inertial State Estimator With LiDAR Enhancement

Original Abstract:
The pose adjustment (PA) with a pose-only visual representation has been proven equivalent to the bundle adjustment (BA), while significantly improving the computational efficiency. However, the pose-only solution has not yet been properly considered in a tightly-coupled visual-inertial state estimator (VISE) with a normal configuration for real-time navigation. In this study, we propose a tightly-coupled LiDAR-enhanced VISE, named PO-VINS, with a full pose-only form for visual and LiDAR-depth measurements. Based on the pose-only visual representation, we derive the analytical depth uncertainty, which is then employed for rejecting LiDAR depth outliers. Besides, we propose a multi-state constraint (MSC)-based LiDAR-depth measurement model with a pose-only form, to balance efficiency and robustness. The pose-only visual and LiDAR-depth measurements and the IMU-preintegration measurements are tightly integrated under the factor graph optimization framework to perform efficient and accurate state estimation. Exhaustive experimental results on private and public datasets indicate that the proposed PO-VINS yields improved or comparable accuracy to sate-of-the-art methods. Compared to the baseline method LE-VINS, the state-estimation efficiency of PO-VINS is improved by 33% and 56% on the laptop PC and the onboard ARM computer, respectively. Besides, PO-VINS yields higher accuracy and robustness than LE-VINS by employing the proposed uncertainty-based outlier-culling method and the MSC-based measurement model for LiDAR depth.

Translated Abstract:
포즈 조정(PA)은 포즈만으로 시각적 표현을 사용했을 때 번들 조정(BA)과 동등한 효과를 보이면서 계산 효율성을 크게 개선하는 것으로 입증되었어. 그런데 포즈만 사용하는 솔루션은 실시간 내비게이션을 위한 일반적인 구성의 밀접 결합된 비주얼-관성 상태 추정기(VISE)에서는 제대로 고려되지 않았어.

이 연구에서는 LiDAR가 강화된 밀접 결합 VISE인 PO-VINS를 제안해. 이 시스템은 비주얼과 LiDAR 깊이 측정을 위한 완전한 포즈만 사용하는 형태야. 포즈만 사용하는 시각적 표현을 바탕으로, 우리는 분석적인 깊이 불확실성을 도출하고, 이를 LiDAR 깊이 아웃라이어를 제거하는 데 사용해. 또한, 우리는 효율성과 강건성을 균형 잡기 위해 포즈만 사용하는 형태의 다중 상태 제약(MSC) 기반 LiDAR 깊이 측정 모델을 제안해.

포즈만 사용하는 비주얼과 LiDAR 깊이 측정, 그리고 IMU 전처리 측정은 팩터 그래프 최적화 프레임워크 아래에서 밀접하게 통합되어 효율적이고 정확한 상태 추정을 수행해. 개인 및 공개 데이터셋에서의 철저한 실험 결과에 따르면, 제안된 PO-VINS는 최신 방법들과 비교했을 때 개선되거나 비슷한 정확도를 보여줘. 기본 방법인 LE-VINS와 비교했을 때, PO-VINS의 상태 추정 효율성은 노트북 PC에서 33%, 탑재된 ARM 컴퓨터에서 56% 향상되었어. 게다가 PO-VINS는 제안된 불확실성 기반 아웃라이어 제거 방법과 MSC 기반 측정 모델을 사용하여 LE-VINS보다 더 높은 정확도와 강건성을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2308.05731.pdf

Title: The Integration of Prediction and Planning in Deep Learning Automated Driving Systems: A Review

Original Abstract:
Automated driving has the potential to revolutionize personal, public, and freight mobility. Beside accurately perceiving the environment, automated vehicles must plan a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential, separate tasks. While this accounts for the influence of surrounding traffic on the ego vehicle, it fails to anticipate the reactions of traffic participants to the ego vehicle's behavior. Recent methods increasingly integrate prediction and planning in a joint or interdependent step to model bidirectional interactions. To date, a comprehensive overview of different integration principles is lacking. We systematically review state-of-the-art deep learning-based planning systems, and focus on how they integrate prediction. Different facets of the integration ranging from system architecture to high-level behavioral aspects are considered and related to each other. Moreover, we discuss the implications, strengths, and limitations of different integration principles. By pointing out research gaps, describing relevant future challenges, and highlighting trends in the research field, we identify promising directions for future research.

Translated Abstract:
자동 운전은 개인, 대중, 화물 이동 방법을 완전히 바꿀 수 있는 가능성이 있어. 환경을 정확하게 인식하는 것 외에도, 자동 차량은 안전하고 편안하며 효율적인 움직임 경로를 계획해야 해. 안전과 발전을 위해 많은 연구들이 주변 교통의 미래 움직임을 예측하는 모듈에 의존하고 있어. 

모듈식 자동 운전 시스템은 일반적으로 예측과 계획을 순차적이고 별개의 작업으로 다뤄. 이렇게 하면 주변 교통이 자차에 미치는 영향을 고려할 수 있지만, 교통 참여자들이 자차의 행동에 어떻게 반응할지를 예측하지는 못해. 최근 방법들은 예측과 계획을 통합하거나 서로 의존하는 단계로 점점 더 많이 결합하고 있어, 양방향 상호작용을 모델링하려고 하지. 

하지만 지금까지 다양한 통합 원칙에 대한 포괄적인 개요는 부족해. 우리는 최첨단 딥러닝 기반 계획 시스템을 체계적으로 검토하고, 이들이 예측을 어떻게 통합하는지를 중점적으로 살펴봐. 시스템 아키텍처부터 고수준 행동적 측면까지 통합의 다양한 측면을 고려하고 서로 연결해. 

또한, 다양한 통합 원칙의 의미, 강점, 한계에 대해서도 이야기해. 연구의 공백을 지적하고, 관련된 미래 도전과제를 설명하며, 연구 분야의 트렌드를 강조하면서, 향후 연구를 위한 유망한 방향을 제시하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2403.06210.pdf

Title: AdaFold: Adapting Folding Trajectories of Cloths via Feedback-loop Manipulation

Original Abstract:
We present AdaFold, a model-based feedback-loop framework for optimizing folding trajectories. AdaFold extracts a particle-based representation of cloth from RGB-D images and feeds back the representation to a model predictive control to replan folding trajectory at every time step. A key component of AdaFold that enables feedback-loop manipulation is the use of semantic descriptors extracted from geometric features. These descriptors enhance the particle representation of the cloth to distinguish between ambiguous point clouds of differently folded cloths. Our experiments demonstrate AdaFold's ability to adapt folding trajectories of cloths with varying physical properties and generalize from simulated training to real-world execution.

Translated Abstract:
우리는 AdaFold라는 모델 기반 피드백 루프 프레임워크를 소개해. 이 시스템은 종이 접기 경로를 최적화하는 데 사용돼. AdaFold는 RGB-D 이미지에서 천의 입자 기반 표현을 추출하고, 이 표현을 모델 예측 제어에 피드백해서 매 시간 단계마다 접기 경로를 다시 계획해.

AdaFold의 핵심 요소는 기하학적 특징에서 추출한 의미론적 설명자를 사용하는 거야. 이 설명자는 천의 입자 표현을 개선해서 서로 다르게 접힌 천의 애매한 점 구름을 구별할 수 있도록 해. 

우리의 실험 결과, AdaFold가 다양한 물리적 특성을 가진 천의 접기 경로를 조정할 수 있는 능력을 보여줬고, 시뮬레이션 훈련에서 실제 환경으로의 일반화도 잘 수행할 수 있음을 증명했어.

================================================================================

URL:
https://arxiv.org/pdf/2403.15333.pdf

Title: Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in Safety Monitoring Applications

Original Abstract:
This paper presents a formation control approach for contactless gesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor Unmanned Aerial Vehicles (UAVs) and a human worker. The approach is designed to monitor the safety of human workers, particularly those operating at heights. In the proposed dynamic formation scheme, one UAV acts as the formation leader, equipped with sensors for detecting human workers and recognizing gestures. The follower UAVs maintain a predetermined formation relative to the worker's position, providing additional perspectives of the monitored scene. Hand gestures enable the human worker to specify movement and action commands for the UAV team and to initiate other mission-related tasks without requiring additional communication channels or specific markers. Combined with a novel unified human detection and tracking algorithm, a human position estimation method, and a gesture detection pipeline, the proposed approach represents the first instance of an HSI system incorporating all these modules onboard real-world UAVs. Simulations and field experiments involving three UAVs and a human worker in a mock-up scenario demonstrate the effectiveness and responsiveness of the proposed approach.

Translated Abstract:
이 논문은 다중 로터 드론 팀과 인간 작업자 간의 비접촉 제스처 기반의 인간-무리 상호작용(HSI)을 위한 형성 제어 접근 방식을 제시해. 이 접근 방식은 특히 높은 곳에서 작업하는 인간 작업자의 안전을 모니터링하도록 설계되었어.

제안된 동적 형성 계획에서는 한 대의 드론이 형성 리더 역할을 하고, 인간 작업자를 감지하고 제스처를 인식하는 센서를 장착하고 있어. 후속 드론들은 작업자의 위치에 따라 미리 정해진 형성을 유지하면서 감시 장면에 대한 추가적인 시각을 제공해.

인간 작업자는 손 제스처를 사용해서 드론 팀에 이동 및 행동 명령을 지정할 수 있고, 추가적인 통신 채널이나 특정 표식 없이도 다른 임무 관련 작업을 시작할 수 있어. 새로운 통합 인간 감지 및 추적 알고리즘, 인간 위치 추정 방법, 제스처 감지 파이프라인과 결합되어, 제안된 접근 방식은 실제 드론에 모든 모듈을 탑재한 HSI 시스템의 첫 사례를 보여줘.

세 대의 드론과 한 명의 인간 작업자가 참여한 모의 실험에서 시뮬레이션과 현장 실험을 통해 제안된 방법의 효과성과 반응성을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2404.01900.pdf

Title: Automatic Derivation of an Optimal Task Frame for Learning and Controlling Contact-Rich Tasks

Original Abstract:
In previous work on learning and controlling contact-rich tasks, the procedure for choosing a proper reference frame to express learned signals for the motion and the interaction wrench is often implicit, requires expert insight, or starts from proposed frame candidates. This article presents an automatic method to derive the optimal reference frame, referred to as optimal task frame, directly from the recorded motion and wrench data of the demonstration. Using screw theory, several origin and orientation candidates are generated that maximize decoupling in the data. These candidates are then processed probabilistically, without needing hyperparameters, to obtain the optimal task frame. Its origin and orientation are independently fixed to either the world or the robot tool. The method works regardless of whether the task involves translation, rotation, force, or moment, or any combination thereof. The method was validated for various tasks, including surface following and manipulation of articulated objects, showing good agreement between derived and assumed expert task frames. To validate the robot's performance, a constraint-based controller was designed based on the data expressed in the derived task frames. These experiments demonstrated the approach's effectiveness and versatility. The automatic task frame derivation approach supports learning methods to design controllers for a wide range of contact-rich tasks.

Translated Abstract:
이전 연구에서는 접촉이 많은 작업을 학습하고 제어할 때, 적절한 기준 프레임을 선택하는 과정이 종종 명확하지 않거나 전문가의 통찰력이 필요했어. 또는 제안된 프레임 후보들로부터 시작해야 했지. 이 논문에서는 시연 데이터에서 직접 최적의 기준 프레임, 즉 최적 작업 프레임을 자동으로 도출하는 방법을 제시해.

스크류 이론을 이용해서 여러 개의 원점과 방향 후보를 생성해, 데이터를 최대한 분리할 수 있도록 해. 그런 다음 이 후보들을 확률적으로 처리해서 최적 작업 프레임을 얻어. 원점과 방향은 세계 기준이든 로봇 도구 기준이든 독립적으로 고정할 수 있어. 이 방법은 작업이 이동, 회전, 힘, 또는 모멘트를 포함하든 상관없이 잘 작동해. 

이 방법은 표면 따라가기와 관절이 있는 물체 조작 같은 다양한 작업에 대해 검증되었고, 도출된 작업 프레임과 전문가가 가정한 작업 프레임 간에 좋은 일치를 보여줬어. 로봇의 성능을 검증하기 위해, 도출된 작업 프레임을 기반으로 제약 기반 제어기도 설계했어. 이 실험들은 이 접근법의 효과성과 유연성을 잘 보여줬어. 자동 작업 프레임 도출 방법은 다양한 접촉이 많은 작업을 위한 컨트롤러 설계를 지원하는 학습 방법을 돕는 역할을 해.

================================================================================

URL:
https://arxiv.org/pdf/2405.14154.pdf

Title: Skip-SCAR: A Modular Approach to ObjectGoal Navigation with Sparsity and Adaptive Skips

Original Abstract:
In ObjectGoal navigation (ObjectNav), agents must locate specific objects within unseen environments, requiring effective observation, prediction, and navigation capabilities. This study found that traditional methods looking only for prediction accuracy often compromise on computational efficiency. To address this, we introduce "Skip-SCAR," a modular framework that enhances efficiency by leveraging sparsity and adaptive skips. The SparseConv-Augmented ResNet (SCAR) at the core of our approach uses sparse and dense feature processing in parallel, optimizing both the computation and memory footprint. Our adaptive skip technique further reduces computational demands by selectively bypassing unnecessary semantic segmentation steps based on environmental constancy. Tested on the HM3D ObjectNav datasets, Skip-SCAR not only minimizes resource use but also sets new performance benchmarks, demonstrating a robust method for improving efficiency and accuracy in robotic navigation tasks.

Translated Abstract:
객체 목표 탐색(ObjectNav)에서는 에이전트가 보지 못한 환경에서 특정 객체를 찾아야 해. 이 과정에서 효과적인 관찰, 예측, 탐색 능력이 필요해. 그런데 전통적인 방법들은 예측 정확성만 따지느라 계산 효율성을 희생하는 경우가 많았어.

이 문제를 해결하기 위해 우리는 "Skip-SCAR"라는 모듈형 프레임워크를 소개해. 이 프레임워크는 희소성과 적응형 스킵을 활용해 효율성을 높여줘. 우리 접근법의 핵심인 SparseConv-Augmented ResNet(SCAR)은 희소한 특징과 밀집된 특징을 동시에 처리해서 계산과 메모리 사용량을 최적화해.

또한, 우리의 적응형 스킵 기술은 환경의 일관성에 따라 불필요한 의미 분할 단계를 선택적으로 건너뛰어서 계산 부담을 줄여줘. HM3D ObjectNav 데이터셋에서 테스트한 결과, Skip-SCAR는 자원 사용량을 최소화할 뿐만 아니라 새로운 성능 기준도 세우면서 로봇 탐색 작업에서 효율성과 정확성을 높이는 강력한 방법임을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2406.04858.pdf

Title: Auto-Multilift: Distributed Learning and Control for Cooperative Load Transportation With Quadrotors

Original Abstract:
Designing motion control and planning algorithms for multilift systems remains challenging due to the complexities of dynamics, collision avoidance, actuator limits, and scalability. Existing methods that use optimization and distributed techniques effectively address these constraints and scalability issues. However, they often require substantial manual tuning, leading to suboptimal performance. This paper proposes Auto-Multilift, a novel framework that automates the tuning of model predictive controllers (MPCs) for multilift systems. We model the MPC cost functions with deep neural networks (DNNs), enabling fast online adaptation to various scenarios. We develop a distributed policy gradient algorithm to train these DNNs efficiently in a closed-loop manner. Central to our algorithm is distributed sensitivity propagation, which is built on fully exploiting the unique dynamic couplings within the multilift system. It parallelizes gradient computation across quadrotors and focuses on actual system state sensitivities relative to key MPC parameters. Extensive simulations demonstrate favorable scalability to a large number of quadrotors. Our method outperforms a state-of-the-art open-loop MPC tuning approach by effectively learning adaptive MPCs from trajectory tracking errors. It also excels in learning an adaptive reference for reconfiguring the system when traversing multiple narrow slots.

Translated Abstract:
다중 리프트 시스템을 위한 모션 제어 및 계획 알고리즘 설계는 동역학의 복잡성, 충돌 회피, 액추에이터 한계, 그리고 확장성 때문에 여전히 어려워. 기존의 최적화 및 분산 기법을 사용하는 방법들은 이러한 제약과 확장성 문제를 잘 해결하고 있어. 하지만, 이 방법들은 보통 많은 수동 조정을 필요로 해서 성능이 최적이 아닌 경우가 많아.

이 논문에서는 다중 리프트 시스템의 모델 예측 제어기(MPC) 조정을 자동화하는 새로운 프레임워크인 Auto-Multilift를 제안해. 우리는 MPC 비용 함수를 딥 뉴럴 네트워크(DNN)로 모델링해서 다양한 상황에 빠르게 적응할 수 있도록 했어. 그리고, 이러한 DNN을 효율적으로 훈련시키기 위해 분산 정책 기울기 알고리즘을 개발했어.

우리 알고리즘의 핵심은 분산 감도 전파인데, 이건 다중 리프트 시스템 내의 독특한 동적 결합을 완전히 활용하는 데 기반하고 있어. 쿼드로터 간의 기울기 계산을 병렬로 진행하고, 주요 MPC 매개변수에 대한 실제 시스템 상태 감도에 초점을 맞추고 있어. 광범위한 시뮬레이션 결과는 많은 쿼드로터에 대한 좋은 확장성을 보여줘.

우리 방법은 궤적 추적 오류로부터 적응형 MPC를 효과적으로 학습함으로써 최신 오픈 루프 MPC 조정 방식을 뛰어넘어. 또한, 여러 좁은 슬롯을 통과할 때 시스템을 재구성하기 위한 적응형 참조를 학습하는 데도 뛰어난 성능을 보여.

================================================================================

URL:
https://arxiv.org/pdf/2406.17768.pdf

Title: EXTRACT: Efficient Policy Learning by Extracting Transferable Robot Skills from Offline Data

Original Abstract:
Most reinforcement learning (RL) methods focus on learning optimal policies over low-level action spaces. While these methods can perform well in their training environments, they lack the flexibility to transfer to new tasks. Instead, RL agents that can act over useful, temporally extended skills rather than low-level actions can learn new tasks more easily. Prior work in skill-based RL either requires expert supervision to define useful skills, which is hard to scale, or learns a skill-space from offline data with heuristics that limit the adaptability of the skills, making them difficult to transfer during downstream RL. Our approach, EXTRACT, instead utilizes pre-trained vision language models to extract a discrete set of semantically meaningful skills from offline data, each of which is parameterized by continuous arguments, without human supervision. This skill parameterization allows robots to learn new tasks by only needing to learn when to select a specific skill and how to modify its arguments for the specific task. We demonstrate through experiments in sparse-reward, image-based, robot manipulation environments that EXTRACT can more quickly learn new tasks than prior works, with major gains in sample efficiency and performance over prior skill-based RL. Website at this https URL.

Translated Abstract:
대부분의 강화 학습(RL) 방법은 낮은 수준의 행동 공간에서 최적의 정책을 배우는 데 집중해. 이런 방법들은 훈련 환경에서는 잘 작동하지만, 새로운 작업으로 전이하는 데 유연성이 부족해. 대신, 유용하고 시간적으로 확장된 기술을 사용할 수 있는 RL 에이전트는 새로운 작업을 더 쉽게 배울 수 있어.

기술 기반 RL에 관한 이전 연구는 유용한 기술을 정의하기 위해 전문가의 감독이 필요했는데, 이건 확장하기 어렵고, 오프라인 데이터에서 휴리스틱을 사용해 기술 공간을 배우는 방식은 기술의 적응성을 제한해. 그래서 기술을 후속 RL에서 전이하기 힘들어. 

우리의 접근 방식인 EXTRACT는 대신에, 사전 훈련된 비전 언어 모델을 사용해서 오프라인 데이터에서 의미 있는 기술의 이산 집합을 추출해. 이 과정에서 인간의 감독이 필요 없어. 이 기술 파라미터화 덕분에 로봇은 특정 기술을 선택하는 시점과 그 기술의 파라미터를 특정 작업에 맞게 조정하는 방법만 배우면 새로운 작업을 학습할 수 있어.

우리는 희소 보상, 이미지 기반 로봇 조작 환경에서 실험을 통해 EXTRACT가 이전 연구보다 새로운 작업을 더 빠르게 배울 수 있음을 보여줬고, 샘플 효율성과 성능에서 큰 향상을 이루었어.

================================================================================

URL:
https://arxiv.org/pdf/2407.02232.pdf

Title: Efficient Extrinsic Self-Calibration of Multiple IMUs using Measurement Subset Selection

Original Abstract:
This paper addresses the problem of choosing a sparse subset of measurements for quick calibration parameter estimation. A standard solution to this is selecting a measurement only if its utility -- the difference between posterior (with the measurement) and prior information (without the measurement) -- exceeds some threshold. Theoretically, utility, a function of the parameter estimate, should be evaluated at the estimate obtained with all measurements selected so far, hence necessitating a recalibration with each new measurement. However, we hypothesize that utility is insensitive to changes in the parameter estimate for many systems of interest, suggesting that evaluating utility at some initial parameter guess would yield equivalent results in practice. We provide evidence supporting this hypothesis for extrinsic calibration of multiple inertial measurement units (IMUs), showing the reduction in calibration time by two orders of magnitude by forgoing recalibration for each measurement.

Translated Abstract:
이 논문은 빠른 보정 파라미터 추정을 위해 측정값의 희소한 부분 집합을 선택하는 문제를 다룬다. 일반적인 해결책은 어떤 측정을 사용할 때 그 유용성, 즉 측정을 포함한 후행 정보와 측정을 포함하지 않은 선행 정보의 차이가 어떤 기준을 초과할 때만 선택하는 것이다.

이론적으로 유용성은 파라미터 추정치의 함수이기 때문에, 지금까지 선택된 모든 측정으로 얻은 추정치에서 평가해야 한다. 그래서 새로운 측정이 들어올 때마다 재보정이 필요하다. 하지만 우리는 많은 시스템에서 유용성이 파라미터 추정치의 변화에 둔감하다고 가정한다. 즉, 초기 파라미터 추정에서 유용성을 평가해도 실제로는 비슷한 결과를 얻을 수 있다는 뜻이다.

우리는 여러 관성 측정 장치(IMU)의 외부 보정에 대한 이 가설을 뒷받침하는 증거를 제시한다. 각 측정에 대해 재보정을 생략함으로써 보정 시간을 100배 줄일 수 있다는 것을 보여준다.

================================================================================

URL:
https://arxiv.org/pdf/2407.15840.pdf

Title: QueST: Self-Supervised Skill Abstractions for Learning Continuous Control

Original Abstract:
Generalization capabilities, or rather a lack thereof, is one of the most important unsolved problems in the field of robot learning, and while several large scale efforts have set out to tackle this problem, unsolved it remains. In this paper, we hypothesize that learning temporal action abstractions using latent variable models (LVMs), which learn to map data to a compressed latent space and back, is a promising direction towards low-level skills that can readily be used for new tasks. Although several works have attempted to show this, they have generally been limited by architectures that do not faithfully capture shareable representations. To address this we present Quantized Skill Transformer (QueST), which learns a larger and more flexible latent encoding that is more capable of modeling the breadth of low-level skills necessary for a variety of tasks. To make use of this extra flexibility, QueST imparts causal inductive bias from the action sequence data into the latent space, leading to more semantically useful and transferable representations. We compare to state-of-the-art imitation learning and LVM baselines and see that QueST's architecture leads to strong performance on several multitask and few-shot learning benchmarks. Further results and videos are available at this https URL

Translated Abstract:
로봇 학습 분야에서 일반화 능력, 즉 일반화 능력이 부족한 것은 아직 해결되지 않은 중요한 문제 중 하나야. 여러 대규모 노력들이 이 문제를 해결하려고 했지만, 아직까지 제대로 해결되진 않았어. 

이 논문에서는 잠재 변수 모델(LVMs)을 사용해 시간적 행동 추상화를 배우는 것이 새로운 작업에 쉽게 적용할 수 있는 저수준 기술로 나아가는 유망한 방향이라고 가정해. 몇몇 연구들이 이 점을 보여주려고 했지만, 일반적으로 공유 가능한 표현을 제대로 담아내지 못하는 구조에 한계가 있었어.

그래서 우리는 Quantized Skill Transformer(QueST)를 제안해. QueST는 다양한 작업에 필요한 저수준 기술의 폭을 모델링할 수 있는 더 크고 유연한 잠재 인코딩을 배워. 이 추가적인 유연성을 활용하기 위해 QueST는 행동 시퀀스 데이터에서 잠재 공간으로 인과적 귀납적 편향을 주입해, 더 의미 있고 전이 가능한 표현을 만들어내.

우리는 최신 모방 학습 및 LVM 기준과 비교했는데, QueST의 구조가 여러 멀티태스크와 적은 샷 학습 벤치마크에서 강력한 성능을 보여준다는 걸 확인했어. 더 많은 결과와 비디오는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2407.21500.pdf

Title: DIABLO: A 6-DoF Wheeled Bipedal Robot Composed Entirely of Direct-Drive Joints

Original Abstract:
Wheeled bipedal robots offer the advantages of both wheeled and legged robots, combining the ability to traverse a wide range of terrains and environments with high efficiency. However, the conventional approach in existing wheeled bipedal robots involves motor-driven joints with high-ratio gearboxes. While this approach provides specific benefits, it also presents several challenges, including increased mechanical complexity, efficiency losses, noise, vibrations, and higher maintenance and lubrication requirements. Addressing the aforementioned concerns, we developed a direct-drive wheeled bipedal robot called DIABLO, which eliminates the use of gearboxes entirely. Our robotic system is simplified as a second-order inverted pendulum, and we have designed an LQR-based balance controller to ensure stability. Additionally, we implemented comprehensive motion controller, including yaw, split-angle, height, and roll controllers. Through expriments in simulations and real-world prototype, we have demonstrated that our platform achieves satisfactory performance.

Translated Abstract:
바퀴가 달린 이족 보행 로봇은 바퀴 로봇과 다리 로봇의 장점을 모두 가지고 있어 다양한 지형과 환경을 효율적으로 탐험할 수 있어. 하지만 기존의 바퀴가 달린 이족 보행 로봇은 모터로 구동되는 관절과 고비율 기어박스를 사용하는 전통적인 방식이야. 이 방법은 몇 가지 장점이 있지만, 기계적 복잡성이 증가하고, 효율이 떨어지며, 소음이나 진동이 생기고, 유지보수와 윤활 요구사항이 더 많아지는 단점도 있어.

이런 문제를 해결하기 위해 우리는 기어박스를 완전히 없앤 DIABLO라는 직접 구동 바퀴가 달린 이족 보행 로봇을 개발했어. 우리 로봇 시스템은 두 번째 순서의 역진자 형태로 단순화되었고, 안정성을 보장하기 위해 LQR 기반의 균형 제어기를 설계했어. 게다가, 방향, 각도, 높이, 롤을 포함한 종합적인 모션 제어기도 구현했어.

시뮬레이션과 실제 프로토타입 실험을 통해 우리 플랫폼이 만족스러운 성능을 보여주었다는 걸 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.02724.pdf

Title: Surgical Task Automation Using Actor-Critic Frameworks and Self-Supervised Imitation Learning

Original Abstract:
Surgical robot task automation has recently attracted great attention due to its potential to benefit both surgeons and patients. Reinforcement learning (RL) based approaches have demonstrated promising ability to provide solutions to automated surgical manipulations on various tasks. To address the exploration challenge, expert demonstrations can be utilized to enhance the learning efficiency via imitation learning (IL) approaches. However, the successes of such methods normally rely on both states and action labels. Unfortunately action labels can be hard to capture or their manual annotation is prohibitively expensive owing to the requirement for expert knowledge. It therefore remains an appealing and open problem to leverage expert demonstrations composed of pure states in RL. In this work, we present an actor-critic RL framework, termed AC-SSIL, to overcome this challenge of learning with state-only demonstrations collected by following an unknown expert policy. It adopts a self-supervised IL method, dubbed SSIL, to effectively incorporate demonstrated states into RL paradigms by retrieving from demonstrates the nearest neighbours of the query state and utilizing the bootstrapping of actor networks. We showcase through experiments on an open-source surgical simulation platform that our method delivers remarkable improvements over the RL baseline and exhibits comparable performance against action based IL methods, which implies the efficacy and potential of our method for expert demonstration-guided learning scenarios.

Translated Abstract:
최근 수술 로봇의 작업 자동화가 의사와 환자 모두에게 이익이 될 수 있다는 점에서 큰 주목을 받고 있어. 강화 학습(RL) 기반의 접근 방식이 여러 작업에서 자동화된 수술 조작을 위한 해결책을 제공하는 데 유망한 능력을 보여줬어. 탐색 문제를 해결하기 위해, 전문가의 시연을 활용해서 모방 학습(IL) 방식으로 학습 효율성을 높일 수 있어.

하지만 이런 방법들이 성공하기 위해서는 상태와 행동 라벨이 모두 필요해. 안타깝게도 행동 라벨을 얻는 건 쉽지 않거나, 전문가의 지식이 필요해서 수작업으로 라벨링하는 게 비용이 많이 들어. 그래서 순수한 상태로 구성된 전문가 시연을 강화 학습에 활용하는 문제는 여전히 매력적이고 열린 문제야.

이번 연구에서는, 알 수 없는 전문가 정책을 따르며 수집한 상태만으로 학습하는 문제를 해결하기 위해 AC-SSIL이라는 액터-크리틱 RL 프레임워크를 제시해. 이 방법은 SSIL이라는 자기 지도 모방 학습 방법을 채택해서, 시연된 상태를 RL 패러다임에 효과적으로 통합하고, 쿼리 상태의 최근접 이웃을 찾아서 액터 네트워크의 부트스트래핑을 활용해.

우리는 오픈 소스 수술 시뮬레이션 플랫폼에서 실험을 통해, 우리의 방법이 RL 기준선보다 놀라운 개선을 보여주고, 행동 기반 IL 방법과 비슷한 성능을 보인다는 걸 보여줬어. 이건 전문가 시연을 통한 학습 시나리오에서 우리의 방법의 효율성과 잠재력을 나타내는 거야.

================================================================================

URL:
https://arxiv.org/pdf/2405.02821.pdf

Title: Sim2Real Transfer for Audio-Visual Navigation with Frequency-Adaptive Acoustic Field Prediction

Original Abstract:
Sim2real transfer has received increasing attention lately due to the success of learning robotic tasks in simulation end-to-end. While there has been a lot of progress in transferring vision-based navigation policies, the existing sim2real strategy for audio-visual navigation performs data augmentation empirically without measuring the acoustic gap. The sound differs from light in that it spans across much wider frequencies and thus requires a different solution for sim2real. We propose the first treatment of sim2real for audio-visual navigation by disentangling it into acoustic field prediction (AFP) and waypoint navigation. We first validate our design choice in the SoundSpaces simulator and show improvement on the Continuous AudioGoal navigation benchmark. We then collect real-world data to measure the spectral difference between the simulation and the real world by training AFP models that only take a specific frequency subband as input. We further propose a frequency-adaptive strategy that intelligently selects the best frequency band for prediction based on both the measured spectral difference and the energy distribution of the received audio, which improves the performance on the real data. Lastly, we build a real robot platform and show that the transferred policy can successfully navigate to sounding objects. This work demonstrates the potential of building intelligent agents that can see, hear, and act entirely from simulation, and transferring them to the real world.

Translated Abstract:
최근 시뮬레이션에서 로봇 작업을 끝에서 끝까지 학습하는 성공 덕분에 sim2real 전이가 많은 주목을 받고 있어. 비전 기반 내비게이션 정책을 전이하는 데는 꽤 많은 발전이 있었지만, 기존의 sim2real 전략은 오디오-비주얼 내비게이션에서 데이터를 경험적으로 증강하고 있어. 소리는 빛과 다르게 훨씬 넓은 주파수 범위를 가지기 때문에 sim2real을 위한 다른 해결책이 필요해.

우리는 오디오-비주얼 내비게이션을 음향 필드 예측(AFP)과 웨이포인트 내비게이션으로 분리하여 sim2real을 처음으로 다뤄봤어. 먼저 SoundSpaces 시뮬레이터에서 우리의 설계 선택을 검증하고 Continuous AudioGoal 내비게이션 기준에서 개선된 성과를 보여줬어. 그런 다음, 실제 데이터를 수집해서 시뮬레이션과 현실 간의 스펙트럴 차이를 측정했어. 이 과정에서 특정 주파수 서브밴드만 입력으로 사용하는 AFP 모델을 훈련시켰지.

우리는 또한 측정된 스펙트럴 차이와 수신된 오디오의 에너지 분포를 바탕으로 예측을 위해 최적의 주파수 대역을 지능적으로 선택하는 주파수 적응 전략을 제안했어. 이 전략은 실제 데이터에서 성능을 향상시켜줘. 마지막으로, 실제 로봇 플랫폼을 만들어서 전이된 정책이 소리가 나는 물체를 성공적으로 내비게이션할 수 있음을 보여줬어. 이 연구는 시뮬레이션에서만 보고, 듣고, 행동할 수 있는 지능형 에이전트를 만드는 가능성을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2407.05717.pdf

Title: A New Framework for Nonlinear Kalman Filters

Original Abstract:
The Kalman filter (KF) is a state estimation algorithm that optimally combines system knowledge and measurements to minimize the mean squared error of the estimated states. While KF was initially designed for linear systems, numerous extensions of it, such as extended Kalman filter (EKF), unscented Kalman filter (UKF), cubature Kalman filter (CKF), etc., have been proposed for nonlinear systems. Although different types of nonlinear KFs have different pros and cons, they all use the same framework of linear KF, which, according to what we found in this paper, tends to give overconfident and less accurate state estimations when the measurement functions are nonlinear. Therefore, in this study, we designed a new framework for nonlinear KFs and showed theoretically and empirically that the new framework estimates the states and covariance matrix more accurately than the old one. The new framework was tested on four different nonlinear KFs and five different tasks, showcasing its ability to reduce the estimation errors by several orders of magnitude in low-measurement-noise conditions, with only about a 10 to 90% increase in computational time. All types of nonlinear KFs can benefit from the new framework, and the benefit will increase as the sensors become more and more accurate in the future. As an example, EKF, the simplest nonlinear KF that was previously believed to work poorly for strongly nonlinear systems, can now provide fast and fairly accurate state estimations with the help of the new framework. The codes are available at this https URL.

Translated Abstract:
칼만 필터(KF)는 시스템 지식과 측정을 최적 결합해서 추정된 상태의 평균 제곱 오차를 최소화하는 상태 추정 알고리즘이야. KF는 처음에 선형 시스템을 위해 설계됐지만, 비선형 시스템을 위한 여러 확장 버전들이 제안됐어. 여기에는 확장 칼만 필터(EKF), 비선형 칼만 필터(UKF), 큐바투르 칼만 필터(CKF) 등이 포함돼.

각기 다른 비선형 KF는 장단점이 있지만, 모두 선형 KF의 같은 프레임워크를 사용해. 우리가 이 논문에서 발견한 바에 따르면, 비선형 측정 함수가 있을 때는 이 프레임워크가 과도하게 자신감 있는 상태 추정치를 제공하고 정확도가 떨어지는 경향이 있어.

그래서 이 연구에서는 비선형 KF를 위한 새로운 프레임워크를 설계했고, 이 새로운 프레임워크가 이전 것보다 상태와 공분산 행렬을 더 정확하게 추정한다는 걸 이론적으로와 경험적으로 보여줬어. 새로운 프레임워크는 네 가지 다른 비선형 KF와 다섯 가지 다른 작업에서 테스트됐고, 측정 노이즈가 낮은 조건에서 추정 오차를 몇 배 줄일 수 있는 능력을 보여줬어. 계산 시간은 약 10%에서 90% 정도 늘어났지만, 효과는 상당했어.

모든 종류의 비선형 KF가 이 새로운 프레임워크의 혜택을 받을 수 있으며, 센서가 점점 더 정확해질수록 그 혜택은 더 커질 거야. 예를 들어, EKF는 예전에는 강한 비선형 시스템에서 잘 작동하지 않는다고 여겨졌지만, 이제는 새로운 프레임워크 덕분에 빠르고 꽤 정확한 상태 추정치를 제공할 수 있게 됐어. 코드는 이 https URL에서 확인할 수 있어.

================================================================================

