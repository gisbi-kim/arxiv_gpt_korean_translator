URL:
https://arxiv.org/pdf/2409.08283.pdf

Title: Activation function optimization method: Learnable series linear units (LSLUs)

Original Abstract:
Effective activation functions introduce non-linear transformations, providing neural networks with stronger fitting capa-bilities, which help them better adapt to real data distributions. Huawei Noah's Lab believes that dynamic activation functions are more suitable than static activation functions for enhancing the non-linear capabilities of neural networks. Tsinghua University's related research also suggests using dynamically adjusted activation functions. Building on the ideas of using fine-tuned activation functions from Tsinghua University and Huawei Noah's Lab, we propose a series-based learnable ac-tivation function called LSLU (Learnable Series Linear Units). This method simplifies deep learning networks while im-proving accuracy. This method introduces learnable parameters {\theta} and {\omega} to control the activation function, adapting it to the current layer's training stage and improving the model's generalization. The principle is to increase non-linearity in each activation layer, boosting the network's overall non-linearity. We evaluate LSLU's performance on CIFAR10, CIFAR100, and specific task datasets (e.g., Silkworm), validating its effectiveness. The convergence behavior of the learnable parameters {\theta} and {\omega}, as well as their effects on generalization, are analyzed. Our empirical results show that LSLU enhances the general-ization ability of the original model in various tasks while speeding up training. In VanillaNet training, parameter {\theta} initially decreases, then increases before stabilizing, while {\omega} shows an opposite trend. Ultimately, LSLU achieves a 3.17% accuracy improvement on CIFAR100 for VanillaNet (Table 3). Codes are available at this https URL.

Translated Abstract:
효과적인 활성화 함수는 비선형 변환을 도입해서 신경망이 더 강력한 적합 능력을 가지도록 도와줍니다. 이 덕분에 신경망이 실제 데이터 분포에 잘 적응할 수 있게 되죠. 화웨이 노아의 연구소는 동적 활성화 함수가 정적 활성화 함수보다 신경망의 비선형 능력을 높이는 데 더 적합하다고 믿고 있습니다. 칭화대의 관련 연구도 동적으로 조정되는 활성화 함수의 사용을 제안하고 있어요.

우리는 칭화대와 화웨이 노아의 아이디어를 바탕으로 LSLU(학습 가능한 시리즈 선형 유닛)라는 시리즈 기반의 학습 가능한 활성화 함수를 제안합니다. 이 방법은 딥러닝 네트워크를 단순화하면서 정확도를 높여줍니다. LSLU는 학습 가능한 파라미터 {\theta}와 {\omega}를 도입해서 활성화 함수를 제어하고, 현재 레이어의 훈련 단계에 맞게 조정하여 모델의 일반화를 개선합니다. 기본 원리는 각 활성화 레이어에서 비선형성을 높여서 네트워크 전체의 비선형성을 증대시키는 것입니다.

우리는 LSLU의 성능을 CIFAR10, CIFAR100, 그리고 특정 작업 데이터셋(예: 누에)에서 평가했습니다. 그 효과를 검증했죠. 학습 가능한 파라미터 {\theta}와 {\omega}의 수렴 행동과 일반화에 미치는 영향을 분석했습니다. 우리의 실험 결과는 LSLU가 다양한 작업에서 원래 모델의 일반화 능력을 향상시키면서 훈련 속도를 높인다는 것을 보여줍니다.

VanillaNet 훈련 과정에서 파라미터 {\theta}는 처음에 감소했다가 증가한 후 안정화되며, 반면 {\omega}는 반대의 경향을 보입니다. 결국, LSLU는 VanillaNet에 대해 CIFAR100에서 3.17%의 정확도 향상을 달성했습니다(표 3 참조). 코드도 이 링크에서 확인할 수 있습니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.08345.pdf

Title: SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition

Original Abstract:
As Artificial Intelligence applications expand, the evaluation of models faces heightened scrutiny. Ensuring public readiness requires evaluation datasets, which differ from training data by being disjoint and ethically sourced in compliance with privacy regulations. The performance and fairness of face recognition systems depend significantly on the quality and representativeness of these evaluation datasets. This data is sometimes scraped from the internet without user's consent, causing ethical concerns that can prohibit its use without proper releases. In rare cases, data is collected in a controlled environment with consent, however, this process is time-consuming, expensive, and logistically difficult to execute. This creates a barrier for those unable to conjure the immense resources required to gather ethically sourced evaluation datasets. To address these challenges, we introduce the Synthetic Identity Generation pipeline, or SIG, that allows for the targeted creation of ethical, balanced datasets for face recognition evaluation. Our proposed and demonstrated pipeline generates high-quality images of synthetic identities with controllable pose, facial features, and demographic attributes, such as race, gender, and age. We also release an open-source evaluation dataset named ControlFace10k, consisting of 10,008 face images of 3,336 unique synthetic identities balanced across race, gender, and age, generated using the proposed SIG pipeline. We analyze ControlFace10k along with a non-synthetic BUPT dataset using state-of-the-art face recognition algorithms to demonstrate its effectiveness as an evaluation tool. This analysis highlights the dataset's characteristics and its utility in assessing algorithmic bias across different demographic groups.

Translated Abstract:
인공지능(AI) 응용 프로그램이 확장됨에 따라, 모델 평가에 대한 관심이 높아지고 있어. 공공의 준비 상태를 보장하려면 평가 데이터셋이 필요한데, 이 데이터들은 훈련 데이터와 달리 서로 분리되어 있고, 개인정보 보호 규정을 준수해서 윤리적으로 수집돼야 해. 얼굴 인식 시스템의 성능과 공정성은 이 평가 데이터셋의 질과 대표성에 크게 의존해. 

그런데 이 데이터는 가끔 사용자의 동의 없이 인터넷에서 긁어오는 경우가 있어, 이로 인해 윤리적인 문제가 발생하고 적절한 허가 없이 사용이 불가능해. 드물게는 통제된 환경에서 동의를 받아서 데이터를 수집하기도 하지만, 이 과정은 시간이 많이 걸리고 비용이 비싸며 실행하기도 어려워. 그래서 윤리적으로 수집된 평가 데이터셋을 모으는 데 필요한 막대한 자원을 마련할 수 없는 사람들에게는 큰 장벽이 돼.

이 문제를 해결하기 위해, 우리는 윤리적이고 균형 잡힌 얼굴 인식 평가를 위한 데이터셋을 목표로 생성할 수 있는 합성 아이덴티티 생성 파이프라인(Synthetic Identity Generation pipeline, SIG)을 소개해. 우리가 제안한 이 파이프라인은 조절 가능한 포즈, 얼굴 특징, 인종, 성별, 나이 같은 인구 통계적 속성을 가진 합성 아이덴티티의 고품질 이미지를 생성해.

또한, 우리는 SIG 파이프라인을 사용해 생성한 3,336개의 독특한 합성 아이덴티티의 10,008개 얼굴 이미지로 구성된 오픈 소스 평가 데이터셋인 ControlFace10k를 공개해. 우리는 ControlFace10k와 비합성 BUPT 데이터셋을 최신 얼굴 인식 알고리즘을 사용해서 분석해, 이 데이터셋이 평가 도구로서 얼마나 효과적인지 보여줄 거야. 이 분석은 데이터셋의 특성과 다양한 인구 통계 그룹 간의 알고리즘 편향을 평가하는 데 유용한 점을 강조해.

================================================================================

URL:
https://arxiv.org/pdf/2409.08381.pdf

Title: Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations

Original Abstract:
Vision-language models (VLMs) like CLIP have been adapted for Multi-Label Recognition (MLR) with partial annotations by leveraging prompt-learning, where positive and negative prompts are learned for each class to associate their embeddings with class presence or absence in the shared vision-text feature space. While this approach improves MLR performance by relying on VLM priors, we hypothesize that learning negative prompts may be suboptimal, as the datasets used to train VLMs lack image-caption pairs explicitly focusing on class absence. To analyze the impact of positive and negative prompt learning on MLR, we introduce PositiveCoOp and NegativeCoOp, where only one prompt is learned with VLM guidance while the other is replaced by an embedding vector learned directly in the shared feature space without relying on the text encoder. Through empirical analysis, we observe that negative prompts degrade MLR performance, and learning only positive prompts, combined with learned negative embeddings (PositiveCoOp), outperforms dual prompt learning approaches. Moreover, we quantify the performance benefits that prompt-learning offers over a simple vision-features-only baseline, observing that the baseline displays strong performance comparable to dual prompt learning approach (DualCoOp), when the proportion of missing labels is low, while requiring half the training compute and 16 times fewer parameters

Translated Abstract:
비전-언어 모델(VLM)인 CLIP 같은 것들은 부분 주석이 있는 다중 레이블 인식(MLR)에 맞춰 조정됐어. 이건 프롬프트 학습을 활용해서, 각 클래스에 대해 긍정적인 프롬프트와 부정적인 프롬프트를 학습해 클래스가 존재하는지 여부를 연결하는 거야. 이 방식은 VLM의 사전 지식을 이용해 MLR 성능을 높이는데 도움이 돼.

하지만 우리는 부정적인 프롬프트를 학습하는 것이 최적이 아닐 수도 있다고 생각해. 왜냐하면 VLM을 훈련하는 데 사용된 데이터셋에는 클래스의 부재에 초점을 맞춘 이미지-캡션 쌍이 부족하거든. 그래서 긍정적이고 부정적인 프롬프트 학습이 MLR에 미치는 영향을 분석하기 위해 PositiveCoOp와 NegativeCoOp를 소개해. 여기서는 하나의 프롬프트만 VLM의 지침을 받아 학습하고, 다른 하나는 텍스트 인코더에 의존하지 않고 공유 피처 공간에서 직접 학습된 임베딩 벡터로 교체해.

실험 분석을 통해 우리는 부정적인 프롬프트가 MLR 성능을 저하시킨다는 걸 알게 됐어. 그리고 긍정적인 프롬프트만 학습하고, 학습된 부정적인 임베딩과 함께(PositiveCoOp) 사용하는 게 두 개의 프롬프트를 학습하는 방법보다 더 나은 성능을 보인다는 걸 확인했어. 게다가, 프롬프트 학습이 단순한 비전 피처만 사용한 기준선에 비해 성능 이점을 제공한다는 걸 수치적으로 평가했어. 기준선은 결측 레이블 비율이 낮을 때는 두 개의 프롬프트 학습 방식(DualCoOp)과 비슷한 강력한 성능을 보여주지만, 훈련에 필요한 계산량은 절반, 파라미터 수는 16배 적어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08388.pdf

Title: Continual Learning in 3D Point Clouds: Employing Spectral Techniques for Exemplar Selection

Original Abstract:
We introduce a novel framework for Continual Learning in 3D object classification (CL3D). Our approach is based on the selection of prototypes from each class using spectral clustering. For non-Euclidean data such as point clouds, spectral clustering can be employed as long as one can define a distance measure between pairs of samples. Choosing the appropriate distance measure enables us to leverage 3D geometric characteristics to identify representative prototypes for each class. We explore the effectiveness of clustering in the input space (3D points), local feature space (1024-dimensional points), and global feature space. We conduct experiments on the ModelNet40, ShapeNet, and ScanNet datasets, achieving state-of-the-art accuracy exclusively through the use of input space features. By leveraging the combined input, local, and global features, we have improved the state-of-the-art on ModelNet and ShapeNet, utilizing nearly half the memory used by competing approaches. For the challenging ScanNet dataset, our method enhances accuracy by 4.1% while consuming just 28% of the memory used by our competitors, demonstrating the scalability of our approach.

Translated Abstract:
새로운 3D 객체 분류를 위한 지속적 학습 프레임워크인 CL3D를 소개해. 이 방법은 스펙트럴 클러스터링을 이용해 각 클래스에서 프로토타입을 선택하는 데 기반하고 있어. 포인트 클라우드 같은 비유클리드 데이터에 대해, 샘플 간의 거리 측정을 정의할 수만 있다면 스펙트럴 클러스터링을 사용할 수 있어. 적절한 거리 측정을 선택하면 3D 기하학적 특성을 활용해 각 클래스의 대표 프로토타입을 찾을 수 있어.

우리는 입력 공간(3D 포인트), 지역 특성 공간(1024차원 포인트), 그리고 글로벌 특성 공간에서 클러스터링의 효과를 살펴봤어. ModelNet40, ShapeNet, ScanNet 데이터셋에서 실험을 진행했는데, 입력 공간 특성만으로도 최첨단 정확도를 달성했어. 입력, 지역, 글로벌 특성을 결합해서 사용함으로써 ModelNet과 ShapeNet에서 최첨단 성능을 개선했어. 경쟁 방법들이 사용하는 메모리의 거의 반밖에 사용하지 않았어.

특히 어려운 ScanNet 데이터셋에서는 우리 방법이 정확도를 4.1% 향상시키면서 경쟁자들이 사용하는 메모리의 28%만 소비했어. 이로써 우리의 접근 방식이 확장 가능하다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08397.pdf

Title: 360PanT: Training-Free Text-Driven 360-Degree Panorama-to-Panorama Translation

Original Abstract:
Preserving boundary continuity in the translation of 360-degree panoramas remains a significant challenge for existing text-driven image-to-image translation methods. These methods often produce visually jarring discontinuities at the translated panorama's boundaries, disrupting the immersive experience. To address this issue, we propose 360PanT, a training-free approach to text-based 360-degree panorama-to-panorama translation with boundary continuity. Our 360PanT achieves seamless translations through two key components: boundary continuity encoding and seamless tiling translation with spatial control. Firstly, the boundary continuity encoding embeds critical boundary continuity information of the input 360-degree panorama into the noisy latent representation by constructing an extended input image. Secondly, leveraging this embedded noisy latent representation and guided by a target prompt, the seamless tiling translation with spatial control enables the generation of a translated image with identical left and right halves while adhering to the extended input's structure and semantic layout. This process ensures a final translated 360-degree panorama with seamless boundary continuity. Experimental results on both real-world and synthesized datasets demonstrate the effectiveness of our 360PanT in translating 360-degree panoramas. Code is available at \href{this https URL}{this https URL}.

Translated Abstract:
360도 파노라마를 번역할 때 경계의 연속성을 유지하는 게 기존의 텍스트 기반 이미지-투-이미지 번역 방법에서 큰 도전 과제가 되고 있어. 이런 방법들은 번역된 파노라마의 경계에서 시각적으로 거슬리는 불연속성을 자주 만들어서 몰입감을 깨뜨려. 이 문제를 해결하기 위해 우리는 경계의 연속성을 유지하는 텍스트 기반 360도 파노라마-투-파노라마 번역을 위한 훈련 없는 접근법인 360PanT를 제안해.

우리의 360PanT는 두 가지 주요 요소를 통해 매끄러운 번역을 달성해: 경계 연속성 인코딩과 공간 제어가 있는 매끄러운 타일 번역. 첫 번째로, 경계 연속성 인코딩은 입력된 360도 파노라마의 중요한 경계 연속성 정보를 잡음이 섞인 잠재 표현에 담기 위해 확장된 입력 이미지를 구성해.

두 번째로, 이 잡음이 섞인 잠재 표현을 활용하고 목표 프롬프트에 의해 안내받아, 공간 제어가 있는 매끄러운 타일 번역이 동일한 왼쪽과 오른쪽 절반을 가진 번역된 이미지를 생성할 수 있게 해. 이 과정은 최종적으로 경계 연속성이 매끄러운 360도 파노라마를 보장해. 실제 데이터셋과 합성 데이터셋에서의 실험 결과는 우리의 360PanT가 360도 파노라마 번역에서 효과적임을 보여줘. 코드도 제공돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.08443.pdf

Title: CF-PRNet: Coarse-to-Fine Prototype Refining Network for Point Cloud Completion and Reconstruction

Original Abstract:
In modern agriculture, precise monitoring of plants and fruits is crucial for tasks such as high-throughput phenotyping and automated harvesting. This paper addresses the challenge of reconstructing accurate 3D shapes of fruits from partial views, which is common in agricultural settings. We introduce CF-PRNet, a coarse-to-fine prototype refining network, leverages high-resolution 3D data during the training phase but requires only a single RGB-D image for real-time inference. Our approach begins by extracting the incomplete point cloud data that constructed from a partial view of a fruit with a series of convolutional blocks. The extracted features inform the generation of scaling vectors that refine two sequentially constructed 3D mesh prototypes - one coarse and one fine-grained. This progressive refinement facilitates the detailed completion of the final point clouds, achieving detailed and accurate reconstructions. CF-PRNet demonstrates excellent performance metrics with a Chamfer Distance of 3.78, an F1 Score of 66.76%, a Precision of 56.56%, and a Recall of 85.31%, and win the first place in the Shape Completion and Reconstruction of Sweet Peppers Challenge.

Translated Abstract:
현대 농업에서 식물과 과일을 정확하게 모니터링하는 건 고속 형질 분석이나 자동 수확 같은 작업에 아주 중요해. 이 논문은 농업 현장에서 흔히 발생하는, 부분적인 시점에서 과일의 정확한 3D 형태를 재구성하는 문제를 다뤄. 

우리는 CF-PRNet이라는 코어스-투-파인 프로토타입 정제 네트워크를 소개하는데, 이건 훈련 단계에서 고해상도 3D 데이터를 활용하지만, 실시간 추론을 위해서는 단 한 장의 RGB-D 이미지만 필요해. 

우리의 접근 방식은, 과일의 부분적인 시점에서 만들어진 불완전한 포인트 클라우드 데이터를 여러 개의 컨볼루션 블록을 통해 추출하는 것으로 시작해. 추출된 특징들은 두 개의 순차적으로 구성된 3D 메쉬 프로토타입, 하나는 거친 형태고 하나는 세밀한 형태를 정제하는 스케일링 벡터 생성을 안내해. 이 단계별 정제 과정 덕분에 최종 포인트 클라우드가 자세하고 정확하게 완성돼. 

CF-PRNet은 챔퍼 거리 3.78, F1 점수 66.76%, 정밀도 56.56%, 재현율 85.31%라는 뛰어난 성능 지표를 보여주고, 스위트 페퍼의 형태 완성 및 재구성 대회에서 1위를 차지했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08444.pdf

Title: Towards Unified Facial Action Unit Recognition Framework by Large Language Models

Original Abstract:
Facial Action Units (AUs) are of great significance in the realm of affective computing. In this paper, we propose AU-LLaVA, the first unified AU recognition framework based on the Large Language Model (LLM). AU-LLaVA consists of a visual encoder, a linear projector layer, and a pre-trained LLM. We meticulously craft the text descriptions and fine-tune the model on various AU datasets, allowing it to generate different formats of AU recognition results for the same input image. On the BP4D and DISFA datasets, AU-LLaVA delivers the most accurate recognition results for nearly half of the AUs. Our model achieves improvements of F1-score up to 11.4% in specific AU recognition compared to previous benchmark results. On the FEAFA dataset, our method achieves significant improvements over all 24 AUs compared to previous benchmark results. AU-LLaVA demonstrates exceptional performance and versatility in AU recognition.

Translated Abstract:
표정 동작 단위(Facial Action Units, AUs)는 감정 컴퓨팅 분야에서 아주 중요해. 이 논문에서는 AU-LLaVA라는 첫 번째 통합 AU 인식 프레임워크를 제안해. 이건 대형 언어 모델(LLM)을 기반으로 하고 있어. 

AU-LLaVA는 시각 인코더, 선형 프로젝터 레이어, 그리고 사전 훈련된 LLM으로 구성되어 있어. 텍스트 설명을 신중하게 만들어서 다양한 AU 데이터셋에서 모델을 미세 조정했어. 이렇게 해서 같은 입력 이미지에 대해 다양한 형식의 AU 인식 결과를 생성할 수 있게 했지.

BP4D와 DISFA 데이터셋에서 AU-LLaVA는 거의 절반의 AU에 대해 가장 정확한 인식 결과를 보여줬어. 우리 모델은 특정 AU 인식에서 이전 벤치마크 결과에 비해 F1 점수에서 최대 11.4% 향상을 달성했어. FEAFA 데이터셋에서는 24개 AU 모두에서 이전 벤치마크 결과보다 큰 향상을 보여줬어. AU-LLaVA는 AU 인식에서 뛰어난 성능과 다양한 활용 가능성을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08461.pdf

Title: VistaFormer: Scalable Vision Transformers for Satellite Image Time Series Segmentation

Original Abstract:
We introduce VistaFormer, a lightweight Transformer-based model architecture for the semantic segmentation of remote-sensing images. This model uses a multi-scale Transformer-based encoder with a lightweight decoder that aggregates global and local attention captured in the encoder blocks. VistaFormer uses position-free self-attention layers which simplifies the model architecture and removes the need to interpolate temporal and spatial codes, which can reduce model performance when training and testing image resolutions differ. We investigate simple techniques for filtering noisy input signals like clouds and demonstrate that improved model scalability can be achieved by substituting Multi-Head Self-Attention (MHSA) with Neighbourhood Attention (NA). Experiments on the PASTIS and MTLCC crop-type segmentation benchmarks show that VistaFormer achieves better performance than comparable models and requires only 8% of the floating point operations using MHSA and 11% using NA while also using fewer trainable parameters. VistaFormer with MHSA improves on state-of-the-art mIoU scores by 0.1% on the PASTIS benchmark and 3% on the MTLCC benchmark while VistaFormer with NA improves on the MTLCC benchmark by 3.7%.

Translated Abstract:
우리는 원격 감지 이미지의 의미 분할을 위한 경량화된 Transformer 기반 모델인 VistaFormer를 소개해. 이 모델은 다중 스케일 Transformer 기반 인코더와 가벼운 디코더를 사용해서 인코더 블록에서 잡은 전역 및 지역 주의를 결합해.

VistaFormer는 위치에 의존하지 않는 자기 주의 레이어를 사용하는데, 이 덕분에 모델 구조가 간단해지고 시간적 및 공간적 코드 보간이 필요 없어져. 이렇게 하면 훈련할 때와 테스트할 때 이미지 해상도가 다를 때 성능이 떨어지는 걸 방지할 수 있어. 우리는 구름 같은 잡음 신호를 필터링하는 간단한 기술을 조사했고, Multi-Head Self-Attention (MHSA)를 이웃 주의(Neighbourhood Attention, NA)로 바꾸면 모델의 확장성을 높일 수 있다는 걸 보여줬어.

PASTIS와 MTLCC 작물 종류 분할 벤치마크에서 실험해 본 결과, VistaFormer는 비슷한 모델보다 더 나은 성능을 내고, MHSA를 사용할 때는 부동 소수점 연산의 8%만 필요하고 NA를 사용할 땐 11%만 필요해. 훈련 가능한 파라미터도 적어. MHSA를 쓴 VistaFormer는 PASTIS 벤치마크에서 최신 기술보다 mIoU 점수를 0.1% 높이고, MTLCC 벤치마크에서는 3% 향상시켰어. NA를 쓴 VistaFormer는 MTLCC 벤치마크에서 3.7% 개선했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08464.pdf

Title: VLTP: Vision-Language Guided Token Pruning for Task-Oriented Segmentation

Original Abstract:
Vision Transformers (ViTs) have emerged as the backbone of many segmentation models, consistently achieving state-of-the-art (SOTA) performance. However, their success comes at a significant computational cost. Image token pruning is one of the most effective strategies to address this complexity. However, previous approaches fall short when applied to more complex task-oriented segmentation (TOS), where the class of each image patch is not predefined but dependent on the specific input task. This work introduces the Vision Language Guided Token Pruning (VLTP), a novel token pruning mechanism that can accelerate ViTbased segmentation models, particularly for TOS guided by multi-modal large language model (MLLM). We argue that ViT does not need to process every image token through all of its layers only the tokens related to reasoning tasks are necessary. We design a new pruning decoder to take both image tokens and vision-language guidance as input to predict the relevance of each image token to the task. Only image tokens with high relevance are passed to deeper layers of the ViT. Experiments show that the VLTP framework reduces the computational costs of ViT by approximately 25% without performance degradation and by around 40% with only a 1% performance drop.

Translated Abstract:
비전 트랜스포머(ViT)는 많은 세분화 모델의 핵심으로 떠올랐고, 항상 최첨단의 성능을 기록하고 있어. 하지만 이게 큰 계산 비용을 요구해. 이미지 토큰 프루닝은 이런 복잡성을 해결할 수 있는 효과적인 전략 중 하나야. 하지만 이전의 방법들은 더 복잡한 작업 지향 세분화(TOS)에는 잘 적용되지 않아. 여기서 각 이미지 패치의 클래스는 미리 정의된 것이 아니라 특정 입력 작업에 따라 달라지거든.

이번 연구는 비전 언어 안내 토큰 프루닝(VLTP)이라는 새로운 토큰 프루닝 메커니즘을 소개해. 이건 ViT 기반 세분화 모델을 가속화할 수 있는 방법으로, 특히 다중 모달 대형 언어 모델(MLLM)에 의해 안내되는 TOS에 적합해. 우리는 ViT가 모든 이미지 토큰을 모든 레이어에서 처리할 필요는 없다고 주장해. 오직 추론 작업과 관련된 토큰만 필요하다는 거지.

새로운 프루닝 디코더를 설계해서 이미지 토큰과 비전-언어 안내를 입력으로 받아 각 이미지 토큰이 작업에 얼마나 관련이 있는지를 예측해. 그래서 높은 관련성을 가진 이미지 토큰만 ViT의 더 깊은 레이어로 전달돼. 실험 결과, VLTP 프레임워크는 ViT의 계산 비용을 약 25% 줄이면서 성능 저하 없이, 약 40%까지 줄일 수 있었고, 이때 성능은 단 1% 떨어졌어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08468.pdf

Title: Generalization Boosted Adapter for Open-Vocabulary Segmentation

Original Abstract:
Vision-language models (VLMs) have demonstrated remarkable open-vocabulary object recognition capabilities, motivating their adaptation for dense prediction tasks like segmentation. However, directly applying VLMs to such tasks remains challenging due to their lack of pixel-level granularity and the limited data available for fine-tuning, leading to overfitting and poor generalization. To address these limitations, we propose Generalization Boosted Adapter (GBA), a novel adapter strategy that enhances the generalization and robustness of VLMs for open-vocabulary segmentation. GBA comprises two core components: (1) a Style Diversification Adapter (SDA) that decouples features into amplitude and phase components, operating solely on the amplitude to enrich the feature space representation while preserving semantic consistency; and (2) a Correlation Constraint Adapter (CCA) that employs cross-attention to establish tighter semantic associations between text categories and target regions, suppressing irrelevant low-frequency ``noise'' information and avoiding erroneous associations. Through the synergistic effect of the shallow SDA and the deep CCA, GBA effectively alleviates overfitting issues and enhances the semantic relevance of feature representations. As a simple, efficient, and plug-and-play component, GBA can be flexibly integrated into various CLIP-based methods, demonstrating broad applicability and achieving state-of-the-art performance on multiple open-vocabulary segmentation benchmarks.

Translated Abstract:
비전-언어 모델(VLMs)은 놀라운 개방형 어휘 객체 인식 능력을 보여줬어. 그래서 이걸 밀집 예측 작업, 예를 들어 세그멘테이션에 적용하려는 시도가 있어. 하지만 VLM을 이런 작업에 직접 사용하는 건 어려워. 픽셀 수준의 세밀함이 부족하고, 미세 조정을 위한 데이터가 제한적이라서 과적합(overfitting) 문제가 생기고 일반화가 잘 안 돼.

이런 한계를 해결하기 위해 우리는 GBA(Generalization Boosted Adapter)를 제안해. 이건 VLM의 개방형 어휘 세그멘테이션을 위한 일반화와 강인성을 높이는 새로운 어댑터 전략이야. GBA는 두 가지 핵심 요소로 구성돼: 

1. 스타일 다양화 어댑터(SDA)는 특징(feature)을 진폭(amplitude)과 위상(phase) 성분으로 분리해. 진폭에만 작업을 해서 특징 공간 표현을 풍부하게 하면서도 의미의 일관성을 유지해.
2. 상관 제약 어댑터(CCA)는 크로스 어텐션을 사용해서 텍스트 카테고리와 목표 영역 간의 더 강한 의미적 연관성을 만들어. 관련 없는 저주파 "잡음" 정보를 억제하고 잘못된 연관을 피해.

얕은 SDA와 깊은 CCA의 시너지 효과 덕분에 GBA는 과적합 문제를 효과적으로 완화하고 특징 표현의 의미적 관련성을 높여. GBA는 간단하고 효율적이며 플러그 앤 플레이 형태로 다양한 CLIP 기반 방법에 유연하게 통합될 수 있어. 그래서 넓은 적용 가능성을 보여주고, 여러 개방형 어휘 세그멘테이션 벤치마크에서 최첨단 성능을 달성하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08475.pdf

Title: RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision

Original Abstract:
RT-DETR is the first real-time end-to-end transformer-based object detector. Its efficiency comes from the framework design and the Hungarian matching. However, compared to dense supervision detectors like the YOLO series, the Hungarian matching provides much sparser supervision, leading to insufficient model training and difficult to achieve optimal results. To address these issues, we proposed a hierarchical dense positive supervision method based on RT-DETR, named RT-DETRv3. Firstly, we introduce a CNN-based auxiliary branch that provides dense supervision that collaborates with the original decoder to enhance the encoder feature representation. Secondly, to address insufficient decoder training, we propose a novel learning strategy involving self-attention perturbation. This strategy diversifies label assignment for positive samples across multiple query groups, thereby enriching positive supervisions. Additionally, we introduce a shared-weight decoder branch for dense positive supervision to ensure more high-quality queries matching each ground truth. Notably, all aforementioned modules are training-only. We conduct extensive experiments to demonstrate the effectiveness of our approach on COCO val2017. RT-DETRv3 significantly outperforms existing real-time detectors, including the RT-DETR series and the YOLO series. For example, RT-DETRv3-R18 achieves 48.1% AP (+1.6%/+1.4%) compared to RT-DETR-R18/RT-DETRv2-R18 while maintaining the same latency. Meanwhile, it requires only half of epochs to attain a comparable performance. Furthermore, RT-DETRv3-R101 can attain an impressive 54.6% AP outperforming YOLOv10-X. Code will be released soon.

Translated Abstract:
RT-DETR는 처음으로 실시간으로 작동하는 엔드 투 엔드 트랜스포머 기반 객체 탐지기야. 이 모델의 효율성은 프레임워크 디자인과 헝가리안 매칭 덕분인데, 하지만 YOLO 시리즈 같은 밀집 감독 탐지기와 비교했을 때 헝가리안 매칭이 제공하는 감독이 훨씬 더 희박해서 모델 훈련이 부족하고 최적의 결과를 내기 어려워.

이런 문제를 해결하기 위해서 우리는 RT-DETR 기반의 계층적 밀집 긍정 감독 방법인 RT-DETRv3를 제안했어. 먼저, 원래 디코더와 협력해서 인코더의 특성 표현을 강화하는 CNN 기반의 보조 브랜치를 도입했어. 두 번째로, 디코더 훈련이 부족한 문제를 해결하기 위해 자기 주의 편향을 포함한 새로운 학습 전략을 제안했어. 이 전략은 여러 쿼리 그룹에 걸쳐 긍정 샘플에 대한 레이블 할당을 다양화해서 긍정 감독을 풍부하게 만들어.

또한, 더 많은 고품질 쿼리가 각 실제 값에 맞춰질 수 있도록 밀집 긍정 감독을 위한 공유 가중치 디코더 브랜치도 도입했어. 흥미롭게도, 이 모든 모듈은 오로지 훈련에만 사용돼. 우리는 COCO val2017에서 이 접근 방식의 효과를 입증하기 위해 광범위한 실험을 진행했어. RT-DETRv3는 RT-DETR 시리즈와 YOLO 시리즈를 포함한 기존의 실시간 탐지기를 크게 능가해.

예를 들어, RT-DETRv3-R18은 RT-DETR-R18/RT-DETRv2-R18에 비해 48.1% AP (+1.6%/+1.4%)를 달성하면서 같은 지연 시간을 유지해. 게다가 비슷한 성능을 얻기 위해서 필요한 에폭 수가 절반밖에 되지 않아. 또한, RT-DETRv3-R101은 놀라운 54.6% AP를 기록해 YOLOv10-X를 능가해. 코드도 곧 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08501.pdf

Title: PSTNet: Enhanced Polyp Segmentation with Multi-scale Alignment and Frequency Domain Integration

Original Abstract:
Accurate segmentation of colorectal polyps in colonoscopy images is crucial for effective diagnosis and management of colorectal cancer (CRC). However, current deep learning-based methods primarily rely on fusing RGB information across multiple scales, leading to limitations in accurately identifying polyps due to restricted RGB domain information and challenges in feature misalignment during multi-scale aggregation. To address these limitations, we propose the Polyp Segmentation Network with Shunted Transformer (PSTNet), a novel approach that integrates both RGB and frequency domain cues present in the images. PSTNet comprises three key modules: the Frequency Characterization Attention Module (FCAM) for extracting frequency cues and capturing polyp characteristics, the Feature Supplementary Alignment Module (FSAM) for aligning semantic information and reducing misalignment noise, and the Cross Perception localization Module (CPM) for synergizing frequency cues with high-level semantics to achieve efficient polyp segmentation. Extensive experiments on challenging datasets demonstrate PSTNet's significant improvement in polyp segmentation accuracy across various metrics, consistently outperforming state-of-the-art methods. The integration of frequency domain cues and the novel architectural design of PSTNet contribute to advancing computer-assisted polyp segmentation, facilitating more accurate diagnosis and management of CRC.

Translated Abstract:
대장 내시경 이미지에서 대장 폴립을 정확하게 분할하는 것은 대장암(CRC)의 효과적인 진단과 관리에 정말 중요해. 그런데 현재의 딥러닝 기반 방법들은 주로 여러 스케일에서 RGB 정보를 결합하는 방식에 의존하고 있어서, RGB 정보가 제한적이고 다중 스케일 집합 과정에서 특성 정렬이 잘 안 되는 문제 때문에 폴립을 정확하게 식별하는 데 한계가 있어.

이런 한계를 해결하기 위해, 우리는 RGB와 주파수 도메인 정보를 모두 활용하는 새로운 접근 방식인 폴립 분할 네트워크(PSTNet)를 제안해. PSTNet은 세 가지 주요 모듈로 구성되어 있어: 주파수 특징 주의 모듈(FCAM)은 주파수 정보를 추출하고 폴립의 특징을 잡아내고, 특징 보조 정렬 모듈(FSAM)은 의미 정보를 정렬하고 정렬 노이즈를 줄여줘. 마지막으로 교차 인식 위치 모듈(CPM)은 주파수 정보를 고급 의미와 결합해 효율적인 폴립 분할을 도와줘.

어려운 데이터셋에서의 광범위한 실험 결과, PSTNet은 다양한 지표에서 폴립 분할 정확도가 크게 향상되었고, 최신 방법들보다 일관되게 더 나은 성능을 보여줬어. 주파수 도메인 정보와 PSTNet의 새로운 구조적 설계가 컴퓨터 보조 폴립 분할을 발전시키는 데 기여하고, 대장암의 더 정확한 진단과 관리에 도움이 될 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08508.pdf

Title: Identifying Human Indoor Daily Life Behavior employing Thermal Sensor Arrays (TSAs)

Original Abstract:
Daily activity monitoring systems used in households provide vital information for health status, particularly with aging residents. Multiple approaches have been introduced to achieve such goals, typically obtrusive and non-obtrusive. Amongst the obtrusive approaches are the wearable devices, and among the non-obtrusive approaches are the movement detection systems, including motion sensors and thermal sensor arrays (TSAs). TSA systems are advantageous when preserving a person's privacy and picking his precise spatial location. In this study, human daily living activities were monitored day and night, constructing the corresponding activity time series and spatial probability distribution and employing a TSA system. The monitored activities are classified into two categories: sleeping and daily activity. Results showed the possibility of distinguishing between classes regardless of day and night. The obtained sleep activity duration was compared with previous research using the same raw data. Results showed that the duration of sleep activity, on average, was 9 hours/day, and daily life activity was 7 hours/day. The person's spatial probability distribution was determined using the bivariate distribution for the monitored location. In conclusion, the results showed that sleeping activity was dominant. Our study showed that TSAs were the optimum choice when monitoring human activity. Our proposed approach tackled limitations encountered by previous human activity monitoring systems, such as preserving human privacy while knowing his precise spatial location.

Translated Abstract:
가정에서 사용되는 일상 활동 모니터링 시스템은 노인들의 건강 상태를 파악하는 데 중요한 정보를 제공해. 여러 가지 방법이 제안되었는데, 크게 방해가 되는 방법과 방해가 되지 않는 방법으로 나뉘어. 방해가 되는 방법에는 착용할 수 있는 기기가 있고, 방해가 되지 않는 방법에는 움직임 감지 시스템이 있어. 여기에는 모션 센서와 열 센서 배열(TSA)이 포함되지.

TSA 시스템은 개인의 프라이버시를 지키면서 정확한 위치를 파악하는 데 유리해. 이 연구에서는 사람의 일상 생활 활동을 낮과 밤에 모니터링해서 해당 활동의 시간 시계열과 공간 확률 분포를 만들었어. 모니터링한 활동은 두 가지 카테고리로 나눌 수 있었는데, 수면과 일상 활동이야. 결과적으로, 낮과 밤에 관계없이 두 가지 활동을 구분할 수 있는 가능성이 있음을 보여줬어.

수면 활동의 지속시간은 이전 연구와 비교했는데, 평균적으로 하루 9시간의 수면 활동을 했고, 일상 활동은 하루 7시간이었어. 사람의 공간 확률 분포는 모니터링한 위치에 대한 이변량 분포를 사용해 결정했어. 결론적으로, 수면 활동이 주로 나타났고, 이 연구는 인간 활동 모니터링 시 TSA가 최적의 선택이라는 것을 보여줬어. 우리가 제안한 방법은 이전의 인간 활동 모니터링 시스템들이 가지고 있던 한계, 즉 개인의 프라이버시를 지키면서도 정확한 위치를 파악할 수 있는 문제를 해결했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08509.pdf

Title: Exploiting Supervised Poison Vulnerability to Strengthen Self-Supervised Defense

Original Abstract:
Availability poisons exploit supervised learning (SL) algorithms by introducing class-related shortcut features in images such that models trained on poisoned data are useless for real-world datasets. Self-supervised learning (SSL), which utilizes augmentations to learn instance discrimination, is regarded as a strong defense against poisoned data. However, by extending the study of SSL across multiple poisons on the CIFAR-10 and ImageNet-100 datasets, we demonstrate that it often performs poorly, far below that of training on clean data. Leveraging the vulnerability of SL to poison attacks, we introduce adversarial training (AT) on SL to obfuscate poison features and guide robust feature learning for SSL. Our proposed defense, designated VESPR (Vulnerability Exploitation of Supervised Poisoning for Robust SSL), surpasses the performance of six previous defenses across seven popular availability poisons. VESPR displays superior performance over all previous defenses, boosting the minimum and average ImageNet-100 test accuracies of poisoned models by 16% and 9%, respectively. Through analysis and ablation studies, we elucidate the mechanisms by which VESPR learns robust class features.

Translated Abstract:
가용성 악성 데이터는 감독 학습(SL) 알고리즘을 이용해 이미지에 클래스 관련 단축키 기능을 추가해서, 악성 데이터로 훈련된 모델이 실제 데이터셋에서는 쓸모없게 만든다. 자기 지도 학습(SSL)은 데이터 증강을 통해 인스턴스 구별을 배우기 때문에 악성 데이터에 대한 강력한 방어책으로 여겨진다. 그런데 CIFAR-10과 ImageNet-100 데이터셋에서 여러 가지 악성 데이터로 SSL을 연구해본 결과, SSL의 성능이 실제로는 좋지 않아서 깨끗한 데이터로 훈련했을 때보다 훨씬 떨어진다는 걸 보여준다.

SL이 악성 공격에 취약한 점을 활용해서, 우리는 SL에서 적대적 훈련(AT)을 도입해 악성 특징을 숨기고 SSL을 위한 강력한 특징 학습을 유도한다. 우리가 제안한 방어책은 VESPR(감독 악성 데이터의 취약성 활용을 통한 강력한 SSL)로, 이전의 여섯 가지 방어책보다 성능이 뛰어난 것으로 나타났다. VESPR는 이전의 모든 방어책보다 우수한 성능을 보이며, 악성 모델의 ImageNet-100 테스트 정확도를 각각 16%와 9% 향상시킨다.

분석과 제거 연구를 통해, VESPR가 어떻게 강력한 클래스 특징을 학습하는지에 대한 메커니즘을 설명한다.

================================================================================

URL:
https://arxiv.org/pdf/2409.08510.pdf

Title: CasDyF-Net: Image Dehazing via Cascaded Dynamic Filters

Original Abstract:
Image dehazing aims to restore image clarity and visual quality by reducing atmospheric scattering and absorption effects. While deep learning has made significant strides in this area, more and more methods are constrained by network depth. Consequently, lots of approaches have adopted parallel branching strategies. however, they often prioritize aspects such as resolution, receptive field, or frequency domain segmentation without dynamically partitioning branches based on the distribution of input features. Inspired by dynamic filtering, we propose using cascaded dynamic filters to create a multi-branch network by dynamically generating filter kernels based on feature map distribution. To better handle branch features, we propose a residual multiscale block (RMB), combining different receptive fields. Furthermore, we also introduce a dynamic convolution-based local fusion method to merge features from adjacent branches. Experiments on RESIDE, Haze4K, and O-Haze datasets validate our method's effectiveness, with our model achieving a PSNR of 43.21dB on the RESIDE-Indoor dataset. The code is available at this https URL.

Translated Abstract:
이미지 디헤이징은 대기에서 발생하는 산란과 흡수 효과를 줄여서 이미지의 선명도와 시각적 품질을 회복하는 걸 목표로 해. 딥러닝이 이 분야에서 많은 발전을 이루긴 했지만, 네트워크 깊이에 제한을 받는 방법들이 많아. 그래서 여러 방법들이 병렬 분기 전략을 채택하고 있어. 하지만 이들 방법은 보통 해상도, 수용 영역, 주파수 도메인 분할 같은 부분을 우선시하면서 입력 특성의 분포에 따라 동적으로 분기를 나누지 않아.

우리는 동적 필터링에서 영감을 받아서, 특성 맵 분포에 따라 필터 커널을 동적으로 생성하는 캐스케이드 동적 필터를 사용해서 다중 분기 네트워크를 만드는 방법을 제안해. 분기 특성을 더 잘 다루기 위해서, 서로 다른 수용 영역을 결합한 잔여 멀티스케일 블록(RMB)을 제안해. 또한, 인접한 분기에서 특징을 합치기 위해 동적 컨볼루션 기반의 로컬 융합 방법도 도입했어.

RESIDE, Haze4K, O-Haze 데이터셋에서 실험한 결과, 우리 방법의 효과가 입증됐어. 우리 모델은 RESIDE-Indoor 데이터셋에서 43.21dB의 PSNR을 달성했어. 코드도 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08513.pdf

Title: Mamba-YOLO-World: Marrying YOLO-World with Mamba for Open-Vocabulary Detection

Original Abstract:
Open-vocabulary detection (OVD) aims to detect objects beyond a predefined set of categories. As a pioneering model incorporating the YOLO series into OVD, YOLO-World is well-suited for scenarios prioritizing speed and efficiency.However, its performance is hindered by its neck feature fusion mechanism, which causes the quadratic complexity and the limited guided receptive this http URL address these limitations, we present Mamba-YOLO-World, a novel YOLO-based OVD model employing the proposed MambaFusion Path Aggregation Network (MambaFusion-PAN) as its neck architecture. Specifically, we introduce an innovative State Space Model-based feature fusion mechanism consisting of a Parallel-Guided Selective Scan algorithm and a Serial-Guided Selective Scan algorithm with linear complexity and globally guided receptive fields. It leverages multi-modal input sequences and mamba hidden states to guide the selective scanning process.Experiments demonstrate that our model outperforms the original YOLO-World on the COCO and LVIS benchmarks in both zero-shot and fine-tuning settings while maintaining comparable parameters and FLOPs. Additionally, it surpasses existing state-of-the-art OVD methods with fewer parameters and FLOPs.

Translated Abstract:
오픈 보카블러리 검출(OVD)은 미리 정해진 카테고리 세트를 넘어서 물체를 검출하는 걸 목표로 해. YOLO 시리즈를 OVD에 처음으로 통합한 모델인 YOLO-World는 속도와 효율성을 중요시하는 상황에 잘 맞아. 하지만, 이 모델의 성능은 목 부분에서 특성 융합 메커니즘 때문에 제한을 받아. 이 메커니즘은 복잡도가 제곱으로 증가하고, 제약된 수용 범위를 초래해.

이런 한계를 해결하기 위해 우리는 Mamba-YOLO-World라는 새로운 YOLO 기반 OVD 모델을 제안해. 이 모델은 MambaFusion Path Aggregation Network(MambaFusion-PAN)를 목 구조로 사용해. 구체적으로, 우리는 선형 복잡도와 전역적으로 안내된 수용 필드를 가진 병렬 안내 선택 스캔 알고리즘과 직렬 안내 선택 스캔 알고리즘으로 구성된 혁신적인 상태 공간 모델 기반 특성 융합 메커니즘을 소개해. 이 과정은 다중 모달 입력 시퀀스와 맘바 숨겨진 상태를 활용해 선택적 스캔 과정을 안내해.

실험 결과, 우리 모델은 COCO와 LVIS 벤치마크에서 제로샷과 파인튜닝 설정 모두에서 원래 YOLO-World보다 성능이 뛰어나면서도, 유사한 파라미터와 FLOPs를 유지해. 게다가, 기존의 최첨단 OVD 방법들을 파라미터와 FLOPs가 더 적으면서도 초월해.

================================================================================

URL:
https://arxiv.org/pdf/2409.08516.pdf

Title: AWF: Adaptive Weight Fusion for Enhanced Class Incremental Semantic Segmentation

Original Abstract:
Class Incremental Semantic Segmentation (CISS) aims to mitigate catastrophic forgetting by maintaining a balance between previously learned and newly introduced knowledge. Existing methods, primarily based on regularization techniques like knowledge distillation, help preserve old knowledge but often face challenges in effectively integrating new knowledge, resulting in limited overall improvement. Endpoints Weight Fusion (EWF) method, while simple, effectively addresses some of these limitations by dynamically fusing the model weights from previous steps with those from the current step, using a fusion parameter alpha determined by the relative number of previously known classes and newly introduced classes. However, the simplicity of the alpha calculation may limit its ability to fully capture the complexities of different task scenarios, potentially leading to suboptimal fusion outcomes. In this paper, we propose an enhanced approach called Adaptive Weight Fusion (AWF), which introduces an alternating training strategy for the fusion parameter, allowing for more flexible and adaptive weight integration. AWF achieves superior performance by better balancing the retention of old knowledge with the learning of new classes, significantly improving results on benchmark CISS tasks compared to the original EWF. And our experiment code will be released on Github.

Translated Abstract:
클래스 점진적 의미 분할(Class Incremental Semantic Segmentation, CISS)은 기존에 배운 지식과 새로 도입된 지식 간의 균형을 유지하면서 재앙적 망각(catastrophic forgetting)을 줄이는 걸 목표로 해. 기존 방법들은 주로 지식 증류(knowledge distillation) 같은 정규화 기법에 기반해 이전 지식을 보존하는 데 도움을 주지만, 새로운 지식을 효과적으로 통합하는 데는 한계가 있어. 그래서 전반적인 개선이 제한적이야.

EndPoints Weight Fusion (EWF) 방법은 간단하지만, 이전 단계의 모델 가중치와 현재 단계의 가중치를 동적으로 융합하는 방식으로 이 문제의 일부를 해결해. 여기서 융합 파라미터인 알파(alpha)는 이전에 알고 있던 클래스와 새로 도입된 클래스의 상대적인 수에 의해 결정돼. 하지만 알파 계산이 너무 단순하다 보니, 다양한 작업 시나리오의 복잡성을 충분히 반영하지 못할 수도 있어, 그래서 최적이 아닌 융합 결과가 나올 수 있어.

이 논문에서는 Adaptive Weight Fusion (AWF)이라고 하는 개선된 접근 방식을 제안해. 이 방법은 융합 파라미터에 대한 교차 훈련 전략을 도입해서 가중치 통합을 더 유연하고 적응적으로 만들어. AWF는 이전 지식을 잘 유지하면서 새로운 클래스 학습을 더 잘 균형 잡아, 원래 EWF에 비해 CISS 벤치마크 작업에서 성능을 크게 개선했어. 그리고 실험 코드는 깃허브(Github)에서 공개할 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08518.pdf

Title: Anytime Continual Learning for Open Vocabulary Classification

Original Abstract:
We propose an approach for anytime continual learning (AnytimeCL) for open vocabulary image classification. The AnytimeCL problem aims to break away from batch training and rigid models by requiring that a system can predict any set of labels at any time and efficiently update and improve when receiving one or more training samples at any time. Despite the challenging goal, we achieve substantial improvements over recent methods. We propose a dynamic weighting between predictions of a partially fine-tuned model and a fixed open vocabulary model that enables continual improvement when training samples are available for a subset of a task's labels. We also propose an attention-weighted PCA compression of training features that reduces storage and computation with little impact to model accuracy. Our methods are validated with experiments that test flexibility of learning and inference. Code is available at this https URL.

Translated Abstract:
우리는 오픈 어휘 이미지 분류를 위한 언제든지 지속적 학습(AnytimeCL) 접근 방식을 제안해. AnytimeCL 문제는 배치 학습이나 고정된 모델에서 벗어나서, 시스템이 언제든지 어떤 레이블 세트를 예측할 수 있고, 동시에 하나 이상의 학습 샘플을 받을 때 효율적으로 업데이트하고 개선할 수 있도록 하는 거야.

이 목표가 어렵긴 하지만, 우리는 최근 방법들보다 상당한 개선을 이뤘어. 우리는 부분적으로 미세 조정된 모델의 예측과 고정된 오픈 어휘 모델 간의 동적 가중치를 제안해서, 작업의 레이블 중 일부에 대한 학습 샘플이 있을 때 지속적으로 개선할 수 있도록 했어.

또한, 학습 특징을 압축하는 주의 가중치 PCA 방법을 제안했는데, 이건 저장 공간과 계산을 줄이면서 모델 정확도에는 큰 영향을 주지 않아. 우리의 방법은 학습과 추론의 유연성을 테스트하는 실험으로 검증됐어. 코드에 대한 링크는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08520.pdf

Title: GroundingBooth: Grounding Text-to-Image Customization

Original Abstract:
Recent studies in text-to-image customization show great success in generating personalized object variants given several images of a subject. While existing methods focus more on preserving the identity of the subject, they often fall short of controlling the spatial relationship between objects. In this work, we introduce GroundingBooth, a framework that achieves zero-shot instance-level spatial grounding on both foreground subjects and background objects in the text-to-image customization task. Our proposed text-image grounding module and masked cross-attention layer allow us to generate personalized images with both accurate layout alignment and identity preservation while maintaining text-image coherence. With such layout control, our model inherently enables the customization of multiple subjects at once. Our model is evaluated on both layout-guided image synthesis and reference-based customization tasks, showing strong results compared to existing methods. Our work is the first work to achieve a joint grounding of both subject-driven foreground generation and text-driven background generation.

Translated Abstract:
최근 텍스트-이미지 맞춤화 연구에서, 여러 이미지로 개인화된 객체 변형을 잘 만들어내는 데 성공을 거두고 있어. 기존 방법들은 주로 주제의 정체성을 유지하는 데 집중하지만, 객체 간의 공간적 관계를 조절하는 데는 부족함이 많아. 

이번 연구에서는 GroundingBooth라는 프레임워크를 소개해. 이 프레임워크는 텍스트-이미지 맞춤화 작업에서 전경 주제와 배경 객체 모두에 대해 제로샷 인스턴스 수준의 공간적 기반을 달성해. 우리가 제안한 텍스트-이미지 기반 모듈과 마스킹된 크로스 어텐션 레이어 덕분에, 정확한 배치 정렬과 정체성 유지가 동시에 가능하면서도 텍스트와 이미지의 일관성을 유지한 개인화된 이미지를 생성할 수 있어. 

이런 배치 조절 덕분에, 우리 모델은 여러 주제를 동시에 맞춤화할 수 있는 기능도 가지고 있어. 우리의 모델은 배치 안내 이미지 합성과 참조 기반 맞춤화 작업에서 평가되었고, 기존 방법들에 비해 강력한 결과를 보여주었어. 우리 연구는 주제 중심 전경 생성과 텍스트 중심 배경 생성을 동시에 이루는 첫 번째 작업이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08557.pdf

Title: DICS: Find Domain-Invariant and Class-Specific Features for Out-of-Distribution Generalization

Original Abstract:
While deep neural networks have made remarkable progress in various vision tasks, their performance typically deteriorates when tested in out-of-distribution (OOD) scenarios. Many OOD methods focus on extracting domain-invariant features but neglect whether these features are unique to each class. Even if some features are domain-invariant, they cannot serve as key classification criteria if shared across different classes. In OOD tasks, both domain-related and class-shared features act as confounders that hinder generalization. In this paper, we propose a DICS model to extract Domain-Invariant and Class-Specific features, including Domain Invariance Testing (DIT) and Class Specificity Testing (CST), which mitigate the effects of spurious correlations introduced by confounders. DIT learns domain-related features of each source domain and removes them from inputs to isolate domain-invariant class-related features. DIT ensures domain invariance by aligning same-class features across different domains. Then, CST calculates soft labels for those features by comparing them with features learned in previous steps. We optimize the cross-entropy between the soft labels and their true labels, which enhances same-class similarity and different-class distinctiveness, thereby reinforcing class specificity. Extensive experiments on widely-used benchmarks demonstrate the effectiveness of our proposed algorithm. Additional visualizations further demonstrate that DICS effectively identifies the key features of each class in target domains.

Translated Abstract:
딥 뉴럴 네트워크는 다양한 비전 작업에서 놀라운 발전을 이뤘지만, 분포 외(OOD) 상황에서 성능이 떨어지는 경향이 있어. 많은 OOD 방법들은 도메인 불변 피처를 추출하는 데 집중하지만, 이런 피처들이 각 클래스에 고유한지에 대해서는 신경 쓰지 않아. 어떤 피처가 도메인 불변이라고 해도, 다른 클래스와 공유되면 중요한 분류 기준이 될 수 없어. OOD 작업에서는 도메인 관련 피처와 클래스 공유 피처가 함께 작용해 일반화를 방해하는 혼란 요소가 돼.

이 논문에서는 DICS 모델을 제안해 도메인 불변과 클래스 특화 피처를 추출해. 여기에는 도메인 불변성 테스트(DIT)와 클래스 특수성 테스트(CST)가 포함돼, 이 테스트들이 혼란 요소가 만들어낸 잘못된 상관관계의 영향을 줄여줘. DIT는 각 소스 도메인의 도메인 관련 피처를 학습하고, 이를 입력에서 제거해 도메인 불변 클래스 관련 피처를 분리해. DIT는 서로 다른 도메인 간의 같은 클래스 피처를 정렬함으로써 도메인 불변성을 보장해. 그 다음 CST는 이전 단계에서 학습한 피처와 비교해 그런 피처에 대한 소프트 레이블을 계산해. 우리는 소프트 레이블과 실제 레이블 간의 크로스 엔트로피를 최적화해서 같은 클래스 간 유사성을 높이고, 다른 클래스 간의 구별성을 강화해. 

많은 실험을 통해 우리가 제안한 알고리즘이 효과적이라는 걸 입증했어. 추가적인 시각화 결과도 DICS가 타겟 도메인에서 각 클래스의 중요한 피처를 효과적으로 식별한다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.08562.pdf

Title: CSS: Overcoming Pose and Scene Challenges in Crowd-Sourced 3D Gaussian Splatting

Original Abstract:
We introduce Crowd-Sourced Splatting (CSS), a novel 3D Gaussian Splatting (3DGS) pipeline designed to overcome the challenges of pose-free scene reconstruction using crowd-sourced imagery. The dream of reconstructing historically significant but inaccessible scenes from collections of photographs has long captivated researchers. However, traditional 3D techniques struggle with missing camera poses, limited viewpoints, and inconsistent lighting. CSS addresses these challenges through robust geometric priors and advanced illumination modeling, enabling high-quality novel view synthesis under complex, real-world conditions. Our method demonstrates clear improvements over existing approaches, paving the way for more accurate and flexible applications in AR, VR, and large-scale 3D reconstruction.

Translated Abstract:
Crowd-Sourced Splatting (CSS)라는 새로운 3D Gaussian Splatting (3DGS) 파이프라인을 소개할게. 이건 사람들이 제공한 이미지를 사용해서 자세가 필요 없는 장면 재구성을 더 쉽게 하려고 만든 거야. 역사적으로 중요한 장면들을 사진 모음으로 재구성하는 꿈은 연구자들을 오랫동안 매료시켰어. 

하지만 기존의 3D 기술은 카메라 자세가 없거나, 시점이 제한적이거나, 조명이 일정하지 않은 경우에 어려움을 겪어. CSS는 강력한 기하학적 정보와 고급 조명 모델링을 통해 이런 문제를 해결해. 그래서 복잡한 현실 조건에서도 높은 품질의 새로운 시점을 만들어낼 수 있어.

우리 방법은 기존 접근 방식보다 분명한 개선점을 보여주고, AR, VR, 그리고 대규모 3D 재구성에 더 정확하고 유연한 응용 가능성을 열어줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.08566.pdf

Title: Hybrid-TTA: Continual Test-time Adaptation via Dynamic Domain Shift Detection

Original Abstract:
Continual Test Time Adaptation (CTTA) has emerged as a critical approach for bridging the domain gap between the controlled training environments and the real-world scenarios, enhancing model adaptability and robustness. Existing CTTA methods, typically categorized into Full-Tuning (FT) and Efficient-Tuning (ET), struggle with effectively addressing domain shifts. To overcome these challenges, we propose Hybrid-TTA, a holistic approach that dynamically selects instance-wise tuning method for optimal adaptation. Our approach introduces the Dynamic Domain Shift Detection (DDSD) strategy, which identifies domain shifts by leveraging temporal correlations in input sequences and dynamically switches between FT and ET to adapt to varying domain shifts effectively. Additionally, the Masked Image Modeling based Adaptation (MIMA) framework is integrated to ensure domain-agnostic robustness with minimal computational overhead. Our Hybrid-TTA achieves a notable 1.6%p improvement in mIoU on the Cityscapes-to-ACDC benchmark dataset, surpassing previous state-of-the-art methods and offering a robust solution for real-world continual adaptation challenges.

Translated Abstract:
연속 테스트 시간 적응(CTTA)은 통제된 훈련 환경과 실제 상황 사이의 도메인 차이를 줄이는 중요한 방법으로 떠올랐어. 이 방법은 모델의 적응성과 강인성을 높여줘. 기존의 CTTA 방법들은 보통 전체 조정(FT)과 효율적 조정(ET)으로 나뉘는데, 도메인 변화에 효과적으로 대응하는 데 어려움을 겪고 있어.

이런 문제를 해결하기 위해 우리는 하이브리드 TTA(Hybrid-TTA)라는 새로운 방법을 제안해. 이 방법은 최적의 적응을 위해 인스턴스별로 조정 방법을 동적으로 선택해. 우리의 접근 방식은 동적 도메인 변화 감지(Dynamic Domain Shift Detection, DDSD) 전략을 도입해, 입력 시퀀스의 시간적 상관관계를 활용해 도메인 변화를 파악하고 FT와 ET 사이를 동적으로 전환해 다양한 도메인 변화에 효과적으로 적응해.

게다가, 마스크 이미지 모델링 기반 적응(Masked Image Modeling based Adaptation, MIMA) 프레임워크를 통합해서 도메인에 구애받지 않는 강인성을 최소한의 계산 비용으로 보장해. 우리의 하이브리드 TTA는 시티스케이프(Cityscapes)에서 ACDC 벤치마크 데이터셋까지 mIoU에서 1.6%p 개선을 달성했어. 이전의 최신 방법들을 초월하며 실제 환경에서의 연속 적응 문제에 대한 강력한 해결책을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.08572.pdf

Title: DiffFAS: Face Anti-Spoofing via Generative Diffusion Models

Original Abstract:
Face anti-spoofing (FAS) plays a vital role in preventing face recognition (FR) systems from presentation attacks. Nowadays, FAS systems face the challenge of domain shift, impacting the generalization performance of existing FAS methods. In this paper, we rethink about the inherence of domain shift and deconstruct it into two factors: image style and image quality. Quality influences the purity of the presentation of spoof information, while style affects the manner in which spoof information is presented. Based on our analysis, we propose DiffFAS framework, which quantifies quality as prior information input into the network to counter image quality shift, and performs diffusion-based high-fidelity cross-domain and cross-attack types generation to counter image style shift. DiffFAS transforms easily collectible live faces into high-fidelity attack faces with precise labels while maintaining consistency between live and spoof face identities, which can also alleviate the scarcity of labeled data with novel type attacks faced by nowadays FAS system. We demonstrate the effectiveness of our framework on challenging cross-domain and cross-attack FAS datasets, achieving the state-of-the-art performance. Available at this https URL.

Translated Abstract:
얼굴 위조 방지(FAS)는 얼굴 인식(FR) 시스템이 프레젠테이션 공격을 막는 데 중요한 역할을 해. 요즘 FAS 시스템은 도메인 변화의 문제에 직면해 있어서 기존 FAS 방법들의 일반화 성능에 영향을 미치고 있어. 

이 논문에서는 도메인 변화의 본질을 다시 생각해보고, 이걸 두 가지 요소로 나눌 수 있다고 주장해. 하나는 이미지 스타일이고, 다른 하나는 이미지 품질이야. 품질은 위조 정보의 순수성에 영향을 미치고, 스타일은 위조 정보가 표현되는 방식을 결정해. 

우리의 분석을 바탕으로 우리는 DiffFAS 프레임워크를 제안해. 이 프레임워크는 품질을 네트워크에 입력하는 사전 정보로 정량화해서 이미지 품질 변화를 막고, 확산 기반의 고충실도 크로스 도메인 및 크로스 공격 유형 생성을 통해 이미지 스타일 변화를 극복해. 

DiffFAS는 쉽게 수집할 수 있는 실제 얼굴을 고충실도 공격 얼굴로 변환하고, 정확한 레이블을 유지하면서 실제 얼굴과 위조 얼굴의 정체성을 일치시켜. 이 방식은 요즘 FAS 시스템이 직면한 새로운 유형의 공격에 대한 레이블이 있는 데이터 부족 문제를 완화하는 데도 도움이 돼. 

우리는 도전적인 크로스 도메인 및 크로스 공격 FAS 데이터셋에서 우리의 프레임워크가 얼마나 효과적인지를 보여주고, 최첨단 성능을 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08573.pdf

Title: HTR-VT: Handwritten Text Recognition with Vision Transformer

Original Abstract:
We explore the application of Vision Transformer (ViT) for handwritten text recognition. The limited availability of labeled data in this domain poses challenges for achieving high performance solely relying on ViT. Previous transformer-based models required external data or extensive pre-training on large datasets to excel. To address this limitation, we introduce a data-efficient ViT method that uses only the encoder of the standard transformer. We find that incorporating a Convolutional Neural Network (CNN) for feature extraction instead of the original patch embedding and employ Sharpness-Aware Minimization (SAM) optimizer to ensure that the model can converge towards flatter minima and yield notable enhancements. Furthermore, our introduction of the span mask technique, which masks interconnected features in the feature map, acts as an effective regularizer. Empirically, our approach competes favorably with traditional CNN-based models on small datasets like IAM and READ2016. Additionally, it establishes a new benchmark on the LAM dataset, currently the largest dataset with 19,830 training text lines. The code is publicly available at: this https URL.

Translated Abstract:
우리는 손글씨 텍스트 인식을 위해 비전 트랜스포머(ViT)의 활용을 연구했어. 이 분야에서 라벨이 붙은 데이터가 제한적이라서 ViT만 가지고는 높은 성능을 내기 어려워. 기존의 트랜스포머 기반 모델들은 외부 데이터나 큰 데이터셋에 대한 광범위한 사전 학습이 필요했어.

이런 한계를 극복하기 위해, 우리는 표준 트랜스포머의 인코더만 사용하는 데이터 효율적인 ViT 방법을 제안했어. 원래 패치 임베딩 대신에 CNN(합성곱 신경망)을 사용해서 특징을 추출하고, Sharpness-Aware Minimization(SAM) 옵티마이저를 활용해 모델이 더 평평한 최소값으로 수렴할 수 있도록 했어. 이로 인해 눈에 띄는 향상을 얻을 수 있었어.

게다가, 우리는 특징 맵에서 서로 연결된 특징들을 마스킹하는 스팬 마스크 기법을 도입했어. 이 기법은 효과적인 정규화 역할을 해. 실험적으로, 우리의 접근 방식은 IAM과 READ2016 같은 작은 데이터셋에서 전통적인 CNN 기반 모델들과 비교했을 때 경쟁력이 있었어. 게다가, 현재 19,830개의 훈련 텍스트 라인을 가진 LAM 데이터셋에서 새로운 기준을 세웠어. 코드는 이 공개된 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08582.pdf

Title: ChangeChat: An Interactive Model for Remote Sensing Change Analysis via Multimodal Instruction Tuning

Original Abstract:
Remote sensing (RS) change analysis is vital for monitoring Earth's dynamic processes by detecting alterations in images over time. Traditional change detection excels at identifying pixel-level changes but lacks the ability to contextualize these alterations. While recent advancements in change captioning offer natural language descriptions of changes, they do not support interactive, user-specific queries. To address these limitations, we introduce ChangeChat, the first bitemporal vision-language model (VLM) designed specifically for RS change analysis. ChangeChat utilizes multimodal instruction tuning, allowing it to handle complex queries such as change captioning, category-specific quantification, and change localization. To enhance the model's performance, we developed the ChangeChat-87k dataset, which was generated using a combination of rule-based methods and GPT-assisted techniques. Experiments show that ChangeChat offers a comprehensive, interactive solution for RS change analysis, achieving performance comparable to or even better than state-of-the-art (SOTA) methods on specific tasks, and significantly surpassing the latest general-domain model, GPT-4. Code and pre-trained weights are available at this https URL.

Translated Abstract:
원격 감지(RS) 변화 분석은 지구의 동적인 과정을 모니터링하는 데 중요해. 시간에 따른 이미지의 변화 감지를 통해 이뤄지지. 전통적인 변화 탐지는 픽셀 수준의 변화를 잘 찾아내지만, 이런 변화의 맥락을 이해하는 데는 한계가 있어. 최근 변화 설명 기술이 발전해서 자연어로 변화를 설명할 수 있지만, 사용자 맞춤형 질문에 대한 지원은 부족해.

이런 한계를 해결하기 위해 우리는 ChangeChat을 소개해. ChangeChat은 RS 변화 분석을 위해 특별히 설계된 첫 번째 이원적 비전-언어 모델(VLM)이야. ChangeChat은 멀티모달 지침 튜닝을 활용해서, 변화 설명, 카테고리 별 양적 측정, 변화 위치 파악 같은 복잡한 질문을 처리할 수 있어.

모델 성능을 높이기 위해 ChangeChat-87k 데이터셋을 만들었어. 이 데이터셋은 규칙 기반 방법과 GPT 지원 기술을 조합해서 생성됐어. 실험 결과, ChangeChat은 RS 변화 분석을 위한 포괄적이고 인터랙티브한 솔루션을 제공하며, 특정 작업에서는 최신 기술(SOTA)과 비슷하거나 더 나은 성능을 보여주고, 일반 도메인 모델인 GPT-4보다도 훨씬 뛰어난 성능을 보였어. 코드와 사전 훈련된 가중치는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08585.pdf

Title: Optimizing 4D Lookup Table for Low-light Video Enhancement via Wavelet Priori

Original Abstract:
Low-light video enhancement is highly demanding in maintaining spatiotemporal color consistency. Therefore, improving the accuracy of color mapping and keeping the latency low is challenging. Based on this, we propose incorporating Wavelet-priori for 4D Lookup Table (WaveLUT), which effectively enhances the color coherence between video frames and the accuracy of color mapping while maintaining low latency. Specifically, we use the wavelet low-frequency domain to construct an optimized lookup prior and achieve an adaptive enhancement effect through a designed Wavelet-prior 4D lookup table. To effectively compensate the a priori loss in the low light region, we further explore a dynamic fusion strategy that adaptively determines the spatial weights based on the correlation between the wavelet lighting prior and the target intensity structure. In addition, during the training phase, we devise a text-driven appearance reconstruction method that dynamically balances brightness and content through multimodal semantics-driven Fourier spectra. Extensive experiments on a wide range of benchmark datasets show that this method effectively enhances the previous method's ability to perceive the color space and achieves metric-favorable and perceptually oriented real-time enhancement while maintaining high efficiency.

Translated Abstract:
저조도 비디오 향상은 공간-시간 색상 일관성을 유지하는 데 많은 요구가 있어. 그래서 색상 매핑의 정확도를 높이고 지연 시간을 낮추는 게 어려워. 이걸 바탕으로, 우리는 4D 룩업 테이블(WaveLUT)에 웨이블릿 프라이어(Wavelet-priori)를 포함시키는 방법을 제안해. 이 방법은 비디오 프레임 간의 색상 일관성을 효과적으로 높이고 색상 매핑의 정확도를 유지하면서 낮은 지연 시간을 유지해.

특히, 우리는 웨이블릿 저주파 도메인을 사용해서 최적화된 룩업 프라이어를 만들고 설계된 웨이블릿 프라이어 4D 룩업 테이블을 통해 적응형 향상 효과를 얻어. 저조도 영역에서의 사전 손실을 보완하기 위해, 우리는 웨이블릿 조명 프라이어와 목표 강도 구조 간의 상관관계를 기반으로 공간 가중치를 적응적으로 결정하는 동적 융합 전략도 탐구해.

또한, 훈련 단계에서는 다중 모달 의미 기반 푸리에 스펙트럼을 통해 밝기와 내용을 동적으로 균형 있게 조절하는 텍스트 주도 외관 재구성 방법을 개발했어. 다양한 벤치마크 데이터셋에서 광범위한 실험을 통해, 이 방법이 이전 방법의 색상 공간 인식 능력을 효과적으로 향상시키고, 메트릭적으로 유리하며 지각적으로 지향하는 실시간 향상을 이루면서 높은 효율성을 유지한다는 것을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08598.pdf

Title: Knowledge-Enhanced Facial Expression Recognition with Emotional-to-Neutral Transformation

Original Abstract:
Existing facial expression recognition (FER) methods typically fine-tune a pre-trained visual encoder using discrete labels. However, this form of supervision limits to specify the emotional concept of different facial expressions. In this paper, we observe that the rich knowledge in text embeddings, generated by vision-language models, is a promising alternative for learning discriminative facial expression representations. Inspired by this, we propose a novel knowledge-enhanced FER method with an emotional-to-neutral transformation. Specifically, we formulate the FER problem as a process to match the similarity between a facial expression representation and text embeddings. Then, we transform the facial expression representation to a neutral representation by simulating the difference in text embeddings from textual facial expression to textual neutral. Finally, a self-contrast objective is introduced to pull the facial expression representation closer to the textual facial expression, while pushing it farther from the neutral representation. We conduct evaluation with diverse pre-trained visual encoders including ResNet-18 and Swin-T on four challenging facial expression datasets. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art FER methods. The code will be publicly available.

Translated Abstract:
기존의 얼굴 표정 인식(FER) 방법은 일반적으로 미리 학습된 비주얼 인코더를 특정한 레이블로 조정하는 방식이에요. 그런데 이런 감독 방식은 다양한 얼굴 표정의 감정 개념을 정확히 나타내는 데 한계가 있어요. 

그래서 우리는 비전-언어 모델에서 생성된 텍스트 임베딩의 풍부한 지식이 차별화된 얼굴 표정 표현을 배우는 데 좋은 대안이 될 수 있다는 점에 주목했어요. 이를 바탕으로 감정에서 중립으로 변환하는 새로운 지식 강화 FER 방법을 제안해요. 

구체적으로, 우리는 FER 문제를 얼굴 표정 표현과 텍스트 임베딩 간의 유사성을 맞추는 과정으로 정의해요. 그 다음, 얼굴 표정 표현을 텍스트의 얼굴 표정과 텍스트의 중립 간의 차이를 시뮬레이션해 중립 표현으로 변환해요. 마지막으로, 자기 대조 목표를 도입해 얼굴 표정 표현을 텍스트의 얼굴 표정에 더 가깝게 만들고, 중립 표현과는 더 멀어지게 해요. 

우리는 ResNet-18과 Swin-T를 포함한 다양한 미리 학습된 비주얼 인코더를 사용해 네 개의 도전적인 얼굴 표정 데이터셋에서 평가를 진행했어요. 실험 결과, 우리의 방법이 최신 FER 방법들보다 상당히 뛰어난 성능을 보여줬어요. 코드도 공개할 예정이에요.

================================================================================

URL:
https://arxiv.org/pdf/2409.08613.pdf

Title: Dense Point Clouds Matter: Dust-GS for Scene Reconstruction from Sparse Viewpoints

Original Abstract:
3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in scene synthesis and novel view synthesis tasks. Typically, the initialization of 3D Gaussian primitives relies on point clouds derived from Structure-from-Motion (SfM) methods. However, in scenarios requiring scene reconstruction from sparse viewpoints, the effectiveness of 3DGS is significantly constrained by the quality of these initial point clouds and the limited number of input images. In this study, we present Dust-GS, a novel framework specifically designed to overcome the limitations of 3DGS in sparse viewpoint conditions. Instead of relying solely on SfM, Dust-GS introduces an innovative point cloud initialization technique that remains effective even with sparse input data. Our approach leverages a hybrid strategy that integrates an adaptive depth-based masking technique, thereby enhancing the accuracy and detail of reconstructed scenes. Extensive experiments conducted on several benchmark datasets demonstrate that Dust-GS surpasses traditional 3DGS methods in scenarios with sparse viewpoints, achieving superior scene reconstruction quality with a reduced number of input images.

Translated Abstract:
3D 가우시안 스플래팅(3DGS)은 장면 합성과 새로운 시점 합성 작업에서 뛰어난 성능을 보여줬어. 보통 3D 가우시안 프리미티브의 초기화는 구조에서 움직임(SfM) 방법으로 얻은 포인트 클라우드에 의존해. 그런데, 시점이 적은 상황에서 장면을 재구성해야 할 때, 3DGS의 효과는 이런 초기 포인트 클라우드의 품질과 입력 이미지의 수에 큰 제약을 받아.

이번 연구에서는 Dust-GS라는 새로운 프레임워크를 소개해. 이건 3DGS의 한계를 극복하기 위해 특별히 설계된 거야. Dust-GS는 SfM에만 의존하는 대신, 희소한 입력 데이터에서도 잘 작동하는 새로운 포인트 클라우드 초기화 기술을 도입해. 우리의 접근법은 적응형 깊이 기반 마스킹 기법을 통합한 하이브리드 전략을 활용해서 재구성된 장면의 정확성과 디테일을 향상시켜.

여러 벤치마크 데이터셋에서 진행된 광범위한 실험 결과, Dust-GS는 시점이 적은 상황에서도 전통적인 3DGS 방법보다 뛰어난 성능을 보여줬어. 입력 이미지 수를 줄이면서도 더 우수한 장면 재구성 품질을 달성했지.

================================================================================

URL:
https://arxiv.org/pdf/2409.08667.pdf

Title: Test-time Training for Hyperspectral Image Super-resolution

Original Abstract:
The progress on Hyperspectral image (HSI) super-resolution (SR) is still lagging behind the research of RGB image SR. HSIs usually have a high number of spectral bands, so accurately modeling spectral band interaction for HSI SR is hard. Also, training data for HSI SR is hard to obtain so the dataset is usually rather small. In this work, we propose a new test-time training method to tackle this problem. Specifically, a novel self-training framework is developed, where more accurate pseudo-labels and more accurate LR-HR relationships are generated so that the model can be further trained with them to improve performance. In order to better support our test-time training method, we also propose a new network architecture to learn HSI SR without modeling spectral band interaction and propose a new data augmentation method Spectral Mixup to increase the diversity of the training data at test time. We also collect a new HSI dataset with a diverse set of images of interesting objects ranging from food to vegetation, to materials, and to general scenes. Extensive experiments on multiple datasets show that our method can improve the performance of pre-trained models significantly after test-time training and outperform competing methods significantly for HSI SR.

Translated Abstract:
하이퍼스펙트럼 이미지(HSI) 초해상도(SR) 연구는 RGB 이미지 SR 연구에 비해 아직 많이 뒤쳐져 있어. HSI는 스펙트럼 밴드가 많아서, HSI SR을 위해 스펙트럼 밴드 간의 상호작용을 정확하게 모델링하는 게 어려워. 게다가 HSI SR에 필요한 훈련 데이터도 구하기 힘들어서 보통 데이터셋이 꽤 작아.

이 연구에서는 이런 문제를 해결하기 위해 새로운 테스트 타임 훈련 방법을 제안해. 구체적으로, 더 정확한 의사 라벨과 더 정확한 저해상도-고해상도 관계를 생성하는 새로운 자기 훈련 프레임워크를 개발했어. 이렇게 생성된 데이터로 모델을 추가로 훈련시켜 성능을 향상시키는 거지.

우리의 테스트 타임 훈련 방식을 잘 지원하기 위해, 스펙트럼 밴드 간의 상호작용을 모델링하지 않고 HSI SR을 학습할 수 있는 새로운 네트워크 아키텍처도 제안했어. 그리고 테스트 시 훈련 데이터의 다양성을 높이기 위해 스펙트럴 믹스업이라는 새로운 데이터 증강 방법도 만들었어.

또한, 음식, 식물, 재료, 일반 장면 등 다양한 흥미로운 물체의 이미지를 포함한 새로운 HSI 데이터셋도 수집했어. 여러 데이터셋에서의 광범위한 실험 결과, 우리의 방법이 테스트 타임 훈련 후에 사전 훈련된 모델의 성능을 크게 향상시킬 수 있고, HSI SR에 대해 경쟁 방법들보다 훨씬 더 좋은 성능을 보인다는 걸 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08669.pdf

Title: AdR-Gaussian: Accelerating Gaussian Splatting with Adaptive Radius

Original Abstract:
3D Gaussian Splatting (3DGS) is a recent explicit 3D representation that has achieved high-quality reconstruction and real-time rendering of complex scenes. However, the rasterization pipeline still suffers from unnecessary overhead resulting from avoidable serial Gaussian culling, and uneven load due to the distinct number of Gaussian to be rendered across pixels, which hinders wider promotion and application of 3DGS. In order to accelerate Gaussian splatting, we propose AdR-Gaussian, which moves part of serial culling in Render stage into the earlier Preprocess stage to enable parallel culling, employing adaptive radius to narrow the rendering pixel range for each Gaussian, and introduces a load balancing method to minimize thread waiting time during the pixel-parallel rendering. Our contributions are threefold, achieving a rendering speed of 310% while maintaining equivalent or even better quality than the state-of-the-art. Firstly, we propose to early cull Gaussian-Tile pairs of low splatting opacity based on an adaptive radius in the Gaussian-parallel Preprocess stage, which reduces the number of affected tile through the Gaussian bounding circle, thus reducing unnecessary overhead and achieving faster rendering speed. Secondly, we further propose early culling based on axis-aligned bounding box for Gaussian splatting, which achieves a more significant reduction in ineffective expenses by accurately calculating the Gaussian size in the 2D directions. Thirdly, we propose a balancing algorithm for pixel thread load, which compresses the information of heavy-load pixels to reduce thread waiting time, and enhance information of light-load pixels to hedge against rendering quality loss. Experiments on three datasets demonstrate that our algorithm can significantly improve the Gaussian Splatting rendering speed.

Translated Abstract:
3D 가우시안 스플래팅(3DGS)은 최근에 등장한 3D 표현 방식으로, 복잡한 장면을 고품질로 재구성하고 실시간으로 렌더링할 수 있어. 하지만, 이 렌더링 과정에서 불필요한 오버헤드가 발생하고, 픽셀마다 렌더링해야 하는 가우시안의 수가 달라서 부하가 불균형하게 걸리는 문제가 있어. 이런 이유로 3DGS의 더 넓은 활용이 어려워.

그래서 우리는 AdR-Gaussian이라는 방법을 제안해. 이 방법은 렌더링 단계에서 일부 연속적인 가우시안 제거 과정을 미리 처리 단계로 옮겨서 병렬 처리가 가능하게 해. 각 가우시안의 렌더링 픽셀 범위를 좁히기 위해 적응형 반경을 사용하고, 픽셀 병렬 렌더링 중 스레드 대기 시간을 최소화하는 부하 균형 방법도 도입했어.

우리의 기여는 세 가지야. 첫째, 적응형 반경을 기반으로 스플래팅 불투명도가 낮은 가우시안-타일 쌍을 미리 제거해서, 영향을 받는 타일의 수를 줄이고, 불필요한 오버헤드를 줄여서 렌더링 속도를 높였어. 둘째, 가우시안 스플래팅을 위해 축 정렬 경계 박스를 기반으로 한 조기 제거 방법을 제안했어. 이 방법은 2D 방향에서 가우시안 크기를 정확하게 계산함으로써 비효율적인 비용을 크게 줄여줘. 셋째, 픽셀 스레드 부하를 균형 있게 조절하는 알고리즘을 제안했어. 이 알고리즘은 부하가 많이 걸린 픽셀의 정보를 압축해 스레드 대기 시간을 줄이고, 부하가 적은 픽셀의 정보를 강화해 렌더링 품질 손실을 방지해.

세 가지 데이터셋에서의 실험 결과, 우리의 알고리즘이 가우시안 스플래팅 렌더링 속도를 크게 개선할 수 있음을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08688.pdf

Title: GenMapping: Unleashing the Potential of Inverse Perspective Mapping for Robust Online HD Map Construction

Original Abstract:
Online High-Definition (HD) maps have emerged as the preferred option for autonomous driving, overshadowing the counterpart offline HD maps due to flexible update capability and lower maintenance costs. However, contemporary online HD map models embed parameters of visual sensors into training, resulting in a significant decrease in generalization performance when applied to visual sensors with different parameters. Inspired by the inherent potential of Inverse Perspective Mapping (IPM), where camera parameters are decoupled from the training process, we have designed a universal map generation framework, GenMapping. The framework is established with a triadic synergy architecture, including principal and dual auxiliary branches. When faced with a coarse road image with local distortion translated via IPM, the principal branch learns robust global features under the state space models. The two auxiliary branches are a dense perspective branch and a sparse prior branch. The former exploits the correlation information between static and moving objects, whereas the latter introduces the prior knowledge of OpenStreetMap (OSM). The triple-enhanced merging module is crafted to synergistically integrate the unique spatial features from all three branches. To further improve generalization capabilities, a Cross-View Map Learning (CVML) scheme is leveraged to realize joint learning within the common space. Additionally, a Bidirectional Data Augmentation (BiDA) module is introduced to mitigate reliance on datasets concurrently. A thorough array of experimental results shows that the proposed model surpasses current state-of-the-art methods in both semantic mapping and vectorized mapping, while also maintaining a rapid inference speed. The source code will be publicly available at this https URL.

Translated Abstract:
온라인 고화질(HD) 맵은 자율주행에 가장 선호되는 옵션으로 부상했어. 오프라인 HD 맵보다 유연한 업데이트 기능과 낮은 유지보수 비용 덕분이지. 하지만 현재의 온라인 HD 맵 모델은 시각 센서의 매개변수를 훈련 과정에 포함시키기 때문에, 다른 매개변수를 가진 시각 센서에 적용할 때 일반화 성능이 크게 떨어져. 

우리는 카메라 매개변수가 훈련 과정에서 분리되는 역투시 매핑(IPM)의 잠재력에 영감을 받아, 보편적인 맵 생성 프레임워크인 GenMapping을 설계했어. 이 프레임워크는 주요 가지와 두 개의 보조 가지를 포함한 삼원 상호작용 구조로 구성돼. IPM을 통해 변환된 지역 왜곡이 있는 도로 이미지에 직면했을 때, 주요 가지는 상태 공간 모델 하에서 강력한 전역 특징을 학습해. 

두 개의 보조 가지는 밀집 투시 가지와 희소 사전 가지야. 밀집 투시 가지는 정적 물체와 이동 물체 간의 상관 정보를 활용하고, 희소 사전 가지는 OpenStreetMap(OSM)의 사전 지식을 도입해. 세 가지의 고유한 공간 특징을 통합하기 위해, 삼중 강화 병합 모듈이 설계됐어. 

일반화 능력을 더욱 향상시키기 위해, 공통 공간 내에서 공동 학습을 실현하는 교차 뷰 맵 학습(CVML) 방식을 활용했어. 또한, 데이터셋 의존도를 줄이기 위해 양방향 데이터 증강(BiDA) 모듈도 도입했어. 

실험 결과를 보면, 제안된 모델이 현재 최첨단 방법들을 초월하며, 의미 맵핑과 벡터화된 맵핑 모두에서 빠른 추론 속도를 유지하고 있어. 소스 코드는 이 URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08691.pdf

Title: Autoregressive Sequence Modeling for 3D Medical Image Representation

Original Abstract:
Three-dimensional (3D) medical images, such as Computed Tomography (CT) and Magnetic Resonance Imaging (MRI), are essential for clinical applications. However, the need for diverse and comprehensive representations is particularly pronounced when considering the variability across different organs, diagnostic tasks, and imaging modalities. How to effectively interpret the intricate contextual information and extract meaningful insights from these images remains an open challenge to the community. While current self-supervised learning methods have shown potential, they often consider an image as a whole thereby overlooking the extensive, complex relationships among local regions from one or multiple images. In this work, we introduce a pioneering method for learning 3D medical image representations through an autoregressive pre-training framework. Our approach sequences various 3D medical images based on spatial, contrast, and semantic correlations, treating them as interconnected visual tokens within a token sequence. By employing an autoregressive sequence modeling task, we predict the next visual token in the sequence, which allows our model to deeply understand and integrate the contextual information inherent in 3D medical images. Additionally, we implement a random startup strategy to avoid overestimating token relationships and to enhance the robustness of learning. The effectiveness of our approach is demonstrated by the superior performance over others on nine downstream tasks in public datasets.

Translated Abstract:
3D 의료 이미지, 예를 들면 CT(컴퓨터 단층촬영)와 MRI(자기 공명 영상)는 임상에서 매우 중요해. 하지만 다양한 장기, 진단 작업, 그리고 영상 모달리티에 따라 다르게 나타나는 특성을 고려할 때, 여러 가지 방식으로 이미지를 잘 표현하는 게 필요해. 복잡한 맥락 정보를 해석하고 의미 있는 통찰을 뽑아내는 건 여전히 해결해야 할 문제야. 현재의 자가 지도 학습 방법들이 잠재력을 보이긴 하지만, 보통 이미지를 전체로 보고 지역 간의 복잡한 관계를 간과하는 경우가 많아.

이 연구에서는 자가 회귀 사전 훈련 프레임워크를 통해 3D 의료 이미지 표현을 학습하는 새로운 방법을 제안해. 우리 방법은 다양한 3D 의료 이미지를 공간적, 대비, 의미적 연관성에 따라 순서대로 배열하고, 이를 서로 연결된 시각적 토큰으로 다뤄. 자가 회귀 시퀀스 모델링 작업을 통해 시퀀스에서 다음 시각적 토큰을 예측함으로써, 모델이 3D 의료 이미지에 내재된 맥락 정보를 깊이 이해하고 통합할 수 있도록 해. 

또한, 토큰 간의 관계를 과대평가하지 않도록 무작위 시작 전략을 구현해 학습의 강건성을 높였어. 우리의 접근 방식은 공공 데이터셋에서 아홉 개의 하위 작업에서 다른 방법들보다 뛰어난 성능을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.08695.pdf

Title: Precision Aquaculture: An Integrated Computer Vision and IoT Approach for Optimized Tilapia Feeding

Original Abstract:
Traditional fish farming practices often lead to inefficient feeding, resulting in environmental issues and reduced productivity. We developed an innovative system combining computer vision and IoT technologies for precise Tilapia feeding. Our solution uses real-time IoT sensors to monitor water quality parameters and computer vision algorithms to analyze fish size and count, determining optimal feed amounts. A mobile app enables remote monitoring and control. We utilized YOLOv8 for keypoint detection to measure Tilapia weight from length, achieving \textbf{94\%} precision on 3,500 annotated images. Pixel-based measurements were converted to centimeters using depth estimation for accurate feeding calculations. Our method, with data collection mirroring inference conditions, significantly improved results. Preliminary estimates suggest this approach could increase production up to 58 times compared to traditional farms. Our models, code, and dataset are open-source~\footnote{The code, dataset, and models are available upon reasonable request.

Translated Abstract:
전통적인 양식 방식은 효율적인 먹이 주기를 방해해서 환경 문제와 생산성 저하를 초래해. 우리는 틸라피아를 정확하게 먹이기 위해 컴퓨터 비전과 IoT 기술을 결합한 혁신적인 시스템을 개발했어. 

우리의 솔루션은 IoT 센서를 사용해 실시간으로 수질 상태를 모니터링하고, 컴퓨터 비전 알고리즘을 통해 물고기의 크기와 수를 분석해서 최적의 먹이 양을 결정해. 모바일 앱을 통해 원격으로 모니터링하고 제어할 수 있어. 

우리는 틸라피아의 길이로부터 무게를 측정하기 위해 YOLOv8를 사용했고, 3,500개의 주석 이미지에서 94%의 정확도를 달성했어. 픽셀 기반 측정값은 깊이 추정을 통해 센티미터로 변환되어 정확한 먹이 계산에 활용됐어. 데이터 수집이 추론 조건과 비슷하게 이루어져서 결과가 크게 개선됐지. 

예비 추정에 따르면, 이 방식은 전통적인 양식장에 비해 생산량을 최대 58배까지 늘릴 수 있을 것 같아. 우리의 모델, 코드, 데이터셋은 오픈 소스야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08744.pdf

Title: Uncertainty and Generalizability in Foundation Models for Earth Observation

Original Abstract:
We take the perspective in which we want to design a downstream task (such as estimating vegetation coverage) on a certain area of interest (AOI) with a limited labeling budget. By leveraging an existing Foundation Model (FM) we must decide whether we train a downstream model on a different but label-rich AOI hoping it generalizes to our AOI, or we split labels in our AOI for training and validating. In either case, we face choices concerning what FM to use, how to sample our AOI for labeling, etc. which affect both the performance and uncertainty of the results. In this work, we perform a large ablative study using eight existing FMs on either Sentinel 1 or Sentinel 2 as input data, and the classes from the ESA World Cover product as downstream tasks across eleven AOIs. We do repeated sampling and training, resulting in an ablation of some 500K simple linear regression models. Our results show both the limits of spatial generalizability across AOIs and the power of FMs where we are able to get over 0.9 correlation coefficient between predictions and targets on different chip level predictive tasks. And still, performance and uncertainty vary greatly across AOIs, tasks and FMs. We believe this is a key issue in practice, because there are many design decisions behind each FM and downstream task (input modalities, sampling, architectures, pretraining, etc.) and usually a downstream task designer is aware of and can decide upon a few of them. Through this work, we advocate for the usage of the methodology herein described (large ablations on reference global labels and simple probes), both when publishing new FMs, and to make informed decisions when designing downstream tasks to use them.

Translated Abstract:
우리는 특정 관심 영역(AOI)에서 식생 커버리지를 추정하는 같은 다운스트림 작업을 제한된 레이블 예산으로 설계하고자 하는 관점을 가지고 있어. 기존의 기본 모델(FM)을 활용하면서, 레이블이 풍부한 다른 AOI에서 다운스트림 모델을 학습할지, 아니면 우리의 AOI에서 레이블을 나누어 학습과 검증을 할지를 결정해야 해. 이 경우, 어떤 FM을 사용할지, AOI에서 레이블을 어떻게 샘플링할지 등 여러 선택을 해야 하고, 이 선택들은 결과의 성능과 불확실성에 영향을 미쳐.

이번 연구에서는 입력 데이터로 Sentinel 1 또는 Sentinel 2를 사용하여 11개의 AOI에서 ESA World Cover 제품의 클래스를 다운스트림 작업으로 설정하고, 8개의 기존 FM을 사용한 대규모 아블레이션 연구를 수행했어. 반복 샘플링과 학습을 통해 약 50만 개의 간단한 선형 회귀 모델을 아블레이션했어. 우리의 결과는 AOI 간의 공간 일반화의 한계를 보여주고, 서로 다른 칩 레벨 예측 작업에서 예측값과 목표값 간의 상관계수가 0.9를 넘는 FM의 힘도 보여줘. 하지만 AOI, 작업, FM에 따라 성능과 불확실성은 크게 달라져.

우리는 이것이 실질적으로 중요한 문제라고 생각해, 왜냐하면 각 FM과 다운스트림 작업 뒤에는 많은 설계 결정이 있고(입력 방식, 샘플링, 아키텍처, 사전 학습 등) 보통 다운스트림 작업 설계자는 그 중 일부에 대해서만 알고 결정할 수 있기 때문이야. 이번 연구를 통해, 새로운 FM을 발표할 때와 다운스트림 작업을 설계할 때 여기서 설명한 방법론(전세계 레이블에 대한 대규모 아블레이션과 간단한 프로브)을 사용하는 것을 권장해.

================================================================================

URL:
https://arxiv.org/pdf/2409.08769.pdf

Title: Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry

Original Abstract:
In recent years, transformer-based architectures become the de facto standard for sequence modeling in deep learning frameworks. Inspired by the successful examples, we propose a causal visual-inertial fusion transformer (VIFT) for pose estimation in deep visual-inertial odometry. This study aims to improve pose estimation accuracy by leveraging the attention mechanisms in transformers, which better utilize historical data compared to the recurrent neural network (RNN) based methods seen in recent methods. Transformers typically require large-scale data for training. To address this issue, we utilize inductive biases for deep VIO networks. Since latent visual-inertial feature vectors encompass essential information for pose estimation, we employ transformers to refine pose estimates by updating latent vectors temporally. Our study also examines the impact of data imbalance and rotation learning methods in supervised end-to-end learning of visual inertial odometry by utilizing specialized gradients in backpropagation for the elements of SE$(3)$ group. The proposed method is end-to-end trainable and requires only a monocular camera and IMU during inference. Experimental results demonstrate that VIFT increases the accuracy of monocular VIO networks, achieving state-of-the-art results when compared to previous methods on the KITTI dataset. The code will be made available at this https URL.

Translated Abstract:
최근 몇 년 동안, 트랜스포머 기반 아키텍처가 딥러닝 프레임워크에서 시퀀스 모델링의 사실상의 표준이 되었어. 이런 성공적인 사례에 영감을 받아, 우리는 포즈 추정을 위한 인과적 비주얼-관성 융합 트랜스포머(VIFT)를 제안해. 이 연구의 목표는 트랜스포머의 주의 메커니즘을 활용해서 포즈 추정 정확도를 높이는 거야. 트랜스포머는 최근에 사용되는 순환 신경망(RNN) 기반 방법들에 비해 역사적 데이터를 더 잘 활용할 수 있어.

트랜스포머는 보통 대규모 데이터가 필요해. 이 문제를 해결하기 위해 우리는 딥 VIO 네트워크에 대한 귀납적 편향을 사용해. 잠재적인 비주얼-관성 특징 벡터는 포즈 추정을 위한 중요한 정보를 담고 있어서, 우리는 시간에 따라 잠재 벡터를 업데이트하며 포즈 추정을 다듬기 위해 트랜스포머를 사용해. 우리의 연구는 시각-관성 오도메트리의 감독된 엔드 투 엔드 학습에서 데이터 불균형과 회전 학습 방법의 영향을 조사해. SE$(3)$ 그룹의 요소에 대해 특수한 그래디언트를 활용해서 역전파를 적용했어.

제안한 방법은 엔드 투 엔드로 학습 가능하고, 추론할 때 단일 카메라와 IMU만 필요해. 실험 결과, VIFT가 단일 카메라 VIO 네트워크의 정확성을 높여주고, KITTI 데이터셋에서 이전 방법들과 비교했을 때 최첨단 결과를 달성했어. 코드는 이 HTTPS URL에서 제공될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08782.pdf

Title: Contactless Fingerprint Recognition Using 3D Graph Matching

Original Abstract:
Contactless fingerprint is a newly developed type of fingerprint, and has gained lots of attention in recent fingerprint studies. However, most existing contactless fingerprint algorithms treat contactless fingerprints as 2D plain fingerprints, and utilize similar recognition methods as traditional contact-based 2D fingerprints. This recognition approach does not consider the modality difference between contactless and contact fingerprints, especially the intrinsic 3D characteristic of contactless fingerprints. This paper proposes a novel contactless fingerprint recognition algorithm that captures the revealed 3D feature of contactless fingerprints rather than the plain 2D feature. The proposed method first recovers 3D features from the input contactless fingerprint, including the 3D shape model and 3D fingerprint feature (minutiae, orientation, etc.). Then, a novel 3D graph matching is conducted in 3D space according to the extracted 3D feature. Our method captures the real 3D nature of contactless fingerprints as the whole feature extraction and matching algorithms are completed in real 3D space. Experiments results on contactless fingerprint databases show that the proposed method successfully improves the matching accuracy of contactless fingerprints. Exceptionally, our method performs stably across multiple poses of contactless fingerprints due to 3D graph matching, which is a great advantage compared to previous contactless fingerprint recognition algorithms.

Translated Abstract:
비접촉식 지문은 새로 개발된 지문 유형으로, 최근 지문 연구에서 많은 관심을 받고 있어. 그런데 기존의 비접촉식 지문 알고리즘은 비접촉식 지문을 2D 평면 지문처럼 다루고, 전통적인 접촉식 2D 지문과 비슷한 인식 방법을 사용해. 이런 인식 방식은 비접촉식 지문과 접촉식 지문 사이의 차이를 고려하지 않고, 특히 비접촉식 지문의 고유한 3D 특성을 간과해.

이 논문에서는 비접촉식 지문의 3D 특징을 포착하는 새로운 인식 알고리즘을 제안해. 제안된 방법은 먼저 입력된 비접촉식 지문에서 3D 특징을 복원해. 여기에는 3D 형태 모델과 3D 지문 특징(미뉴티아, 방향 등)이 포함돼. 그 다음, 추출된 3D 특징에 따라 3D 공간에서 새로운 3D 그래프 매칭을 진행해. 이 방법은 비접촉식 지문의 실제 3D 특성을 잡아내며, 전체 특징 추출과 매칭 알고리즘이 실제 3D 공간에서 이루어져.

비접촉식 지문 데이터베이스에서의 실험 결과, 제안된 방법이 비접촉식 지문의 매칭 정확도를 성공적으로 향상시킨다고 보여. 특히, 우리의 방법은 3D 그래프 매칭 덕분에 다양한 비접촉식 지문의 자세에서도 안정적으로 작동하는데, 이는 기존의 비접촉식 지문 인식 알고리즘에 비해 큰 장점이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08800.pdf

Title: Task-Specific Data Preparation for Deep Learning to Reconstruct Structures of Interest from Severely Truncated CBCT Data

Original Abstract:
Cone-beam computed tomography (CBCT) is widely used in interventional surgeries and radiation oncology. Due to the limited size of flat-panel detectors, anatomical structures might be missing outside the limited field-of-view (FOV), which restricts the clinical applications of CBCT systems. Recently, deep learning methods have been proposed to extend the FOV for multi-slice CT systems. However, in mobile CBCT system with a smaller FOV size, projection data is severely truncated and it is challenging for a network to restore all missing structures outside the FOV. In some applications, only certain structures outside the FOV are of interest, e.g., ribs in needle path planning for liver/lung cancer diagnosis. Therefore, a task-specific data preparation method is proposed in this work, which automatically let the network focus on structures of interest instead of all the structures. Our preliminary experiment shows that Pix2pixGAN with a conventional training has the risk to reconstruct false positive and false negative rib structures from severely truncated CBCT data, whereas Pix2pixGAN with the proposed task-specific training can reconstruct all the ribs reliably. The proposed method is promising to empower CBCT with more clinical applications.

Translated Abstract:
콘빔 컴퓨터 단층 촬영(CBCT)은 중재 수술과 방사선 종양학에서 널리 사용돼. 하지만 평면 탐지기의 크기가 한정적이어서, 제한된 시야(FOV) 밖의 해부학적 구조는 놓칠 수 있어. 이 때문에 CBCT 시스템의 임상 활용에 제약이 생겨. 최근에는 다중 슬라이스 CT 시스템의 FOV를 확장하기 위해 딥러닝 방법이 제안됐어.

하지만 FOV가 작은 이동식 CBCT 시스템에서는 투영 데이터가 심하게 잘리기 때문에, 네트워크가 FOV 밖의 모든 누락된 구조를 복원하는 게 어려워. 어떤 경우에는 FOV 밖의 특정 구조만 관심이 있을 때도 있어. 예를 들어, 간이나 폐암 진단을 위한 바늘 경로 계획에서 갈비뼈가 그런 경우지. 그래서 이 연구에서는 네트워크가 모든 구조 대신 관심 있는 구조에 집중할 수 있도록 자동으로 데이터 준비 방법을 제안했어.

우리의 초기 실험 결과를 보면, 전통적인 훈련을 받은 Pix2pixGAN은 심하게 잘린 CBCT 데이터에서 잘못된 양성 및 음성 갈비뼈 구조를 복원할 위험이 있어. 반면, 제안한 작업 특화 훈련을 받은 Pix2pixGAN은 모든 갈비뼈를 신뢰성 있게 복원할 수 있어. 이 방법은 CBCT의 임상 활용을 더 넓힐 수 있는 가능성이 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08824.pdf

Title: Pathfinder for Low-altitude Aircraft with Binary Neural Network

Original Abstract:
A prior global topological map (e.g., the OpenStreetMap, OSM) can boost the performance of autonomous mapping by a ground mobile robot. However, the prior map is usually incomplete due to lacking labeling in partial paths. To solve this problem, this paper proposes an OSM maker using airborne sensors carried by low-altitude aircraft, where the core of the OSM maker is a novel efficient pathfinder approach based on LiDAR and camera data, i.e., a binary dual-stream road segmentation model. Specifically, a multi-scale feature extraction based on the UNet architecture is implemented for images and point clouds. To reduce the effect caused by the sparsity of point cloud, an attention-guided gated block is designed to integrate image and point-cloud features. For enhancing the efficiency of the model, we propose a binarization streamline to each model component, including a variant of vision transformer (ViT) architecture as the encoder of the image branch, and new focal and perception losses to optimize the model training. The experimental results on two datasets demonstrate that our pathfinder method achieves SOTA accuracy with high efficiency in finding paths from the low-level airborne sensors, and we can create complete OSM prior maps based on the segmented road skeletons. Code and data are available at:this https URL}{this https URL.

Translated Abstract:
이전에 만들어진 글로벌 토폴로지 맵(예: OpenStreetMap, OSM)은 지상 모바일 로봇의 자율 맵핑 성능을 높일 수 있어. 하지만 기존 맵은 부분적인 경로에 라벨링이 부족해서 보통 불완전해. 이 문제를 해결하기 위해, 이 논문에서는 저고도 항공기에 장착된 공중 센서를 이용한 OSM 제작기를 제안해. OSM 제작기의 핵심은 LiDAR와 카메라 데이터를 기반으로 한 새로운 효율적인 경로 탐색 방법인 이진 이중 스트림 도로 세분화 모델이야.

특히, 이미지와 포인트 클라우드를 위한 멀티 스케일 특징 추출이 UNet 아키텍처를 기반으로 구현됐어. 포인트 클라우드의 희소성으로 인한 영향을 줄이기 위해, 이미지와 포인트 클라우드 특징을 통합하는 주의 기반 게이트 블록이 설계됐어. 모델의 효율성을 높이기 위해, 각 모델 구성요소에 이진화 간소화 방법을 제안하고, 이미지 브랜치의 인코더로 비전 트랜스포머(ViT) 아키텍처 변형을 사용했어. 그리고 모델 훈련을 최적화하기 위해 새로운 포컬 손실과 지각 손실을 도입했어.

두 개의 데이터셋에서 실험한 결과, 우리의 경로 탐색 방법이 저고도 공중 센서에서 경로를 찾는 데 높은 효율로 SOTA 정확도를 달성했어. 그리고 세분화된 도로 골격을 기반으로 완전한 OSM 이전 맵을 만들 수 있어. 코드와 데이터는 여기에 있어: 이 링크에서 확인해봐.

================================================================================

URL:
https://arxiv.org/pdf/2409.08831.pdf

Title: Breaking reCAPTCHAv2

Original Abstract:
Our work examines the efficacy of employing advanced machine learning methods to solve captchas from Google's reCAPTCHAv2 system. We evaluate the effectiveness of automated systems in solving captchas by utilizing advanced YOLO models for image segmentation and classification. Our main result is that we can solve 100% of the captchas, while previous work only solved 68-71%. Furthermore, our findings suggest that there is no significant difference in the number of challenges humans and bots must solve to pass the captchas in reCAPTCHAv2. This implies that current AI technologies can exploit advanced image-based captchas. We also look under the hood of reCAPTCHAv2, and find evidence that reCAPTCHAv2 is heavily based on cookie and browser history data when evaluating whether a user is human or not. The code is provided alongside this paper.

Translated Abstract:
우리 연구는 구글의 reCAPTCHAv2 시스템에서 캡차를 푸는 데 고급 머신러닝 방법을 사용하는 효과를 살펴봤어. 우리는 YOLO 모델을 활용해서 이미지 분할과 분류를 통해 자동화 시스템이 캡차를 푸는 효율성을 평가했어.

주요 결과는 우리가 100%의 캡차를 풀 수 있다는 거야. 이전 연구에서는 68-71%만 풀었거든. 게다가, 우리의 발견은 인간과 봇이 reCAPTCHAv2의 캡차를 통과하기 위해 풀어야 하는 도전 과제의 수에 큰 차이가 없다는 걸 보여줘. 이건 현재의 AI 기술이 고급 이미지 기반 캡차를 이용할 수 있다는 걸 의미해.

또한, reCAPTCHAv2의 내부 구조를 살펴봤는데, 사용자에게 인간인지 아닌지를 평가할 때 쿠키와 브라우저 기록 데이터에 많이 의존한다는 증거를 찾았어. 이 논문과 함께 코드도 제공돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.08840.pdf

Title: Direct-CP: Directed Collaborative Perception for Connected and Autonomous Vehicles via Proactive Attention

Original Abstract:
Collaborative perception (CP) leverages visual data from connected and autonomous vehicles (CAV) to enhance an ego vehicle's field of view (FoV). Despite recent progress, current CP methods expand the ego vehicle's 360-degree perceptual range almost equally, which faces two key challenges. Firstly, in areas with uneven traffic distribution, focusing on directions with little traffic offers limited benefits. Secondly, under limited communication budgets, allocating excessive bandwidth to less critical directions lowers the perception accuracy in more vital areas. To address these issues, we propose Direct-CP, a proactive and direction-aware CP system aiming at improving CP in specific directions. Our key idea is to enable an ego vehicle to proactively signal its interested directions and readjust its attention to enhance local directional CP performance. To achieve this, we first propose an RSU-aided direction masking mechanism that assists an ego vehicle in identifying vital directions. Additionally, we design a direction-aware selective attention module to wisely aggregate pertinent features based on ego vehicle's directional priorities, communication budget, and the positional data of CAVs. Moreover, we introduce a direction-weighted detection loss (DWLoss) to capture the divergence between directional CP outcomes and the ground truth, facilitating effective model training. Extensive experiments on the V2X-Sim 2.0 dataset demonstrate that our approach achieves 19.8\% higher local perception accuracy in interested directions and 2.5\% higher overall perception accuracy than the state-of-the-art methods in collaborative 3D object detection tasks.

Translated Abstract:
협력 인식(CP)은 연결된 자율주행차(CAV)에서 시각 데이터를 활용해 자차의 시야(FoV)를 넓히는 기술이에요. 최근에 발전이 있었지만, 현재의 CP 방법은 자차의 360도 인식 범위를 거의 동일하게 확장하기 때문에 두 가지 주요 문제에 직면해 있어요.

첫째, 교통이 고르지 않은 지역에서는 교통이 적은 방향에 집중해도 큰 도움이 안 돼요. 둘째, 통신 예산이 제한된 상황에서 덜 중요한 방향에 너무 많은 대역폭을 할당하면, 더 중요한 영역의 인식 정확도가 떨어져요. 이런 문제를 해결하기 위해, 우리는 Direct-CP라는 시스템을 제안해요. 이 시스템은 특정 방향에서 CP를 개선하는 것을 목표로 해요.

우리의 주요 아이디어는 자차가 관심 있는 방향을 미리 신호를 보내고, 주의를 조정해 지역 방향 CP 성능을 높이는 거예요. 이를 위해 먼저, 자차가 중요한 방향을 식별하는 데 도움을 주는 RSU 지원 방향 마스킹 메커니즘을 제안해요. 그리고 자차의 방향 우선순위, 통신 예산, CAV의 위치 데이터를 바탕으로 관련된 특징들을 현명하게 모으는 방향 인식 선택적 주의 모듈을 설계해요.

또한, 방향 가중치 검출 손실(DWLoss)을 도입해 방향별 CP 결과와 실제 정답 간의 차이를 포착해 모델 훈련을 효과적으로 돕고 있어요. V2X-Sim 2.0 데이터셋에 대한 광범위한 실험 결과, 우리의 접근 방식이 관심 있는 방향에서 19.8% 더 높은 지역 인식 정확도와, 협력 3D 객체 탐지 작업에서 기존 최첨단 방법보다 2.5% 더 높은 전체 인식 정확도를 달성했어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.08847.pdf

Title: Kinect Calibration and Data Optimization For Anthropometric Parameters

Original Abstract:
Recently, through development of several 3d vision systems, widely used in various applications, medical and biometric fields. Microsoft kinect sensor have been most of used camera among 3d vision systems. Microsoft kinect sensor can obtain depth images of a scene and 3d coordinates of human joints. Thus, anthropometric features can extractable easily. Anthropometric feature and 3d joint coordinate raw datas which captured from kinect sensor is unstable. The strongest reason for this, datas vary by distance between joints of individual and location of kinect sensor. Consequently, usage of this datas without kinect calibration and data optimization does not result in sufficient and healthy. In this study, proposed a novel method to calibrating kinect sensor and optimizing skeleton features. Results indicate that the proposed method is quite effective and worthy of further study in more general scenarios.

Translated Abstract:
최근에 여러 3D 비전 시스템이 개발되면서 다양한 분야에서 많이 사용되고 있어. 의료나 생체 인식 분야에서도 그렇고, 그중에서 마이크로소프트 키넥트 센서가 3D 비전 시스템 중에서 가장 많이 쓰이는 카메라야. 

키넥트 센서는 장면의 깊이 이미지와 사람 관절의 3D 좌표를 얻을 수 있어서, 인체 측정 특성을 쉽게 추출할 수 있어. 하지만 키넥트 센서로 얻은 인체 측정 특성과 3D 관절 좌표 데이터는 불안정해. 그 이유는 개인마다 관절 간의 거리나 키넥트 센서의 위치에 따라 데이터가 다르게 나오기 때문이야. 그래서 키넥트를 보정하고 데이터를 최적화하지 않으면 이 데이터를 사용하는 게 충분하거나 건강하지 않아.

이 연구에서는 키넥트 센서를 보정하고 스켈레톤 특성을 최적화하는 새로운 방법을 제안했어. 결과적으로 이 방법이 꽤 효과적이고, 더 일반적인 상황에서도 연구할 가치가 있다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08849.pdf

Title: DeCLIP: Decoding CLIP representations for deepfake localization

Original Abstract:
Generative models can create entirely new images, but they can also partially modify real images in ways that are undetectable to the human eye. In this paper, we address the challenge of automatically detecting such local manipulations. One of the most pressing problems in deepfake detection remains the ability of models to generalize to different classes of generators. In the case of fully manipulated images, representations extracted from large self-supervised models (such as CLIP) provide a promising direction towards more robust detectors. Here, we introduce DeCLIP, a first attempt to leverage such large pretrained features for detecting local manipulations. We show that, when combined with a reasonably large convolutional decoder, pretrained self-supervised representations are able to perform localization and improve generalization capabilities over existing methods. Unlike previous work, our approach is able to perform localization on the challenging case of latent diffusion models, where the entire image is affected by the fingerprint of the generator. Moreover, we observe that this type of data, which combines local semantic information with a global fingerprint, provides more stable generalization than other categories of generative methods.

Translated Abstract:
생성 모델은 완전히 새로운 이미지를 만들 수 있을 뿐만 아니라, 실제 이미지를 사람의 눈으로는 감지할 수 없는 방식으로 부분적으로 수정할 수도 있어. 이 논문에서는 이런 지역적인 조작을 자동으로 감지하는 문제를 다뤘어. 딥페이크 탐지에서 가장 시급한 문제 중 하나는 모델이 다양한 생성기 클래스에 일반화할 수 있는 능력이야.

완전히 조작된 이미지의 경우, 대규모 자기 지도 학습 모델(예: CLIP)에서 추출한 표현이 더 강력한 탐지기로 나아가는 유망한 방향을 제공해. 여기서 우리는 DeCLIP을 소개하는데, 이건 그런 큰 사전 학습된 특징들을 활용해서 지역적인 조작을 감지하려는 첫 번째 시도야.

우리는 적당히 큰 합성곱 디코더와 결합했을 때, 사전 학습된 자기 지도 표현이 위치를 파악하고 기존 방법들보다 일반화 능력을 향상시킬 수 있다는 걸 보여줬어. 이전 연구들과는 달리, 우리의 접근 방식은 생성기의 지문으로 인해 전체 이미지가 영향을 받는 어려운 경우인 잠재 확산 모델에서도 위치를 파악할 수 있어. 게다가, 지역적인 의미 정보를 글로벌 지문과 결합한 이런 타입의 데이터가 다른 생성 방법들보다 더 안정적인 일반화를 제공한다는 걸 관찰했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08857.pdf

Title: InstantDrag: Improving Interactivity in Drag-based Image Editing

Original Abstract:
Drag-based image editing has recently gained popularity for its interactivity and precision. However, despite the ability of text-to-image models to generate samples within a second, drag editing still lags behind due to the challenge of accurately reflecting user interaction while maintaining image content. Some existing approaches rely on computationally intensive per-image optimization or intricate guidance-based methods, requiring additional inputs such as masks for movable regions and text prompts, thereby compromising the interactivity of the editing process. We introduce InstantDrag, an optimization-free pipeline that enhances interactivity and speed, requiring only an image and a drag instruction as input. InstantDrag consists of two carefully designed networks: a drag-conditioned optical flow generator (FlowGen) and an optical flow-conditioned diffusion model (FlowDiffusion). InstantDrag learns motion dynamics for drag-based image editing in real-world video datasets by decomposing the task into motion generation and motion-conditioned image generation. We demonstrate InstantDrag's capability to perform fast, photo-realistic edits without masks or text prompts through experiments on facial video datasets and general scenes. These results highlight the efficiency of our approach in handling drag-based image editing, making it a promising solution for interactive, real-time applications.

Translated Abstract:
드래그 기반 이미지 편집이 요즘 대세인데, 그 이유는 상호작용성과 정밀함 때문이야. 하지만 텍스트-투-이미지 모델이 1초 안에 샘플을 생성할 수 있는 것에 비해, 드래그 편집은 사용자 상호작용을 정확하게 반영하면서 이미지 내용을 유지하는 게 쉽지 않아서 뒤처져 있어. 

기존의 몇몇 방법들은 이미지마다 계산이 많이 필요한 최적화나 복잡한 가이드를 사용하는데, 이 과정에서 이동 가능한 영역의 마스크나 텍스트 프롬프트 같은 추가 입력이 필요해서 상호작용성이 떨어져. 

우리는 InstantDrag라는 최적화가 필요 없는 파이프라인을 소개해. 이건 상호작용성과 속도를 높여주고, 입력으로는 단지 이미지와 드래그 지시만 있으면 돼. InstantDrag는 드래그 조건의 광학 흐름 생성기(FlowGen)와 광학 흐름에 조건화된 확산 모델(FlowDiffusion)이라는 두 개의 잘 설계된 네트워크로 이루어져 있어. InstantDrag는 실제 비디오 데이터셋에서 드래그 기반 이미지 편집을 위해 모션 동역학을 배우고, 이 작업을 모션 생성과 모션 조건화 이미지 생성으로 나눠서 처리해. 

우리는 InstantDrag가 마스크나 텍스트 프롬프트 없이도 빠르고 사실적인 편집을 할 수 있다는 걸 얼굴 비디오 데이터셋과 일반 장면에 대한 실험을 통해 보여줬어. 이런 결과는 드래그 기반 이미지 편집을 다루는 데 있어서 우리의 접근 방식이 얼마나 효율적인지를 잘 보여주고, 실시간 상호작용 애플리케이션에 대한 유망한 솔루션이 될 수 있음을 나타내.

================================================================================

URL:
https://arxiv.org/pdf/2409.08884.pdf

Title: Detect Fake with Fake: Leveraging Synthetic Data-driven Representation for Synthetic Image Detection

Original Abstract:
Are general-purpose visual representations acquired solely from synthetic data useful for detecting fake images? In this work, we show the effectiveness of synthetic data-driven representations for synthetic image detection. Upon analysis, we find that vision transformers trained by the latest visual representation learners with synthetic data can effectively distinguish fake from real images without seeing any real images during pre-training. Notably, using SynCLR as the backbone in a state-of-the-art detection method demonstrates a performance improvement of +10.32 mAP and +4.73% accuracy over the widely used CLIP, when tested on previously unseen GAN models. Code is available at this https URL.

Translated Abstract:
합성 데이터만으로 얻은 일반적인 시각적 표현이 가짜 이미지를 탐지하는 데 유용할까요? 이 연구에서는 합성 데이터 기반 표현이 합성 이미지 탐지에 효과적임을 보여줍니다. 분석해보니, 최신 시각적 표현 학습기로 합성 데이터로 훈련된 비전 트랜스포머가 실제 이미지를 전혀 보지 않고도 가짜 이미지와 진짜 이미지를 효과적으로 구별할 수 있더라고요.

특히 SynCLR을 최첨단 탐지 방법의 백본으로 사용할 경우, 널리 사용되는 CLIP보다 +10.32 mAP와 +4.73% 정확도 향상을 보여줍니다. 이 성능은 이전에 본 적 없는 GAN 모델에서 테스트했을 때 나타났어요. 코드도 이 링크에서 확인할 수 있습니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.08885.pdf

Title: Interactive Masked Image Modeling for Multimodal Object Detection in Remote Sensing

Original Abstract:
Object detection in remote sensing imagery plays a vital role in various Earth observation applications. However, unlike object detection in natural scene images, this task is particularly challenging due to the abundance of small, often barely visible objects across diverse terrains. To address these challenges, multimodal learning can be used to integrate features from different data modalities, thereby improving detection accuracy. Nonetheless, the performance of multimodal learning is often constrained by the limited size of labeled datasets. In this paper, we propose to use Masked Image Modeling (MIM) as a pre-training technique, leveraging self-supervised learning on unlabeled data to enhance detection performance. However, conventional MIM such as MAE which uses masked tokens without any contextual information, struggles to capture the fine-grained details due to a lack of interactions with other parts of image. To address this, we propose a new interactive MIM method that can establish interactions between different tokens, which is particularly beneficial for object detection in remote sensing. The extensive ablation studies and evluation demonstrate the effectiveness of our approach.

Translated Abstract:
원격 탐사 이미지에서 물체 탐지는 다양한 지구 관측 응용 프로그램에서 매우 중요해. 하지만 자연 장면 이미지에서의 물체 탐지와는 달리, 이 작업은 다양한 지형에 작고 잘 보이지 않는 물체들이 많아서 특히 어려워. 이런 문제를 해결하기 위해서, 다양한 데이터 모달리티에서 특징을 통합하는 다중 모달 학습을 사용할 수 있어. 하지만 다중 모달 학습의 성능은 라벨이 붙은 데이터셋의 크기가 제한적이라서 종종 제약을 받아.

이 논문에서는 물체 탐지 성능을 높이기 위해 마스크 이미지 모델링(MIM)을 사전 훈련 기법으로 사용하자고 제안해. 이 방법은 라벨이 없는 데이터에서 자기 지도 학습을 활용해. 하지만 전통적인 MIM인 MAE는 맥락 정보 없이 마스크된 토큰만 사용하기 때문에 이미지의 세밀한 디테일을 잡아내는 데 어려움을 겪어. 그래서 우리는 서로 다른 토큰 간의 상호작용을 설정할 수 있는 새로운 인터랙티브 MIM 방법을 제안해. 이 방법은 원격 탐사에서 물체 탐지에 특히 유용해. 광범위한 제거 연구와 평가 결과는 우리의 접근 방식이 효과적임을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.08887.pdf

Title: Visual Language Tracking with Multi-modal Interaction: A Robust Benchmark

Original Abstract:
Visual Language Tracking (VLT) enhances tracking by mitigating the limitations of relying solely on the visual modality, utilizing high-level semantic information through language. This integration of the language enables more advanced human-machine interaction. The essence of interaction is cognitive alignment, which typically requires multiple information exchanges, especially in the sequential decision-making process of VLT. However, current VLT benchmarks do not account for multi-round interactions during tracking. They provide only an initial text and bounding box (bbox) in the first frame, with no further interaction as tracking progresses, deviating from the original motivation of the VLT task. To address these limitations, we propose a novel and robust benchmark, VLT-MI (Visual Language Tracking with Multi-modal Interaction), which introduces multi-round interaction into the VLT task for the first time. (1) We generate diverse, multi-granularity texts for multi-round, multi-modal interaction based on existing mainstream VLT benchmarks using DTLLM-VLT, leveraging the world knowledge of LLMs. (2) We propose a new VLT interaction paradigm that achieves multi-round interaction through text updates and object recovery. When multiple tracking failures occur, we provide the tracker with more aligned texts and corrected bboxes through interaction, thereby expanding the scope of VLT downstream tasks. (3) We conduct comparative experiments on both traditional VLT benchmarks and VLT-MI, evaluating and analyzing the accuracy and robustness of trackers under the interactive paradigm. This work offers new insights and paradigms for the VLT task, enabling a fine-grained evaluation of multi-modal trackers. We believe this approach can be extended to additional datasets in the future, supporting broader evaluations and comparisons of video-language model capabilities.

Translated Abstract:
비주얼 언어 추적(Visual Language Tracking, VLT)은 시각적 모드에만 의존하는 한계를 극복하고, 언어를 통해 고급 의미 정보를 활용하여 추적을 향상시킵니다. 이렇게 언어를 통합하면 인간과 기계 간의 상호작용이 더 발전하게 됩니다. 상호작용의 본질은 인지적 정렬인데, 이 과정은 여러 정보 교환을 필요로 하며, 특히 VLT의 순차적 의사결정 과정에서 그렇습니다. 하지만 현재의 VLT 벤치마크는 추적 중에 여러 번의 상호작용을 고려하지 않고 있습니다. 처음 프레임에서 초기 텍스트와 바운딩 박스(bbox)만 제공하고, 추적이 진행되면서는 더 이상의 상호작용이 없어서 VLT 작업의 원래 동기와는 다르게 진행됩니다.

이러한 한계를 해결하기 위해, 우리는 VLT 작업에 처음으로 다중 라운드 상호작용을 도입하는 새로운 벤치마크인 VLT-MI(Visual Language Tracking with Multi-modal Interaction)를 제안합니다. (1) 우리는 기존의 주요 VLT 벤치마크를 기반으로 DTLLM-VLT를 사용하여 다채롭고 다양한 텍스트를 생성하고, 이를 통해 다중 라운드, 다중 모달 상호작용을 가능하게 합니다. 이 과정에서 LLM의 세계 지식을 활용합니다. (2) 우리는 텍스트 업데이트와 객체 회복을 통해 다중 라운드 상호작용을 달성하는 새로운 VLT 상호작용 패러다임을 제안합니다. 여러 추적 실패가 발생할 때, 상호작용을 통해 더 정렬된 텍스트와 수정된 bbox를 제공하여 VLT의 하위 작업 범위를 확장합니다. (3) 우리는 전통적인 VLT 벤치마크와 VLT-MI에서 비교 실험을 수행하여 상호작용 패러다임 하에서 추적기의 정확성과 강인성을 평가하고 분석합니다.

이 연구는 VLT 작업에 대한 새로운 통찰력과 패러다임을 제공하여 다중 모달 추적기를 세밀하게 평가할 수 있게 합니다. 우리는 이 접근 방식이 향후 추가 데이터 세트로 확장될 수 있다고 믿으며, 비디오-언어 모델의 능력을 더 넓게 평가하고 비교하는 데 기여할 수 있습니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.08943.pdf

Title: Pushing Joint Image Denoising and Classification to the Edge

Original Abstract:
In this paper, we jointly combine image classification and image denoising, aiming to enhance human perception of noisy images captured by edge devices, like low-light security cameras. In such settings, it is important to retain the ability of humans to verify the automatic classification decision and thus jointly denoise the image to enhance human perception. Since edge devices have little computational power, we explicitly optimize for efficiency by proposing a novel architecture that integrates the two tasks. Additionally, we alter a Neural Architecture Search (NAS) method, which searches for classifiers to search for the integrated model while optimizing for a target latency, classification accuracy, and denoising performance. The NAS architectures outperform our manually designed alternatives in both denoising and classification, offering a significant improvement to human perception. Our approach empowers users to construct architectures tailored to domains like medical imaging, surveillance systems, and industrial inspections.

Translated Abstract:
이 논문에서는 이미지 분류와 이미지 노이즈 제거를 함께 다루고 있어. 목표는 저조도 보안 카메라 같은 엣지 장치로 찍힌 노이즈가 많은 이미지를 사람이 더 잘 인식할 수 있도록 하는 거야. 이런 상황에서는 자동으로 분류한 결과를 사람이 잘 검증할 수 있어야 하고, 그래서 이미지를 동시에 노이즈 제거해서 인식을 개선하는 게 중요해.

엣지 장치는 처리 능력이 낮기 때문에, 두 가지 작업을 통합하는 새로운 구조를 제안하면서 효율성을 높이도록 명시적으로 최적화했어. 그리고 신경망 구조 검색(NAS) 방법도 바꿨고, 이 방법으로 분류기를 찾으면서 목표 지연 시간, 분류 정확도, 노이즈 제거 성능을 최적화하고 있어.

NAS로 설계한 구조는 우리가 수작업으로 만든 것보다 노이즈 제거와 분류 모두에서 더 나은 성능을 보여줘. 이는 인간의 인식을 크게 향상시키는 결과를 가져와. 우리의 접근 방식은 사용자들이 의료 영상, 감시 시스템, 산업 검사 같은 분야에 맞춰 구조를 만들 수 있도록 해줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.08947.pdf

Title: A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis

Original Abstract:
Relighting radiance fields is severely underconstrained for multi-view data, which is most often captured under a single illumination condition; It is especially hard for full scenes containing multiple objects. We introduce a method to create relightable radiance fields using such single-illumination data by exploiting priors extracted from 2D image diffusion models. We first fine-tune a 2D diffusion model on a multi-illumination dataset conditioned by light direction, allowing us to augment a single-illumination capture into a realistic -- but possibly inconsistent -- multi-illumination dataset from directly defined light directions. We use this augmented data to create a relightable radiance field represented by 3D Gaussian splats. To allow direct control of light direction for low-frequency lighting, we represent appearance with a multi-layer perceptron parameterized on light direction. To enforce multi-view consistency and overcome inaccuracies we optimize a per-image auxiliary feature vector. We show results on synthetic and real multi-view data under single illumination, demonstrating that our method successfully exploits 2D diffusion model priors to allow realistic 3D relighting for complete scenes. Project site this https URL

Translated Abstract:
다중 시점 데이터에서 조명 재조정은 매우 제약이 많아. 대부분의 데이터는 단일 조명 조건에서 촬영되거든. 여러 개의 객체가 있는 전체 장면일수록 더 어려워. 우리는 2D 이미지 확산 모델에서 추출한 선행 정보를 활용해 단일 조명 데이터로 재조정 가능한 복사광장(field)을 만드는 방법을 소개해.

먼저, 우리는 다양한 조명 조건에 맞춰 조명 방향으로 조건화된 다중 조명 데이터셋에서 2D 확산 모델을 미세 조정해. 이렇게 하면 단일 조명 촬영을 현실적이지만 다소 일관성이 없을 수 있는 다중 조명 데이터셋으로 확장할 수 있어. 이 증강된 데이터를 사용해 3D 가우시안 스플랫으로 표현된 재조정 가능한 복사광장을 만들어.

저주파 조명에 대한 조명 방향을 직접 조절할 수 있게 하기 위해, 우리는 조명 방향에 따라 매개변수가 설정된 다중 레이어 퍼셉트론으로 외관을 표현해. 다중 시점 일관성을 유지하고 부정확성을 극복하기 위해, 우리는 각 이미지에 대한 보조 특성 벡터를 최적화해. 우리는 단일 조명 하의 합성 및 실제 다중 시점 데이터에서 결과를 보여주며, 우리의 방법이 2D 확산 모델의 선행 정보를 잘 활용해 전체 장면에 대해 현실적인 3D 조명 재조정을 가능하게 한다는 것을 입증해. 프로젝트 사이트는 이 URL이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08953.pdf

Title: Pushing the boundaries of event subsampling in event-based video classification using CNNs

Original Abstract:
Event cameras offer low-power visual sensing capabilities ideal for edge-device applications. However, their high event rate, driven by high temporal details, can be restrictive in terms of bandwidth and computational resources. In edge AI applications, determining the minimum amount of events for specific tasks can allow reducing the event rate to improve bandwidth, memory, and processing efficiency. In this paper, we study the effect of event subsampling on the accuracy of event data classification using convolutional neural network (CNN) models. Surprisingly, across various datasets, the number of events per video can be reduced by an order of magnitude with little drop in accuracy, revealing the extent to which we can push the boundaries in accuracy vs. event rate trade-off. Additionally, we also find that lower classification accuracy in high subsampling rates is not solely attributable to information loss due to the subsampling of the events, but that the training of CNNs can be challenging in highly subsampled scenarios, where the sensitivity to hyperparameters increases. We quantify training instability across multiple event-based classification datasets using a novel metric for evaluating the hyperparameter sensitivity of CNNs in different subsampling settings. Finally, we analyze the weight gradients of the network to gain insight into this instability.

Translated Abstract:
이벤트 카메라는 저전력 비주얼 센싱 기능을 제공해서 엣지 디바이스에 적합해. 하지만, 높은 시간적 세부 사항 때문에 이벤트 발생률이 높아서 대역폭과 계산 자원에 제한이 될 수 있어. 엣지 AI 애플리케이션에서는 특정 작업에 필요한 최소 이벤트 수를 결정하면, 이벤트 발생률을 줄여서 대역폭, 메모리, 처리 효율성을 높일 수 있어.

이 논문에서는 CNN 모델을 사용해서 이벤트 데이터 분류의 정확도에 대한 이벤트 서브샘플링의 영향을 연구했어. 여러 데이터셋에서 놀랍게도, 비디오당 이벤트 수를 10배 줄여도 정확도가 크게 떨어지지 않는다는 결과를 발견했어. 이건 정확도와 이벤트 발생률 간의 균형을 얼마나 조정할 수 있는지를 보여줘.

또한, 높은 서브샘플링 비율에서 분류 정확도가 낮은 이유는 단순히 이벤트 서브샘플링으로 인한 정보 손실 때문만이 아니라, CNN을 훈련하는 게 서브샘플링이 많이 된 경우에 더 어려워진다는 것도 알아냈어. 이럴 때 하이퍼파라미터에 대한 민감도가 증가해. 우리는 여러 이벤트 기반 분류 데이터셋에서 훈련의 불안정성을 정량화하고, 다양한 서브샘플링 설정에서 CNN의 하이퍼파라미터 민감도를 평가하기 위해 새로운 지표를 사용했어.

마지막으로, 이 불안정성에 대한 통찰을 얻기 위해 네트워크의 가중치 기울기도 분석했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09018.pdf

Title: An Efficient and Streaming Audio Visual Active Speaker Detection System

Original Abstract:
This paper delves into the challenging task of Active Speaker Detection (ASD), where the system needs to determine in real-time whether a person is speaking or not in a series of video frames. While previous works have made significant strides in improving network architectures and learning effective representations for ASD, a critical gap exists in the exploration of real-time system deployment. Existing models often suffer from high latency and memory usage, rendering them impractical for immediate applications. To bridge this gap, we present two scenarios that address the key challenges posed by real-time constraints. First, we introduce a method to limit the number of future context frames utilized by the ASD model. By doing so, we alleviate the need for processing the entire sequence of future frames before a decision is made, significantly reducing latency. Second, we propose a more stringent constraint that limits the total number of past frames the model can access during inference. This tackles the persistent memory issues associated with running streaming ASD systems. Beyond these theoretical frameworks, we conduct extensive experiments to validate our approach. Our results demonstrate that constrained transformer models can achieve performance comparable to or even better than state-of-the-art recurrent models, such as uni-directional GRUs, with a significantly reduced number of context frames. Moreover, we shed light on the temporal memory requirements of ASD systems, revealing that larger past context has a more profound impact on accuracy than future context. When profiling on a CPU we find that our efficient architecture is memory bound by the amount of past context it can use and that the compute cost is negligible as compared to the memory cost.

Translated Abstract:
이 논문은 능동 화자 탐지(Active Speaker Detection, ASD)의 어려운 과제를 다루고 있어. 이 시스템은 영상 프레임을 실시간으로 분석해서 누군가 말을 하고 있는지 아닌지를 판단해야 해. 이전 연구들은 네트워크 구조를 개선하고 ASD를 위한 효과적인 표현을 배우는 데 큰 발전을 이뤘지만, 실시간 시스템 배치에 대한 탐구는 부족해. 기존 모델들은 대개 높은 지연 시간과 메모리 사용으로 인해 즉각적인 응용에 실용적이지 않아.

이 문제를 해결하기 위해 우리는 실시간 제약에 대응하는 두 가지 시나리오를 제안해. 첫째, ASD 모델이 사용하는 미래 컨텍스트 프레임의 수를 제한하는 방법을 소개해. 이렇게 하면 결정을 내리기 전에 전체 미래 프레임을 처리할 필요가 없어져서 지연 시간을 크게 줄일 수 있어. 둘째, 추론 중 모델이 접근할 수 있는 과거 프레임의 총 수를 제한하는 더 엄격한 제약을 제안해. 이건 스트리밍 ASD 시스템에서 발생하는 메모리 문제를 해결하는 데 도움을 줘.

이론적인 틀을 넘어서, 우리는 우리의 접근 방식을 검증하기 위해 광범위한 실험을 진행했어. 그 결과, 제약이 있는 변환기 모델이 최신 순환 모델인 일방향 GRU와 비슷하거나 더 나은 성능을 내면서도 훨씬 적은 수의 컨텍스트 프레임을 사용한다는 것을 보여줬어. 더욱이, ASD 시스템의 시간적 메모리 요구 사항에 대해 밝혀냈는데, 과거 컨텍스트가 미래 컨텍스트보다 정확도에 더 큰 영향을 미친다는 사실이야. CPU에서 프로파일링을 진행해본 결과, 우리 효율적인 아키텍처는 사용할 수 있는 과거 컨텍스트의 양에 의해 메모리 제약을 받으며, 계산 비용은 메모리 비용에 비해 미미하다는 것을 발견했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08301.pdf

Title: Gaussian Differentially Private Human Faces Under a Face Radial Curve Representation

Original Abstract:
In this paper we consider the problem of releasing a Gaussian Differentially Private (GDP) 3D human face. The human face is a complex structure with many features and inherently tied to one's identity. Protecting this data, in a formally private way, is important yet challenging given the dimensionality of the problem. We extend approximate DP techniques for functional data to the GDP framework. We further propose a novel representation, face radial curves, of a 3D face as a set of functions and then utilize our proposed GDP functional data mechanism. To preserve the shape of the face while injecting noise we rely on tools from shape analysis for our novel representation of the face. We show that our method preserves the shape of the average face and injects less noise than traditional methods for the same privacy budget. Our mechanism consists of two primary components, the first is generally applicable to function value summaries (as are commonly found in nonparametric statistics or functional data analysis) while the second is general to disk-like surfaces and hence more applicable than just to human faces.

Translated Abstract:
이 논문에서는 가우시안 차등 프라이버시(GDP)를 적용한 3D 인간 얼굴 데이터를 보호하는 문제를 다룹니다. 인간 얼굴은 많은 특징을 가진 복잡한 구조로, 개인의 정체성과도 밀접하게 연결되어 있어요. 이런 데이터를 공식적으로 보호하는 것은 매우 중요하지만, 문제의 차원 때문에 도전적입니다.

우리는 기능 데이터에 대한 근사 DP 기술을 GDP 프레임워크로 확장했습니다. 그리고 3D 얼굴을 함수의 집합으로 나타내는 새로운 방법인 얼굴 방사 곡선을 제안하고, 이 방법을 통해 우리의 GDP 기능 데이터 메커니즘을 활용합니다. 얼굴의 모양을 유지하면서 노이즈를 추가하기 위해, 우리는 얼굴의 새로운 표현을 위해 형태 분석 도구를 사용합니다.

우리의 방법은 평균 얼굴의 모양을 잘 유지하면서, 같은 프라이버시 예산 하에 전통적인 방법보다 더 적은 노이즈를 추가하는 것을 보여줍니다. 우리의 메커니즘은 두 가지 주요 요소로 구성되어 있습니다. 첫 번째는 일반적으로 함수 값 요약에 적용 가능하고, 두 번째는 디스크 모양의 표면에 일반적으로 적용 가능하므로 단순히 인간 얼굴에만 국한되지 않습니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.08307.pdf

Title: MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation

Original Abstract:
Widely used traditional pipelines for subcortical brain segmentation are often inefficient and slow, particularly when processing large datasets. Furthermore, deep learning models face challenges due to the high resolution of MRI images and the large number of anatomical classes involved. To address these limitations, we developed a 3D patch-based hybrid CNN-Mamba model that leverages Mamba's selective scan algorithm, thereby enhancing segmentation accuracy and efficiency for 3D inputs. This retrospective study utilized 1784 T1-weighted MRI scans from a diverse, multi-site dataset of healthy individuals. The dataset was divided into training, validation, and testing sets with a 1076/345/363 split. The scans were obtained from 1.5T and 3T MRI machines. Our model's performance was validated against several benchmarks, including other CNN-Mamba, CNN-Transformer, and pure CNN networks, using FreeSurfer-generated ground truths. We employed the Dice Similarity Coefficient (DSC), Volume Similarity (VS), and Average Symmetric Surface Distance (ASSD) as evaluation metrics. Statistical significance was determined using the Wilcoxon signed-rank test with a threshold of P < 0.05. The proposed model achieved the highest overall performance across all metrics (DSC 0.88383; VS 0.97076; ASSD 0.33604), significantly outperforming all non-Mamba-based models (P < 0.001). While the model did not show significant improvement in DSC or VS compared to another Mamba-based model (P-values of 0.114 and 0.425), it demonstrated a significant enhancement in ASSD (P < 0.001) with approximately 20% fewer parameters. In conclusion, our proposed hybrid CNN-Mamba architecture offers an efficient and accurate approach for 3D subcortical brain segmentation, demonstrating potential advantages over existing methods.

Translated Abstract:
전통적인 서브코르티컬 뇌 분할 방법은 대규모 데이터셋을 처리할 때 비효율적이고 느린 경우가 많아. 게다가 딥러닝 모델은 MRI 이미지의 높은 해상도와 많은 해부학적 클래스 때문에 어려움을 겪고 있어. 이런 한계를 극복하기 위해 우리는 3D 패치 기반 하이브리드 CNN-Mamba 모델을 개발했어. 이 모델은 Mamba의 선택적 스캔 알고리즘을 활용해서 3D 입력의 분할 정확도와 효율성을 높였어.

이번 연구는 건강한 개인들로부터 수집된 다양한 다기관 데이터셋의 1784개의 T1 가중 MRI 스캔을 사용했어. 데이터셋은 훈련, 검증, 테스트 세트로 나누었고, 비율은 1076/345/363이었어. 스캔은 1.5T와 3T MRI 기계에서 얻어졌어. 우리의 모델 성능은 다른 CNN-Mamba, CNN-Transformer, 순수 CNN 네트워크와 비교했고, FreeSurfer에서 생성된 기준 데이터와 비교했어. 평가 지표로는 Dice 유사도 계수(DSC), 볼륨 유사도(VS), 평균 대칭 표면 거리(ASSD)를 사용했어. 통계적 유의성은 Wilcoxon 부호 순위 검정을 통해 P < 0.05 기준으로 평가했어.

제안한 모델은 모든 지표에서 가장 높은 성능을 기록했어 (DSC 0.88383; VS 0.97076; ASSD 0.33604) 그리고 모든 비-Mamba 기반 모델보다 유의미하게 뛰어났어 (P < 0.001). 하지만 다른 Mamba 기반 모델과 비교했을 때 DSC나 VS에서 큰 개선은 없었어 (P값이 각각 0.114와 0.425였어). 대신 ASSD에서 유의미한 향상을 보여주었고 (P < 0.001), 약 20% 적은 파라미터로도 가능했어. 

결론적으로, 우리가 제안한 하이브리드 CNN-Mamba 구조는 3D 서브코르티컬 뇌 분할을 위한 효율적이고 정확한 접근법을 제공하며 기존 방법들보다 잠재적인 장점을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.08331.pdf

Title: Digital Volumetric Biopsy Cores Improve Gleason Grading of Prostate Cancer Using Deep Learning

Original Abstract:
Prostate cancer (PCa) was the most frequently diagnosed cancer among American men in 2023. The histological grading of biopsies is essential for diagnosis, and various deep learning-based solutions have been developed to assist with this task. Existing deep learning frameworks are typically applied to individual 2D cross-sections sliced from 3D biopsy tissue specimens. This process impedes the analysis of complex tissue structures such as glands, which can vary depending on the tissue slice examined. We propose a novel digital pathology data source called a "volumetric core," obtained via the extraction and co-alignment of serially sectioned tissue sections using a novel morphology-preserving alignment framework. We trained an attention-based multiple-instance learning (ABMIL) framework on deep features extracted from volumetric patches to automatically classify the Gleason Grade Group (GGG). To handle volumetric patches, we used a modified video transformer with a deep feature extractor pretrained using self-supervised learning. We ran our morphology-preserving alignment framework to construct 10,210 volumetric cores, leaving out 30% for pretraining. The rest of the dataset was used to train ABMIL, which resulted in a 0.958 macro-average AUC, 0.671 F1 score, 0.661 precision, and 0.695 recall averaged across all five GGG significantly outperforming the 2D baselines.

Translated Abstract:
2023년, 전립선암(PCa)은 미국 남성들 사이에서 가장 많이 진단된 암이었어. 생검의 조직학적 등급 매기는 게 진단에 정말 중요하고, 이걸 도와주기 위해 여러 딥러닝 기반 솔루션들이 개발됐어. 기존의 딥러닝 프레임워크는 보통 3D 생검 조직 샘플에서 잘라낸 개별 2D 단면에 적용되는데, 이 과정이 복잡한 조직 구조, 예를 들어 샘 같은 것들의 분석을 방해해. 왜냐하면 조직 절편에 따라 다를 수 있으니까.

우리는 "부피 코어"라는 새로운 디지털 병리학 데이터 소스를 제안해. 이건 새로운 형태 보존 정렬 프레임워크를 사용해 연속적으로 절단한 조직 섹션을 추출하고 정렬해서 얻은 거야. 우리는 부피 패치에서 추출한 딥 특징을 이용해 자동으로 글리슨 등급 그룹(GGG)을 분류하는 주의 기반 다중 인스턴스 학습(ABMIL) 프레임워크를 훈련했어. 부피 패치를 다루기 위해, 우리는 자기 지도 학습을 통해 사전 훈련된 딥 특징 추출기를 사용한 수정된 비디오 변환기를 썼어.

형태 보존 정렬 프레임워크를 이용해 10,210개의 부피 코어를 만들었고, 그 중 30%는 사전 훈련용으로 남겨두었어. 나머지 데이터셋은 ABMIL 훈련에 사용됐고, 그 결과 0.958의 매크로 평균 AUC, 0.671의 F1 점수, 0.661의 정밀도, 0.695의 재현율을 기록했어. 이건 모든 다섯 개 GGG에서 2D 기준보다 훨씬 뛰어난 성과야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08351.pdf

Title: Bayesian Inverse Graphics for Few-Shot Concept Learning

Original Abstract:
Humans excel at building generalizations of new concepts from just one single example. Contrary to this, current computer vision models typically require large amount of training samples to achieve a comparable accuracy. In this work we present a Bayesian model of perception that learns using only minimal data, a prototypical probabilistic program of an object. Specifically, we propose a generative inverse graphics model of primitive shapes, to infer posterior distributions over physically consistent parameters from one or several images. We show how this representation can be used for downstream tasks such as few-shot classification and pose estimation. Our model outperforms existing few-shot neural-only classification algorithms and demonstrates generalization across varying lighting conditions, backgrounds, and out-of-distribution shapes. By design, our model is uncertainty-aware and uses our new differentiable renderer for optimizing global scene parameters through gradient descent, sampling posterior distributions over object parameters with Markov Chain Monte Carlo (MCMC), and using a neural based likelihood function.

Translated Abstract:
사람들은 단 한 번의 예시로 새로운 개념을 일반화하는 데 매우 뛰어나. 반면에 현재의 컴퓨터 비전 모델은 비슷한 정확도를 얻으려면 많은 훈련 샘플이 필요해. 

이번 연구에서는 최소한의 데이터만으로 학습하는 베이지안 인식 모델을 제안해. 이 모델은 물체의 전형적인 확률적 프로그램을 사용해. 특히, 우리는 기본 도형에 대한 생성적 역 그래픽 모델을 제안해서, 하나 또는 여러 개의 이미지에서 물리적으로 일관된 매개변수에 대한 후방 분포를 유추할 수 있어. 

이 표현 방식이 몇 장의 예시로 분류하거나 자세 추정 같은 후속 작업에 어떻게 사용될 수 있는지를 보여줄 거야. 우리 모델은 기존의 몇 장의 예시로만 학습하는 신경망 기반 분류 알고리즘보다 성능이 뛰어나고, 다양한 조명 조건, 배경, 그리고 분포 밖의 도형에서도 일반화할 수 있는 능력을 보여줘. 

우리 모델은 불확실성을 인식할 수 있도록 설계되었고, 새로운 미분 가능한 렌더러를 사용해서 그래디언트 하강법으로 전역 장면 매개변수를 최적화하고, 마르코프 체인 몬테 카를로(MCMC)를 통해 물체 매개변수에 대한 후방 분포를 샘플링해. 그리고 신경망 기반의 가능성 함수를 사용해.

================================================================================

URL:
https://arxiv.org/pdf/2409.08353.pdf

Title: Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos

Original Abstract:
Volumetric video represents a transformative advancement in visual media, enabling users to freely navigate immersive virtual experiences and narrowing the gap between digital and real worlds. However, the need for extensive manual intervention to stabilize mesh sequences and the generation of excessively large assets in existing workflows impedes broader adoption. In this paper, we present a novel Gaussian-based approach, dubbed \textit{DualGS}, for real-time and high-fidelity playback of complex human performance with excellent compression ratios. Our key idea in DualGS is to separately represent motion and appearance using the corresponding skin and joint Gaussians. Such an explicit disentanglement can significantly reduce motion redundancy and enhance temporal coherence. We begin by initializing the DualGS and anchoring skin Gaussians to joint Gaussians at the first frame. Subsequently, we employ a coarse-to-fine training strategy for frame-by-frame human performance modeling. It includes a coarse alignment phase for overall motion prediction as well as a fine-grained optimization for robust tracking and high-fidelity rendering. To integrate volumetric video seamlessly into VR environments, we efficiently compress motion using entropy encoding and appearance using codec compression coupled with a persistent codebook. Our approach achieves a compression ratio of up to 120 times, only requiring approximately 350KB of storage per frame. We demonstrate the efficacy of our representation through photo-realistic, free-view experiences on VR headsets, enabling users to immersively watch musicians in performance and feel the rhythm of the notes at the performers' fingertips.

Translated Abstract:
볼륨 비디오는 시각 미디어에서 큰 발전을 가져왔어요. 이걸 통해 사용자들은 몰입감 있는 가상 경험을 자유롭게 탐험할 수 있고, 디지털 세계와 현실 세계의 간극이 줄어들게 되죠. 하지만, 메쉬 시퀀스를 안정화하려면 많은 수동 작업이 필요하고, 기존 작업 흐름에서 너무 큰 자산이 생성되는 문제가 있어요. 이로 인해 더 넓은 사용이 어렵습니다.

이번 논문에서는 \textit{DualGS}라는 새로운 가우시안 기반 접근 방식을 제안합니다. 이 방법은 복잡한 인간의 동작을 실시간으로 고화질로 재생할 수 있게 해주고, 압축 비율도 훌륭해요. DualGS의 핵심 아이디어는 동작과 외형을 각각 해당하는 피부와 관절 가우시안을 사용해 따로 표현하는 거예요. 이렇게 명확하게 분리하면 동작의 중복을 줄이고 시간적 일관성을 높일 수 있어요.

우리는 DualGS를 초기화하고 첫 프레임에서 피부 가우시안을 관절 가우시안에 고정하는 것으로 시작해요. 그 다음, 프레임별 인간 성능 모델링을 위한 거칠게 맞추는 단계와 정밀한 최적화를 포함한 훈련 전략을 사용해요. 이 과정에서 전체적인 동작 예측을 위한 거친 정렬 단계와 강력한 추적 및 고화질 렌더링을 위한 세밀한 최적화가 이루어집니다.

볼륨 비디오를 VR 환경에 원활하게 통합하기 위해 우리는 엔트로피 인코딩을 사용해 동작을 효율적으로 압축하고, 지속적인 코드북과 함께 코덱 압축을 통해 외형을 압축해요. 우리의 접근 방식은 최대 120배의 압축 비율을 달성하며, 프레임당 약 350KB의 저장 공간만 필요해요. 우리는 VR 헤드셋에서 포토리얼리스틱한 자유 시청 경험을 통해 우리의 표현 방식이 효과적임을 보여줍니다. 이 덕분에 사용자들은 공연 중인 음악가들을 몰입감 있게 감상하고, 음악가의 손끝에서 느껴지는 음의 리듬을 체험할 수 있어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.08376.pdf

Title: Learned Compression for Images and Point Clouds

Original Abstract:
Over the last decade, deep learning has shown great success at performing computer vision tasks, including classification, super-resolution, and style transfer. Now, we apply it to data compression to help build the next generation of multimedia codecs. This thesis provides three primary contributions to this new field of learned compression. First, we present an efficient low-complexity entropy model that dynamically adapts the encoding distribution to a specific input by compressing and transmitting the encoding distribution itself as side information. Secondly, we propose a novel lightweight low-complexity point cloud codec that is highly specialized for classification, attaining significant reductions in bitrate compared to non-specialized codecs. Lastly, we explore how motion within the input domain between consecutive video frames is manifested in the corresponding convolutionally-derived latent space.

Translated Abstract:
지난 10년 동안, 딥러닝은 분류, 슈퍼 해상도, 스타일 전이 같은 컴퓨터 비전 작업에서 큰 성공을 거두었어. 이제 우리는 이 기술을 데이터 압축에 적용해서 차세대 멀티미디어 코덱을 만드는 데 도움을 주려고 해. 이 논문은 새로운 학습 압축 분야에 세 가지 주요 기여를 해.

첫째, 우리는 특정 입력에 맞게 인코딩 분포를 동적으로 조정할 수 있는 효율적이고 저복잡도의 엔트로피 모델을 제안해. 이 모델은 인코딩 분포 자체를 부가 정보로 압축하고 전송함으로써 작동해.

둘째, 우리는 분류에 특화된 새로운 경량 저복잡도 포인트 클라우드 코덱을 제안해. 이 코덱은 비특화된 코덱에 비해 비트 전송률을 크게 줄일 수 있어.

마지막으로, 우리는 연속 비디오 프레임 사이의 입력 도메인 내에서의 움직임이 어떻게 대응되는 컨볼루션 기반의 잠재 공간에서 나타나는지를 탐구해.

================================================================================

URL:
https://arxiv.org/pdf/2409.08474.pdf

Title: Rethinking Meta-Learning from a Learning Lens

Original Abstract:
Meta-learning has emerged as a powerful approach for leveraging knowledge from previous tasks to solve new tasks. The mainstream methods focus on training a well-generalized model initialization, which is then adapted to different tasks with limited data and updates. However, it pushes the model overfitting on the training tasks. Previous methods mainly attributed this to the lack of data and used augmentations to address this issue, but they were limited by sufficient training and effective augmentation strategies. In this work, we focus on the more fundamental ``learning to learn'' strategy of meta-learning to explore what causes errors and how to eliminate these errors without changing the environment. Specifically, we first rethink the algorithmic procedure of meta-learning from a ``learning'' lens. Through theoretical and empirical analyses, we find that (i) this paradigm faces the risk of both overfitting and underfitting and (ii) the model adapted to different tasks promote each other where the effect is stronger if the tasks are more similar. Based on this insight, we propose using task relations to calibrate the optimization process of meta-learning and propose a plug-and-play method called Task Relation Learner (TRLearner) to achieve this goal. Specifically, it first obtains task relation matrices from the extracted task-specific meta-data. Then, it uses the obtained matrices with relation-aware consistency regularization to guide optimization. Extensive theoretical and empirical analyses demonstrate the effectiveness of TRLearner.

Translated Abstract:
메타 학습은 이전 작업에서 얻은 지식을 활용해 새로운 작업을 해결하는 강력한 방법으로 떠올랐어. 대부분의 방법들은 잘 일반화된 모델 초기화를 훈련시키고, 그 후에 제한된 데이터와 업데이트로 다른 작업에 적응시키는 데 집중해. 하지만 이 과정에서 모델이 훈련 작업에 과적합되는 문제가 생겨. 이전 방법들은 주로 데이터 부족을 문제로 보고 데이터 증강을 통해 이 문제를 해결하려 했지만, 충분한 훈련이나 효과적인 증강 전략에 한계가 있었어.

이 연구에서는 메타 학습의 근본적인 "학습을 학습하는" 전략에 집중해서 오류의 원인이 무엇인지, 환경을 바꾸지 않고 이러한 오류를 어떻게 없앨 수 있는지를 탐구했어. 구체적으로, 메타 학습의 알고리즘 절차를 "학습"의 관점에서 다시 생각해봤어. 이론적 및 실증 분석을 통해 우리는 (i) 이 패러다임이 과적합과 과소적합의 위험에 직면해 있고, (ii) 서로 다른 작업에 적응된 모델들이 서로 영향을 주고, 작업이 더 비슷할수록 그 영향이 더 강하다는 걸 발견했어.

이 통찰을 바탕으로, 우리는 작업 간의 관계를 사용해 메타 학습의 최적화 과정을 조정하는 방법을 제안하고, 이를 위한 플러그 앤 플레이 방법인 Task Relation Learner (TRLearner)를 개발했어. 구체적으로, 이 방법은 먼저 추출된 작업 특화 메타 데이터에서 작업 관계 행렬을 얻어. 그 다음, 얻은 행렬을 사용해서 관계 인식 일관성 정규화를 통해 최적화를 안내해. 광범위한 이론적 및 실증 분석을 통해 TRLearner의 효과를 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08481.pdf

Title: USTC-TD: A Test Dataset and Benchmark for Image and Video Coding in 2020s

Original Abstract:
Image/video coding has been a remarkable research area for both academia and industry for many years. Testing datasets, especially high-quality image/video datasets are desirable for the justified evaluation of coding-related research, practical applications, and standardization activities. We put forward a test dataset namely USTC-TD, which has been successfully adopted in the practical end-to-end image/video coding challenge of the IEEE International Conference on Visual Communications and Image Processing in 2022 and 2023. USTC-TD contains 40 images at 4K spatial resolution and 10 video sequences at 1080p spatial resolution, featuring various content due to the diverse environmental factors (scene type, texture, motion, view) and the designed imaging factors (illumination, shadow, lens). We quantitatively evaluate USTC-TD on different image/video features (spatial, temporal, color, lightness), and compare it with the previous image/video test datasets, which verifies the wider coverage and more diversity of the proposed dataset. We also evaluate both classic standardized and recent learned image/video coding schemes on USTC-TD with PSNR and MS-SSIM, and provide an extensive benchmark for the evaluated schemes. Based on the characteristics and specific design of the proposed test dataset, we analyze the benchmark performance and shed light on the future research and development of image/video coding. All the data are released online: this https URL.

Translated Abstract:
이미지/비디오 코딩은 학계와 산업 모두에서 오랫동안 주목받아온 연구 분야야. 테스트 데이터셋, 특히 고품질 이미지/비디오 데이터셋은 코딩 관련 연구를 제대로 평가하고, 실용적인 응용 프로그램과 표준화 활동에 필수적이야.

우리는 USTC-TD라는 테스트 데이터셋을 제안하는데, 이 데이터셋은 2022년과 2023년 IEEE 국제 시각 통신 및 이미지 처리 학회에서 실제 엔드 투 엔드 이미지/비디오 코딩 챌린지에 성공적으로 사용됐어. USTC-TD는 4K 해상도의 이미지 40장과 1080p 해상도의 비디오 10개 시퀀스로 구성되어 있어. 다양한 환경 요인(장면 유형, 질감, 움직임, 시점)과 설계된 이미징 요인(조명, 그림자, 렌즈) 덕분에 다양한 콘텐츠를 보여줘.

우리는 USTC-TD의 다양한 이미지/비디오 특징(공간, 시간, 색상, 밝기)을 정량적으로 평가하고, 이전의 이미지/비디오 테스트 데이터셋과 비교해봤어. 그 결과, 제안한 데이터셋이 더 넓은 범위와 다양한 특성을 가지고 있음을 확인했어. 또, USTC-TD에서 클래식한 표준화된 코딩 방식과 최근의 학습된 코딩 방식에 대해 PSNR과 MS-SSIM을 사용해 평가하고, 평가된 방식에 대한 광범위한 벤치마크를 제공했어.

제안한 테스트 데이터셋의 특징과 구체적인 설계를 바탕으로, 우리는 벤치마크 성능을 분석하고 이미지/비디오 코딩의 미래 연구 및 개발에 대한 통찰을 제공해. 모든 데이터는 온라인에서 공개돼 있어: 이 https URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.08482.pdf

Title: Risks When Sharing LoRA Fine-Tuned Diffusion Model Weights

Original Abstract:
With the emerging trend in generative models and convenient public access to diffusion models pre-trained on large datasets, users can fine-tune these models to generate images of personal faces or items in new contexts described by natural language. Parameter efficient fine-tuning (PEFT) such as Low Rank Adaptation (LoRA) has become the most common way to save memory and computation usage on the user end during fine-tuning. However, a natural question is whether the private images used for fine-tuning will be leaked to adversaries when sharing model weights. In this paper, we study the issue of privacy leakage of a fine-tuned diffusion model in a practical setting, where adversaries only have access to model weights, rather than prompts or images used for fine-tuning. We design and build a variational network autoencoder that takes model weights as input and outputs the reconstruction of private images. To improve the efficiency of training such an autoencoder, we propose a training paradigm with the help of timestep embedding. The results give a surprising answer to this research question: an adversary can generate images containing the same identities as the private images. Furthermore, we demonstrate that no existing defense method, including differential privacy-based methods, can preserve the privacy of private data used for fine-tuning a diffusion model without compromising the utility of a fine-tuned model.

Translated Abstract:
최근 생성 모델과 대규모 데이터셋으로 사전 훈련된 확산 모델에 대한 접근이 쉬워지면서, 사용자들은 이러한 모델을 조정해서 개인 얼굴이나 물건을 자연어로 설명된 새로운 맥락에서 생성할 수 있게 됐어. 메모리와 계산 자원을 절약하기 위해 파라미터 효율적인 미세 조정 방법인 LoRA 같은 기법이 많이 사용되고 있어. 하지만, 여기서 한 가지 중요한 질문은 미세 조정에 사용된 개인 이미지가 모델 가중치를 공유할 때 해커에게 유출될 수 있는가 하는 거야.

이 논문에서는 해커가 모델 가중치만 접근할 수 있는 실제 환경에서 미세 조정된 확산 모델의 프라이버시 유출 문제를 연구했어. 우리는 모델 가중치를 입력으로 받고 개인 이미지를 재구성하는 변분 네트워크 오토인코더를 설계하고 구축했어. 이런 오토인코더의 훈련 효율을 높이기 위해 타임스탬프 임베딩을 활용한 훈련 방식을 제안했어.

결과는 이 연구 질문에 놀라운 답을 줘: 해커는 개인 이미지와 같은 정체성을 가진 이미지를 생성할 수 있다는 거야. 게다가, 차별적 개인 정보 보호 같은 기존 방어 방법들이 미세 조정을 위한 개인 데이터의 프라이버시를 유지할 수 없으며, 이는 미세 조정된 모델의 유용성을 저하시키지 않고는 불가능하다는 것을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08492.pdf

Title: Tri-Plane Mamba: Efficiently Adapting Segment Anything Model for 3D Medical Images

Original Abstract:
General networks for 3D medical image segmentation have recently undergone extensive exploration. Behind the exceptional performance of these networks lies a significant demand for a large volume of pixel-level annotated data, which is time-consuming and labor-intensive. The emergence of the Segment Anything Model (SAM) has enabled this model to achieve superior performance in 2D medical image segmentation tasks via parameter- and data-efficient feature adaptation. However, the introduction of additional depth channels in 3D medical images not only prevents the sharing of 2D pre-trained features but also results in a quadratic increase in the computational cost for adapting SAM. To overcome these challenges, we present the Tri-Plane Mamba (TP-Mamba) adapters tailored for the SAM, featuring two major innovations: 1) multi-scale 3D convolutional adapters, optimized for efficiently processing local depth-level information, 2) a tri-plane mamba module, engineered to capture long-range depth-level representation without significantly increasing computational costs. This approach achieves state-of-the-art performance in 3D CT organ segmentation tasks. Remarkably, this superior performance is maintained even with scarce training data. Specifically using only three CT training samples from the BTCV dataset, it surpasses conventional 3D segmentation networks, attaining a Dice score that is up to 12% higher.

Translated Abstract:
최근 3D 의료 이미지 분할을 위한 일반 네트워크에 대한 연구가 활발히 진행되고 있어. 이런 네트워크들이 뛰어난 성능을 보이려면 많은 양의 픽셀 수준 주석 데이터가 필요한데, 이건 시간도 많이 걸리고 힘든 작업이야. 

Segment Anything Model (SAM)이 등장하면서 2D 의료 이미지 분할 작업에서 파라미터와 데이터 효율적으로 기능을 조정해 뛰어난 성과를 내고 있어. 하지만 3D 의료 이미지에서 추가적인 깊이 채널이 생기면 2D에서 이미 훈련된 기능을 공유할 수 없고, SAM을 조정하는 데 드는 계산 비용이 기하급수적으로 늘어나. 

이런 문제를 해결하기 위해 우리는 SAM에 맞춰진 Tri-Plane Mamba (TP-Mamba) 어댑터를 제안해. 여기엔 두 가지 주요 혁신이 있어: 1) 지역 깊이 정보를 효율적으로 처리하도록 최적화된 다중 스케일 3D 컨볼루션 어댑터, 2) 계산 비용을 크게 늘리지 않으면서 긴 범위의 깊이 표현을 캡처하도록 설계된 삼면 마바 모듈. 

이 방법은 3D CT 장기 분할 작업에서 최첨단 성능을 달성해. 특히, 훈련 데이터가 부족해도 이 뛰어난 성능을 유지해. BTCV 데이터셋에서 단 세 개의 CT 훈련 샘플만 사용했는데도 전통적인 3D 분할 네트워크를 초월해서 Dice 점수를 최대 12% 더 높게 기록했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08494.pdf

Title: WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users

Original Abstract:
Despite researchers having extensively studied various ways to track body pose on-the-go, most prior work does not take into account wheelchair users, leading to poor tracking performance. Wheelchair users could greatly benefit from this pose information to prevent injuries, monitor their health, identify environmental accessibility barriers, and interact with gaming and VR experiences. In this work, we present WheelPoser, a real-time pose estimation system specifically designed for wheelchair users. Our system uses only four strategically placed IMUs on the user's body and wheelchair, making it far more practical than prior systems using cameras and dense IMU arrays. WheelPoser is able to track a wheelchair user's pose with a mean joint angle error of 14.30 degrees and a mean joint position error of 6.74 cm, more than three times better than similar systems using sparse IMUs. To train our system, we collect a novel WheelPoser-IMU dataset, consisting of 167 minutes of paired IMU sensor and motion capture data of people in wheelchairs, including wheelchair-specific motions such as propulsion and pressure relief. Finally, we explore the potential application space enabled by our system and discuss future opportunities. Open-source code, models, and dataset can be found here: this https URL.

Translated Abstract:
연구자들이 몸의 자세를 추적하는 다양한 방법을 많이 연구했지만, 휠체어 사용자를 고려한 연구는 거의 없어서 추적 성능이 좋지 않았어. 휠체어 사용자들은 부상을 방지하고, 건강을 모니터링하며, 환경 접근성 문제를 식별하고, 게임이나 VR 환경과 상호작용하기 위해 자세 정보를 많이 필요로 해. 

이 연구에서는 WheelPoser라는 실시간 자세 추정 시스템을 소개해. 이 시스템은 사용자 몸과 휠체어에 전략적으로 배치된 네 개의 IMU만 사용해서, 카메라나 밀집 IMU 배열을 사용하는 이전 시스템보다 훨씬 실용적이야. WheelPoser는 휠체어 사용자의 자세를 평균 관절 각도 오차 14.30도, 평균 관절 위치 오차 6.74cm로 추적할 수 있어. 이는 드문 IMU를 사용하는 유사 시스템보다 세 배 이상 나은 성능이야.

우리 시스템을 훈련시키기 위해 WheelPoser-IMU라는 새로운 데이터셋을 수집했어. 이 데이터셋은 휠체어 사용자의 IMU 센서와 모션 캡처 데이터를 167분 동안 기록한 거고, 휠체어 특유의 동작인 추진이나 압력 해소 같은 것도 포함되어 있어. 마지막으로, 우리 시스템이 가능하게 하는 응용 분야를 탐색하고, 미래의 기회에 대해 논의할 거야. 오픈소스 코드, 모델, 데이터셋은 여기에서 확인할 수 있어: 이 https URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.08500.pdf

Title: Cross-conditioned Diffusion Model for Medical Image to Image Translation

Original Abstract:
Multi-modal magnetic resonance imaging (MRI) provides rich, complementary information for analyzing diseases. However, the practical challenges of acquiring multiple MRI modalities, such as cost, scan time, and safety considerations, often result in incomplete datasets. This affects both the quality of diagnosis and the performance of deep learning models trained on such data. Recent advancements in generative adversarial networks (GANs) and denoising diffusion models have shown promise in natural and medical image-to-image translation tasks. However, the complexity of training GANs and the computational expense associated with diffusion models hinder their development and application in this task. To address these issues, we introduce a Cross-conditioned Diffusion Model (CDM) for medical image-to-image translation. The core idea of CDM is to use the distribution of target modalities as guidance to improve synthesis quality while achieving higher generation efficiency compared to conventional diffusion models. First, we propose a Modality-specific Representation Model (MRM) to model the distribution of target modalities. Then, we design a Modality-decoupled Diffusion Network (MDN) to efficiently and effectively learn the distribution from MRM. Finally, a Cross-conditioned UNet (C-UNet) with a Condition Embedding module is designed to synthesize the target modalities with the source modalities as input and the target distribution for guidance. Extensive experiments conducted on the BraTS2023 and UPenn-GBM benchmark datasets demonstrate the superiority of our method.

Translated Abstract:
다중 양상 자기공명영상(MRI)은 질병 분석을 위한 풍부하고 보완적인 정보를 제공해. 하지만 여러 MRI 양상을 얻는 데 드는 비용, 스캔 시간, 안전 문제 같은 현실적인 도전 때문에 데이터셋이 불완전해지는 경우가 많아. 이건 진단의 질과 그런 데이터를 기반으로 훈련된 딥러닝 모델의 성능에 영향을 미쳐. 

최근 생성적 적대 신경망(GAN)과 노이즈 제거 확산 모델이 자연 이미지와 의료 이미지 간의 변환 작업에서 가능성을 보여줬어. 하지만 GAN을 훈련하는 게 복잡하고, 확산 모델이 계산 비용이 많이 드는 문제 때문에 이 작업에서 적용하기 어렵지. 

이런 문제를 해결하기 위해 우리는 의료 이미지 간 변환을 위한 교차 조건부 확산 모델(CDM)을 소개해. CDM의 핵심 아이디어는 목표 양상의 분포를 가이드로 사용해 합성 품질을 개선하고 기존 확산 모델보다 더 높은 생성 효율을 달성하는 거야. 먼저, 목표 양상의 분포를 모델링하기 위해 양상 특화 표현 모델(MRM)을 제안해. 그 다음, MRM에서 분포를 효율적이고 효과적으로 학습하기 위해 양상 분리 확산 네트워크(MDN)를 설계했어. 마지막으로, 조건 임베딩 모듈이 있는 교차 조건부 UNet(C-UNet)을 설계해서 소스 양상과 목표 분포를 입력으로 하여 목표 양상을 합성해. 

BraTS2023와 UPenn-GBM 벤치마크 데이터셋에서 진행한 실험 결과, 우리의 방법이 우수성을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08537.pdf

Title: SRE-CNN: A Spatiotemporal Rotation-Equivariant CNN for Cardiac Cine MR Imaging

Original Abstract:
Dynamic MR images possess various transformation symmetries,including the rotation symmetry of local features within the image and along the temporal dimension. Utilizing these symmetries as prior knowledge can facilitate dynamic MR imaging with high spatiotemporal resolution. Equivariant CNN is an effective tool to leverage the symmetry priors. However, current equivariant CNN methods fail to fully exploit these symmetry priors in dynamic MR imaging. In this work, we propose a novel framework of Spatiotemporal Rotation-Equivariant CNN (SRE-CNN), spanning from the underlying high-precision filter design to the construction of the temporal-equivariant convolutional module and imaging model, to fully harness the rotation symmetries inherent in dynamic MR images. The temporal-equivariant convolutional module enables exploitation the rotation symmetries in both spatial and temporal dimensions, while the high-precision convolutional filter, based on parametrization strategy, enhances the utilization of rotation symmetry of local features to improve the reconstruction of detailed anatomical structures. Experiments conducted on highly undersampled dynamic cardiac cine data (up to 20X) have demonstrated the superior performance of our proposed approach, both quantitatively and qualitatively.

Translated Abstract:
동적 MRI 이미지는 이미지 안의 지역 특징들에서 회전 대칭과 시간 차원에서도 다양한 변환 대칭을 가지고 있어. 이런 대칭을 사전 지식으로 활용하면 동적 MRI 이미지를 높은 공간-시간 해상도로 촬영하는 데 도움이 돼. 대칭 사전 지식을 활용하는 데 효과적인 도구가 바로 공변 CNN이야. 하지만 현재의 공변 CNN 방법들은 동적 MRI 이미징에서 이 대칭 사전 지식을 제대로 활용하지 못하고 있어.

우리는 이 문제를 해결하기 위해 Spatiotemporal Rotation-Equivariant CNN (SRE-CNN)이라는 새로운 프레임워크를 제안해. 이 프레임워크는 고정밀 필터 디자인부터 시작해서 시간 공변 합성곱 모듈과 이미징 모델 구축에 이르기까지, 동적 MRI 이미지에 내재된 회전 대칭을 완전히 활용할 수 있게 해줘. 시간 공변 합성곱 모듈은 공간과 시간 차원 모두에서 회전 대칭을 활용할 수 있게 해주고, 고정밀 합성곱 필터는 매개변수화 전략을 기반으로 지역 특징의 회전 대칭 이용을 높여서 정밀한 해부학적 구조 복원을 개선해.

우리가 진행한 실험에서는 동적 심장 시네 데이터(최대 20배 저샘플링)에 대해 우리의 제안 방법이 정량적, 정성적으로 더 우수한 성능을 나타냈어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08563.pdf

Title: Second-order difference subspace

Original Abstract:
Subspace representation is a fundamental technique in various fields of machine learning. Analyzing a geometrical relationship among multiple subspaces is essential for understanding subspace series' temporal and/or spatial dynamics. This paper proposes the second-order difference subspace, a higher-order extension of the first-order difference subspace between two subspaces that can analyze the geometrical difference between them. As a preliminary for that, we extend the definition of the first-order difference subspace to the more general setting that two subspaces with different dimensions have an intersection. We then define the second-order difference subspace by combining the concept of first-order difference subspace and principal component subspace (Karcher mean) between two subspaces, motivated by the second-order central difference method. We can understand that the first/second-order difference subspaces correspond to the velocity and acceleration of subspace dynamics from the viewpoint of a geodesic on a Grassmann manifold. We demonstrate the validity and naturalness of our second-order difference subspace by showing numerical results on two applications: temporal shape analysis of a 3D object and time series analysis of a biometric signal.

Translated Abstract:
서브스페이스 표현은 머신러닝의 여러 분야에서 기본적인 기술이야. 여러 서브스페이스 간의 기하학적 관계를 분석하는 것은 서브스페이스 시리즈의 시간적 또는 공간적 동역학을 이해하는 데 중요해. 

이 논문에서는 두 서브스페이스 사이의 기하학적 차이를 분석할 수 있는 고차원 확장인 2차 차분 서브스페이스를 제안해. 이를 위해 먼저 서로 다른 차원을 가진 두 서브스페이스가 교차하는 더 일반적인 상황으로 1차 차분 서브스페이스의 정의를 확장해. 

그 다음에 2차 차분 서브스페이스를 정의하는데, 이는 1차 차분 서브스페이스와 두 서브스페이스 간의 주성분 서브스페이스(카르셔 평균)를 결합한 개념이야. 이건 2차 중심 차분 방법에 의해 동기를 부여받았어. 

1차 및 2차 차분 서브스페이스는 그래스만 다양체의 지오데식 관점에서 서브스페이스 동역학의 속도와 가속도에 해당한다고 이해할 수 있어. 우리는 3D 객체의 시간적 형태 분석과 생체 신호의 시계열 분석이라는 두 가지 응용 사례에 대한 수치 결과를 보여주면서 우리의 2차 차분 서브스페이스의 유효성과 자연스러움을 입증해.

================================================================================

URL:
https://arxiv.org/pdf/2409.08588.pdf

Title: Improved Unet model for brain tumor image segmentation based on ASPP-coordinate attention mechanism

Original Abstract:
In this paper, we propose an improved Unet model for brain tumor image segmentation, which combines coordinate attention mechanism and ASPP module to improve the segmentation effect. After the data set is divided, we do the necessary preprocessing to the image and use the improved model to experiment. First, we trained and validated the traditional Unet model. By analyzing the loss curve of the training set and the validation set, we can see that the loss value continues to decline at the first epoch and becomes stable at the eighth epoch. This process shows that the model constantly optimizes its parameters to improve performance. At the same time, the change in the miou (mean Intersection over Union) index shows that the miou value exceeded 0.6 at the 15th epoch, remained above 0.6 thereafter, and reached above 0.7 at the 46th epoch. These results indicate that the basic Unet model is effective in brain tumor image segmentation. Next, we introduce an improved Unet algorithm based on coordinate attention mechanism and ASPP module for experiments. By observing the loss change curves of the training set and the verification set, it is found that the loss value reaches the lowest point at the sixth epoch and then remains relatively stable. At the same time, the miou indicator has stabilized above 0.7 since the 20th epoch and has reached a maximum of 0.76. These results show that the new mechanism introduced significantly improves the segmentation ability of the model. Finally, we apply the trained traditional Unet model and the improved Unet model based on the coordinate attention mechanism and ASPP module to the test set for brain tumor image segmentation prediction. Compared to the traditional Unet, the enhanced model offers superior segmentation and edge accuracy, providing a more reliable method for medical image analysis with the coordinate attention mechanism and ASPP module.

Translated Abstract:
이 논문에서는 뇌 종양 이미지 분할을 위한 개선된 Unet 모델을 제안해. 이 모델은 좌표 주의 메커니즘과 ASPP 모듈을 결합해서 분할 효과를 높여. 데이터 세트를 나눈 후, 이미지를 필요한 대로 전처리하고 개선된 모델로 실험을 진행했어.

먼저, 전통적인 Unet 모델을 훈련하고 검증했어. 훈련 세트와 검증 세트의 손실 곡선을 분석해보니, 첫 번째 에폭에서 손실 값이 계속 감소하다가 여덟 번째 에폭에서 안정화되는 걸 볼 수 있었어. 이 과정은 모델이 성능을 높이기 위해 계속 파라미터를 최적화하고 있다는 걸 보여줘. 동시에 miou(평균 교차 비율) 지표의 변화도 보였는데, 15번째 에폭에서 miou 값이 0.6을 넘고 이후로도 계속 0.6 이상을 유지하다가 46번째 에폭에서 0.7을 초과했어. 이 결과들은 기본 Unet 모델이 뇌 종양 이미지 분할에 효과적이라는 걸 증명해.

다음에는 좌표 주의 메커니즘과 ASPP 모듈을 기반으로 한 개선된 Unet 알고리즘을 도입해서 실험을 진행했어. 훈련 세트와 검증 세트의 손실 변화 곡선을 관찰해보니, 손실 값이 여섯 번째 에폭에서 최저점을 찍고 그 후에는 안정적인 상태를 유지했어. 동시에 miou 지표는 20번째 에폭부터 0.7 이상으로 안정화되었고 최대 0.76에 도달했어. 이 결과들은 새로 도입한 메커니즘이 모델의 분할 능력을 크게 향상시켰다는 걸 보여줘.

마지막으로 훈련된 전통적인 Unet 모델과 개선된 Unet 모델을 테스트 세트에 적용해서 뇌 종양 이미지 분할 예측을 했어. 전통적인 Unet과 비교했을 때, 개선된 모델이 분할과 경계 정확도에서 더 우수한 성능을 보여줬고, 좌표 주의 메커니즘과 ASPP 모듈을 통해 의료 이미지 분석에 더 신뢰할 수 있는 방법을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.08618.pdf

Title: TapToTab : Video-Based Guitar Tabs Generation using AI and Audio Analysis

Original Abstract:
The automation of guitar tablature generation from video inputs holds significant promise for enhancing music education, transcription accuracy, and performance analysis. Existing methods face challenges with consistency and completeness, particularly in detecting fretboards and accurately identifying notes. To address these issues, this paper introduces an advanced approach leveraging deep learning, specifically YOLO models for real-time fretboard detection, and Fourier Transform-based audio analysis for precise note identification. Experimental results demonstrate substantial improvements in detection accuracy and robustness compared to traditional techniques. This paper outlines the development, implementation, and evaluation of these methodologies, aiming to revolutionize guitar instruction by automating the creation of guitar tabs from video recordings.

Translated Abstract:
비디오 입력으로부터 기타 타블렛을 자동으로 생성하는 기술은 음악 교육, 악보 전사 정확도, 그리고 연주 분석을 향상시킬 수 있는 큰 가능성을 가지고 있어. 기존 방법들은 일관성과 완전성에서 어려움을 겪고 있는데, 특히 프렛보드 감지와 정확한 노트 인식에서 그렇지.

이 논문에서는 이런 문제들을 해결하기 위해 딥러닝을 활용한 고급 접근 방식을 소개해. 구체적으로는, 실시간 프렛보드 감지를 위해 YOLO 모델을 사용하고, 정확한 노트 인식을 위해 푸리에 변환 기반의 오디오 분석을 활용해. 실험 결과, 전통적인 기술에 비해 감지 정확도와 강인성이 크게 개선된 것을 보여줘.

이 논문은 이러한 방법론의 개발, 구현, 평가 과정을 정리하고, 비디오 녹화로부터 기타 타블렛을 자동으로 생성함으로써 기타 교육을 혁신하는 목표를 가지고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08619.pdf

Title: Joint image reconstruction and segmentation of real-time cardiac MRI in free-breathing using a model based on disentangled representation learning

Original Abstract:
A joint image reconstruction and segmentation approach based on disentangled representation learning was trained to enable cardiac cine MR imaging in real-time and under free-breathing. An exploratory feasibility study tested the proposed method in undersampled real-time acquisitions based on an in-house developed spiral bSSFP pulse sequence in eight healthy participants and five patients with intermittent atrial fibrillation. Images and predicted LV segmentations were compared to the reference standard of ECG-gated segmented Cartesian cine in repeated breath-holds and corresponding manual segmentation. On a 5-point Likert scale, image quality of the real-time breath-hold approach and Cartesian cine was comparable in healthy participants (RT-BH: 1.99 $\pm$ .98, Cartesian: 1.94 $\pm$ .86, p=.052), but slightly inferior in free-breathing (RT-FB: 2.40 $\pm$ .98, p<.001). In patients with arrhythmia, image quality from both real-time approaches was favourable (RT-BH: 2.10 $\pm$ 1.28, p<.001, RT-FB: 2.40 $\pm$ 1.13, p<.001, Cartesian: 2.68 $\pm$ 1.13). Intra-observer reliability was good (ICC=.77, 95%-confidence interval [.75, .79], p<.001). In functional analysis, a positive bias was observed for ejection fractions derived from the proposed model compared to the clinical reference standard (RT-BH mean EF: 58.5 $\pm$ 5.6%, bias: +3.47%, 95%-confidence interval [-.86, 7.79%], RT-FB mean: 57.9 $\pm$ 10.6%, bias: +1.45%, [-3.02, 5.91%], Cartesian mean: 54.9 $\pm$ 6.7%). The introduced real-time MR imaging technique is capable of acquiring high-quality cardiac cine data in 1-2 minutes without the need for ECG gating and breath-holds. It thus offers a promising alternative to the current clinical practice of segmented acquisition, with shorter scan times, higher patient comfort and increased robustness to arrhythmia and patient incompliance.

Translated Abstract:
심장 cine MR 이미징을 실시간으로 자유 호흡 상태에서 가능하게 하기 위해 분리된 표현 학습 기반의 공동 이미지 재구성 및 분할 접근 방식을 훈련했습니다. 탐색적인 가능성 연구에서는 자체 개발한 나선형 bSSFP 펄스 시퀀스를 기반으로 한 언더샘플링된 실시간 획득에서 제안된 방법을 테스트했습니다. 이 연구는 8명의 건강한 참가자와 5명의 간헐적 심방세동 환자를 대상으로 진행되었습니다.

이미지와 예측된 좌심실(LV) 분할 결과를 ECG-게이티드 세그먼트 카르테시안 cine와 비교했습니다. 반복적인 호흡 정지 동안의 결과와 수동 분할 결과를 비교했어요. 5점 리커트 척도에서, 건강한 참가자들 사이에서 실시간 호흡 정지 접근법(RT-BH)과 카르테시안 cine의 이미지 품질은 비슷했어요 (RT-BH: 1.99 ± 0.98, 카르테시안: 1.94 ± 0.86, p=.052). 하지만 자유 호흡 상태에서는 약간 떨어졌어요 (RT-FB: 2.40 ± 0.98, p<.001).

부정맥 환자들에게서 두 가지 실시간 접근법 모두 좋은 이미지 품질을 보였어요 (RT-BH: 2.10 ± 1.28, p<.001, RT-FB: 2.40 ± 1.13, p<.001, 카르테시안: 2.68 ± 1.13). 내부 관찰자 신뢰도도 좋았어요 (ICC=.77, 95% 신뢰 구간 [.75, .79], p<.001). 기능 분석에서 제안된 모델로부터 도출된 박출 분율은 임상 기준에 비해 약간의 긍정적 편향이 있었어요 (RT-BH 평균 EF: 58.5 ± 5.6%, 편향: +3.47%, 95% 신뢰 구간 [-.86, 7.79%], RT-FB 평균: 57.9 ± 10.6%, 편향: +1.45%, [-3.02, 5.91%], 카르테시안 평균: 54.9 ± 6.7%).

이제 소개된 실시간 MR 이미징 기술은 ECG 게이팅과 호흡 정지 없이 1-2분 안에 고품질의 심장 cine 데이터를 얻을 수 있어요. 그래서 현재의 세그먼트 획득 임상 실습에 비해 스캔 시간이 짧고, 환자 편안함이 높아지며, 부정맥이나 환자 불복종에 더 강한 대안을 제공할 수 있어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.08652.pdf

Title: SkinFormer: Learning Statistical Texture Representation with Transformer for Skin Lesion Segmentation

Original Abstract:
Accurate skin lesion segmentation from dermoscopic images is of great importance for skin cancer diagnosis. However, automatic segmentation of melanoma remains a challenging task because it is difficult to incorporate useful texture representations into the learning process. Texture representations are not only related to the local structural information learned by CNN, but also include the global statistical texture information of the input image. In this paper, we propose a trans\textbf{Former} network (\textbf{SkinFormer}) that efficiently extracts and fuses statistical texture representation for \textbf{Skin} lesion segmentation. Specifically, to quantify the statistical texture of input features, a Kurtosis-guided Statistical Counting Operator is designed. We propose Statistical Texture Fusion Transformer and Statistical Texture Enhance Transformer with the help of Kurtosis-guided Statistical Counting Operator by utilizing the transformer's global attention mechanism. The former fuses structural texture information and statistical texture information, and the latter enhances the statistical texture of multi-scale features. {Extensive experiments on three publicly available skin lesion datasets validate that our SkinFormer outperforms other SOAT methods, and our method achieves 93.2\% Dice score on ISIC 2018. It can be easy to extend SkinFormer to segment 3D images in the future.} Our code is available at this https URL.

Translated Abstract:
피부 병변을 정확하게 분할하는 것은 피부암 진단에 매우 중요해. 하지만 멜라노마의 자동 분할은 여전히 어려운 과제야. 왜냐하면 유용한 텍스처 표현을 학습 과정에 포함시키기가 어렵거든. 텍스처 표현은 CNN이 학습한 지역 구조 정보와 입력 이미지의 전반적인 통계적 텍스처 정보 모두와 관련이 있어.

이 논문에서는 피부 병변 분할을 위해 통계적 텍스처 표현을 효율적으로 추출하고 융합하는 전이 네트워크인 SkinFormer를 제안해. 특히 입력 특징의 통계적 텍스처를 정량화하기 위해 Kurtosis-guided Statistical Counting Operator를 설계했어. 우리는 변환기의 전역 주의 메커니즘을 활용해서 통계적 텍스처 융합 트랜스포머와 통계적 텍스처 강화 트랜스포머를 제안해. 첫 번째는 구조적 텍스처 정보와 통계적 텍스처 정보를 융합하고, 두 번째는 다중 스케일 특징의 통계적 텍스처를 향상시켜.

세 가지 공개된 피부 병변 데이터셋에서 광범위한 실험을 통해 SkinFormer가 다른 SOAT 방법들보다 뛰어난 성능을 보인다는 것을 확인했어. 그리고 우리 방법은 ISIC 2018에서 93.2%의 Dice 점수를 기록했어. 앞으로 3D 이미지를 분할하는 데 SkinFormer를 쉽게 확장할 수 있을 것 같아. 우리 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08712.pdf

Title: Layerwise Change of Knowledge in Neural Networks

Original Abstract:
This paper aims to explain how a deep neural network (DNN) gradually extracts new knowledge and forgets noisy features through layers in forward propagation. Up to now, although the definition of knowledge encoded by the DNN has not reached a consensus, Previous studies have derived a series of mathematical evidence to take interactions as symbolic primitive inference patterns encoded by a DNN. We extend the definition of interactions and, for the first time, extract interactions encoded by intermediate layers. We quantify and track the newly emerged interactions and the forgotten interactions in each layer during the forward propagation, which shed new light on the learning behavior of DNNs. The layer-wise change of interactions also reveals the change of the generalization capacity and instability of feature representations of a DNN.

Translated Abstract:
이 논문은 딥 뉴럴 네트워크(DNN)가 어떻게 새로운 지식을 점차 추출하고, 노이즈가 있는 특징은 잊어버리는지를 설명하려고 해. 지금까지 DNN이 인코딩하는 지식의 정의에 대해 합의가 이루어지지 않았지만, 이전 연구들은 DNN이 인코딩하는 상징적 기본 추론 패턴으로 상호작용을 다룬 수 있는 수학적 증거를 제시했어.

우리는 상호작용의 정의를 확장하고, 처음으로 중간 층에서 인코딩된 상호작용을 추출했어. 그리고 각 층에서 새로 나타난 상호작용과 잊혀진 상호작용을 정량화하고 추적했어. 이 과정은 DNN의 학습 행동에 대한 새로운 통찰을 제공해.

또한, 층별로 상호작용의 변화는 DNN의 일반화 능력과 특징 표현의 불안정성 변화도 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.08772.pdf

Title: On the Computation of BD-Rate over a Set of Videos for Fair Assessment of Performance of Learned Video Codecs

Original Abstract:
The Bjøntegaard Delta (BD) measure is widely employed to evaluate and quantify the variations in the rate-distortion(RD) performance across different codecs. Many researchers report the average BD value over multiple videos within a dataset for different codecs. We claim that the current practice in the learned video compression community of computing the average BD value over a dataset based on the average RD curve of multiple videos can lead to misleading conclusions. We show both by analysis of a simplistic case of linear RD curves and experimental results with two recent learned video codecs that averaging RD curves can lead to a single video to disproportionately influence the average BD value especially when the operating bitrate range of different codecs do not exactly match. Instead, we advocate for calculating the BD measure per-video basis, as commonly done by the traditional video compression community, followed by averaging the individual BD values over videos, to provide a fair comparison of learned video codecs. Our experimental results demonstrate that the comparison of two recent learned video codecs is affected by how we evaluate the average BD measure.

Translated Abstract:
Bjøntegaard Delta (BD) 측정값은 다양한 코덱의 비율-왜곡(RD) 성능 변화를 평가하고 수치화하는 데 널리 사용돼. 많은 연구자들이 데이터셋 내 여러 비디오에 대해 다른 코덱의 평균 BD 값을 보고해. 

우리는 현재 학습된 비디오 압축 분야에서 여러 비디오의 평균 RD 곡선을 기반으로 데이터셋의 평균 BD 값을 계산하는 방식이 잘못된 결론을 초래할 수 있다고 주장해. 간단한 선형 RD 곡선의 분석과 최근 두 개의 학습된 비디오 코덱을 사용한 실험 결과를 통해, RD 곡선을 평균내면 한 개의 비디오가 평균 BD 값에 과도한 영향을 미칠 수 있다는 걸 보여줘. 특히 서로 다른 코덱의 비트레이트 범위가 정확히 일치하지 않을 때 더 그렇지. 

대신, 우리는 전통적인 비디오 압축 분야에서 흔히 하는 것처럼 비디오별로 BD 측정값을 계산한 다음, 개별 BD 값을 평균내는 방식이 더 공정한 비교를 제공한다고 주장해. 우리의 실험 결과는 두 개의 최근 학습된 비디오 코덱 비교가 평균 BD 측정값을 평가하는 방법에 따라 영향을 받는다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.08850.pdf

Title: DX2CT: Diffusion Model for 3D CT Reconstruction from Bi or Mono-planar 2D X-ray(s)

Original Abstract:
Computational tomography (CT) provides high-resolution medical imaging, but it can expose patients to high radiation. X-ray scanners have low radiation exposure, but their resolutions are low. This paper proposes a new conditional diffusion model, DX2CT, that reconstructs three-dimensional (3D) CT volumes from bi or mono-planar X-ray image(s). Proposed DX2CT consists of two key components: 1) modulating feature maps extracted from two-dimensional (2D) X-ray(s) with 3D positions of CT volume using a new transformer and 2) effectively using the modulated 3D position-aware feature maps as conditions of DX2CT. In particular, the proposed transformer can provide conditions with rich information of a target CT slice to the conditional diffusion model, enabling high-quality CT reconstruction. Our experiments with the bi or mono-planar X-ray(s) benchmark datasets show that proposed DX2CT outperforms several state-of-the-art methods. Our codes and model will be available at: this https URL.

Translated Abstract:
계산 단층촬영(CT)은 고해상도의 의료 이미지를 제공하지만, 환자에게 높은 방사선을 노출시킬 수 있어. 엑스레이 스캐너는 방사선 노출이 적지만 해상도가 낮아. 이 논문에서는 DX2CT라는 새로운 조건부 확산 모델을 제안해, 이 모델은 이차 또는 단일 평면 X-ray 이미지로부터 3D CT 볼륨을 재구성해.

DX2CT는 두 가지 주요 구성 요소로 이루어져 있어: 

1) 2D X-ray에서 추출한 특징 맵을 CT 볼륨의 3D 위치로 변조하는 새로운 트랜스포머를 사용하고, 
2) 변조된 3D 위치 인식 특징 맵을 DX2CT의 조건으로 효과적으로 활용해.

특히, 제안한 트랜스포머는 조건부 확산 모델에 목표 CT 슬라이스에 대한 풍부한 정보를 제공할 수 있어, 이를 통해 고품질 CT 재구성이 가능해. 

우리의 실험 결과, 이차 또는 단일 평면 X-ray 벤치마크 데이터셋에서 DX2CT가 여러 최신 기법들보다 뛰어난 성능을 보였어. 우리의 코드와 모델은 이 링크에서 확인할 수 있어: 이 https URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.08905.pdf

Title: D2-MLP: Dynamic Decomposed MLP Mixer for Medical Image Segmentation

Original Abstract:
Convolutional neural networks are widely used in various segmentation tasks in medical images. However, they are challenged to learn global features adaptively due to the inherent locality of convolutional operations. In contrast, MLP Mixers are proposed as a backbone to learn global information across channels with low complexity. However, they cannot capture spatial features efficiently. Additionally, they lack effective mechanisms to fuse and mix features adaptively. To tackle these limitations, we propose a novel Dynamic Decomposed Mixer module. It is designed to employ novel Mixers to extract features and aggregate information across different spatial locations and channels. Additionally, it employs novel dynamic mixing mechanisms to model inter-dependencies between channel and spatial feature representations and to fuse them adaptively. Subsequently, we incorporate it into a U-shaped Transformer-based architecture to generate a novel network, termed the Dynamic Decomposed MLP Mixer. We evaluated it for medical image segmentation on two datasets, and it achieved superior segmentation performance than other state-of-the-art methods.

Translated Abstract:
합성곱 신경망은 의료 이미지에서 다양한 세분화 작업에 널리 사용되고 있어. 하지만 합성곱 연산의 특성 때문에 전역적인 특징을 적응적으로 학습하는 데 어려움이 있어. 반면, MLP 믹서는 낮은 복잡도로 채널 간의 전역 정보를 학습하기 위해 제안되었지만, 공간적 특징을 효율적으로 포착하지 못해. 게다가, 특징을 적응적으로 융합하고 혼합할 수 있는 효과적인 메커니즘도 부족해.

이런 한계를 해결하기 위해, 우리는 새로운 '동적 분해 믹서' 모듈을 제안해. 이 모듈은 서로 다른 공간 위치와 채널에서 특징을 추출하고 정보를 집계하기 위해 새로운 믹서를 사용하도록 설계되었어. 또한, 채널과 공간적 특징 표현 간의 상호 의존성을 모델링하고 이를 적응적으로 융합하기 위한 새로운 동적 혼합 메커니즘도 포함돼.

이후, 우리는 이 모듈을 U자형 트랜스포머 기반 아키텍처에 통합해서 '동적 분해 MLP 믹서'라는 새로운 네트워크를 만들어냈어. 우리는 두 개의 데이터셋에서 의료 이미지 세분화를 평가했는데, 그 결과 다른 최신 방법들보다 우수한 세분화 성능을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08906.pdf

Title: Gaussian is All You Need: A Unified Framework for Solving Inverse Problems via Diffusion Posterior Sampling

Original Abstract:
Diffusion models can generate a variety of high-quality images by modeling complex data distributions. Trained diffusion models can also be very effective image priors for solving inverse problems. Most of the existing diffusion-based methods integrate data consistency steps within the diffusion reverse sampling process. The data consistency steps rely on an approximate likelihood function. In this paper, we show that the existing approximations are either insufficient or computationally inefficient. To address these issues, we propose a unified likelihood approximation method that incorporates a covariance correction term to enhance the performance and avoids propagating gradients through the diffusion model. The correction term, when integrated into the reverse diffusion sampling process, achieves better convergence towards the true data posterior for selected distributions and improves performance on real-world natural image datasets. Furthermore, we present an efficient way to factorize and invert the covariance matrix of the likelihood function for several inverse problems. We present comprehensive experiments to demonstrate the effectiveness of our method over several existing approaches.

Translated Abstract:
확산 모델은 복잡한 데이터 분포를 모델링해서 다양한 고품질 이미지를 생성할 수 있어. 훈련된 확산 모델은 역문제를 해결하는 데에도 효과적인 이미지 사전으로 활용될 수 있어. 대부분의 기존 확산 기반 방법들은 확산 역 샘플링 과정 안에 데이터 일관성 단계를 통합하고 있어. 이 데이터 일관성 단계는 근사 우도 함수에 의존해. 

이 논문에서는 기존의 근사가 부족하거나 계산 효율성이 떨어진다는 걸 보여줘. 이 문제를 해결하기 위해, 우리는 공분산 보정 항을 포함한 통합된 우도 근사 방법을 제안해. 이 방법은 성능을 향상시키고 확산 모델을 통해 그래디언트를 전파하는 걸 피해. 이 보정 항을 역 확산 샘플링 과정에 통합하면 선택된 분포에 대해 실제 데이터 후방 분포에 더 잘 수렴하게 되고, 실제 자연 이미지 데이터셋에서 성능이 향상돼.

또한, 우리는 여러 역문제에 대한 우도 함수의 공분산 행렬을 효율적으로 분해하고 역산하는 방법도 제시해. 여러 기존 방법들과 비교해서 우리의 방법이 효과적임을 보여주는 포괄적인 실험 결과도 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.08926.pdf

Title: ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation

Original Abstract:
Transparent object depth perception poses a challenge in everyday life and logistics, primarily due to the inability of standard 3D sensors to accurately capture depth on transparent or reflective surfaces. This limitation significantly affects depth map and point cloud-reliant applications, especially in robotic manipulation. We developed a vision transformer-based algorithm for stereo depth recovery of transparent objects. This approach is complemented by an innovative feature post-fusion module, which enhances the accuracy of depth recovery by structural features in images. To address the high costs associated with dataset collection for stereo camera-based perception of transparent objects, our method incorporates a parameter-aligned, domain-adaptive, and physically realistic Sim2Real simulation for efficient data generation, accelerated by AI algorithm. Our experimental results demonstrate the model's exceptional Sim2Real generalizability in real-world scenarios, enabling precise depth mapping of transparent objects to assist in robotic manipulation. Project details are available at this https URL .

Translated Abstract:
투명한 물체의 깊이 인식은 일상생활이나 물류에서 어려운 문제야. 주로 일반 3D 센서가 투명하거나 반사되는 표면에서 깊이를 정확하게 측정하지 못하기 때문이지. 이 제한은 깊이 맵이나 포인트 클라우드에 의존하는 응용 프로그램에 큰 영향을 미쳐. 

우리는 투명 물체의 스테레오 깊이를 회복하기 위한 비전 트랜스포머 기반 알고리즘을 개발했어. 이 방법은 이미지의 구조적 특징을 통해 깊이 회복의 정확성을 높이는 혁신적인 피처 포스트퓨전 모듈로 보완돼. 

또한, 투명 물체에 대한 스테레오 카메라 기반 인식을 위한 데이터셋 수집의 높은 비용 문제를 해결하기 위해, 우리의 방법은 매개변수 정렬, 도메인 적응, 그리고 물리적으로 현실적인 Sim2Real 시뮬레이션을 활용해서 효율적인 데이터 생성을 가능하게 해. 이 과정은 AI 알고리즘에 의해 가속화돼. 

실험 결과는 우리 모델이 실제 상황에서도 뛰어난 Sim2Real 일반화를 보여주고, 투명 물체의 정확한 깊이 맵핑을 통해 로봇 조작을 도울 수 있다는 걸 증명해. 프로젝트에 대한 자세한 내용은 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2211.10881.pdf

Title: Deepfake Detection: A Comprehensive Survey from the Reliability Perspective

Original Abstract:
The mushroomed Deepfake synthetic materials circulated on the internet have raised a profound social impact on politicians, celebrities, and individuals worldwide. In this survey, we provide a thorough review of the existing Deepfake detection studies from the reliability perspective. We identify three reliability-oriented research challenges in the current Deepfake detection domain: transferability, interpretability, and robustness. Moreover, while solutions have been frequently addressed regarding the three challenges, the general reliability of a detection model has been barely considered, leading to the lack of reliable evidence in real-life usages and even for prosecutions on Deepfake-related cases in court. We, therefore, introduce a model reliability study metric using statistical random sampling knowledge and the publicly available benchmark datasets to review the reliability of the existing detection models on arbitrary Deepfake candidate suspects. Case studies are further executed to justify the real-life Deepfake cases including different groups of victims with the help of the reliably qualified detection models as reviewed in this survey. Reviews and experiments on the existing approaches provide informative discussions and future research directions for Deepfake detection.

Translated Abstract:
인터넷에 퍼진 딥페이크 합성 자료는 전 세계 정치인, 유명인사, 개인들에게 큰 사회적 영향을 미쳤어. 이 조사에서는 현재의 딥페이크 탐지 연구를 신뢰성 관점에서 자세히 살펴볼 거야. 

먼저, 현재 딥페이크 탐지 분야에서 신뢰성과 관련된 세 가지 연구 과제를 찾아냈어: 전이 가능성, 해석 가능성, 그리고 강인성. 이 세 가지 문제에 대한 해결책은 종종 다뤄지지만, 탐지 모델의 전반적인 신뢰성은 거의 고려되지 않았어. 그래서 실제 사용이나 법원에서의 딥페이크 관련 사건에 대한 기소를 위한 신뢰할 만한 증거가 부족해.

그래서 우리는 통계적 무작위 샘플링 지식을 활용한 모델 신뢰성 연구 지표를 소개할 거야. 이를 통해 기존 탐지 모델의 신뢰성을 임의의 딥페이크 용의자에 대해 검토할 수 있어. 또한, 이 조사에서 검토한 신뢰성 있는 탐지 모델을 활용해 다양한 피해자 그룹을 포함한 실제 딥페이크 사례를 정당화하는 사례 연구도 진행할 거야.

기존 접근 방법에 대한 리뷰와 실험은 딥페이크 탐지에 대한 유익한 논의와 미래 연구 방향을 제시할 거야.

================================================================================

URL:
https://arxiv.org/pdf/2212.14181.pdf

Title: Efficient Image Super-Resolution with Feature Interaction Weighted Hybrid Network

Original Abstract:
Lightweight image super-resolution aims to reconstruct high-resolution images from low-resolution images using low computational costs. However, existing methods result in the loss of middle-layer features due to activation functions. To minimize the impact of intermediate feature loss on reconstruction quality, we propose a Feature Interaction Weighted Hybrid Network (FIWHN), which comprises a series of Wide-residual Distillation Interaction Block (WDIB) as the backbone. Every third WDIB forms a Feature Shuffle Weighted Group (FSWG) by applying mutual information shuffle and fusion. Moreover, to mitigate the negative effects of intermediate feature loss, we introduce Wide Residual Weighting units within WDIB. These units effectively fuse features of varying levels of detail through a Wide-residual Distillation Connection (WRDC) and a Self-Calibrating Fusion (SCF). To compensate for global feature deficiencies, we incorporate a Transformer and explore a novel architecture to combine CNN and Transformer. We show that our FIWHN achieves a favorable balance between performance and efficiency through extensive experiments on low-level and high-level tasks. Codes will be available at \url{this https URL}.

Translated Abstract:
경량 이미지 초해상도는 저해상도 이미지를 사용해서 고해상도 이미지를 재구성하는 걸 목표로 하면서도 계산 비용을 낮추는 방법이야. 하지만 기존 방법들은 활성화 함수 때문에 중간 레이어의 특징이 손실되는 문제가 있어.

그래서 우리는 중간 특징 손실이 재구성 품질에 미치는 영향을 최소화하기 위해 Feature Interaction Weighted Hybrid Network (FIWHN)을 제안해. 이 네트워크는 Wide-residual Distillation Interaction Block (WDIB)이라는 블록을 여러 개 쌓아서 만들어졌어. 매 세 번째 WDIB는 상호 정보 셔플과 융합을 적용해서 Feature Shuffle Weighted Group (FSWG)을 형성해.

또한, 중간 특징 손실의 부정적인 영향을 줄이기 위해 WDIB 안에 Wide Residual Weighting 유닛을 도입했어. 이 유닛은 다양한 디테일 레벨의 특징을 Wide-residual Distillation Connection (WRDC)과 Self-Calibrating Fusion (SCF)을 통해 효과적으로 융합해.

전반적인 특징의 부족함을 보충하기 위해 Transformer를 추가하고 CNN과 Transformer를 결합하는 새로운 아키텍처를 탐구했어. 우리는 FIWHN이 저수준 및 고수준 작업에서 성능과 효율성 간의 좋은 균형을 이룬다는 걸 광범위한 실험을 통해 보여줬어. 코드도 \url{this https URL}에서 사용할 수 있을 거야.

================================================================================

URL:
https://arxiv.org/pdf/2305.06110.pdf

Title: Pavlok-Nudge: A Feedback Mechanism for Atomic Behaviour Modification with Snoring Usecase

Original Abstract:
This paper proposes a feedback mechanism to change behavioural patterns using the Pavlok device. Pavlok utilises beeps, vibration and shocks as a mode of aversion technique to help individuals with behaviour modification. While the device can be useful in certain periodic daily life situations, like alarms and exercise notifications, the device relies on manual operations that limit its usage. To automate behaviour modification, we propose a framework that first detects targeted behaviours through a lightweight deep learning model and subsequently nudges the user through Pavlok. Our proposed solution is implemented and verified in the context of snoring, which captures audio from the environment following a prediction of whether the audio content is a snore or not using a 1D convolutional neural network. Based on the prediction, we use Pavlok to nudge users for preventive measures, such as a change in sleeping posture. We believe that this simple solution can help people to change their atomic habits, which may lead to long-term health benefits. Our proposed real-time, lightweight model (99.8% less parameters over SOTA; 1,278,049 --> 1337) achieves SOTA performance (test accuracy of 0.99) on a public domain benchmark. The code and model are publicly available at this https URL.

Translated Abstract:
이 논문은 Pavlok 장치를 이용해 행동 패턴을 변경하는 피드백 메커니즘을 제안해. Pavlok은 비프음, 진동, 전기충격 같은 방법을 사용해서 사람들이 행동을 수정할 수 있도록 도와줘. 이 장치는 알람이나 운동 알림 같은 특정한 일상 상황에서는 유용하지만, 수동 조작에 의존해서 사용이 제한돼.

우리는 행동 수정을 자동화하기 위해, 먼저 가벼운 딥러닝 모델로 목표 행동을 감지하고, 그 다음에 Pavlok을 통해 사용자에게 알림을 주는 프레임워크를 제안해. 이 해결책은 코골이에 대한 맥락에서 구현하고 검증했어. 코골이인지 아닌지를 예측하기 위해 1D 합성곱 신경망을 사용해서 환경에서 오디오를 캡처해. 예측 결과에 따라 Pavlok을 사용해 사용자가 예방 조치를 취할 수 있도록 알림을 줘, 예를 들어 수면 자세를 바꾸는 거야.

이 간단한 해결책이 사람들이 작은 습관을 바꾸는 데 도움이 될 수 있다고 생각해. 이런 변화가 장기적으로 건강에 이로운 결과를 가져올 수 있을 거라고 믿어. 우리가 제안한 실시간 경량 모델은 (SOTA보다 99.8% 적은 파라미터 수; 1,278,049에서 1,337로 줄어듦) 공공 데이터셋에서 SOTA 성능(테스트 정확도 0.99)을 달성했어. 코드와 모델은 이 링크에서 공개되어 있어.

================================================================================

URL:
https://arxiv.org/pdf/2310.07248.pdf

Title: IBoxCLA: Towards Robust Box-supervised Segmentation of Polyp via Improved Box-dice and Contrastive Latent-anchors

Original Abstract:
Box-supervised polyp segmentation attracts increasing attention for its cost-effective potential. Existing solutions often rely on learning-free methods or pretrained models to laboriously generate pseudo masks, triggering Dice constraint subsequently. In this paper, we found that a model guided by the simplest box-filled masks can accurately predict polyp locations/sizes, but suffers from shape collapsing. In response, we propose two innovative learning fashions, Improved Box-dice (IBox) and Contrastive Latent-Anchors (CLA), and combine them to train a robust box-supervised model IBoxCLA. The core idea behind IBoxCLA is to decouple the learning of location/size and shape, allowing for focused constraints on each of them. Specifically, IBox transforms the segmentation map into a proxy map using shape decoupling and confusion-region swapping sequentially. Within the proxy map, shapes are disentangled, while locations/sizes are encoded as box-like responses. By constraining the proxy map instead of the raw prediction, the box-filled mask can well supervise IBoxCLA without misleading its shape learning. Furthermore, CLA contributes to shape learning by generating two types of latent anchors, which are learned and updated using momentum and segmented polyps to steadily represent polyp and background features. The latent anchors facilitate IBoxCLA to capture discriminative features within and outside boxes in a contrastive manner, yielding clearer boundaries. We benchmark IBoxCLA on five public polyp datasets. The experimental results demonstrate the competitive performance of IBoxCLA compared to recent fully-supervised polyp segmentation methods, and its superiority over other box-supervised state-of-the-arts with a relative increase of overall mDice and mIoU by at least 6.5% and 7.5%, respectively.

Translated Abstract:
박스-감독 폴립 세분화는 비용 효율적인 가능성 덕분에 점점 더 많은 관심을 받고 있어. 기존의 해결책들은 종종 학습이 필요 없는 방법이나 사전 학습된 모델에 의존해서 힘들게 유사 마스크를 생성해. 이 과정에서 Dice 제약이 발생해. 

이 논문에서는 가장 단순한 박스가 채워진 마스크에 의해 안내된 모델이 폴립의 위치와 크기를 정확하게 예측할 수 있지만, 형태가 무너지는 문제를 겪는다는 걸 발견했어. 그래서 우리는 두 가지 혁신적인 학습 방식을 제안했어: 개선된 박스-다이스(IBox)와 대조적 잠재 앵커(CLA). 이 두 가지를 결합해서 강력한 박스-감독 모델인 IBoxCLA를 훈련시켰어.

IBoxCLA의 핵심 아이디어는 위치/크기와 형태 학습을 분리하는 거야. 이렇게 함으로써 각 요소에 집중할 수 있는 제한을 두는 거지. 구체적으로, IBox는 세분화 맵을 형태 분리와 혼동 지역 스와핑을 통해 프록시 맵으로 변환해. 프록시 맵 안에서는 형태가 분리되고, 위치/크기는 박스와 비슷한 반응으로 인코딩돼. 원래 예측 대신 프록시 맵을 제한함으로써 박스가 채워진 마스크가 IBoxCLA를 잘 감독할 수 있도록 하면서 형태 학습에 혼란을 주지 않아.

게다가 CLA는 두 가지 유형의 잠재 앵커를 생성함으로써 형태 학습에 기여해. 이 잠재 앵커는 모멘텀과 세분화된 폴립을 사용해 배우고 업데이트되어 폴립과 배경 특징을 안정적으로 표현해. 잠재 앵커는 IBoxCLA가 박스 안팎에서 차별적인 특징을 대조적으로 포착하도록 도와주면서 더 명확한 경계를 만들어.

우리는 IBoxCLA를 다섯 개의 공개 폴립 데이터셋에서 벤치마킹했어. 실험 결과는 IBoxCLA가 최근의 완전 감독 폴립 세분화 방법에 비해 경쟁력 있는 성능을 보였고, 다른 박스-감독 최신 기술에 비해 전체 mDice와 mIoU가 각각 최소 6.5%와 7.5% 증가하면서 우수하다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2311.17744.pdf

Title: Variational Bayes image restoration with compressive autoencoders

Original Abstract:
Regularization of inverse problems is of paramount importance in computational imaging. The ability of neural networks to learn efficient image representations has been recently exploited to design powerful data-driven regularizers. While state-of-the-art plug-and-play methods rely on an implicit regularization provided by neural denoisers, alternative Bayesian approaches consider Maximum A Posteriori (MAP) estimation in the latent space of a generative model, thus with an explicit regularization. However, state-of-the-art deep generative models require a huge amount of training data compared to denoisers. Besides, their complexity hampers the optimization involved in latent MAP derivation. In this work, we first propose to use compressive autoencoders instead. These networks, which can be seen as variational autoencoders with a flexible latent prior, are smaller and easier to train than state-of-the-art generative models. As a second contribution, we introduce the Variational Bayes Latent Estimation (VBLE) algorithm, which performs latent estimation within the framework of variational inference. Thanks to a simple yet efficient parameterization of the variational posterior, VBLE allows for fast and easy (approximate) posterior sampling.Experimental results on image datasets BSD and FFHQ demonstrate that VBLE reaches similar performance than state-of-the-art plug-and-play methods, while being able to quantify uncertainties significantly faster than other existing posterior sampling techniques.

Translated Abstract:
역 문제의 정규화는 계산 이미징에서 정말 중요해. 최근에 신경망이 효율적인 이미지 표현을 학습하는 능력을 활용해서 강력한 데이터 기반 정규화기를 설계하게 되었어. 최신의 플러그 앤 플레이 방법들은 신경 잡음 제거기에서 제공하는 암묵적인 정규화에 의존하고, 다른 Bayesian 접근법들은 생성 모델의 잠재 공간에서 최대 사후 확률(MAP) 추정을 고려해. 이건 명시적인 정규화 방식이야.

하지만 최신의 딥 생성 모델들은 잡음 제거기보다 훨씬 많은 훈련 데이터를 필요로 해. 게다가 그 복잡성 때문에 잠재 MAP 유도 과정에서 최적화가 힘들어. 그래서 이 연구에서는 먼저 압축 오토인코더를 사용하자고 제안했어. 이 네트워크는 유연한 잠재 사전이 있는 변분 오토인코더로 볼 수 있는데, 최신 생성 모델보다 작고 훈련하기 쉬워.

두 번째 기여로는 변분 베이즈 잠재 추정(VBLE) 알고리즘을 소개해. 이 알고리즘은 변분 추론 프레임워크 내에서 잠재 추정을 수행해. 간단하면서도 효율적인 변분 사후 확률의 매개변수화 덕분에, VBLE는 빠르고 쉽게 (근사) 사후 샘플링을 할 수 있어.

이미지 데이터셋 BSD와 FFHQ에서의 실험 결과에 따르면, VBLE는 최신의 플러그 앤 플레이 방법들과 비슷한 성능을 내면서 다른 기존의 사후 샘플링 기법들보다 불확실성을 훨씬 더 빨리 정량화할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2403.05402.pdf

Title: DualBEV: Unifying Dual View Transformation with Probabilistic Correspondences

Original Abstract:
Camera-based Bird's-Eye-View (BEV) perception often struggles between adopting 3D-to-2D or 2D-to-3D view transformation (VT). The 3D-to-2D VT typically employs resource-intensive Transformer to establish robust correspondences between 3D and 2D features, while the 2D-to-3D VT utilizes the Lift-Splat-Shoot (LSS) pipeline for real-time application, potentially missing distant information. To address these limitations, we propose DualBEV, a unified framework that utilizes a shared feature transformation incorporating three probabilistic measurements for both strategies. By considering dual-view correspondences in one stage, DualBEV effectively bridges the gap between these strategies, harnessing their individual strengths. Our method achieves state-of-the-art performance without Transformer, delivering comparable efficiency to the LSS approach, with 55.2% mAP and 63.4% NDS on the nuScenes test set. Code is available at \url{this https URL}

Translated Abstract:
카메라 기반의 새 시점(BEV) 인식은 보통 3D에서 2D로 또는 2D에서 3D로 변환하는 데 어려움을 겪어. 3D에서 2D로 변환할 때는 보통 리소스를 많이 사용하는 Transformer를 사용해서 3D와 2D 특징 간의 강력한 대응 관계를 만들고, 2D에서 3D로 변환할 때는 Lift-Splat-Shoot(LSS) 방식을 이용해 실시간으로 처리하는데, 이 과정에서 먼 정보는 놓칠 수 있어. 

이러한 한계를 해결하기 위해 우리는 DualBEV라는 통합 프레임워크를 제안해. 이 프레임워크는 두 가지 전략을 위해 세 가지 확률적 측정을 포함한 공유 특징 변환을 사용해. DualBEV는 한 단계에서 두 개의 시점 대응 관계를 고려함으로써 이 두 전략 간의 간극을 효과적으로 연결하고, 각 전략의 장점을 활용해. 

우리 방법은 Transformer 없이도 최첨단 성능을 달성하고, LSS 접근 방식과 비슷한 효율성을 보여줘. nuScenes 테스트 세트에서 55.2% mAP와 63.4% NDS를 기록했어. 코드도 제공되어 있어.

================================================================================

URL:
https://arxiv.org/pdf/2403.07319.pdf

Title: Efficient Diffusion Model for Image Restoration by Residual Shifting

Original Abstract:
While diffusion-based image restoration (IR) methods have achieved remarkable success, they are still limited by the low inference speed attributed to the necessity of executing hundreds or even thousands of sampling steps. Existing acceleration sampling techniques, though seeking to expedite the process, inevitably sacrifice performance to some extent, resulting in over-blurry restored outcomes. To address this issue, this study proposes a novel and efficient diffusion model for IR that significantly reduces the required number of diffusion steps. Our method avoids the need for post-acceleration during inference, thereby avoiding the associated performance deterioration. Specifically, our proposed method establishes a Markov chain that facilitates the transitions between the high-quality and low-quality images by shifting their residuals, substantially improving the transition efficiency. A carefully formulated noise schedule is devised to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experimental evaluations demonstrate that the proposed method achieves superior or comparable performance to current state-of-the-art methods on three classical IR tasks, namely image super-resolution, image inpainting, and blind face restoration, \textit{\textbf{even only with four sampling steps}}. Our code and model are publicly available at \url{this https URL}.

Translated Abstract:
확산 기반 이미지 복원(IR) 방법들은 꽤 성공적이긴 하지만, 수백 또는 수천 번의 샘플링 단계를 실행해야 해서 inference 속도가 느린 문제가 있어. 기존의 가속 샘플링 기법들은 프로세스를 빠르게 하려 하지만, 성능을 어느 정도 희생하게 되고 결국 복원된 이미지가 너무 흐릿해지는 결과를 낳아. 

이 문제를 해결하기 위해, 이 연구는 IR을 위한 새롭고 효율적인 확산 모델을 제안해. 이 모델은 필요한 확산 단계 수를 크게 줄여줘. 우리 방법은 inference 중에 후속 가속이 필요 없어서 성능 저하도 피할 수 있어. 

구체적으로, 우리가 제안한 방법은 고품질 이미지와 저품질 이미지 간의 전환을 돕는 마르코프 체인을 설정해. 이 과정에서 잔여값을 이동시켜 전환 효율을 크게 향상시켜. 또한, 확산 과정 중 이동 속도와 노이즈 강도를 유연하게 조절할 수 있도록 잘 설계된 노이즈 스케줄을 마련했어.

광범위한 실험 평가를 통해, 제안한 방법이 이미지 슈퍼 해상도, 이미지 인페인팅, 블라인드 얼굴 복원이라는 세 가지 전통적인 IR 작업에서 현재 최첨단 방법들과 비교해 우수하거나 동등한 성능을 달성한다는 걸 보여줬어. \textit{\textbf{심지어 네 번의 샘플링 단계만으로도}} 말이야. 우리 코드와 모델은 공개적으로 이용 가능해.

================================================================================

URL:
https://arxiv.org/pdf/2403.08557.pdf

Title: OC4-ReID: Occluded Cloth-Changing Person Re-Identification

Original Abstract:
The study of Cloth-Changing Person Re-identification (CC-ReID) focuses on retrieving specific pedestrians when their clothing has changed, typically under the assumption that the entire pedestrian images are visible. Pedestrian images in real-world scenarios, however, are often partially obscured by obstacles, presenting a significant challenge to existing CC-ReID systems. In this paper, we introduce a more challenging task termed Occluded Cloth-Changing Person Re-Identification (OC4-ReID), which simultaneously addresses two challenges of clothing changes and occlusion. Concretely, we construct two new datasets, Occ-LTCC and Occ-PRCC, based on original CC-ReID datasets to include random occlusions of key pedestrians components (e.g., head, torso). Moreover, a novel benchmark is proposed for OC4-ReID incorporating a Train-Test Micro Granularity Screening (T2MGS) module to mitigate the influence of occlusion and proposing a Part-Robust Triplet (PRT) loss for partial features learning. Comprehensive experiments on the proposed datasets, as well as on two CC-ReID benchmark datasets demonstrate the superior performance of proposed method against other state-of-the-art methods. The codes and datasets are available at: this https URL.

Translated Abstract:
CC-ReID, 즉 옷 갈아입는 사람 재식별 연구는 옷이 바뀌었을 때 특정 보행자를 찾는 데 초점을 맞추고 있어. 일반적으로는 보행자 이미지를 전체적으로 볼 수 있다고 가정해. 하지만 실제 상황에서는 장애물 때문에 보행자 이미지가 부분적으로 가려지는 경우가 많아서, 기존 CC-ReID 시스템에는 큰 도전이 돼.

이 논문에서는 Occluded Cloth-Changing Person Re-Identification (OC4-ReID)라는 더 어려운 작업을 소개해. 이 작업은 옷 변화와 가림 현상의 두 가지 문제를 동시에 다뤄. 구체적으로, 우리는 원래 CC-ReID 데이터셋을 기반으로 두 개의 새로운 데이터셋, Occ-LTCC와 Occ-PRCC를 만들어서 주요 보행자 구성 요소(예: 머리, 몸통)가 무작위로 가려지는 상황을 포함했어.

또한, OC4-ReID를 위한 새로운 벤치마크를 제안해. 여기에는 가림의 영향을 줄이기 위한 Train-Test Micro Granularity Screening (T2MGS) 모듈과 부분 특징 학습을 위한 Part-Robust Triplet (PRT) 손실 함수가 포함돼. 제안한 데이터셋과 두 개의 CC-ReID 벤치마크 데이터셋에서 종합적인 실험을 통해, 우리 방법이 다른 최신 방법들보다 우수한 성능을 보였다는 걸 증명했어. 코드와 데이터셋은 이 링크에서 확인할 수 있어: 이 https URL.

================================================================================

URL:
https://arxiv.org/pdf/2403.12839.pdf

Title: Global-guided Focal Neural Radiance Field for Large-scale Scene Rendering

Original Abstract:
Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes. However, their limited model capacity typically results in blurred rendering results. Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in geometry and appearance across the scene. Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity. In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy. The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders. Leveraging this two-stage architecture, sub-encoders only need fine-tuning based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency. Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes. Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes. We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets. Our project page: this https URL

Translated Abstract:
신경 방사장(NeRF)은 최근 대규모 장면 렌더링에 사용되고 있어. 하지만 모델 용량이 제한적이라서 렌더링 결과가 흐릿해지는 문제가 있어. 기존 대규모 NeRF는 이 문제를 장면을 블록으로 나누고, 각 블록을 별도의 서브 NeRF로 처리하는 방식으로 해결하려고 해. 이렇게 독립적으로 훈련된 서브 NeRF들은 장면의 기하학적 일관성과 외관에서 차이를 초래하게 돼. 그래서 모델 용량을 늘려도 렌더링 품질이 크게 개선되지 않아.

이번 연구에서는 대규모 장면의 고충실도 렌더링을 달성하는 글로벌 가이드 초점 신경 방사장(GF-NeRF)을 제안해. GF-NeRF는 두 단계(글로벌과 초점) 아키텍처와 글로벌 가이드 훈련 전략을 활용해. 글로벌 단계에서는 전체 장면의 연속적 표현을 얻고, 초점 단계에서는 장면을 여러 블록으로 나누고 각각을 다른 서브 인코더로 처리해.

이 두 단계 아키텍처를 활용하면, 서브 인코더는 글로벌 인코더를 기반으로 미세 조정만 하면 돼서 초점 단계에서 훈련 복잡성이 줄어들면서도 장면 전체의 일관성을 유지할 수 있어. 글로벌 단계에서 얻은 공간 정보와 오류 정보도 서브 인코더가 중요한 영역에 집중하고 대규모 장면의 세부 사항을 잘 포착하는 데 도움을 줘.

특히, 우리 방법은 목표 장면에 대한 사전 지식을 필요로 하지 않아서, GF-NeRF가 거리 뷰나 공중 뷰 같은 다양한 대규모 장면 유형에 적응할 수 있어. 우리는 우리 방법이 다양한 대규모 데이터셋에서 고충실도이고 자연스러운 렌더링 결과를 달성한다는 것을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2404.11256.pdf

Title: MMCBE: Multi-modality Dataset for Crop Biomass Prediction and Beyond

Original Abstract:
Crop biomass, a critical indicator of plant growth, health, and productivity, is invaluable for crop breeding programs and agronomic research. However, the accurate and scalable quantification of crop biomass remains inaccessible due to limitations in existing measurement methods. One of the obstacles impeding the advancement of current crop biomass prediction methodologies is the scarcity of publicly available datasets. Addressing this gap, we introduce a new dataset in this domain, i.e. Multi-modality dataset for crop biomass estimation (MMCBE). Comprising 216 sets of multi-view drone images, coupled with LiDAR point clouds, and hand-labelled ground truth, MMCBE represents the first multi-modality one in the field. This dataset aims to establish benchmark methods for crop biomass quantification and foster the development of vision-based approaches. We have rigorously evaluated state-of-the-art crop biomass estimation methods using MMCBE and ventured into additional potential applications, such as 3D crop reconstruction from drone imagery and novel-view rendering. With this publication, we are making our comprehensive dataset available to the broader community.

Translated Abstract:
작물 생체량은 식물의 성장, 건강, 생산성을 나타내는 중요한 지표로, 농작물 육종 프로그램과 농업 연구에 매우 중요해. 하지만 현재의 측정 방법 때문에 작물 생체량을 정확하고 쉽게 측정하는 게 어렵지. 현재의 작물 생체량 예측 방법이 발전하지 못하는 이유 중 하나는 공개된 데이터셋이 부족하다는 거야.

이 문제를 해결하기 위해, 우리는 새로운 데이터셋을 만들었어. 이 데이터셋의 이름은 작물 생체량 추정을 위한 다중 모달리티 데이터셋(MMCBE)야. 이 데이터셋은 216개의 다중 시점 드론 이미지와 LiDAR 포인트 구름, 그리고 수동으로 라벨링된 실제 데이터를 포함하고 있어. MMCBE는 이 분야에서 첫 번째 다중 모달리티 데이터셋이야. 

이 데이터셋의 목표는 작물 생체량 측정을 위한 기준 방법을 설정하고, 비전 기반 접근법의 개발을 촉진하는 거야. 우리는 MMCBE를 사용해서 최신 작물 생체량 추정 방법들을 엄격하게 평가했고, 드론 이미지를 활용한 3D 작물 재구성이나 새로운 시점 렌더링 같은 추가적인 가능성도 탐구했어. 이번 발표를 통해, 우리는 우리 데이터셋을 더 넓은 커뮤니티에 공개하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2404.16571.pdf

Title: MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth Estimation of Endoscopic Images

Original Abstract:
Photometric constraint is indispensable for self-supervised monocular depth estimation. It involves warping a source image onto a target view using estimated depth&pose, and then minimizing the difference between the warped and target images. However, the endoscopic built-in light causes significant brightness fluctuations, and thus makes the photometric constraint unreliable. Previous efforts only mitigate this relying on extra models to calibrate image brightness. In this paper, we propose MonoPCC to address the brightness inconsistency radically by reshaping the photometric constraint into a cycle form. Instead of only warping the source image, MonoPCC constructs a closed loop consisting of two opposite forward-backward warping paths: from target to source and then back to target. Thus, the target image finally receives an image cycle-warped from itself, which naturally makes the constraint invariant to brightness changes. Moreover, MonoPCC transplants the source image's phase-frequency into the intermediate warped image to avoid structure lost, and also stabilizes the training via an exponential moving average (EMA) strategy to avoid frequent changes in the forward warping. The comprehensive and extensive experimental results on four endoscopic datasets demonstrate that our proposed MonoPCC shows a great robustness to the brightness inconsistency, and exceeds other state-of-the-arts by reducing the absolute relative error by at least 7.27%, 9.38%, 9.90% and 3.17%, respectively.

Translated Abstract:
광학 제약은 자기 지도식 단안 깊이 추정에 필수적이야. 이건 추정된 깊이와 자세를 사용해서 소스 이미지를 타겟 뷰로 변형하고, 그 변형된 이미지와 타겟 이미지의 차이를 최소화하는 방식이야. 하지만 내시경의 내장 조명 때문에 밝기가 많이 변동해서 광학 제약이 신뢰할 수 없게 돼. 이전 연구들은 이미지 밝기를 보정하기 위해 추가 모델에 의존해서 이 문제를 완화하려고 했어.

이번 논문에서는 MonoPCC라는 방법을 제안해. 이 방법은 광학 제약을 사이클 형태로 바꿔서 밝기 불일치를 근본적으로 해결해. MonoPCC는 단순히 소스 이미지를 변형하는 게 아니라, 타겟에서 소스로, 다시 타겟으로 가는 두 개의 반대 방향 변형 경로로 이루어진 폐쇄 루프를 만들어. 이렇게 해서 최종적으로 타겟 이미지는 자기 자신으로부터 사이클 형태로 변형된 이미지를 받게 되니까, 밝기 변화에 강하게 만들어져.

게다가 MonoPCC는 소스 이미지의 주파수 정보를 중간 변형 이미지에 이식해서 구조 손실을 막고, 지수 이동 평균(EMA) 전략을 통해 훈련을 안정화시켜서 변형 과정에서 잦은 변화를 피해. 네 개의 내시경 데이터셋에서 진행한 종합적이고 광범위한 실험 결과에 따르면, 제안한 MonoPCC는 밝기 불일치에 대해 뛰어난 강인성을 보여주고 다른 최신 기술들보다 절대 상대 오차를 최소 7.27%, 9.38%, 9.90%, 3.17% 줄이는 성능을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2405.06468.pdf

Title: Pseudo-Prompt Generating in Pre-trained Vision-Language Models for Multi-Label Medical Image Classification

Original Abstract:
The task of medical image recognition is notably complicated by the presence of varied and multiple pathological indications, presenting a unique challenge in multi-label classification with unseen labels. This complexity underlines the need for computer-aided diagnosis methods employing multi-label zero-shot learning. Recent advancements in pre-trained vision-language models (VLMs) have showcased notable zero-shot classification abilities on medical images. However, these methods have limitations on leveraging extensive pre-trained knowledge from broader image datasets, and often depend on manual prompt construction by expert radiologists. By automating the process of prompt tuning, prompt learning techniques have emerged as an efficient way to adapt VLMs to downstream tasks. Yet, existing CoOp-based strategies fall short in performing class-specific prompts on unseen categories, limiting generalizability in fine-grained scenarios. To overcome these constraints, we introduce a novel prompt generation approach inspirited by text generation in natural language processing (NLP). Our method, named Pseudo-Prompt Generating (PsPG), capitalizes on the priori knowledge of multi-modal features. Featuring a RNN-based decoder, PsPG autoregressively generates class-tailored embedding vectors, i.e., pseudo-prompts. Comparative evaluations on various multi-label chest radiograph datasets affirm the superiority of our approach against leading medical vision-language and multi-label prompt learning methods. The source code is available at this https URL

Translated Abstract:
의료 이미지 인식 작업은 다양한 병리학적 징후 때문에 꽤 복잡해. 특히, 보지 못한 레이블이 있는 다중 레이블 분류는 독특한 도전 과제가 돼. 이런 복잡성 때문에 다중 레이블 제로샷 학습을 활용한 컴퓨터 보조 진단 방법이 필요해. 최근에 사전 훈련된 비전-언어 모델(VLMs)이 의료 이미지에서 제로샷 분류 능력을 보여줬지만, 넓은 이미지 데이터셋에서 얻은 방대한 사전 훈련 지식을 잘 활용하지 못해. 또, 전문가 방사선의사들이 수동으로 프롬프트를 만들어야 하는 경우가 많아.

프롬프트 조정 과정을 자동화함으로써, 프롬프트 학습 기술이 VLMs를 하위 작업에 적응시키는 효율적인 방법으로 떠올랐어. 하지만 기존의 CoOp 기반 전략은 보지 못한 카테고리에서 클래스별 프롬프트를 수행하는 데 한계가 있어서, 세밀한 상황에서 일반화하기 어려워. 이런 제약을 극복하기 위해 우리는 자연어 처리(NLP)에서 텍스트 생성을 영감을 받아 새로운 프롬프트 생성 방법을 제안해. 

우리의 방법은 Pseudo-Prompt Generating (PsPG)라고 부르며, 다중 모달 특징에 대한 사전 지식을 활용해. RNN 기반 디코더를 특징으로 하는 PsPG는 클래스 맞춤형 임베딩 벡터, 즉, 의사 프롬프트를 자가 회귀적으로 생성해. 다양한 다중 레이블 흉부 X선 데이터셋에서 비교 평가를 해본 결과, 우리의 방법이 주요 의료 비전-언어 및 다중 레이블 프롬프트 학습 방법보다 우수하다는 것을 확인했어. 소스 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2405.19331.pdf

Title: NPGA: Neural Parametric Gaussian Avatars

Original Abstract:
The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives. Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance. In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings. We build our method around 3D Gaussian splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds. In contrast to previous work, we condition our avatars' dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering. All remaining fine-scale, expression-dependent details are learned from the multi-view videos. For increased representational capacity of our avatars, we propose per-Gaussian latent features that condition each primitives dynamic behavior. To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos.

Translated Abstract:
인간 머리의 고충실도 디지털 버전을 만드는 것은 가상 요소를 우리의 일상생활에 더 잘 통합하기 위한 중요한 단계야. 이런 아바타를 만드는 건 쉽지 않은 연구 문제인데, 사진 같은 사실감과 실시간 렌더링 성능에 대한 높은 수요 때문이야.

이번 연구에서는 Neural Parametric Gaussian Avatars (NPGA)라는 데이터 기반 접근 방식을 제안해. 이 방법은 여러 각도에서 촬영한 비디오를 사용해 고충실도, 조절 가능한 아바타를 만드는 거야. 우리는 3D 가우시안 스플래팅을 사용해서 효율적인 렌더링을 하고, 포인트 클라우드의 토폴로지 유연성을 유지하는 방식으로 방법을 구성했어.

이전 연구와는 다르게, 우리는 아바타의 동작을 메쉬 기반의 3DMM이 아니라 신경 매개 헤드 모델(NPHM)의 풍부한 표현 공간에 기반해 조정해. 이를 위해 NPHM의 역변형 필드를 전방 변형으로 정제해서 래스터화 기반 렌더링에 맞게 만들었어. 나머지 미세한 표현 의존 세부 사항은 다중 시점 비디오에서 학습해.

아바타의 표현 능력을 높이기 위해, 각 기본 요소의 동적 행동을 조절하는 개별 가우시안 잠재 특징을 제안해. 이 증가된 동적 표현력을 규제하기 위해, 우리는 잠재 특징과 예측된 동작에 라플라시안 항을 추가했어. 우리 방법을 공개된 NeRSemble 데이터셋에서 평가했는데, NPGA가 자기 재현 작업에서 이전의 최첨단 아바타보다 2.6 PSNR이 더 뛰어난 성능을 보여줬어. 게다가, 실제 모노큘러 비디오에서 정확한 애니메이션 기능도 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2406.11445.pdf

Title: Solving the Inverse Problem of Electrocardiography for Cardiac Digital Twins: A Survey

Original Abstract:
Cardiac digital twins (CDTs) are personalized virtual representations used to understand complex cardiac mechanisms. A critical component of CDT development is solving the ECG inverse problem, which enables the reconstruction of cardiac sources and the estimation of patient-specific electrophysiology (EP) parameters from surface ECG data. Despite challenges from complex cardiac anatomy, noisy ECG data, and the ill-posed nature of the inverse problem, recent advances in computational methods have greatly improved the accuracy and efficiency of ECG inverse inference, strengthening the fidelity of CDTs. This paper aims to provide a comprehensive review of the methods of solving ECG inverse problem, the validation strategies, the clinical applications, and future perspectives. For the methodologies, we broadly classify state-of-the-art approaches into two categories: deterministic and probabilistic methods, including both conventional and deep learning-based techniques. Integrating physics laws with deep learning models holds promise, but challenges such as capturing dynamic electrophysiology accurately, accessing accurate domain knowledge, and quantifying prediction uncertainty persist. Integrating models into clinical workflows while ensuring interpretability and usability for healthcare professionals is essential. Overcoming these challenges will drive further research in CDTs.

Translated Abstract:
심장 디지털 트윈(CDTs)은 복잡한 심장 메커니즘을 이해하기 위해 개인화된 가상 모델이야. CDT 개발의 중요한 부분은 ECG 역문제를 해결하는 건데, 이걸 통해 심장 소스를 재구성하고 표면 ECG 데이터로부터 환자 맞춤형 전기 생리학(EP) 매개변수를 추정할 수 있어. 복잡한 심장 해부학, 시끄러운 ECG 데이터, 그리고 역문제의 잘못 정립된 특성 때문에 어려움이 있지만, 최근의 계산 방법 발전 덕분에 ECG 역 추론의 정확성과 효율성이 크게 향상되었어. 이로 인해 CDT의 신뢰성이 강화됐지.

이 논문은 ECG 역문제를 해결하는 방법들, 검증 전략, 임상 응용, 그리고 향후 전망에 대한 포괄적인 리뷰를 제공하는 게 목표야. 방법론에 대해선 최첨단 접근 방식을 크게 두 가지로 나눌 수 있어: 결정론적 방법과 확률론적 방법인데, 여기엔 전통적인 기술과 딥러닝 기반 기술이 모두 포함돼.

물리 법칙을 딥러닝 모델과 결합하는 게 가능성이 있지만, 동적인 전기 생리학을 정확하게 잡아내거나, 정확한 도메인 지식에 접근하거나, 예측 불확실성을 정량화하는 데는 여전히 어려움이 있어. 모델을 임상 작업 흐름에 통합하면서도 의료 전문가들이 해석하고 사용할 수 있도록 하는 게 중요해. 이러한 문제들을 해결하는 게 CDT 연구를 더 발전시키는 데 큰 도움이 될 거야.

================================================================================

URL:
https://arxiv.org/pdf/2406.17323.pdf

Title: XAMI -- A Benchmark Dataset for Artefact Detection in XMM-Newton Optical Images

Original Abstract:
Reflected or scattered light produce artefacts in astronomical observations that can negatively impact the scientific study. Hence, automated detection of these artefacts is highly beneficial, especially with the increasing amounts of data gathered. Machine learning methods are well-suited to this problem, but currently there is a lack of annotated data to train such approaches to detect artefacts in astronomical observations. In this work, we present a dataset of images from the XMM-Newton space telescope Optical Monitoring camera showing different types of artefacts. We hand-annotated a sample of 1000 images with artefacts which we use to train automated ML methods. We further demonstrate techniques tailored for accurate detection and masking of artefacts using instance segmentation. We adopt a hybrid approach, combining knowledge from both convolutional neural networks (CNNs) and transformer-based models and use their advantages in segmentation. The presented method and dataset will advance artefact detection in astronomical observations by providing a reproducible baseline. All code and data are made available (this https URL and this https URL).

Translated Abstract:
천문 관측에서 반사되거나 산란된 빛은 인공물(아티팩트)을 만들어내서 과학적 연구에 부정적인 영향을 미칠 수 있어. 그래서 이런 인공물을 자동으로 감지하는 게 매우 중요해, 특히 데이터 양이 점점 많아지고 있으니까. 

기계 학습 방법이 이 문제에 잘 맞긴 한데, 현재 천문 관측에서 인공물을 감지하기 위해 학습할 수 있는 주석이 달린 데이터가 부족해. 그래서 우리는 XMM-Newton 우주 망원경의 광학 모니터링 카메라에서 찍힌 다양한 인공물 이미지 데이터셋을 만들었어. 우리는 1000개의 인공물 이미지 샘플에 손으로 주석을 달아서 자동화된 기계 학습 방법을 학습하는 데 사용할 거야. 

또한, 인스턴스 분할(instance segmentation)을 활용해서 인공물을 정확하게 감지하고 마스킹하는 기술도 보여줄 거야. 컨볼루션 신경망(CNN)과 변환기 기반 모델의 지식을 결합한 하이브리드 접근 방식을 사용하고, 세분화에서 이들의 장점을 활용할 거야. 

이 방법과 데이터셋은 천문 관측에서 인공물 감지를 발전시켜서 재현 가능한 기준을 제공할 거야. 모든 코드와 데이터는 공개되어 있어 (이 https URL과 이 https URL).

================================================================================

URL:
https://arxiv.org/pdf/2406.18140.pdf

Title: Exclusive Style Removal for Cross Domain Novel Class Discovery

Original Abstract:
As a promising field in open-world learning, \textit{Novel Class Discovery} (NCD) is usually a task to cluster unseen novel classes in an unlabeled set based on the prior knowledge of labeled data within the same domain. However, the performance of existing NCD methods could be severely compromised when novel classes are sampled from a different distribution with the labeled ones. In this paper, we explore and establish the solvability of NCD in cross domain setting with the necessary condition that style information must be removed. Based on the theoretical analysis, we introduce an exclusive style removal module for extracting style information that is distinctive from the baseline features, thereby facilitating inference. Moreover, this module is easy to integrate with other NCD methods, acting as a plug-in to improve performance on novel classes with different distributions compared to the seen labeled set. Additionally, recognizing the non-negligible influence of different backbones and pre-training strategies on the performance of the NCD methods, we build a fair benchmark for future NCD research. Extensive experiments on three common datasets demonstrate the effectiveness of our proposed module.

Translated Abstract:
오픈 월드 학습에서 유망한 분야인 \textit{새로운 클래스 발견} (NCD)은 보통 같은 도메인 내의 레이블이 있는 데이터의 기존 지식을 바탕으로, 레이블이 없는 세트에서 보지 못한 새로운 클래스를 클러스터링하는 작업이야. 하지만 기존 NCD 방법들은 새로운 클래스가 레이블 있는 데이터와 다른 분포에서 샘플링되면 성능이 크게 저하될 수 있어.

이 논문에서는 크로스 도메인 환경에서 NCD의 해결 가능성을 탐구하고, 스타일 정보를 제거해야 한다는 필수 조건을 세웠어. 이론적 분석을 바탕으로, 기준 피처와 구별되는 스타일 정보를 추출하기 위해 독점적인 스타일 제거 모듈을 소개해. 이 모듈은 추론을 더 쉽게 만들어 주는 역할을 해. 게다가, 이 모듈은 다른 NCD 방법과 통합하기도 쉬워서, 보지 못한 클래스의 성능을 개선하는 플러그인 역할을 할 수 있어.

또한, 서로 다른 백본과 사전 훈련 전략이 NCD 방법의 성능에 미치는 영향을 무시할 수 없다는 점을 인식하고, 앞으로의 NCD 연구를 위한 공정한 벤치마크를 만들었어. 세 가지 일반적인 데이터셋에서 진행한 광범위한 실험을 통해, 우리가 제안한 모듈의 효과를 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2407.08061.pdf

Title: Geospecific View Generation -- Geometry-Context Aware High-resolution Ground View Inference from Satellite Views

Original Abstract:
Predicting realistic ground views from satellite imagery in urban scenes is a challenging task due to the significant view gaps between satellite and ground-view images. We propose a novel pipeline to tackle this challenge, by generating geospecifc views that maximally respect the weak geometry and texture from multi-view satellite images. Different from existing approaches that hallucinate images from cues such as partial semantics or geometry from overhead satellite images, our method directly predicts ground-view images at geolocation by using a comprehensive set of information from the satellite image, resulting in ground-level images with a resolution boost at a factor of ten or more. We leverage a novel building refinement method to reduce geometric distortions in satellite data at ground level, which ensures the creation of accurate conditions for view synthesis using diffusion networks. Moreover, we proposed a novel geospecific prior, which prompts distribution learning of diffusion models to respect image samples that are closer to the geolocation of the predicted images. We demonstrate our pipeline is the first to generate close-to-real and geospecific ground views merely based on satellite images.

Translated Abstract:
위성 이미지로부터 도시 장면의 현실적인 지상 뷰를 예측하는 건 어려운 일이야. 왜냐하면 위성과 지상 이미지 간의 큰 차이가 있기 때문이지. 우리는 이 문제를 해결하기 위해 새로운 방법을 제안해. 이 방법은 여러 위성 이미지에서 얻은 약한 기하학과 텍스처를 최대한 존중하면서 지리적 특성에 맞는 뷰를 생성해.

기존 방법들은 위에서 본 위성 이미지의 부분적인 의미나 기하학 정보를 바탕으로 이미지를 상상하는데, 우리의 방법은 위성 이미지에서 얻은 다양한 정보를 사용해서 직접적으로 지리 위치에 맞는 지상 이미지를 예측해. 이 덕분에 지상 이미지의 해상도가 10배 이상 향상돼.

우리는 새로운 건물 정제 방법을 활용해 위성 데이터의 기하학적 왜곡을 줄여. 이 방법은 확산 네트워크를 사용해 뷰 합성을 위한 정확한 조건을 만들어줘. 그리고 우리는 새로운 지리적 사전도 제안했어. 이 건 확산 모델의 학습을 도와서 예측한 이미지의 지리적 위치에 더 가까운 이미지 샘플을 존중하게 해.

우리의 방법은 위성 이미지만으로 현실적이고 지리적 특성이 있는 지상 뷰를 생성하는 첫 번째 방법임을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2407.12568.pdf

Title: LTRL: Boosting Long-tail Recognition via Reflective Learning

Original Abstract:
In real-world scenarios, where knowledge distributions exhibit long-tail. Humans manage to master knowledge uniformly across imbalanced distributions, a feat attributed to their diligent practices of reviewing, summarizing, and correcting errors. Motivated by this learning process, we propose a novel learning paradigm, called reflecting learning, in handling long-tail recognition. Our method integrates three processes for reviewing past predictions during training, summarizing and leveraging the feature relation across classes, and correcting gradient conflict for loss functions. These designs are lightweight enough to plug and play with existing long-tail learning methods, achieving state-of-the-art performance in popular long-tail visual benchmarks. The experimental results highlight the great potential of reflecting learning in dealing with long-tail recognition.

Translated Abstract:
현실 세계에서는 지식 분포가 긴 꼬리를 가지는 경우가 많아. 사람들은 불균형한 분포 속에서도 지식을 고루 익히는 데, 이는 그들이 복습하고 요약하고 오류를 수정하는 꾸준한 연습 덕분이야. 이런 학습 과정을 바탕으로, 우리는 긴 꼬리 인식을 다루기 위해 '반영 학습(reflecting learning)'이라는 새로운 학습 패러다임을 제안해.

우리 방법은 세 가지 과정을 통합해. 첫째, 훈련 중에 과거 예측을 복습하는 과정이고, 둘째, 클래스 간의 특징 관계를 요약하고 활용하는 거야. 마지막으로, 손실 함수에 대한 그래디언트 충돌을 수정하는 과정이야. 이 디자인은 기존의 긴 꼬리 학습 방법과 쉽게 결합할 수 있을 만큼 가벼워. 그래서 인기 있는 긴 꼬리 시각 벤치마크에서 최첨단 성능을 달성했어. 실험 결과는 반영 학습이 긴 꼬리 인식을 다루는 데 큰 잠재력을 가지고 있음을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2407.12632.pdf

Title: CerberusDet: Unified Multi-Dataset Object Detection

Original Abstract:
Conventional object detection models are usually limited by the data on which they were trained and by the category logic they define. With the recent rise of Language-Visual Models, new methods have emerged that are not restricted to these fixed categories. Despite their flexibility, such Open Vocabulary detection models still fall short in accuracy compared to traditional models with fixed classes. At the same time, more accurate data-specific models face challenges when there is a need to extend classes or merge different datasets for training. The latter often cannot be combined due to different logics or conflicting class definitions, making it difficult to improve a model without compromising its performance. In this paper, we introduce CerberusDet, a framework with a multi-headed model designed for handling multiple object detection tasks. Proposed model is built on the YOLO architecture and efficiently shares visual features from both backbone and neck components, while maintaining separate task heads. This approach allows CerberusDet to perform very efficiently while still delivering optimal results. We evaluated the model on the PASCAL VOC dataset and Objects365 dataset to demonstrate its abilities. CerberusDet achieved state-of-the-art results with 36% less inference time. The more tasks are trained together, the more efficient the proposed model becomes compared to running individual models sequentially. The training and inference code, as well as the model, are available as open-source (this https URL).

Translated Abstract:
전통적인 객체 탐지 모델은 보통 훈련된 데이터와 정의된 카테고리에 제한을 받는다. 최근에 언어-비주얼 모델이 발전하면서 이런 고정된 카테고리에 얽매이지 않는 새로운 방법들이 등장했다. 하지만 이런 오픈 보캐뷸러리 탐지 모델은 여전히 고정 클래스 모델에 비해 정확도가 떨어진다.  

한편, 더 정확한 데이터 특정 모델은 클래스 확장이나 서로 다른 데이터셋을 합쳐서 훈련할 필요가 있을 때 문제에 직면한다. 서로 다른 논리나 충돌하는 클래스 정의 때문에 데이터셋을 합치기 힘든 경우가 많아서 성능을 유지하면서 모델을 개선하기 어렵다.  

이 논문에서는 여러 객체 탐지 작업을 처리할 수 있는 다중 헤드 모델인 CerberusDet 프레임워크를 소개한다. 제안된 모델은 YOLO 아키텍처를 기반으로 하고, 백본과 넥 컴포넌트에서 시각적 특징을 효율적으로 공유하면서 각 작업에 대해 별도의 헤드를 유지한다. 이 방식 덕분에 CerberusDet는 매우 효율적으로 작동하면서도 최적의 결과를 제공한다.  

PASCAL VOC 데이터셋과 Objects365 데이터셋에서 모델을 평가하여 그 능력을 입증했다. CerberusDet는 36% 더 적은 추론 시간으로 최신 기술 수준의 결과를 달성했다. 여러 작업을 함께 훈련할수록 제안된 모델이 개별 모델을 순차적으로 실행하는 것보다 더 효율적으로 작동한다. 훈련과 추론 코드, 그리고 모델은 오픈소스로 제공된다.

================================================================================

URL:
https://arxiv.org/pdf/2407.13863.pdf

Title: A Closer Look at GAN Priors: Exploiting Intermediate Features for Enhanced Model Inversion Attacks

Original Abstract:
Model Inversion (MI) attacks aim to reconstruct privacy-sensitive training data from released models by utilizing output information, raising extensive concerns about the security of Deep Neural Networks (DNNs). Recent advances in generative adversarial networks (GANs) have contributed significantly to the improved performance of MI attacks due to their powerful ability to generate realistic images with high fidelity and appropriate semantics. However, previous MI attacks have solely disclosed private information in the latent space of GAN priors, limiting their semantic extraction and transferability across multiple target models and datasets. To address this challenge, we propose a novel method, Intermediate Features enhanced Generative Model Inversion (IF-GMI), which disassembles the GAN structure and exploits features between intermediate blocks. This allows us to extend the optimization space from latent code to intermediate features with enhanced expressive capabilities. To prevent GAN priors from generating unrealistic images, we apply a L1 ball constraint to the optimization process. Experiments on multiple benchmarks demonstrate that our method significantly outperforms previous approaches and achieves state-of-the-art results under various settings, especially in the out-of-distribution (OOD) scenario. Our code is available at: this https URL

Translated Abstract:
모델 역전(MI) 공격은 공개된 모델의 출력 정보를 이용해 개인 정보가 포함된 훈련 데이터를 재구성하려고 해. 이 때문에 딥 뉴럴 네트워크(DNN)의 보안에 대한 우려가 커지고 있어. 최근 생성적 적대 신경망(GAN)의 발전 덕분에 MI 공격의 성능이 크게 향상됐어. GAN은 현실감 있고 적절한 의미를 가진 이미지를 생성하는 능력이 뛰어나거든.

하지만 이전의 MI 공격은 GAN의 잠재 공간에서만 개인 정보를 공개했기 때문에, 의미를 추출하거나 여러 목표 모델과 데이터셋 간에 전이하는 데 한계가 있었어. 이 문제를 해결하기 위해 우리는 Intermediate Features enhanced Generative Model Inversion (IF-GMI)라는 새로운 방법을 제안해. 이 방법은 GAN 구조를 분해하고 중간 블록 사이의 특징을 활용해. 이렇게 하면 최적화 공간을 잠재 코드에서 중간 특징으로 확장할 수 있어, 표현 능력이 더 강화된 거지.

GAN의 잠재 정보가 비현실적인 이미지를 생성하지 않도록 L1 볼 제약을 최적화 과정에 적용했어. 여러 벤치마크에서 실험한 결과, 우리의 방법이 이전 방법들보다 훨씬 뛰어난 성과를 보였고, 다양한 설정에서 최신 기술을 달성했어. 특히, 배포되지 않은(out-of-distribution, OOD) 상황에서 그 효과가 두드러져. 우리의 코드는 이 링크에서 확인할 수 있어: this https URL.

================================================================================

URL:
https://arxiv.org/pdf/2407.15589.pdf

Title: Exploring the Effectiveness of Object-Centric Representations in Visual Question Answering: Comparative Insights with Foundation Models

Original Abstract:
Object-centric (OC) representations, which represent the state of a visual scene by modeling it as a composition of objects, have the potential to be used in various downstream tasks to achieve systematic compositional generalization and facilitate reasoning. However, these claims have not been thoroughly analyzed yet. Recently, foundation models have demonstrated unparalleled capabilities across diverse domains from language to computer vision, marking them as a potential cornerstone of future research for a multitude of computational tasks. In this paper, we conduct an extensive empirical study on representation learning for downstream Visual Question Answering (VQA), which requires an accurate compositional understanding of the scene. We thoroughly investigate the benefits and trade-offs of OC models and alternative approaches including large pre-trained foundation models on both synthetic and real-world data, and demonstrate a viable way to achieve the best of both worlds. The extensiveness of our study, encompassing over 800 downstream VQA models and 15 different types of upstream representations, also provides several additional insights that we believe will be of interest to the community at large.

Translated Abstract:
객체 중심(OC) 표현은 시각 장면의 상태를 객체의 조합으로 모델링해서 표현하는 방법이야. 이 방법은 여러 가지 후속 작업에 사용될 수 있는 가능성이 있는데, 시스템적인 조합 일반화와 추론을 쉽게 해줄 수 있어. 하지만 이런 주장들은 아직 충분히 분석되지 않았어.

최근에 기초 모델들이 언어부터 컴퓨터 비전까지 다양한 분야에서 엄청난 능력을 보여줬어. 그래서 이 모델들이 앞으로의 많은 계산 작업에 중요한 기초가 될 수 있다는 기대를 받고 있어.

이번 논문에서는 후속 비주얼 질문 응답(VQA)을 위한 표현 학습에 대한 광범위한 실험 연구를 진행했어. VQA는 장면에 대한 정확한 조합 이해가 필요하거든. 우리는 OC 모델과 대체 접근법, 그리고 대규모 사전 학습된 기초 모델의 장점과 단점을 합성 데이터와 실제 데이터 모두에서 자세히 조사했어. 그리고 두 가지 방법의 장점을 모두 살릴 수 있는 방법을 보여줬어.

우리 연구는 800개 이상의 후속 VQA 모델과 15가지의 다양한 상류 표현을 포함하고 있어서, 커뮤니티에 도움이 될 만한 여러 추가적인 통찰도 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2407.15794.pdf

Title: Disentangling spatio-temporal knowledge for weakly supervised object detection and segmentation in surgical video

Original Abstract:
Weakly supervised video object segmentation (WSVOS) enables the identification of segmentation maps without requiring an extensive training dataset of object masks, relying instead on coarse video labels indicating object presence. Current state-of-the-art methods either require multiple independent stages of processing that employ motion cues or, in the case of end-to-end trainable networks, lack in segmentation accuracy, in part due to the difficulty of learning segmentation maps from videos with transient object presence. This limits the application of WSVOS for semantic annotation of surgical videos where multiple surgical tools frequently move in and out of the field of view, a problem that is more difficult than typically encountered in WSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks (VDST-Net), a framework to disentangle spatiotemporal information using semi-decoupled knowledge distillation to predict high-quality class activation maps (CAMs). A teacher network designed to resolve temporal conflicts when specifics about object location and timing in the video are not provided works with a student network that integrates information over time by leveraging temporal dependencies. We demonstrate the efficacy of our framework on a public reference dataset and on a more challenging surgical video dataset where objects are, on average, present in less than 60\% of annotated frames. Our method outperforms state-of-the-art techniques and generates superior segmentation masks under video-level weak supervision.

Translated Abstract:
약한 감독 하의 비디오 객체 분할(WSVOS)은 객체 마스크의 방대한 훈련 데이터셋 없이도 분할 맵을 식별할 수 있게 해줘. 대신 객체가 존재한다는 대충의 비디오 레이블에 의존해. 현재의 최신 기술들은 보통 여러 독립적인 처리 단계를 필요로 하거나, 끝에서 끝으로 훈련 가능한 네트워크의 경우에는 분할 정확도가 떨어져. 이는 일시적으로 존재하는 객체의 비디오에서 분할 맵을 배우는 게 어렵기 때문이야. 이로 인해 WSVOS의 활용이 제한되는데, 특히 여러 수술 도구들이 자주 시야에 들어오고 나가는 수술 비디오의 의미 레이블 작업에서는 더 힘들어.

이 논문에서는 비디오 시공간 분리 네트워크(VDST-Net)를 소개해. 이 프레임워크는 반분리 지식 증류를 사용해서 시공간 정보를 분리하고 고품질 클래스 활성화 맵(CAM)을 예측해. 객체 위치와 비디오의 시간에 대한 구체적인 정보가 없을 때, 시간적 갈등을 해결하도록 설계된 교사 네트워크가 있고, 이 네트워크는 시간적 의존성을 활용해 정보를 통합하는 학생 네트워크와 함께 작동해. 

우리는 우리의 프레임워크가 공개 참조 데이터셋과, 평균적으로 주석이 달린 프레임의 60% 미만에서 객체가 존재하는 더 도전적인 수술 비디오 데이터셋에서 효과적임을 보여줘. 우리의 방법은 최신 기술들보다 성능이 뛰어나고, 비디오 수준의 약한 감독 하에서도 우수한 분할 마스크를 생성해.

================================================================================

URL:
https://arxiv.org/pdf/2408.05008.pdf

Title: FlowDreamer: exploring high fidelity text-to-3D generation via rectified flow

Original Abstract:
Recent advances in text-to-3D generation have made significant progress. In particular, with the pretrained diffusion models, existing methods predominantly use Score Distillation Sampling (SDS) to train 3D models such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3D GS). However, a hurdle is that they often encounter difficulties with over-smoothing textures and over-saturating colors. The rectified flow model - which utilizes a simple ordinary differential equation (ODE) to represent a linear trajectory - shows promise as an alternative prior to text-to-3D generation. It learns a time-independent vector field, thereby reducing the ambiguity in 3D model update gradients that are calculated using time-dependent scores in the SDS framework. In light of this, we first develop a mathematical analysis to seamlessly integrate SDS with rectified flow model, paving the way for our initial framework known as Vector Field Distillation Sampling (VFDS). However, empirical findings indicate that VFDS still results in over-smoothing outcomes. Therefore, we analyze the grounding reasons for such a failure from the perspective of ODE trajectories. On top, we propose a novel framework, named FlowDreamer, which yields high-fidelity results with richer textual details and faster convergence. The key insight is to leverage the coupling and reversible properties of the rectified flow model to search for the corresponding noise, rather than using randomly sampled noise as in VFDS. Accordingly, we introduce a novel Unique Couple Matching (UCM) loss, which guides the 3D model to optimize along the same trajectory. Our FlowDreamer is superior in its flexibility to be applied to both NeRF and 3D GS. Extensive experiments demonstrate the high-fidelity outcomes and accelerated convergence of FlowDreamer.

Translated Abstract:
최근 텍스트-3D 생성 기술이 크게 발전했어. 특히, 사전 훈련된 확산 모델을 사용해서 기존 방법들은 주로 Score Distillation Sampling (SDS)을 이용해 Neural Radiance Fields (NeRF)와 3D Gaussian Splatting (3D GS) 같은 3D 모델을 훈련하고 있어. 하지만, 이 과정에서 텍스처가 너무 부드러워지거나 색상이 과포화되는 문제에 자주 부딪혀.

여기서, 수정된 흐름 모델이란 간단한 보통 미분 방정식(ODE)을 이용해 선형 경로를 표현하는 방법이 주목받고 있어. 이 모델은 시간에 의존하지 않는 벡터 필드를 학습해서, SDS 프레임워크에서 시간 의존적인 점수로 계산한 3D 모델 업데이트의 모호성을 줄여줘. 그래서 우리는 먼저 SDS와 수정된 흐름 모델을 매끄럽게 통합하기 위한 수학적 분석을 개발했고, 이를 바탕으로 Vector Field Distillation Sampling (VFDS)라는 초기 프레임워크를 만들었어.

하지만 실험 결과 VFDS도 여전히 과하게 부드러운 결과를 내는 문제가 있었어. 그래서 우리는 ODE 경로 관점에서 이 실패의 근본적인 이유를 분석했어. 그리고 새로운 프레임워크인 FlowDreamer를 제안했는데, 이건 더 풍부한 텍스트 세부 정보와 빠른 수렴을 통해 높은 충실도의 결과를 내. 핵심 아이디어는 수정된 흐름 모델의 결합 및 가역 속성을 활용해서 VFDS처럼 무작위로 샘플링한 노이즈 대신에 해당하는 노이즈를 검색하는 거야.

따라서 우리는 3D 모델이 같은 경로를 따라 최적화하도록 안내하는 새로운 Unique Couple Matching (UCM) 손실을 도입했어. FlowDreamer는 NeRF와 3D GS 모두에 유연하게 적용할 수 있는 점에서 우수해. 다양한 실험을 통해 FlowDreamer의 높은 충실도와 가속된 수렴을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2408.05752.pdf

Title: RTF-Q: Efficient Unsupervised Domain Adaptation with Retraining-free Quantization

Original Abstract:
Performing unsupervised domain adaptation on resource-constrained edge devices is challenging. Existing research typically adopts architecture optimization (e.g., designing slimmable networks) but requires expensive training costs. Moreover, it does not consider the considerable precision redundancy of parameters and activations. To address these limitations, we propose efficient unsupervised domain adaptation with ReTraining-Free Quantization (RTF-Q). Our approach uses low-precision quantization architectures with varying computational costs, adapting to devices with dynamic computation budgets. We subtly configure subnet dimensions and leverage weight-sharing to optimize multiple architectures within a single set of weights, enabling the use of pre-trained models from open-source repositories. Additionally, we introduce multi-bitwidth joint training and the SandwichQ rule, both of which are effective in handling multiple quantization bit-widths across subnets. Experimental results demonstrate that our network achieves competitive accuracy with state-of-the-art methods across three benchmarks while significantly reducing memory and computational costs.

Translated Abstract:
자원 제약이 있는 엣지 디바이스에서 비지도 도메인 적응을 수행하는 것은 어려워. 기존 연구들은 보통 아키텍처 최적화(예: 슬리머블 네트워크 설계)를 사용하는데, 이게 비싼 훈련 비용이 들어. 게다가, 파라미터와 활성화의 정확성 중복성을 많이 고려하지 않아. 

이런 한계를 해결하기 위해 우리는 ReTraining-Free Quantization (RTF-Q)를 이용한 효율적인 비지도 도메인 적응 방법을 제안해. 우리의 접근법은 다양한 계산 비용을 가진 저정밀 양자화 아키텍처를 사용해서 동적인 계산 예산에 맞춰 디바이스에 적응해. 우리는 서브넷 크기를 세밀하게 설정하고 가중치 공유를 활용해서 한 세트의 가중치로 여러 아키텍처를 최적화할 수 있게 해, 오픈 소스 저장소에서 가져온 사전 훈련된 모델도 사용할 수 있어.

게다가, 우리는 다중 비트 너비 공동 훈련과 SandwichQ 규칙을 도입했어. 이 두 가지 방법은 서브넷 간에 여러 양자화 비트 너비를 다루는 데 효과적이야. 실험 결과, 우리의 네트워크는 세 가지 벤치마크에서 최첨단 방법들과 경쟁력 있는 정확도를 달성하면서 메모리와 계산 비용을 크게 줄이는 것을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2408.10060.pdf

Title: Facial Wrinkle Segmentation for Cosmetic Dermatology: Pretraining with Texture Map-Based Weak Supervision

Original Abstract:
Facial wrinkle detection plays a crucial role in cosmetic dermatology. Precise manual segmentation of facial wrinkles is challenging and time-consuming, with inherent subjectivity leading to inconsistent results among graders. To address this issue, we propose two solutions. First, we build and release the first public facial wrinkle dataset, 'FFHQ-Wrinkle', an extension of the NVIDIA FFHQ dataset. It includes 1,000 images with human labels and 50,000 images with automatically generated weak labels. This dataset could serve as a foundation for the research community to develop advanced wrinkle detection algorithms. Second, we introduce a simple training strategy utilizing texture maps, applicable to various segmentation models, to detect wrinkles across the face. Our two-stage training strategy first pretrain models on a large dataset with weak labels (N=50k), or masked texture maps generated through computer vision techniques, without human intervention. We then finetune the models using human-labeled data (N=1k), which consists of manually labeled wrinkle masks. The network takes as input a combination of RGB and masked texture map of the image, comprising four channels, in finetuning. We effectively combine labels from multiple annotators to minimize subjectivity in manual labeling. Our strategies demonstrate improved segmentation performance in facial wrinkle segmentation both quantitatively and visually compared to existing pretraining methods. The dataset is available at this https URL.

Translated Abstract:
얼굴 주름 감지는 피부과에서 정말 중요한 역할을 해. 얼굴 주름을 정확하게 구분하는 건 쉽지 않고 시간이 많이 걸려. 사람마다 주관적인 차이가 있어서 평가 결과도 일관성이 없어. 이 문제를 해결하기 위해 우리는 두 가지 방법을 제안해.

첫째, 우리는 'FFHQ-Wrinkle'이라는 첫 번째 공개 얼굴 주름 데이터셋을 만들고 배포했어. 이건 NVIDIA FFHQ 데이터셋의 확장이야. 여기에는 사람의 레이블이 붙은 1,000장의 이미지와 자동으로 생성된 약한 레이블이 붙은 50,000장의 이미지가 포함돼. 이 데이터셋은 연구자들이 더 발전된 주름 감지 알고리즘을 개발하는 데 기초가 될 수 있어.

둘째, 우리는 다양한 세분화 모델에 적용 가능한 텍스처 맵을 활용한 간단한 훈련 전략을 소개해. 이 전략은 얼굴 전체의 주름을 감지하는 데 사용돼. 우리의 두 단계 훈련 전략은 먼저, 약한 레이블(N=50k)이 있는 큰 데이터셋이나 컴퓨터 비전 기술로 생성된 마스크 텍스처 맵에서 모델을 사전 훈련해. 이 과정에서는 인간의 개입이 없어. 그 다음, 우리는 사람 레이블이 붙은 데이터(N=1k)를 사용해 모델을 미세 조정해. 이 데이터는 수동으로 레이블이 붙은 주름 마스크로 구성돼. 네트워크는 RGB 이미지와 마스크된 텍스처 맵을 결합한 네 가지 채널을 입력으로 받아 미세 조정해. 우리는 여러 주석자의 레이블을 효과적으로 결합해서 수동 레이블링의 주관성을 최소화했어. 우리의 전략은 기존의 사전 훈련 방법에 비해 얼굴 주름 세분화에서 정량적으로나 시각적으로 개선된 성능을 보여줘. 데이터셋은 이 URL에서 사용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2408.11447.pdf

Title: GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation with Gaussian Splatting

Original Abstract:
We introduce GaussianOcc, a systematic method that investigates the two usages of Gaussian splatting for fully self-supervised and efficient 3D occupancy estimation in surround views. First, traditional methods for self-supervised 3D occupancy estimation still require ground truth 6D poses from sensors during training. To address this limitation, we propose Gaussian Splatting for Projection (GSP) module to provide accurate scale information for fully self-supervised training from adjacent view projection. Additionally, existing methods rely on volume rendering for final 3D voxel representation learning using 2D signals (depth maps, semantic maps), which is both time-consuming and less effective. We propose Gaussian Splatting from Voxel space (GSV) to leverage the fast rendering properties of Gaussian splatting. As a result, the proposed GaussianOcc method enables fully self-supervised (no ground truth pose) 3D occupancy estimation in competitive performance with low computational cost (2.7 times faster in training and 5 times faster in rendering). The relevant code will be available in this https URL.

Translated Abstract:
우리는 GaussianOcc라는 방법을 소개해. 이 방법은 주변 뷰에서 완전히 자기 지도 학습을 하면서 효율적으로 3D 점유율을 추정하는 두 가지 사용법을 조사해.

첫째, 기존의 자기 지도 3D 점유율 추정 방법은 훈련하는 동안 센서에서 6D 포즈의 실제 데이터가 필요해. 이 한계를 극복하기 위해, 우리는 인접한 뷰의 프로젝션에서 정확한 스케일 정보를 제공하는 Gaussian Splatting for Projection (GSP) 모듈을 제안해.

또한, 기존 방법들은 최종 3D 복셀 표현 학습을 위해 2D 신호(깊이 맵, 의미 맵)를 사용하는 볼륨 렌더링에 의존하는데, 이게 시간도 많이 걸리고 효과적이지 않아. 그래서 우리는 Gaussian Splatting from Voxel space (GSV)를 제안했어. 이 방법은 Gaussian splatting의 빠른 렌더링 특성을 활용해.

결과적으로, 제안한 GaussianOcc 방법은 완전히 자기 지도 학습으로(실제 포즈 없이) 경쟁력 있는 성능으로 3D 점유율 추정을 가능하게 해. 계산 비용이 낮아서 훈련은 2.7배, 렌더링은 5배 더 빠르다고. 관련 코드는 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2408.11559.pdf

Title: Semi-supervised 3D Semantic Scene Completion with 2D Vision Foundation Model Guidance

Original Abstract:
Accurate prediction of 3D semantic occupancy from 2D visual images is vital in enabling autonomous agents to comprehend their surroundings for planning and navigation. State-of-the-art methods typically employ fully supervised approaches, necessitating a huge labeled dataset acquired through expensive LiDAR sensors and meticulous voxel-wise labeling by human annotators. The resource-intensive nature of this annotating process significantly hampers the application and scalability of these methods. We introduce a novel semi-supervised framework to alleviate the dependency on densely annotated data. Our approach leverages 2D foundation models to generate essential 3D scene geometric and semantic cues, facilitating a more efficient training process. Our framework exhibits notable properties: (1) Generalizability, applicable to various 3D semantic scene completion approaches, including 2D-3D lifting and 3D-2D transformer methods. (2) Effectiveness, as demonstrated through experiments on SemanticKITTI and NYUv2, wherein our method achieves up to 85% of the fully-supervised performance using only 10% labeled data. This approach not only reduces the cost and labor associated with data annotation but also demonstrates the potential for broader adoption in camera-based systems for 3D semantic occupancy prediction.

Translated Abstract:
2D 이미지에서 3D 의미적 점유 예측을 정확하게 하는 건 자율 에이전트가 주변을 이해하고 계획 및 내비게이션을 하는 데 정말 중요해. 최신 방법들은 주로 완전 감독 방식에 의존하는데, 이 방식은 비싼 LiDAR 센서를 통해 얻은 큰 양의 레이블 데이터와 사람이 직접 꼼꼼하게 레이블링한 데이터를 필요로 해. 이 레이블 작업이 자원 소모가 많아서 이런 방법들의 적용과 확장이 어렵게 만들어.

우리는 밀집한 레이블 데이터 의존도를 줄이기 위해 새로운 반감독 프레임워크를 소개해. 이 방법은 2D 기초 모델을 활용해 3D 장면의 기하학적 정보와 의미적 단서를 생성해서 더 효율적인 훈련 과정을 가능하게 해. 우리의 프레임워크는 몇 가지 주목할 만한 특징이 있어: 

(1) 일반화 가능성 - 여러 3D 의미적 장면 완성 접근 방식에 적용할 수 있어, 2D-3D 리프팅이나 3D-2D 변환 방법도 포함해. 

(2) 효과성 - SemanticKITTI와 NYUv2에서 실험을 통해 입증되었고, 우리의 방법이 단 10%의 레이블 데이터로 완전 감독 성능의 85%까지 달성할 수 있어. 

이 접근 방식은 데이터 레이블링에 드는 비용과 노동을 줄일 뿐만 아니라 카메라 기반 시스템에서 3D 의미적 점유 예측의 더 넓은 채택 가능성도 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.03487.pdf

Title: ScreenMark: Watermarking Arbitrary Visual Content on Screen

Original Abstract:
Digital watermarking has demonstrated its effectiveness in protecting multimedia content. However, existing watermarking are predominantly tailored for specific media types, rendering them less effective for the protection of content displayed on computer screens, which is often multimodal and dynamic. Visual Screen Content (VSC), is particularly susceptible to theft and leakage via screenshots, a vulnerability that current watermarking methods fail to adequately address. To tackle these challenges, we propose ScreenMark, a robust and practical watermarking method designed specifically for arbitrary VSC protection. ScreenMark utilizes a three-stage progressive watermarking framework. Initially, inspired by diffusion principles, we initialize the mutual transformation between regular watermark information and irregular watermark patterns. Subsequently, these patterns are integrated with screen content using a pre-multiplication alpha blending technique, supported by a pre-trained screen decoder for accurate watermark retrieval. The progressively complex distorter enhances the robustness of the watermark in real-world screenshot scenarios. Finally, the model undergoes fine-tuning guided by a joint-level distorter to ensure optimal performance. To validate the effectiveness of ScreenMark, we compiled a dataset comprising 100,000 screenshots from various devices and resolutions. Extensive experiments across different datasets confirm the method's superior robustness, imperceptibility, and practical applicability.

Translated Abstract:
디지털 워터마크는 멀티미디어 콘텐츠를 보호하는 데 효과적이라는 걸 보여줬어. 하지만 기존의 워터마크 기술은 특정 미디어 유형에 맞춰져 있어서, 컴퓨터 화면에 표시되는 콘텐츠를 보호하는 데는 잘 안 먹혀. 컴퓨터 화면 콘텐츠(Visual Screen Content, VSC)는 특히 스크린샷을 통해 도용이나 유출에 취약한데, 지금의 워터마크 방법들은 이 문제를 제대로 해결하지 못하고 있어.

그래서 우리는 ScreenMark라는 새로운 워터마크 방법을 제안해. 이건 특히 다양한 VSC를 보호하기 위해 설계된 튼튼하고 실용적인 방법이야. ScreenMark는 세 단계의 점진적인 워터마크 프레임워크를 사용해. 처음에는 확산 원리에 영감을 받아서, 일반적인 워터마크 정보와 비정형 워터마크 패턴 간의 상호 변환을 시작해. 그 다음, 이 패턴들을 화면 콘텐츠와 결합하는데, 이때는 사전 곱셈 알파 블렌딩 기법을 써. 그리고 정확한 워터마크 복원을 위해 미리 훈련된 스크린 디코더를 활용해.

점진적으로 복잡해지는 왜곡기가 실제 스크린샷 상황에서 워터마크의 강인함을 높여줘. 마지막으로, 모델은 최적의 성능을 보장하기 위해 공동 수준의 왜곡기에 의해 미세 조정을 거쳐. ScreenMark의 효과를 검증하기 위해 10만 개의 다양한 기기와 해상도에서 찍은 스크린샷 데이터셋을 만들었어. 여러 데이터셋에 대한 폭넓은 실험을 통해 이 방법의 뛰어난 강인함, 인지 불가능성, 그리고 실용성을 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06442.pdf

Title: Prompt2Fashion: An automatically generated fashion dataset

Original Abstract:
Despite the rapid evolution and increasing efficacy of language and vision generative models, there remains a lack of comprehensive datasets that bridge the gap between personalized fashion needs and AI-driven design, limiting the potential for truly inclusive and customized fashion solutions. In this work, we leverage generative models to automatically construct a fashion image dataset tailored to various occasions, styles, and body types as instructed by users. We use different Large Language Models (LLMs) and prompting strategies to offer personalized outfits of high aesthetic quality, detail, and relevance to both expert and non-expert users' requirements, as demonstrated by qualitative analysis. Up until now the evaluation of the generated outfits has been conducted by non-expert human subjects. Despite the provided fine-grained insights on the quality and relevance of generation, we extend the discussion on the importance of expert knowledge for the evaluation of artistic AI-generated datasets such as this one. Our dataset is publicly available on GitHub at this https URL.

Translated Abstract:
빠르게 발전하고 효과성이 높아진 언어와 비전 생성 모델에도 불구하고, 개인의 패션 요구와 AI 디자인을 연결하는 포괄적인 데이터셋이 부족해. 이 때문에 진정으로 포괄적이고 맞춤화된 패션 솔루션을 만드는 데 한계가 있어.

이번 연구에서는 생성 모델을 활용해서 사용자가 요청하는 다양한 상황, 스타일, 체형에 맞춘 패션 이미지 데이터셋을 자동으로 만들었어. 여러 종류의 대형 언어 모델(LLMs)과 프롬프트 전략을 사용해서 전문가와 비전문가 모두의 요구에 맞는 고품질의 개인화된 의상을 제공해. 질적 분석을 통해 이를 입증했어.

지금까지 생성된 의상에 대한 평가는 비전문가인 사람들에 의해 이루어졌어. 생성물의 품질과 관련성에 대한 세밀한 통찰이 제공되긴 했지만, 예술적인 AI 생성 데이터셋 평가에서 전문가 지식의 중요성에 대해서도 이야기하고 싶어. 우리 데이터셋은 GitHub에서 공개되어 있어, 이 링크를 통해 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07003.pdf

Title: ODYSSEE: Oyster Detection Yielded by Sensor Systems on Edge Electronics

Original Abstract:
Oysters are a vital keystone species in coastal ecosystems, providing significant economic, environmental, and cultural benefits. As the importance of oysters grows, so does the relevance of autonomous systems for their detection and monitoring. However, current monitoring strategies often rely on destructive methods. While manual identification of oysters from video footage is non-destructive, it is time-consuming, requires expert input, and is further complicated by the challenges of the underwater environment.
To address these challenges, we propose a novel pipeline using stable diffusion to augment a collected real dataset with realistic synthetic data. This method enhances the dataset used to train a YOLOv10-based vision model. The model is then deployed and tested on an edge platform in underwater robotics, achieving a state-of-the-art 0.657 mAP@50 for oyster detection on the Aqua2 platform.

Translated Abstract:
굴은 해안 생태계에서 중요한 핵심 종으로, 경제적, 환경적, 문화적 이점을 많이 제공합니다. 굴의 중요성이 커짐에 따라, 이를 탐지하고 모니터링하기 위한 자율 시스템의 필요성도 증가하고 있습니다. 하지만 현재의 모니터링 방법은 종종 파괴적인 방식에 의존하고 있습니다. 영상에서 굴을 수동으로 식별하는 방법은 파괴적이지 않지만, 시간이 많이 걸리고 전문가의 도움이 필요하며, 수중 환경의 어려움 때문에 더 복잡해집니다.

이런 문제를 해결하기 위해, 우리는 안정적인 확산(stable diffusion)을 이용해 실제로 수집한 데이터셋을 현실적인 합성 데이터로 보강하는 새로운 파이프라인을 제안합니다. 이 방법은 YOLOv10 기반의 비전 모델을 훈련하는 데 사용되는 데이터셋을 향상시킵니다. 그런 다음, 이 모델을 수중 로봇의 엣지 플랫폼에서 배포하고 테스트하여 Aqua2 플랫폼에서 굴 탐지에 대해 최첨단 0.657 mAP@50을 달성했습니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.07825.pdf

Title: A Comprehensive Survey on Deep Multimodal Learning with Missing Modality

Original Abstract:
During multimodal model training and reasoning, data samples may miss certain modalities and lead to compromised model performance due to sensor limitations, cost constraints, privacy concerns, data loss, and temporal and spatial factors. This survey provides an overview of recent progress in Multimodal Learning with Missing Modality (MLMM), focusing on deep learning techniques. It is the first comprehensive survey that covers the historical background and the distinction between MLMM and standard multimodal learning setups, followed by a detailed analysis of current MLMM methods, applications, and datasets, concluding with a discussion about challenges and potential future directions in the field.

Translated Abstract:
다중 모달 모델 훈련과 추론 과정에서 데이터 샘플이 특정 모달리티를 놓칠 수 있어. 이로 인해 센서 한계, 비용 문제, 프라이버시 걱정, 데이터 손실, 그리고 시간적, 공간적 요인 때문에 모델 성능이 저하될 수 있어. 

이 서베이는 누락된 모달리티를 가진 다중 모달 학습(MLMM)에서 최근의 발전을 다루고 있어. 주로 딥러닝 기술에 초점을 맞추고 있어. MLMM과 일반 다중 모달 학습 설정의 역사적 배경과 차이를 다룬 첫 번째 포괄적인 서베이야. 

그리고 현재의 MLMM 방법, 응용, 데이터셋에 대한 자세한 분석을 이어가고, 마지막으로 이 분야의 도전 과제와 앞으로의 방향에 대한 논의로 마무리하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07961.pdf

Title: Estimating Atmospheric Variables from Digital Typhoon Satellite Images via Conditional Denoising Diffusion Models

Original Abstract:
This study explores the application of diffusion models in the field of typhoons, predicting multiple ERA5 meteorological variables simultaneously from Digital Typhoon satellite images. The focus of this study is taken to be Taiwan, an area very vulnerable to typhoons. By comparing the performance of Conditional Denoising Diffusion Probability Model (CDDPM) with Convolutional Neural Networks (CNN) and Squeeze-and-Excitation Networks (SENet), results suggest that the CDDPM performs best in generating accurate and realistic meteorological data. Specifically, CDDPM achieved a PSNR of 32.807, which is approximately 7.9% higher than CNN and 5.5% higher than SENet. Furthermore, CDDPM recorded an RMSE of 0.032, showing a 11.1% improvement over CNN and 8.6% improvement over SENet. A key application of this research can be for imputation purposes in missing meteorological datasets and generate additional high-quality meteorological data using satellite images. It is hoped that the results of this analysis will enable more robust and detailed forecasting, reducing the impact of severe weather events on vulnerable regions. Code accessible at this https URL.

Translated Abstract:
이 연구는 태풍 분야에서 확산 모델의 적용을 살펴보는 거야. 디지털 태풍 위성 이미지에서 여러 가지 ERA5 기상 변수를 동시에 예측하는 내용이야. 연구의 초점은 태풍에 매우 취약한 대만 지역이야.

조건부 노이즈 제거 확산 확률 모델(CDDPM)의 성능을 컨볼루션 신경망(CNN)과 스퀴즈 앤 이그자이팅 네트워크(SENet)와 비교해봤더니, CDDPM이 가장 정확하고 현실적인 기상 데이터를 생성하는 데 가장 잘 작동하는 것 같아. 특히, CDDPM은 PSNR이 32.807로, CNN보다 약 7.9% 높고 SENet보다 5.5% 높았어. 게다가 CDDPM은 RMSE가 0.032로, CNN보다 11.1% 개선됐고 SENet보다 8.6% 개선된 결과를 보였어.

이 연구의 주요 응용은 결측 기상 데이터의 보완이나 위성 이미지를 사용해 추가적인 고품질 기상 데이터를 생성하는 데 사용할 수 있어. 분석 결과가 더 강력하고 세부적인 예측을 가능하게 해서 취약한 지역에 대한 심각한 기상 사건의 영향을 줄이는 데 도움이 되길 바래. 코드 접근은 이 URL에서 가능해.

================================================================================

URL:
https://arxiv.org/pdf/2302.13080.pdf

Title: Does a Neural Network Really Encode Symbolic Concepts?

Original Abstract:
Recently, a series of studies have tried to extract interactions between input variables modeled by a DNN and define such interactions as concepts encoded by the DNN. However, strictly speaking, there still lacks a solid guarantee whether such interactions indeed represent meaningful concepts. Therefore, in this paper, we examine the trustworthiness of interaction concepts from four perspectives. Extensive empirical studies have verified that a well-trained DNN usually encodes sparse, transferable, and discriminative concepts, which is partially aligned with human intuition.

Translated Abstract:
최근에 여러 연구들이 DNN(딥 뉴럴 네트워크)으로 모델링된 입력 변수 간의 상호작용을 뽑아내고, 그런 상호작용을 DNN이 인코딩한 개념으로 정의하려고 했어. 하지만, 엄밀히 말하면, 이런 상호작용이 실제로 의미 있는 개념을 나타내는지에 대한 확실한 보장이 부족해. 

그래서 이 논문에서는 상호작용 개념의 신뢰성을 네 가지 관점에서 살펴보았어. 많은 실험 연구들이 잘 훈련된 DNN이 보통 희소하고, 전이 가능하며, 구별 가능한 개념을 인코딩한다는 것을 확인했어. 이건 부분적으로 인간의 직관과도 일치해.

================================================================================

URL:
https://arxiv.org/pdf/2302.13091.pdf

Title: Explaining Generalization Power of a DNN Using Interactive Concepts

Original Abstract:
This paper explains the generalization power of a deep neural network (DNN) from the perspective of interactions. Although there is no universally accepted definition of the concepts encoded by a DNN, the sparsity of interactions in a DNN has been proved, i.e., the output score of a DNN can be well explained by a small number of interactions between input variables. In this way, to some extent, we can consider such interactions as interactive concepts encoded by the DNN. Therefore, in this paper, we derive an analytic explanation of inconsistency of concepts of different complexities. This may shed new lights on using the generalization power of concepts to explain the generalization power of the entire DNN. Besides, we discover that the DNN with stronger generalization power usually learns simple concepts more quickly and encodes fewer complex concepts. We also discover the detouring dynamics of learning complex concepts, which explains both the high learning difficulty and the low generalization power of complex concepts. The code will be released when the paper is accepted.

Translated Abstract:
이 논문은 딥 뉴럴 네트워크(DNN)의 일반화 능력을 상호작용의 관점에서 설명해. DNN이 인코딩하는 개념에 대한 보편적으로 accepted된 정의는 없지만, DNN의 상호작용이 희소하다는 건 입증됐어. 즉, DNN의 출력 점수는 입력 변수 간의 적은 수의 상호작용으로 잘 설명될 수 있다는 거지. 이렇게 보면, 어느 정도는 이런 상호작용을 DNN이 인코딩한 상호작용 개념으로 생각할 수 있어.

그래서 이 논문에서는 서로 다른 복잡성을 가진 개념의 불일치에 대한 분석적인 설명을 도출해. 이게 개념의 일반화 능력을 사용해서 전체 DNN의 일반화 능력을 설명하는 데 새로운 시각을 제공할 수 있을 거야. 

또, 일반화 능력이 강한 DNN은 보통 간단한 개념을 더 빨리 배우고, 복잡한 개념은 덜 인코딩하는 경향이 있다는 것도 발견했어. 복잡한 개념을 배우는 데는 우회적인 동학도 발견했는데, 이게 복잡한 개념의 높은 학습 난이도와 낮은 일반화 능력을 설명해. 논문이 승인되면 코드를 공개할 거야.

================================================================================

URL:
https://arxiv.org/pdf/2305.01939.pdf

Title: Where We Have Arrived in Proving the Emergence of Sparse Symbolic Concepts in AI Models

Original Abstract:
This study aims to prove the emergence of symbolic concepts (or more precisely, sparse primitive inference patterns) in well-trained deep neural networks (DNNs). Specifically, we prove the following three conditions for the emergence. (i) The high-order derivatives of the network output with respect to the input variables are all zero. (ii) The DNN can be used on occluded samples and when the input sample is less occluded, the DNN will yield higher confidence. (iii) The confidence of the DNN does not significantly degrade on occluded samples. These conditions are quite common, and we prove that under these conditions, the DNN will only encode a relatively small number of sparse interactions between input variables. Moreover, we can consider such interactions as symbolic primitive inference patterns encoded by a DNN, because we show that inference scores of the DNN on an exponentially large number of randomly masked samples can always be well mimicked by numerical effects of just a few interactions.

Translated Abstract:
이 연구는 잘 훈련된 딥 뉴럴 네트워크(DNN)에서 상징적 개념, 즉 희소한 기본 추론 패턴이 나타나는 걸 증명하는 걸 목표로 해. 구체적으로, 우리는 다음 세 가지 조건이 충족될 때 이게 나타난다는 걸 증명해.

(i) 네트워크 출력의 고차 미분이 입력 변수에 대해 모두 0이야.  
(ii) DNN은 가려진 샘플에서도 사용할 수 있고, 입력 샘플이 덜 가려져 있을 때 DNN의 신뢰도가 더 높아져.  
(iii) DNN의 신뢰도가 가려진 샘플에서는 크게 떨어지지 않아.  

이 조건들은 꽤 흔하고, 우리는 이 조건이 충족될 때 DNN이 입력 변수 간의 상대적으로 적은 수의 희소한 상호작용만 인코딩한다고 증명해. 더 나아가, 이런 상호작용을 DNN이 인코딩한 상징적 기본 추론 패턴으로 볼 수 있어. 왜냐하면, DNN이 무작위로 가려진 샘플에 대해 추론 점수를 내는 걸 몇 개의 상호작용으로 잘 모방할 수 있다는 걸 보여줬거든.

================================================================================

URL:
https://arxiv.org/pdf/2401.16318.pdf

Title: Defining and Extracting generalizable interaction primitives from DNNs

Original Abstract:
Faithfully summarizing the knowledge encoded by a deep neural network (DNN) into a few symbolic primitive patterns without losing much information represents a core challenge in explainable AI. To this end, Ren et al. (2024) have derived a series of theorems to prove that the inference score of a DNN can be explained as a small set of interactions between input variables. However, the lack of generalization power makes it still hard to consider such interactions as faithful primitive patterns encoded by the DNN. Therefore, given different DNNs trained for the same task, we develop a new method to extract interactions that are shared by these DNNs. Experiments show that the extracted interactions can better reflect common knowledge shared by different DNNs.

Translated Abstract:
딥 뉴럴 네트워크(DNN)가 담고 있는 지식을 몇 가지 기호 패턴으로 요약하는 건 설명 가능한 AI에서 중요한 도전 과제야. 이를 위해 Ren 외 연구팀(2024)은 DNN의 추론 점수가 입력 변수 간의 작은 상호작용으로 설명될 수 있다는 일련의 정리를 도출했어.

하지만 일반화 능력이 부족해서 이런 상호작용을 DNN이 인코딩한 신뢰할 수 있는 기본 패턴으로 보기 어려워. 그래서 동일한 작업을 위해 훈련된 여러 DNN을 고려해서, 우리는 이 DNN들이 공유하는 상호작용을 추출하는 새로운 방법을 개발했어.

실험 결과, 추출된 상호작용이 서로 다른 DNN들이 공유하는 공통 지식을 더 잘 반영할 수 있다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2405.01607.pdf

Title: Wildfire Risk Prediction: A Review

Original Abstract:
Wildfires have significant impacts on global vegetation, wildlife, and humans. They destroy plant communities and wildlife habitats and contribute to increased emissions of carbon dioxide, nitrogen oxides, methane, and other pollutants. The prediction of wildfires relies on various independent variables combined with regression or machine learning methods. In this technical review, we describe the options for independent variables, data processing techniques, models, independent variables collinearity and importance estimation methods, and model performance evaluation metrics. First, we divide the independent variables into 4 aspects, including climate and meteorology conditions, socio-economical factors, terrain and hydrological features, and wildfire historical records. Second, preprocessing methods are described for different magnitudes, different spatial-temporal resolutions, and different formats of data. Third, the collinearity and importance evaluation methods of independent variables are also considered. Fourth, we discuss the application of statistical models, traditional machine learning models, and deep learning models in wildfire risk prediction. In this subsection, compared with other reviews, this manuscript particularly discusses the evaluation metrics and recent advancements in deep learning methods. Lastly, addressing the limitations of current research, this paper emphasizes the need for more effective deep learning time series forecasting algorithms, the utilization of three-dimensional data including ground and trunk fuel, extraction of more accurate historical fire point data, and improved model evaluation metrics.

Translated Abstract:
산불은 전 세계의 식생, 야생동물, 그리고 인간에게 큰 영향을 미쳐. 식물 군락과 야생동물 서식지를 파괴하고, 이산화탄소, 질소산화물, 메탄 같은 오염물질의 배출을 증가시켜. 산불 예측은 여러 독립 변수를 사용해서 회귀나 머신러닝 방법과 결합해 이루어져. 

이 기술 리뷰에서는 독립 변수 선택, 데이터 처리 기법, 모델, 독립 변수 간의 다중공선성과 중요도 평가 방법, 그리고 모델 성능 평가 지표들을 설명할 거야. 첫째, 독립 변수를 기후 및 기상 조건, 사회 경제적 요인, 지형 및 수문학적 특성, 그리고 산불 이력 기록의 4가지 측면으로 나눌 수 있어. 

둘째, 데이터의 크기, 공간-시간 해상도, 포맷에 따라 다른 전처리 방법을 설명해. 셋째, 독립 변수의 다중공선성과 중요도 평가 방법도 고려해. 넷째, 산불 위험 예측에 통계 모델, 전통적인 머신러닝 모델, 그리고 딥러닝 모델의 적용을 논의해. 특히, 다른 리뷰와 비교했을 때, 이 논문은 평가 지표와 딥러닝 방법의 최근 발전에 대해 특별히 다루고 있어. 

마지막으로, 현재 연구의 한계를 짚으면서, 보다 효과적인 딥러닝 시계열 예측 알고리즘, 지면과 줄기 연료를 포함한 3차원 데이터 활용, 더 정확한 역사적 화재 포인트 데이터 추출, 그리고 개선된 모델 평가 지표의 필요성을 강조해.

================================================================================

URL:
https://arxiv.org/pdf/2406.01829.pdf

Title: FaçAID: A Transformer Model for Neuro-Symbolic Facade Reconstruction

Original Abstract:
We introduce a neuro-symbolic transformer-based model that converts flat, segmented facade structures into procedural definitions using a custom-designed split grammar. To facilitate this, we first develop a semi-complex split grammar tailored for architectural facades and then generate a dataset comprising of facades alongside their corresponding procedural representations. This dataset is used to train our transformer model to convert segmented, flat facades into the procedural language of our grammar. During inference, the model applies this learned transformation to new facade segmentations, providing a procedural representation that users can adjust to generate varied facade designs. This method not only automates the conversion of static facade images into dynamic, editable procedural formats but also enhances the design flexibility, allowing for easy modifications.

Translated Abstract:
우리는 평평하고 세분화된 건물 외관 구조를 절차적 정의로 변환하는 신경-기호 변환기 모델을 소개해. 이 모델은 특별히 설계된 분할 문법을 사용해.

먼저, 건축 외관에 맞춘 반복잡한 분할 문법을 개발해. 그리고 외관과 그에 맞는 절차적 표현을 포함한 데이터셋을 만들어. 이 데이터셋은 우리 변환기 모델을 훈련시키는 데 사용돼, 세분화된 평면 외관을 우리 문법의 절차적 언어로 변환할 수 있게 해.

추론할 때, 모델은 학습한 변환을 새로운 외관 세분화에 적용해. 이렇게 하면 사용자가 조정할 수 있는 절차적 표현이 생성돼서 다양한 외관 디자인을 만들 수 있어. 이 방법은 정적인 외관 이미지를 동적이고 수정 가능한 절차적 포맷으로 자동 변환할 뿐만 아니라 디자인의 유연성을 높여서 쉽게 수정할 수 있게 해.

================================================================================

URL:
https://arxiv.org/pdf/2407.12405.pdf

Title: Fisheye-Calib-Adapter: An Easy Tool for Fisheye Camera Model Conversion

Original Abstract:
The increasing necessity for fisheye cameras in fields such as robotics and autonomous driving has led to the proposal of various fisheye camera models. While the evolution of camera models has facilitated the development of diverse systems in the field, the lack of adaptation between different fisheye camera models means that recalibration is always necessary, which is cumbersome. This paper introduces a conversion tool for various previously proposed fisheye camera models. It is user-friendly, simple, yet extremely fast and accurate, offering conversion capabilities for a broader range of models compared to existing tools. We have verified that models converted using our system perform correctly in applications such as SLAM. By utilizing our system, researchers can obtain output parameters directly from input parameters without the need for an image set and any recalibration processes, thus serving as a bridge across different fisheye camera models in various research fields. We provide our system as an open source tool available at: this https URL

Translated Abstract:
로봇공학과 자율주행 같은 분야에서 어안 카메라의 필요성이 커지면서 여러 종류의 어안 카메라 모델이 제안되고 있어. 카메라 모델이 발전하면서 다양한 시스템 개발이 가능해졌지만, 서로 다른 어안 카메라 모델 간의 적응이 부족해서 항상 재조정이 필요해. 이게 좀 번거롭지.

이 논문에서는 기존에 제안된 다양한 어안 카메라 모델을 변환할 수 있는 도구를 소개해. 사용하기 쉽고, 간단하면서도 매우 빠르고 정확해. 기존 도구에 비해 더 많은 모델을 변환할 수 있는 기능을 제공해. 우리 시스템을 이용해 변환한 모델들이 SLAM 같은 응용 분야에서 제대로 작동하는 걸 확인했어.

이 시스템을 사용하면 연구자들이 이미지 세트나 재조정 과정 없이 입력 파라미터에서 직접 출력 파라미터를 얻을 수 있어. 그래서 여러 연구 분야에서 다양한 어안 카메라 모델 간의 다리 역할을 해줄 수 있어. 우리는 이 시스템을 오픈 소스 도구로 제공하고 있어: 이 주소를 참고해.

================================================================================

URL:
https://arxiv.org/pdf/2407.15861.pdf

Title: Adversarial Attacks and Defenses on Text-to-Image Diffusion Models: A Survey

Original Abstract:
Recently, the text-to-image diffusion model has gained considerable attention from the community due to its exceptional image generation capability. A representative model, Stable Diffusion, amassed more than 10 million users within just two months of its release. This surge in popularity has facilitated studies on the robustness and safety of the model, leading to the proposal of various adversarial attack methods. Simultaneously, there has been a marked increase in research focused on defense methods to improve the robustness and safety of these models. In this survey, we provide a comprehensive review of the literature on adversarial attacks and defenses targeting text-to-image diffusion models. We begin with an overview of text-to-image diffusion models, followed by an introduction to a taxonomy of adversarial attacks and an in-depth review of existing attack methods. We then present a detailed analysis of current defense methods that improve model robustness and safety. Finally, we discuss ongoing challenges and explore promising future research directions. For a complete list of the adversarial attack and defense methods covered in this survey, please refer to our curated repository at this https URL.

Translated Abstract:
최근 텍스트-이미지 확산 모델이 뛰어난 이미지 생성 능력 덕분에 많은 주목을 받고 있어. 대표적인 모델인 스테이블 디퓨전은 출시된 지 두 달 만에 1천만 명 이상의 사용자를 모았어. 이런 인기에 힘입어 모델의 견고성과 안전성에 대한 연구가 활발해졌고, 다양한 적대적 공격 방법도 제안되고 있어.

동시에 이러한 모델들의 견고성과 안전성을 높이기 위한 방어 방법에 대한 연구도 증가하고 있어. 이 서베이에서는 텍스트-이미지 확산 모델을 목표로 하는 적대적 공격과 방어에 관한 문헌을 종합적으로 리뷰할 거야. 먼저 텍스트-이미지 확산 모델에 대한 개요를 소개한 후, 적대적 공격의 분류법을 소개하고 기존의 공격 방법들에 대해 깊이 있는 리뷰를 진행할 거야.

그 다음에는 모델의 견고성과 안전성을 개선하는 현재 방어 방법들에 대한 자세한 분석을 제공할 거고, 마지막으로 현재의 도전 과제를 논의하고 앞으로의 연구 방향에 대해 탐구할 거야. 이 서베이에 포함된 적대적 공격 및 방어 방법의 전체 목록은 우리가 정리한 저장소에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2407.18456.pdf

Title: Diffusion-driven lensless fiber endomicroscopic quantitative phase imaging towards digital pathology

Original Abstract:
Lensless fiber endomicroscope is an emerging tool for in-vivo microscopic imaging, where quantitative phase imaging (QPI) can be utilized as a label-free method to enhance image contrast. However, existing single-shot phase reconstruction methods through lensless fiber endomicroscope typically perform well on simple images but struggle with complex microscopic structures. Here, we propose a speckle-conditioned diffusion model (SpecDiffusion), which reconstructs phase images directly from speckles captured at the detection side of a multi-core fiber (MCF). Unlike conventional neural networks, SpecDiffusion employs iterative phase denoising steps for speckle-driven phase reconstruction. The iteration scheme allows SpecDiffusion to break down the phase reconstruction process into multiple steps, gradually building up to the final phase image. This attribute alleviates the computation challenge at each step and enables the reconstruction of rich details in complex microscopic images. To validate its efficacy, we build an optical system to capture speckles from MCF and construct a dataset consisting of 100,000 paired images. SpecDiffusion provides high-fidelity phase reconstruction results and shows powerful generalization capacity for unseen objects, such as test charts and biological tissues, reducing the average mean absolute error of the reconstructed tissue images by 7 times. Furthermore, the reconstructed tissue images using SpecDiffusion shows higher accuracy in zero-shot cell segmentation tasks compared to the conventional method, demonstrating the potential for further cell morphology analysis through the learning-based lensless fiber endomicroscope. SpecDiffusion offers a precise and generalized method to phase reconstruction through scattering media, including MCFs, opening new perspective in lensless fiber endomicroscopic imaging.

Translated Abstract:
렌즈 없는 섬유 내시경은 인체 내부에서 미세 이미지를 촬영하는 새로운 도구야. 여기서 양적 위상 이미징(QPI)을 사용하면 라벨 없이도 이미지의 대비를 높일 수 있어. 하지만 기존의 단일 촬영 위상 재구성 방법은 간단한 이미지에서는 잘 작동하지만 복잡한 미세 구조에 대해서는 힘들어.

그래서 우리는 스펙클 조건화 확산 모델(SpecDiffusion)을 제안해. 이 모델은 다중 코어 섬유(MCF)의 감지 측에서 캡처한 스펙클로부터 직접 위상 이미지를 재구성해. 기존의 신경망과는 다르게, SpecDiffusion은 스펙클 기반 위상 재구성을 위해 반복적인 위상 노이즈 제거 단계를 사용해. 이 반복 과정 덕분에 위상 재구성 과정을 여러 단계로 나눌 수 있어서, 최종 위상 이미지로 점진적으로 발전해 나가.

이런 특징 덕분에 각 단계에서 계산의 부담을 줄일 수 있고, 복잡한 미세 이미지의 세부 사항을 풍부하게 재구성할 수 있어. 효과를 검증하기 위해 우리는 MCF에서 스펙클을 잡아내는 광학 시스템을 만들고, 10만 쌍의 이미지를 포함하는 데이터셋을 구축했어. SpecDiffusion은 높은 충실도의 위상 재구성 결과를 제공하고, 테스트 차트와 생물 조직 같은 보지 못한 물체에 대해서도 강력한 일반화 능력을 보여줘. 재구성된 조직 이미지의 평균 절대 오차를 7배 줄였어.

게다가 SpecDiffusion을 사용한 재구성된 조직 이미지는 기존 방법에 비해 제로 샷 세포 분할 작업에서 더 높은 정확도를 보여줘. 이는 렌즈 없는 섬유 내시경을 통해 세포 형태 분석을 더 발전시킬 수 있는 가능성을 보여주는 거야. SpecDiffusion은 MCF를 포함한 산란 매체를 통한 위상 재구성에 대해 정확하고 일반화된 방법을 제공해, 렌즈 없는 섬유 내시경 이미징의 새로운 관점을 열어줘.

================================================================================

