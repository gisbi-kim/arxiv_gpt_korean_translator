URL:
https://arxiv.org/pdf/2409.09144.pdf

Title: PrimeDepth: Efficient Monocular Depth Estimation with a Stable Diffusion Preimage

Original Abstract:
This work addresses the task of zero-shot monocular depth estimation. A recent advance in this field has been the idea of utilising Text-to-Image foundation models, such as Stable Diffusion. Foundation models provide a rich and generic image representation, and therefore, little training data is required to reformulate them as a depth estimation model that predicts highly-detailed depth maps and has good generalisation capabilities. However, the realisation of this idea has so far led to approaches which are, unfortunately, highly inefficient at test-time due to the underlying iterative denoising process. In this work, we propose a different realisation of this idea and present PrimeDepth, a method that is highly efficient at test time while keeping, or even enhancing, the positive aspects of diffusion-based approaches. Our key idea is to extract from Stable Diffusion a rich, but frozen, image representation by running a single denoising step. This representation, we term preimage, is then fed into a refiner network with an architectural inductive bias, before entering the downstream task. We validate experimentally that PrimeDepth is two orders of magnitude faster than the leading diffusion-based method, Marigold, while being more robust for challenging scenarios and quantitatively marginally superior. Thereby, we reduce the gap to the currently leading data-driven approach, Depth Anything, which is still quantitatively superior, but predicts less detailed depth maps and requires 20 times more labelled data. Due to the complementary nature of our approach, even a simple averaging between PrimeDepth and Depth Anything predictions can improve upon both methods and sets a new state-of-the-art in zero-shot monocular depth estimation. In future, data-driven approaches may also benefit from integrating our preimage.

Translated Abstract:
이 연구는 제로샷 단안 깊이 추정 작업에 대해 다룬다. 최근 이 분야에서의 발전 중 하나는 Stable Diffusion 같은 텍스트-이미지 기반 모델을 활용하는 아이디어야. 이런 기초 모델들은 풍부하고 일반적인 이미지 표현을 제공하기 때문에, 깊이 추정 모델로 재구성하는 데 필요한 훈련 데이터가 거의 없어. 이 모델은 고해상도 깊이 맵을 예측하고 일반화 능력이 뛰어나. 

하지만, 이 아이디어를 현실화하는 과정에서 기존 방법들은 테스트할 때 비효율적이었어. 그 이유는 기본적으로 반복적인 노이즈 제거 과정이 필요하기 때문이야. 그래서 우리는 이 아이디어를 다른 방식으로 실현하기 위해 PrimeDepth라는 방법을 제안해. 이 방법은 테스트할 때 매우 효율적이며, 확산 기반 접근법의 긍정적인 측면을 유지하거나 심지어 향상시켜. 

우리의 핵심 아이디어는 Stable Diffusion에서 단 한 번의 노이즈 제거 단계를 수행하여 풍부하지만 고정된 이미지 표현을 추출하는 거야. 이 표현을 우리는 '프리이미지'라고 부르며, 이걸 구조적 인덕티브 바이어스를 가진 정제 네트워크에 넣고 이후 작업에 들어가게 해. 실험적으로 PrimeDepth가 기존의 확산 기반 방법인 Marigold보다 두 배 빠르면서도 도전적인 상황에서 더 강인하고, 정량적으로도 약간 더 우수하다는 것을 검증했어. 

그렇게 우리는 현재의 데이터 기반 접근법인 Depth Anything과의 격차를 줄였어. Depth Anything은 여전히 정량적으로 우수하지만, 깊이 맵의 세부 묘사가 덜하고 레이블이 된 데이터가 20배 더 필요해. 우리 접근법의 보완적인 성격 덕분에 PrimeDepth와 Depth Anything의 예측을 단순히 평균 내기만 해도 두 방법 모두를 개선할 수 있고, 제로샷 단안 깊이 추정에서 새로운 최첨단을 세울 수 있어. 앞으로 데이터 기반 접근법도 우리의 프리이미지를 통합함으로써 이점을 얻을 수 있을 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09149.pdf

Title: Adaptive Multi-Modal Control of Digital Human Hand Synthesis Using a Region-Aware Cycle Loss

Original Abstract:
Diffusion models have shown their remarkable ability to synthesize images, including the generation of humans in specific poses. However, current models face challenges in adequately expressing conditional control for detailed hand pose generation, leading to significant distortion in the hand regions. To tackle this problem, we first curate the How2Sign dataset to provide richer and more accurate hand pose annotations. In addition, we introduce adaptive, multi-modal fusion to integrate characters' physical features expressed in different modalities such as skeleton, depth, and surface normal. Furthermore, we propose a novel Region-Aware Cycle Loss (RACL) that enables the diffusion model training to focus on improving the hand region, resulting in improved quality of generated hand gestures. More specifically, the proposed RACL computes a weighted keypoint distance between the full-body pose keypoints from the generated image and the ground truth, to generate higher-quality hand poses while balancing overall pose accuracy. Moreover, we use two hand region metrics, named hand-PSNR and hand-Distance for hand pose generation evaluations. Our experimental evaluations demonstrate the effectiveness of our proposed approach in improving the quality of digital human pose generation using diffusion models, especially the quality of the hand region. The source code is available at this https URL.

Translated Abstract:
확산 모델은 이미지를 합성하는 데 뛰어난 능력을 보여주고, 특정 자세의 인간을 생성하는 것도 가능해. 하지만 현재 모델들은 손 자세를 세밀하게 생성하는 데 어려움이 있어서 손 부분에서 큰 왜곡이 발생해. 이 문제를 해결하기 위해, 먼저 How2Sign 데이터셋을 정리해서 더 풍부하고 정확한 손 자세 주석을 제공해.

또한, 우리는 서로 다른 방식으로 표현된 캐릭터의 신체 특징을 통합하기 위해 적응형 다중 모드 융합을 도입해. 예를 들어, 뼈대, 깊이, 표면 법선 같은 여러 모드들을 활용해. 그리고 우리는 새로운 지역 인식 사이클 손실(RACL)을 제안하는데, 이건 확산 모델 훈련이 손 영역 개선에 집중할 수 있게 해줘, 그래서 생성된 손 제스처의 품질이 향상돼.

좀 더 구체적으로 말하자면, 제안된 RACL은 생성된 이미지의 전체 신체 자세 키포인트와 실제 키포인트 간의 가중치가 적용된 거리 계산을 통해 더 높은 품질의 손 자세를 생성하면서 전체적인 자세 정확성을 유지하게 해. 또한, 손 자세 생성을 평가하기 위해 손-PSNR과 손-거리라는 두 가지 손 영역 지표를 사용해.

우리의 실험 결과는 제안된 접근 방식이 확산 모델을 사용한 디지털 인간 자세 생성의 품질, 특히 손 영역의 품질을 개선하는 데 효과적임을 보여줘. 소스 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09177.pdf

Title: Transformer with Controlled Attention for Synchronous Motion Captioning

Original Abstract:
In this paper, we address a challenging task, synchronous motion captioning, that aim to generate a language description synchronized with human motion sequences. This task pertains to numerous applications, such as aligned sign language transcription, unsupervised action segmentation and temporal grounding. Our method introduces mechanisms to control self- and cross-attention distributions of the Transformer, allowing interpretability and time-aligned text generation. We achieve this through masking strategies and structuring losses that push the model to maximize attention only on the most important frames contributing to the generation of a motion word. These constraints aim to prevent undesired mixing of information in attention maps and to provide a monotonic attention distribution across tokens. Thus, the cross attentions of tokens are used for progressive text generation in synchronization with human motion sequences. We demonstrate the superior performance of our approach through evaluation on the two available benchmark datasets, KIT-ML and HumanML3D. As visual evaluation is essential for this task, we provide a comprehensive set of animated visual illustrations in the code repository: this https URL.

Translated Abstract:
이 논문에서는 동기화된 모션 캡셔닝이라는 어려운 작업을 다뤄. 이 작업은 인간의 동작 시퀀스와 함께 언어 설명을 생성하는 걸 목표로 해. 이건 정렬된 수화 전사, 비지도 행동 분할, 시간 기반 구속 같은 다양한 응용 프로그램에 관련돼.

우리 방법은 Transformer의 자기 주의(attention)와 교차 주의 분포를 조절하는 메커니즘을 도입했어. 이를 통해 해석 가능성과 시간에 맞춘 텍스트 생성을 가능하게 해. 우리는 마스킹 전략과 손실 구조를 통해 모델이 모션 단어 생성에 가장 중요한 프레임에만 최대한 주의를 기울이도록 유도해. 이런 제약은 주의 맵에서 원치 않는 정보 혼합을 방지하고, 토큰 간에 단조로운 주의 분포를 제공하는 데 도움을 줘.

따라서 토큰의 교차 주의는 인간 동작 시퀀스와 동기화된 텍스트 생성을 위해 사용돼. 우리는 KIT-ML과 HumanML3D라는 두 개의 벤치마크 데이터셋에서 우리의 접근 방식이 뛰어난 성능을 보여준다는 걸 입증했어. 이 작업에는 시각적인 평가가 중요하니까, 코드 저장소에 애니메이션 시각화 자료를 포괄적으로 제공하고 있어: 이 https URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.09196.pdf

Title: Are Sparse Neural Networks Better Hard Sample Learners?

Original Abstract:
While deep learning has demonstrated impressive progress, it remains a daunting challenge to learn from hard samples as these samples are usually noisy and intricate. These hard samples play a crucial role in the optimal performance of deep neural networks. Most research on Sparse Neural Networks (SNNs) has focused on standard training data, leaving gaps in understanding their effectiveness on complex and challenging data. This paper's extensive investigation across scenarios reveals that most SNNs trained on challenging samples can often match or surpass dense models in accuracy at certain sparsity levels, especially with limited data. We observe that layer-wise density ratios tend to play an important role in SNN performance, particularly for methods that train from scratch without pre-trained initialization. These insights enhance our understanding of SNNs' behavior and potential for efficient learning approaches in data-centric AI. Our code is publicly available at: \url{this https URL}.

Translated Abstract:
딥러닝은 놀라운 발전을 보여줬지만, 어려운 샘플에서 배우는 건 여전히 힘든 과제야. 이런 샘플들은 보통 노이즈가 많고 복잡하거든. 하지만 이 어려운 샘플들은 딥 뉴럴 네트워크가 최적의 성능을 내는 데 중요한 역할을 해. 

대부분의 희소 신경망(SNN)에 대한 연구는 일반적인 훈련 데이터에 집중해왔고, 복잡하고 도전적인 데이터에서의 효과를 이해하는 데는 부족한 부분이 있어. 이 논문은 다양한 상황에서의 광범위한 조사를 통해, 어려운 샘플로 훈련된 대부분의 SNN들이 특정한 희소성 수준에서 밀집 모델보다 정확도가 같거나 더 높을 수 있다는 걸 보여줘, 특히 데이터가 제한된 경우에.

우리는 레이어별 밀도 비율이 SNN 성능에서 중요한 역할을 한다는 걸 관찰했어. 특히, 사전 훈련 초기화 없이 처음부터 훈련하는 방법에서 그러해. 이런 통찰들은 SNN의 행동과 데이터 중심 AI에서 효율적인 학습 접근 방식의 가능성을 이해하는 데 도움을 줘. 

우리의 코드는 공개적으로 사용할 수 있어: \url{this https URL}.

================================================================================

URL:
https://arxiv.org/pdf/2409.09221.pdf

Title: Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?

Original Abstract:
Decoder-only discrete-token language models have recently achieved significant success in automatic speech recognition. However, systematic analyses of how different modalities impact performance in specific scenarios remain limited. In this paper, we investigate the effects of multiple modalities on recognition accuracy on both synthetic and real-world datasets. Our experiments suggest that: (1) Integrating more modalities can increase accuracy; in particular, our paper is, to our best knowledge, the first to show the benefit of combining audio, image context, and lip information; (2) Images as a supplementary modality for speech recognition provide the greatest benefit at moderate noise levels, moreover, they exhibit a different trend compared to inherently synchronized modalities like lip movements; (3) Performance improves on both synthetic and real-world datasets when the most relevant visual information is filtered as a preprocessing step.

Translated Abstract:
디코더 전용 이산 토큰 언어 모델이 최근 자동 음성 인식에서 큰 성공을 거두었어. 하지만 다양한 모달리티가 특정 상황에서 성능에 어떻게 영향을 미치는지에 대한 체계적인 분석은 아직 부족해. 이 논문에서는 여러 모달리티가 인식 정확도에 미치는 영향을 합성 데이터와 실제 데이터셋에서 조사했어.

우리 실험 결과는 다음과 같아: 

(1) 더 많은 모달리티를 통합하면 정확도가 증가할 수 있어. 특히, 우리 논문은 오디오, 이미지 컨텍스트, 입술 정보를 결합하는 것의 이점을 보여주는 첫 번째 연구야. 

(2) 이미지가 음성 인식의 보조 모달리티로서 제공하는 이점은 중간 수준의 소음에서 가장 커. 게다가, 입술 움직임 같은 본질적으로 동기화된 모달리티와는 다른 경향을 보여. 

(3) 가장 관련성 높은 시각 정보를 필터링하는 전처리 단계를 거치면 합성 데이터와 실제 데이터셋 모두에서 성능이 향상돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.09244.pdf

Title: Investigation of Hierarchical Spectral Vision Transformer Architecture for Classification of Hyperspectral Imagery

Original Abstract:
In the past three years, there has been significant interest in hyperspectral imagery (HSI) classification using vision Transformers for analysis of remotely sensed data. Previous research predominantly focused on the empirical integration of convolutional neural networks (CNNs) to augment the network's capability to extract local feature information. Yet, the theoretical justification for vision Transformers out-performing CNN architectures in HSI classification remains a question. To address this issue, a unified hierarchical spectral vision Transformer architecture, specifically tailored for HSI classification, is investigated. In this streamlined yet effective vision Transformer architecture, multiple mixer modules are strategically integrated separately. These include the CNN-mixer, which executes convolution operations; the spatial self-attention (SSA)-mixer and channel self-attention (CSA)-mixer, both of which are adaptations of classical self-attention blocks; and hybrid models such as the SSA+CNN-mixer and CSA+CNN-mixer, which merge convolution with self-attention operations. This integration facilitates the development of a broad spectrum of vision Transformer-based models tailored for HSI classification. In terms of the training process, a comprehensive analysis is performed, contrasting classical CNN models and vision Transformer-based counterparts, with particular attention to disturbance robustness and the distribution of the largest eigenvalue of the Hessian. From the evaluations conducted on various mixer models rooted in the unified architecture, it is concluded that the unique strength of vision Transformers can be attributed to their overarching architecture, rather than being exclusively reliant on individual multi-head self-attention (MSA) components.

Translated Abstract:
지난 3년 동안, 원거리 감지 데이터 분석을 위한 하이퍼스펙트럼 이미지(HSI) 분류에 비전 트랜스포머를 활용하는 데 큰 관심이 많았어. 이전 연구들은 주로 CNN(합성곱 신경망)을 활용해서 네트워크가 지역 특성 정보를 잘 뽑아낼 수 있도록 하는 데 집중했어. 그런데 HSI 분류에서 비전 트랜스포머가 CNN 아키텍처보다 더 뛰어난 이유에 대한 이론적인 설명은 아직 명확하지 않아.

이 문제를 해결하기 위해, HSI 분류에 맞춰진 통합 계층 스펙트럴 비전 트랜스포머 아키텍처를 연구했어. 이 구조는 여러 믹서 모듈을 효과적으로 통합해. 여기에는 합성곱 연산을 수행하는 CNN 믹서, 그리고 고전적인 자기 주의 블록을 변형한 공간 자기 주의(SSA) 믹서와 채널 자기 주의(CSA) 믹서가 포함돼. 또한, 합성곱과 자기 주의 연산을 결합한 하이브리드 모델인 SSA+CNN 믹서와 CSA+CNN 믹서도 있어. 이런 통합 덕분에 HSI 분류에 맞춘 다양한 비전 트랜스포머 기반 모델을 개발할 수 있게 됐어.

훈련 과정에 대해서는 고전적인 CNN 모델과 비전 트랜스포머 기반 모델을 비교 분석했어. 특히, 방해에 대한 강인성과 헤시안의 가장 큰 고유값 분포에 주목했어. 통합 아키텍처에 기반한 다양한 믹서 모델에 대한 평가 결과, 비전 트랜스포머의 독특한 강점은 개별 멀티헤드 자기 주의(MSA) 구성요소에만 의존하는 것이 아니라, 전체 아키텍처에 있다고 결론지었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09254.pdf

Title: VSFormer: Mining Correlations in Flexible View Set for Multi-view 3D Shape Understanding

Original Abstract:
View-based methods have demonstrated promising performance in 3D shape understanding. However, they tend to make strong assumptions about the relations between views or learn the multi-view correlations indirectly, which limits the flexibility of exploring inter-view correlations and the effectiveness of target tasks. To overcome the above problems, this paper investigates flexible organization and explicit correlation learning for multiple views. In particular, we propose to incorporate different views of a 3D shape into a permutation-invariant set, referred to as \emph{View Set}, which removes rigid relation assumptions and facilitates adequate information exchange and fusion among views. Based on that, we devise a nimble Transformer model, named \emph{VSFormer}, to explicitly capture pairwise and higher-order correlations of all elements in the set. Meanwhile, we theoretically reveal a natural correspondence between the Cartesian product of a view set and the correlation matrix in the attention mechanism, which supports our model design. Comprehensive experiments suggest that VSFormer has better flexibility, efficient inference efficiency and superior performance. Notably, VSFormer reaches state-of-the-art results on various 3d recognition datasets, including ModelNet40, ScanObjectNN and RGBD. It also establishes new records on the SHREC'17 retrieval benchmark. The code and datasets are available at \url{this https URL}.

Translated Abstract:
뷰 기반 방법들은 3D 형태 이해에서 좋은 성능을 보여줬어. 하지만 이 방법들은 뷰 간의 관계에 대해 강한 가정을 하거나, 다중 뷰의 상관관계를 간접적으로 학습하는 경향이 있어. 이 때문에 뷰 간 상관관계를 탐색하는 유연성과 목표 작업의 효과가 제한돼. 

이 문제를 해결하기 위해, 이 논문에서는 여러 뷰를 위한 유연한 조직과 명시적인 상관관계 학습을 조사해. 특히, 3D 형태의 다양한 뷰를 순열 불변 집합인 \emph{View Set}으로 통합하는 방법을 제안해. 이 방법은 고정된 관계 가정을 없애고 뷰 간의 적절한 정보 교환과 융합을 촉진해.

그걸 바탕으로, 모든 요소의 쌍별 및 고차 상관관계를 명시적으로 캡처하는 민첩한 Transformer 모델인 \emph{VSFormer}를 고안했어. 한편, 뷰 집합의 카르테시안 곱과 주의 메커니즘의 상관관계 행렬 사이에 자연스러운 대응 관계를 이론적으로 밝혀냈어. 이게 모델 설계를 지지해줘.

종합적인 실험 결과, VSFormer가 더 나은 유연성과 효율적인 추론 성능, 그리고 뛰어난 성능을 보여줬어. 특히, VSFormer는 ModelNet40, ScanObjectNN, RGBD 같은 다양한 3D 인식 데이터셋에서 최첨단 결과를 달성했고, SHREC'17 검색 벤치마크에서도 새로운 기록을 세웠어. 코드와 데이터셋은 \url{this https URL}에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09269.pdf

Title: Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types

Original Abstract:
Visual Question-Answering (VQA) has become a key use-case in several applications to aid user experience, particularly after Vision-Language Models (VLMs) achieving good results in zero-shot inference. But evaluating different VLMs for an application requirement using a standardized framework in practical settings is still challenging. This paper introduces a comprehensive framework for evaluating VLMs tailored to VQA tasks in practical settings. We present a novel dataset derived from established VQA benchmarks, annotated with task types, application domains, and knowledge types, three key practical aspects on which tasks can vary. We also introduce GoEval, a multimodal evaluation metric developed using GPT-4o, achieving a correlation factor of 56.71% with human judgments. Our experiments with ten state-of-the-art VLMs reveals that no single model excelling universally, making appropriate selection a key design decision. Proprietary models such as Gemini-1.5-Pro and GPT-4o-mini generally outperform others, though open-source models like InternVL-2-8B and CogVLM-2-Llama-3-19B demonstrate competitive strengths in specific contexts, while providing additional advantages. This study guides the selection of VLMs based on specific task requirements and resource constraints, and can also be extended to other vision-language tasks.

Translated Abstract:
시각 질문 응답(Visual Question-Answering, VQA)은 사용자 경험을 향상시키기 위해 여러 응용 프로그램에서 중요한 사용 사례가 되었어. 특히 비전-언어 모델(Vision-Language Models, VLMs)이 제로샷 추론에서 좋은 성과를 거둔 이후로 그렇지. 하지만 실제 환경에서 표준화된 프레임워크를 사용해 다양한 VLM을 평가하는 건 여전히 어려워.

이 논문은 VQA 작업에 맞춰진 VLM 평가를 위한 포괄적인 프레임워크를 소개해. 우리는 기존 VQA 벤치마크에서 파생된 새로운 데이터셋을 제시하는데, 여기에는 작업 유형, 응용 도메인, 지식 유형 같은 세 가지 중요한 실용적 측면이 주석으로 달려 있어. 그리고 GoEval이라는 멀티모달 평가 지표도 소개하는데, 이건 GPT-4o를 사용해서 개발된 거야. 인간의 판단과 56.71%의 상관 계수를 달성했어.

우리가 실험한 열 개의 최신 VLM을 보니, 어떤 모델도 모든 상황에서 뛰어난 건 없더라. 그래서 적절한 모델 선택이 중요한 디자인 결정이 돼. Gemini-1.5-Pro나 GPT-4o-mini 같은 독점 모델이 일반적으로 다른 모델들보다 성능이 좋지만, InternVL-2-8B나 CogVLM-2-Llama-3-19B 같은 오픈소스 모델도 특정 상황에서 경쟁력을 보여주고 추가적인 장점도 가지고 있어.

이 연구는 특정 작업 요구 사항과 자원 제약에 따라 VLM 선택을 안내하고, 다른 비전-언어 작업에도 확장될 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09274.pdf

Title: LabellessFace: Fair Metric Learning for Face Recognition without Attribute Labels

Original Abstract:
Demographic bias is one of the major challenges for face recognition systems. The majority of existing studies on demographic biases are heavily dependent on specific demographic groups or demographic classifier, making it difficult to address performance for unrecognised groups. This paper introduces ``LabellessFace'', a novel framework that improves demographic bias in face recognition without requiring demographic group labeling typically required for fairness considerations. We propose a novel fairness enhancement metric called the class favoritism level, which assesses the extent of favoritism towards specific classes across the dataset. Leveraging this metric, we introduce the fair class margin penalty, an extension of existing margin-based metric learning. This method dynamically adjusts learning parameters based on class favoritism levels, promoting fairness across all attributes. By treating each class as an individual in facial recognition systems, we facilitate learning that minimizes biases in authentication accuracy among individuals. Comprehensive experiments have demonstrated that our proposed method is effective for enhancing fairness while maintaining authentication accuracy.

Translated Abstract:
인구 통계적 편향은 얼굴 인식 시스템에서 큰 문제 중 하나야. 기존 연구들은 특정 인구 집단이나 분류기에 많이 의존해서, 잘 알려지지 않은 그룹에 대한 성능 문제를 해결하기 어려워. 

이 논문에서는 "LabellessFace"라는 새로운 프레임워크를 소개해. 이 프레임워크는 공정성을 고려할 때 보통 필요한 인구 집단 레이블 없이도 얼굴 인식에서 인구 통계적 편향을 개선할 수 있어. 우리는 "클래스 편애 수준"이라는 새로운 공정성 향상 메트릭을 제안하는데, 이건 데이터셋에서 특정 클래스에 대한 편애 정도를 평가해.

이 메트릭을 활용해서, 우리는 공정 클래스 마진 패널티를 도입해. 이건 기존의 마진 기반 메트릭 학습을 확장한 거야. 이 방법은 클래스 편애 수준에 따라 학습 매개변수를 동적으로 조정해서, 모든 속성에서 공정성을 촉진해. 

각 클래스를 얼굴 인식 시스템의 개별로 취급함으로써, 개인 간 인증 정확도에서 편향을 최소화하는 학습을 가능하게 해. 다양한 실험 결과를 통해 우리 방법이 공정성을 높이면서도 인증 정확도를 유지하는 데 효과적이라는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09286.pdf

Title: SAM-OCTA2: Layer Sequence OCTA Segmentation with Fine-tuned Segment Anything Model 2

Original Abstract:
Segmentation of indicated targets aids in the precise analysis of optical coherence tomography angiography (OCTA) samples. Existing segmentation methods typically perform on 2D projection targets, making it challenging to capture the variance of segmented objects through the 3D volume. To address this limitation, the low-rank adaptation technique is adopted to fine-tune the Segment Anything Model (SAM) version 2, enabling the tracking and segmentation of specified objects across the OCTA scanning layer sequence. To further this work, a prompt point generation strategy in frame sequence and a sparse annotation method to acquire retinal vessel (RV) layer masks are proposed. This method is named SAM-OCTA2 and has been experimented on the OCTA-500 dataset. It achieves state-of-the-art performance in segmenting the foveal avascular zone (FAZ) on regular 2D en-face and effectively tracks local vessels across scanning layer sequences. The code is available at: this https URL.

Translated Abstract:
표적 세분화는 광학 간섭 단층 촬영 혈관 조영술(OCTA) 샘플을 정확하게 분석하는 데 도움을 줘. 기존의 세분화 방법들은 주로 2D 투영 표적에서 수행되다 보니, 3D 볼륨을 통해 세분화된 물체의 변화를 포착하기가 어려워. 

이런 한계를 해결하기 위해 저희는 저순위 적응(low-rank adaptation) 기술을 사용해 Segment Anything Model(SAM) 버전 2를 조정했어. 이 기술은 OCTA 스캐닝 레이어 순서에서 지정된 물체를 추적하고 세분화할 수 있게 해줘. 

더 나아가서, 프레임 시퀀스에서 프롬프트 포인트 생성 전략과 망막 혈관(RV) 레이어 마스크를 얻기 위한 희소 주석 방법도 제안했어. 이 방법은 SAM-OCTA2라고 이름 붙였고, OCTA-500 데이터셋에서 실험했어. 

이 방법은 일반 2D 엔페이스에서 중심와 무혈관 영역(FAZ)을 세분화하는 데 뛰어난 성능을 보였고, 스캐닝 레이어 시퀀스 전반에 걸쳐 지역 혈관을 효과적으로 추적할 수 있어. 코드도 여기에서 확인할 수 있어: 이 URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.09291.pdf

Title: Infrared and Visible Image Fusion with Hierarchical Human Perception

Original Abstract:
Image fusion combines images from multiple domains into one image, containing complementary information from source domains. Existing methods take pixel intensity, texture and high-level vision task information as the standards to determine preservation of information, lacking enhancement for human perception. We introduce an image fusion method, Hierarchical Perception Fusion (HPFusion), which leverages Large Vision-Language Model to incorporate hierarchical human semantic priors, preserving complementary information that satisfies human visual system. We propose multiple questions that humans focus on when viewing an image pair, and answers are generated via the Large Vision-Language Model according to images. The texts of answers are encoded into the fusion network, and the optimization also aims to guide the human semantic distribution of the fused image more similarly to source images, exploring complementary information within the human perception domain. Extensive experiments demonstrate our HPFusoin can achieve high-quality fusion results both for information preservation and human visual enhancement.

Translated Abstract:
이미지 융합은 여러 도메인에서 온 이미지를 하나로 합쳐서, 각 출처에서의 보완 정보를 포함하는 방식이야. 기존 방법들은 픽셀 강도나 텍스처, 그리고 고급 비전 작업 정보를 기준으로 정보를 유지하는 걸 판단하지만, 인간의 인식을 더 잘 향상시키는 방법은 부족해.

우리는 계층적 인간 의미 우선 순위를 활용하는 이미지 융합 방법인 Hierarchical Perception Fusion(HPFusion)을 소개할 거야. 이 방법은 인간의 시각 시스템을 만족시키는 보완 정보를 유지할 수 있어. 사람들이 이미지 쌍을 볼 때 집중하는 여러 질문들을 제안하고, 그에 대한 답변은 대형 비전-언어 모델을 통해 이미지에 맞춰 생성돼. 이 답변의 텍스트는 융합 네트워크에 인코딩되고, 최적화 과정은 융합된 이미지의 인간 의미 분포가 출처 이미지와 더 비슷하게 되도록 유도해. 이렇게 해서 인간 인식 영역 내에서 보완 정보를 탐색할 수 있어.

많은 실험 결과, 우리의 HPFusion이 정보 보존과 인간의 시각 향상 모두에서 고품질의 융합 결과를 얻을 수 있음을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09292.pdf

Title: StyleTalk++: A Unified Framework for Controlling the Speaking Styles of Talking Heads

Original Abstract:
Individuals have unique facial expression and head pose styles that reflect their personalized speaking styles. Existing one-shot talking head methods cannot capture such personalized characteristics and therefore fail to produce diverse speaking styles in the final videos. To address this challenge, we propose a one-shot style-controllable talking face generation method that can obtain speaking styles from reference speaking videos and drive the one-shot portrait to speak with the reference speaking styles and another piece of audio. Our method aims to synthesize the style-controllable coefficients of a 3D Morphable Model (3DMM), including facial expressions and head movements, in a unified framework. Specifically, the proposed framework first leverages a style encoder to extract the desired speaking styles from the reference videos and transform them into style codes. Then, the framework uses a style-aware decoder to synthesize the coefficients of 3DMM from the audio input and style codes. During decoding, our framework adopts a two-branch architecture, which generates the stylized facial expression coefficients and stylized head movement coefficients, respectively. After obtaining the coefficients of 3DMM, an image renderer renders the expression coefficients into a specific person's talking-head video. Extensive experiments demonstrate that our method generates visually authentic talking head videos with diverse speaking styles from only one portrait image and an audio clip.

Translated Abstract:
개개인은 자신만의 독특한 얼굴 표정과 머리 움직임 스타일을 가지고 있어요. 이런 스타일은 개인의 말하는 방식과 연결돼 있죠. 기존의 한 번만 촬영하는 토킹 헤드 방법들은 이런 개인화된 특성을 잡아내지 못해서 최종 비디오에서 다양한 말하는 스타일을 만들어내지 못해요.

이 문제를 해결하기 위해, 우리는 스타일을 조절할 수 있는 한 번 촬영하는 토킹 얼굴 생성 방법을 제안해요. 이 방법은 참고용 말하는 비디오에서 말하는 스타일을 얻고, 이를 바탕으로 한 번 촬영한 초상화가 다른 오디오와 함께 말할 수 있도록 해줘요. 우리의 방법은 3D 변형 모델(3DMM)의 스타일 조절 계수를 합쳐서 얼굴 표정과 머리 움직임을 생성하는 걸 목표로 해요.

구체적으로, 제안하는 프레임워크는 먼저 스타일 인코더를 사용해서 참고 비디오에서 원하는 말하는 스타일을 추출하고, 이를 스타일 코드로 변환해요. 그 다음, 스타일 인식 디코더를 사용해서 오디오 입력과 스타일 코드로부터 3DMM의 계수를 합성해요. 디코딩하는 동안, 우리의 프레임워크는 두 가지 가지 구조를 채택해서 각각 스타일화된 얼굴 표정 계수와 스타일화된 머리 움직임 계수를 생성해요.

3DMM의 계수를 얻은 후, 이미지 렌더러가 그 표정 계수를 특정 인물의 토킹 헤드 비디오로 렌더링해요. 많은 실험 결과, 우리의 방법은 단 한 장의 초상 이미지와 오디오 클립만으로도 다양한 말하는 스타일을 가진 시각적으로 진짜 같은 토킹 헤드 비디오를 생성한다는 걸 보여줬어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.09293.pdf

Title: Associate Everything Detected: Facilitating Tracking-by-Detection to the Unknown

Original Abstract:
Multi-object tracking (MOT) emerges as a pivotal and highly promising branch in the field of computer vision. Classical closed-vocabulary MOT (CV-MOT) methods aim to track objects of predefined categories. Recently, some open-vocabulary MOT (OV-MOT) methods have successfully addressed the problem of tracking unknown categories. However, we found that the CV-MOT and OV-MOT methods each struggle to excel in the tasks of the other. In this paper, we present a unified framework, Associate Everything Detected (AED), that simultaneously tackles CV-MOT and OV-MOT by integrating with any off-the-shelf detector and supports unknown categories. Different from existing tracking-by-detection MOT methods, AED gets rid of prior knowledge (e.g. motion cues) and relies solely on highly robust feature learning to handle complex trajectories in OV-MOT tasks while keeping excellent performance in CV-MOT tasks. Specifically, we model the association task as a similarity decoding problem and propose a sim-decoder with an association-centric learning mechanism. The sim-decoder calculates similarities in three aspects: spatial, temporal, and cross-clip. Subsequently, association-centric learning leverages these threefold similarities to ensure that the extracted features are appropriate for continuous tracking and robust enough to generalize to unknown categories. Compared with existing powerful OV-MOT and CV-MOT methods, AED achieves superior performance on TAO, SportsMOT, and DanceTrack without any prior knowledge. Our code is available at this https URL.

Translated Abstract:
다중 객체 추적(MOT)은 컴퓨터 비전 분야에서 매우 중요한 연구 주제야. 전통적인 폐쇄 어휘 MOT(CV-MOT) 방법은 미리 정의된 카테고리의 객체를 추적하는 데 초점을 맞추고 있어. 최근에는 알려지지 않은 카테고리를 추적할 수 있는 개방형 어휘 MOT(OV-MOT) 방법도 등장했어. 하지만 우리는 CV-MOT과 OV-MOT 방법이 서로의 작업에서 잘 수행하지 못한다는 걸 발견했어.

이 논문에서는 CV-MOT과 OV-MOT을 동시에 해결할 수 있는 통합 프레임워크인 '모든 것을 감지하여 연결하기(Associate Everything Detected, AED)'를 소개할 거야. 이 프레임워크는 어떤 상용 탐지기와도 통합할 수 있고, 알려지지 않은 카테고리를 지원해. 기존의 탐지 기반 MOT 방법과는 다르게, AED는 이전 지식(예: 움직임 단서)을 필요로 하지 않고, 강력한 특징 학습만으로 OV-MOT 작업에서 복잡한 궤적을 처리하면서도 CV-MOT 작업에서도 뛰어난 성능을 유지해.

구체적으로, 우리는 연관 작업을 유사성 디코딩 문제로 모델링하고, 연관 중심 학습 메커니즘을 가진 sim-decoder를 제안해. 이 sim-decoder는 공간, 시간, 그리고 교차 클립의 세 가지 측면에서 유사성을 계산해. 그 후, 연관 중심 학습은 이 세 가지 유사성을 활용해서 추출된 특징이 지속적인 추적에 적합하고 알려지지 않은 카테고리에 잘 일반화될 수 있도록 보장해.

기존의 강력한 OV-MOT와 CV-MOT 방법들과 비교했을 때, AED는 TAO, SportsMOT, 그리고 DanceTrack에서 뛰어난 성능을 보여줘. 이 연구의 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09300.pdf

Title: ManiDext: Hand-Object Manipulation Synthesis via Continuous Correspondence Embeddings and Residual-Guided Diffusion

Original Abstract:
Dynamic and dexterous manipulation of objects presents a complex challenge, requiring the synchronization of hand motions with the trajectories of objects to achieve seamless and physically plausible interactions. In this work, we introduce ManiDext, a unified hierarchical diffusion-based framework for generating hand manipulation and grasp poses based on 3D object trajectories. Our key insight is that accurately modeling the contact correspondences between objects and hands during interactions is crucial. Therefore, we propose a continuous correspondence embedding representation that specifies detailed hand correspondences at the vertex level between the object and the hand. This embedding is optimized directly on the hand mesh in a self-supervised manner, with the distance between embeddings reflecting the geodesic distance. Our framework first generates contact maps and correspondence embeddings on the object's surface. Based on these fine-grained correspondences, we introduce a novel approach that integrates the iterative refinement process into the diffusion process during the second stage of hand pose generation. At each step of the denoising process, we incorporate the current hand pose residual as a refinement target into the network, guiding the network to correct inaccurate hand poses. Introducing residuals into each denoising step inherently aligns with traditional optimization process, effectively merging generation and refinement into a single unified framework. Extensive experiments demonstrate that our approach can generate physically plausible and highly realistic motions for various tasks, including single and bimanual hand grasping as well as manipulating both rigid and articulated objects. Code will be available for research purposes.

Translated Abstract:
물체를 동적으로 조작하는 건 복잡한 도전 과제야. 손 움직임과 물체의 경로를 잘 맞춰야 자연스럽고 현실적인 상호작용이 가능하거든. 이번 연구에서는 ManiDext라는 통합된 계층적 확산 기반 프레임워크를 소개해. 이건 3D 물체 경로에 기반해서 손 조작과 그립 자세를 생성하는 거야.

우리의 주요 아이디어는 상호작용 중에 물체와 손 사이의 접촉 관계를 정확하게 모델링하는 게 중요하다는 거야. 그래서 우리는 객체와 손 사이의 정점 수준에서 자세한 손 관계를 지정하는 연속 접촉 관계 임베딩 표현을 제안해. 이 임베딩은 손 메시에서 직접 최적화되는데, 임베딩 간의 거리는 기하학적 거리를 반영해.

우리 프레임워크는 먼저 물체 표면에서 접촉 맵과 관계 임베딩을 생성해. 그리고 이 세밀한 관계를 바탕으로 손 자세 생성을 위한 두 번째 단계에서 반복적 정제 과정을 확산 과정에 통합하는 새로운 접근법을 소개해. 각 디노이징 과정의 단계에서 현재 손 자세의 잔여물을 정제 목표로 네트워크에 통합해, 잘못된 손 자세를 수정하도록 유도해.

각 디노이징 단계에 잔여물을 도입하는 건 전통적인 최적화 과정과 잘 맞아떨어져서 생성과 정제를 하나의 통합된 프레임워크로 합치는 효과가 있어. 다양한 작업, 예를 들어 단일 및 양손 그립, 강체 및 관절 물체 조작을 포함한 신뢰할 수 있고 매우 현실적인 동작을 생성할 수 있다는 걸 실험을 통해 보여줬어. 연구 목적으로 코드는 제공할 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09306.pdf

Title: Keypoints-Integrated Instruction-Following Data Generation for Enhanced Human Pose Understanding in Multimodal Models

Original Abstract:
Current multimodal models are well-suited for general visual understanding tasks. However, they perform inadequately when handling complex visual tasks related to human poses and actions, primarily due to the lack of specialized instruction-following data. We introduce a new method for generating such data by integrating human keypoints with traditional visual features like captions and bounding boxes. Our approach produces datasets designed for fine-tuning models to excel in human-centric activities, focusing on three specific types: conversation, detailed description, and complex reasoning. We fine-tuned the LLaVA-7B model with this novel dataset, achieving significant improvements across various human pose-related tasks. Experimental results show an overall improvement of 21.18% compared to the original LLaVA-7B model. These findings demonstrate the effectiveness of keypoints-assisted data in enhancing multimodal models.

Translated Abstract:
현재의 멀티모달 모델은 일반적인 시각 이해 작업에는 잘 맞지만, 사람의 자세와 행동 같은 복잡한 시각 작업에서는 성능이 부족해. 주된 이유는 전문적인 지시를 따르는 데이터가 부족하기 때문이야. 

우리는 인간의 주요 포인트를 전통적인 시각적 특징인 캡션이나 바운딩 박스와 결합해서 새로운 데이터를 생성하는 방법을 소개해. 이 접근 방식은 인간 중심의 활동에 맞춰 모델을 미세 조정할 수 있는 데이터셋을 만들어. 특히 대화, 자세한 설명, 복잡한 추론의 세 가지 유형에 초점을 맞추고 있어.

우리는 이 새로운 데이터셋으로 LLaVA-7B 모델을 미세 조정했더니, 사람의 자세와 관련된 다양한 작업에서 큰 개선을 이루었어. 실험 결과, 원래 LLaVA-7B 모델에 비해 21.18%의 전반적인 개선이 있었어. 이 결과는 주요 포인트가 도움이 된 데이터가 멀티모달 모델의 성능을 높이는 데 효과적임을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09312.pdf

Title: Registration between Point Cloud Streams and Sequential Bounding Boxes via Gradient Descent

Original Abstract:
In this paper, we propose an algorithm for registering sequential bounding boxes with point cloud streams. Unlike popular point cloud registration techniques, the alignment of the point cloud and the bounding box can rely on the properties of the bounding box, such as size, shape, and temporal information, which provides substantial support and performance gains. Motivated by this, we propose a new approach to tackle this problem. Specifically, we model the registration process through an overall objective function that includes the final goal and all constraints. We then optimize the function using gradient descent. Our experiments show that the proposed method performs remarkably well with a 40\% improvement in IoU and demonstrates more robust registration between point cloud streams and sequential bounding boxes

Translated Abstract:
이 논문에서는 연속적인 바운딩 박스와 포인트 클라우드 스트림을 등록하는 알고리즘을 제안해. 일반적인 포인트 클라우드 등록 기법과는 달리, 포인트 클라우드와 바운딩 박스의 정렬은 바운딩 박스의 크기, 모양, 시간 정보 같은 특성을 활용할 수 있어서 성능 향상에 큰 도움이 돼.

이런 점에서 우리는 이 문제를 해결하기 위한 새로운 접근 방식을 생각해냈어. 구체적으로, 우리는 최종 목표와 모든 제약 조건을 포함한 전체 목적 함수를 통해 등록 과정을 모델링해. 그리고 이 함수를 경량 하강법을 사용해서 최적화해.

실험 결과, 제안한 방법이 IoU에서 40% 개선되는 등 매우 뛰어난 성능을 보였고, 포인트 클라우드 스트림과 연속적인 바운딩 박스 간의 등록이 더 견고하게 이루어졌어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09313.pdf

Title: Tensor-Based Synchronization and the Low-Rankness of the Block Trifocal Tensor

Original Abstract:
The block tensor of trifocal tensors provides crucial geometric information on the three-view geometry of a scene. The underlying synchronization problem seeks to recover camera poses (locations and orientations up to a global transformation) from the block trifocal tensor. We establish an explicit Tucker factorization of this tensor, revealing a low multilinear rank of $(6,4,4)$ independent of the number of cameras under appropriate scaling conditions. We prove that this rank constraint provides sufficient information for camera recovery in the noiseless case. The constraint motivates a synchronization algorithm based on the higher-order singular value decomposition of the block trifocal tensor. Experimental comparisons with state-of-the-art global synchronization methods on real datasets demonstrate the potential of this algorithm for significantly improving location estimation accuracy. Overall this work suggests that higher-order interactions in synchronization problems can be exploited to improve performance, beyond the usual pairwise-based approaches.

Translated Abstract:
트리포컬 텐서의 블록 텐서는 장면의 세 가지 시점 기하학에 대한 중요한 기하학적 정보를 제공해. 기본적으로 동기화 문제는 블록 트리포컬 텐서에서 카메라의 위치와 방향(전역 변환을 제외한)을 회복하는 거야. 

우리는 이 텐서의 명시적인 터커 분해를 설정하고, 적절한 스케일링 조건 하에서 카메라 수와는 무관한 낮은 다중 선형 계급 $(6,4,4)$를 드러냈어. 이 계급 제약이 노이즈 없는 경우에 카메라 회복에 충분한 정보를 제공한다는 걸 증명했어. 이 제약은 블록 트리포컬 텐서의 고차 단일값 분해를 기반으로 한 동기화 알고리즘을 유도해. 

실제 데이터셋에서 최첨단 글로벌 동기화 방법들과의 실험 비교를 통해 이 알고리즘이 위치 추정 정확도를 크게 향상시킬 수 있는 가능성을 보여줬어. 전반적으로 이 연구는 동기화 문제에서 고차 상호작용을 활용해 기존의 쌍 기반 접근 방식 이상으로 성능을 개선할 수 있음을 제안해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09319.pdf

Title: ChildPlay-Hand: A Dataset of Hand Manipulations in the Wild

Original Abstract:
Hand-Object Interaction (HOI) is gaining significant attention, particularly with the creation of numerous egocentric datasets driven by AR/VR applications. However, third-person view HOI has received less attention, especially in terms of datasets. Most third-person view datasets are curated for action recognition tasks and feature pre-segmented clips of high-level daily activities, leaving a gap for in-the-wild datasets. To address this gap, we propose ChildPlay-Hand, a novel dataset that includes person and object bounding boxes, as well as manipulation actions. ChildPlay-Hand is unique in: (1) providing per-hand annotations; (2) featuring videos in uncontrolled settings with natural interactions, involving both adults and children; (3) including gaze labels from the ChildPlay-Gaze dataset for joint modeling of manipulations and gaze. The manipulation actions cover the main stages of an HOI cycle, such as grasping, holding or operating, and different types of releasing. To illustrate the interest of the dataset, we study two tasks: object in hand detection (OiH), i.e. if a person has an object in their hand, and manipulation stages (ManiS), which is more fine-grained and targets the main stages of manipulation. We benchmark various spatio-temporal and segmentation networks, exploring body vs. hand-region information and comparing pose and RGB modalities. Our findings suggest that ChildPlay-Hand is a challenging new benchmark for modeling HOI in the wild.

Translated Abstract:
Hand-Object Interaction (HOI)은 요즘 많이 주목받고 있어. 특히 AR/VR 관련해서 개인 중심의 데이터셋이 많이 생기고 있거든. 근데 제3자 시점에서의 HOI는 데이터셋 측면에서 많이 연구되지 않았어. 대부분의 제3자 시점 데이터셋은 행동 인식 작업을 위해 만들어졌고, 고수준 일상 활동의 사전 분할된 클립만 포함되어 있어. 그래서 자연환경에서 사용할 수 있는 데이터셋이 부족한 상황이야.

이 문제를 해결하기 위해 우리는 ChildPlay-Hand라는 새로운 데이터셋을 제안해. 이 데이터셋은 사람과 물체의 바운딩 박스, 그리고 조작 행동을 포함하고 있어. ChildPlay-Hand의 특징은: (1) 각 손에 대한 주석을 제공하고; (2) 자연스러운 상호작용이 있는 통제되지 않은 환경에서 촬영된 비디오를 포함하며, 어른과 아이 모두가 참여하고; (3) 조작과 시선을 함께 모델링하기 위해 ChildPlay-Gaze 데이터셋의 시선 라벨을 포함한다는 거야.

조작 행동은 HOI 사이클의 주요 단계인 잡기, 들기 또는 조작하기, 그리고 다양한 방출 방식 등을 다루고 있어. 이 데이터셋의 흥미로운 점을 보여주기 위해 두 가지 작업을 연구했어: 손에 있는 물체 탐지(OiH), 즉 사람이 손에 물체를 쥐고 있는지와 조작 단계(ManiS), 더 세부적으로 조작의 주요 단계를 목표로 해. 우리는 다양한 시공간적 및 분할 네트워크를 벤치마킹하면서 몸과 손 영역 정보를 비교하고, 포즈와 RGB 모달리티를 비교했어. 우리의 결과는 ChildPlay-Hand가 자연환경에서 HOI를 모델링하는 데 도전적인 새로운 기준이 될 것임을 시사해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09323.pdf

Title: Implicit Neural Representations with Fourier Kolmogorov-Arnold Networks

Original Abstract:
Implicit neural representations (INRs) use neural networks to provide continuous and resolution-independent representations of complex signals with a small number of parameters. However, existing INR models often fail to capture important frequency components specific to each task. To address this issue, in this paper, we propose a Fourier Kolmogorov Arnold network (FKAN) for INRs. The proposed FKAN utilizes learnable activation functions modeled as Fourier series in the first layer to effectively control and learn the task-specific frequency components. In addition, the activation functions with learnable Fourier coefficients improve the ability of the network to capture complex patterns and details, which is beneficial for high-resolution and high-dimensional data. Experimental results show that our proposed FKAN model outperforms three state-of-the-art baseline schemes, and improves the peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) for the image representation task and intersection over union (IoU) for the 3D occupancy volume representation task, respectively.

Translated Abstract:
암묵적 신경 표현(Implicit Neural Representations, INRs)은 신경망을 사용해서 복잡한 신호를 연속적이고 해상도에 상관없이 표현할 수 있게 해줘. 이 과정에서 필요한 파라미터 개수도 적어. 하지만 기존의 INR 모델들은 각 작업에 특화된 중요한 주파수 요소들을 잘 잡아내지 못하는 경우가 많아.

그래서 이 논문에서는 Fourier Kolmogorov Arnold 네트워크(FKAN)를 제안해. 이 FKAN은 첫 번째 층에서 푸리에 급수로 모델링된 학습 가능한 활성화 함수를 사용해서 작업에 맞는 주파수 요소들을 효과적으로 조절하고 학습할 수 있어. 또한, 학습 가능한 푸리에 계수를 가진 활성화 함수 덕분에 네트워크가 복잡한 패턴과 세부사항을 더 잘 잡아낼 수 있게 돼. 이건 고해상도와 고차원 데이터에 유리해.

실험 결과를 보면, 우리가 제안한 FKAN 모델이 세 가지 최첨단 기준 모델보다 성능이 더 뛰어나고, 이미지 표현 작업에서는 신호 대 잡음 비율(PSNR)과 구조적 유사도 지수(SSIM)가 개선됐어. 그리고 3D 점유 볼륨 표현 작업에서는 교차 비율(IoU)도 향상됐어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09326.pdf

Title: LawDNet: Enhanced Audio-Driven Lip Synthesis via Local Affine Warping Deformation

Original Abstract:
In the domain of photorealistic avatar generation, the fidelity of audio-driven lip motion synthesis is essential for realistic virtual interactions. Existing methods face two key challenges: a lack of vivacity due to limited diversity in generated lip poses and noticeable anamorphose motions caused by poor temporal coherence. To address these issues, we propose LawDNet, a novel deep-learning architecture enhancing lip synthesis through a Local Affine Warping Deformation mechanism. This mechanism models the intricate lip movements in response to the audio input by controllable non-linear warping fields. These fields consist of local affine transformations focused on abstract keypoints within deep feature maps, offering a novel universal paradigm for feature warping in networks. Additionally, LawDNet incorporates a dual-stream discriminator for improved frame-to-frame continuity and employs face normalization techniques to handle pose and scene variations. Extensive evaluations demonstrate LawDNet's superior robustness and lip movement dynamism performance compared to previous methods. The advancements presented in this paper, including the methodologies, training data, source codes, and pre-trained models, will be made accessible to the research community.

Translated Abstract:
포토리얼리스틱 아바타 생성에서는 오디오 기반의 입술 움직임 합성이 현실적인 가상 상호작용에 매우 중요해. 기존 방법들은 두 가지 큰 문제에 직면해 있어: 첫째, 생성되는 입술 포즈의 다양성이 부족해서 생동감이 떨어지고, 둘째, 시간적 일관성이 떨어져서 입술 움직임이 이상하게 보이는 경우가 많아.

이런 문제를 해결하기 위해 우리는 LawDNet이라는 새로운 딥러닝 구조를 제안해. 이 구조는 Local Affine Warping Deformation 메커니즘을 통해 입술 합성을 개선해. 이 메커니즘은 오디오 입력에 반응하는 복잡한 입술 움직임을 조절 가능한 비선형 왜곡 필드를 사용해서 모델링해. 이 필드는 딥 피처 맵 내의 추상적인 키포인트에 초점을 맞춘 지역 아핀 변환으로 구성되어 있어서 네트워크에서 피처 왜곡을 위한 새로운 보편적인 패러다임을 제공해.

또한, LawDNet은 프레임 간 연속성을 개선하기 위해 이중 스트림 판별기를 포함하고, 포즈와 장면 변화를 처리하기 위해 얼굴 정규화 기술을 사용해. 다양한 평가 결과, LawDNet은 이전 방법들에 비해 더 뛰어난 강인성과 입술 움직임의 역동성을 보여줘. 이 논문에서 제시된 기술, 훈련 데이터, 소스 코드, 사전 훈련된 모델들은 연구 커뮤니티에 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09348.pdf

Title: QTG-VQA: Question-Type-Guided Architectural for VideoQA Systems

Original Abstract:
In the domain of video question answering (VideoQA), the impact of question types on VQA systems, despite its critical importance, has been relatively under-explored to date. However, the richness of question types directly determines the range of concepts a model needs to learn, thereby affecting the upper limit of its learning capability. This paper focuses on exploring the significance of different question types for VQA systems and their impact on performance, revealing a series of issues such as insufficient learning and model degradation due to uneven distribution of question types. Particularly, considering the significant variation in dependency on temporal information across different question types, and given that the representation of such information coincidentally represents a principal challenge and difficulty for VideoQA as opposed to ImageQA. To address these challenges, we propose QTG-VQA, a novel architecture that incorporates question-type-guided attention and adaptive learning mechanism. Specifically, as to temporal-type questions, we design Masking Frame Modeling technique to enhance temporal modeling, aimed at encouraging the model to grasp richer visual-language relationships and manage more intricate temporal dependencies. Furthermore, a novel evaluation metric tailored to question types is introduced. Experimental results confirm the effectiveness of our approach.

Translated Abstract:
비디오 질문 답변(VideoQA) 분야에서 질문 유형이 VQA 시스템에 미치는 영향은 굉장히 중요한데, 아직 충분히 연구되지 않았어. 질문 유형의 다양성은 모델이 배워야 할 개념의 범위를 직접 결정하고, 그로 인해 학습 능력의 한계를 영향을 미쳐.

이 논문에서는 VQA 시스템에 대한 다양한 질문 유형의 중요성과 성능에 미치는 영향을 탐구해. 여기서 발견된 문제들은 질문 유형의 불균형 배포로 인한 학습 부족과 모델의 저하 같은 것들이야. 특히, 질문 유형에 따라 시간 정보에 대한 의존성이 크게 달라지는데, 이런 시간 정보를 표현하는 게 ImageQA와 비교할 때 VideoQA에서 가장 큰 도전이야.

이런 문제들을 해결하기 위해 QTG-VQA라는 새로운 아키텍처를 제안해. 이건 질문 유형에 기반한 주의(attention)와 적응형 학습 메커니즘을 포함하고 있어. 특히, 시간 관련 질문에 대해선 Masking Frame Modeling 기법을 설계해서 시간 모델링을 강화하고, 모델이 더 풍부한 시각-언어 관계를 이해하고 복잡한 시간 의존성을 관리하도록 도와.

또한, 질문 유형에 맞춘 새로운 평가 지표도 도입했어. 실험 결과, 우리의 접근 방식이 효과적이라는 걸 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09350.pdf

Title: OPUS: Occupancy Prediction Using a Sparse Set

Original Abstract:
Occupancy prediction, aiming at predicting the occupancy status within voxelized 3D environment, is quickly gaining momentum within the autonomous driving community. Mainstream occupancy prediction works first discretize the 3D environment into voxels, then perform classification on such dense grids. However, inspection on sample data reveals that the vast majority of voxels is unoccupied. Performing classification on these empty voxels demands suboptimal computation resource allocation, and reducing such empty voxels necessitates complex algorithm designs. To this end, we present a novel perspective on the occupancy prediction task: formulating it as a streamlined set prediction paradigm without the need for explicit space modeling or complex sparsification procedures. Our proposed framework, called OPUS, utilizes a transformer encoder-decoder architecture to simultaneously predict occupied locations and classes using a set of learnable queries. Firstly, we employ the Chamfer distance loss to scale the set-to-set comparison problem to unprecedented magnitudes, making training such model end-to-end a reality. Subsequently, semantic classes are adaptively assigned using nearest neighbor search based on the learned locations. In addition, OPUS incorporates a suite of non-trivial strategies to enhance model performance, including coarse-to-fine learning, consistent point sampling, and adaptive re-weighting, etc. Finally, compared with current state-of-the-art methods, our lightest model achieves superior RayIoU on the Occ3D-nuScenes dataset at near 2x FPS, while our heaviest model surpasses previous best results by 6.1 RayIoU.

Translated Abstract:
점유 예측은 3D 환경의 점유 상태를 예측하는 거고, 자율주행 분야에서 빠르게 주목받고 있어. 일반적으로 점유 예측은 3D 환경을 먼저 격자 형태로 나눈 다음, 이렇게 만들어진 격자에서 분류 작업을 해. 그런데 샘플 데이터를 살펴보면 대부분의 격자는 비어있다는 걸 알 수 있어. 이런 빈 격자에서 분류를 하려면 비효율적으로 계산 자원을 써야 하고, 빈 격자를 줄이려면 복잡한 알고리즘이 필요해.

그래서 우리는 점유 예측 작업에 대한 새로운 관점을 제시해. 명확한 공간 모델링이나 복잡한 희소화 절차 없이 간소화된 세트 예측 패러다임으로 이 작업을 정의하는 거야. 우리가 제안한 프레임워크, OPUS는 트랜스포머 인코더-디코더 구조를 활용해서, 학습 가능한 쿼리 세트를 사용해 점유된 위치와 클래스 모두를 동시에 예측해.

먼저, 챔퍼 거리 손실을 사용해 세트 간 비교 문제를 엄청난 규모로 확장해서, 이런 모델을 엔드 투 엔드로 훈련하는 게 가능해졌어. 그 다음, 학습된 위치를 기반으로 최근접 이웃 검색을 통해 의미적 클래스를 유동적으로 할당해. 또한, OPUS는 모델 성능을 높이기 위해 정교한 전략들을 포함하고 있어. 예를 들어, 거친 것에서 세밀한 것으로 학습, 일관된 포인트 샘플링, 적응형 재가중치 조정 등이 있어.

마지막으로, 현재의 최첨단 방법들과 비교했을 때, 우리의 가장 가벼운 모델은 Occ3D-nuScenes 데이터셋에서 RayIoU가 더 뛰어나고, 거의 2배의 FPS를 기록해. 그리고 가장 무거운 모델은 이전 최상의 결과보다 6.1 RayIoU 더 나은 성능을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09360.pdf

Title: LACOSTE: Exploiting stereo and temporal contexts for surgical instrument segmentation

Original Abstract:
Surgical instrument segmentation is instrumental to minimally invasive surgeries and related applications. Most previous methods formulate this task as single-frame-based instance segmentation while ignoring the natural temporal and stereo attributes of a surgical video. As a result, these methods are less robust against the appearance variation through temporal motion and view change. In this work, we propose a novel LACOSTE model that exploits Location-Agnostic COntexts in Stereo and TEmporal images for improved surgical instrument segmentation. Leveraging a query-based segmentation model as core, we design three performance-enhancing modules. Firstly, we design a disparity-guided feature propagation module to enhance depth-aware features explicitly. To generalize well for even only a monocular video, we apply a pseudo stereo scheme to generate complementary right images. Secondly, we propose a stereo-temporal set classifier, which aggregates stereo-temporal contexts in a universal way for making a consolidated prediction and mitigates transient failures. Finally, we propose a location-agnostic classifier to decouple the location bias from mask prediction and enhance the feature semantics. We extensively validate our approach on three public surgical video datasets, including two benchmarks from EndoVis Challenges and one real radical prostatectomy surgery dataset GraSP. Experimental results demonstrate the promising performances of our method, which consistently achieves comparable or favorable results with previous state-of-the-art approaches.

Translated Abstract:
수술 도구 분할은 최소 침습 수술과 관련된 응용 프로그램에 매우 중요해. 대부분의 이전 방법들은 이 작업을 단일 프레임 기반의 인스턴스 분할로 정의했지만, 수술 비디오의 자연스러운 시간적 특성과 스테레오 특성을 무시했어. 그래서 이런 방법들은 시간에 따른 움직임이나 시점 변화에 대한 외관 변동에 잘 견디지 못해.

이번 연구에서는 수술 도구 분할을 개선하기 위해 Location-Agnostic COntexts in Stereo and TEmporal images를 활용하는 새로운 LACOSTE 모델을 제안해. 쿼리 기반 분할 모델을 핵심으로 삼고, 세 개의 성능 향상 모듈을 디자인했어. 

첫째, 깊이 인식 특성을 명시적으로 강화하기 위해 불일치 가이드 피쳐 전파 모듈을 설계했어. 단안 비디오에서도 잘 일반화할 수 있도록, 보조적인 오른쪽 이미지를 생성하는 의사 스테레오 방식을 적용했어. 

둘째, 스테레오-시간 집합 분류기를 제안해. 이 분류기는 스테레오-시간 맥락을 보편적인 방식으로 집계하여 통합된 예측을 하고, 일시적인 실패를 완화해. 

마지막으로, 위치 편향을 마스크 예측에서 분리하고 특성 의미를 강화하기 위해 위치 불가지론 분류기를 제안했어. 우리는 EndoVis Challenge의 두 개 벤치마크와 실제 근치적 전립선 절제술 데이터셋 GraSP를 포함한 세 개의 공공 수술 비디오 데이터셋에서 우리의 접근 방식을 광범위하게 검증했어. 실험 결과는 우리의 방법이 이전의 최첨단 접근 방식과 비교해서도 일관되게 유사하거나 더 좋은 성능을 보여준다는 것을 입증해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09366.pdf

Title: MHAD: Multimodal Home Activity Dataset with Multi-Angle Videos and Synchronized Physiological Signals

Original Abstract:
Video-based physiology, exemplified by remote photoplethysmography (rPPG), extracts physiological signals such as pulse and respiration by analyzing subtle changes in video recordings. This non-contact, real-time monitoring method holds great potential for home settings. Despite the valuable contributions of public benchmark datasets to this technology, there is currently no dataset specifically designed for passive home monitoring. Existing datasets are often limited to close-up, static, frontal recordings and typically include only 1-2 physiological signals. To advance video-based physiology in real home settings, we introduce the MHAD dataset. It comprises 1,440 videos from 40 subjects, capturing 6 typical activities from 3 angles in a real home environment. Additionally, 5 physiological signals were recorded, making it a comprehensive video-based physiology dataset. MHAD is compatible with the rPPG-toolbox and has been validated using several unsupervised and supervised methods. Our dataset is publicly available at this https URL.

Translated Abstract:
비디오 기반 생리학, 특히 원거리 광용적맥파측정법(rPPG)을 예로 들면, 비디오 녹화에서 미세한 변화를 분석해 맥박이나 호흡 같은 생리 신호를 추출해. 이 방법은 비접촉식이고 실시간 모니터링이 가능해서 가정에서 활용할 가능성이 커. 

하지만 이 기술에 기여한 공공 벤치마크 데이터셋이 있지만, 현재로서는 수동적인 가정 모니터링을 위해 특별히 설계된 데이터셋은 없어. 기존 데이터셋은 보통 가까운 거리에서 정면으로 촬영된 정적인 영상으로 제한되고, 보통 1-2개의 생리 신호만 포함돼.

우리는 실제 가정 환경에서 비디오 기반 생리학을 발전시키기 위해 MHAD 데이터셋을 소개해. 이 데이터셋은 40명의 피실험자로부터 1,440개의 비디오를 포함하고, 3개의 각도에서 6가지 전형적인 활동을 촬영했어. 게다가 5개의 생리 신호도 기록되어서 포괄적인 비디오 기반 생리학 데이터셋이야. MHAD는 rPPG-toolbox와 호환되며 여러 비지도 및 지도 방법을 통해 검증되었어. 

우리 데이터셋은 이 URL에서 공개적으로 이용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09369.pdf

Title: Interpretable Vision-Language Survival Analysis with Ordinal Inductive Bias for Computational Pathology

Original Abstract:
Histopathology Whole-Slide Images (WSIs) provide an important tool to assess cancer prognosis in computational pathology (CPATH). While existing survival analysis (SA) approaches have made exciting progress, they are generally limited to adopting highly-expressive architectures and only coarse-grained patient-level labels to learn prognostic visual representations from gigapixel WSIs. Such learning paradigm suffers from important performance bottlenecks, when facing present scarce training data and standard multi-instance learning (MIL) framework in CPATH. To break through it, this paper, for the first time, proposes a new Vision-Language-based SA (VLSA) paradigm. Concretely, (1) VLSA is driven by pathology VL foundation models. It no longer relies on high-capability networks and shows the advantage of data efficiency. (2) In vision-end, VLSA encodes prognostic language prior and then employs it as auxiliary signals to guide the aggregating of prognostic visual features at instance level, thereby compensating for the weak supervision in MIL. Moreover, given the characteristics of SA, we propose i) ordinal survival prompt learning to transform continuous survival labels into textual prompts; and ii) ordinal incidence function as prediction target to make SA compatible with VL-based prediction. VLSA's predictions can be interpreted intuitively by our Shapley values-based method. The extensive experiments on five datasets confirm the effectiveness of our scheme. Our VLSA could pave a new way for SA in CPATH by offering weakly-supervised MIL an effective means to learn valuable prognostic clues from gigapixel WSIs. Our source code is available at this https URL.

Translated Abstract:
조직병리학 전체 슬라이드 이미지(WSI)는 컴퓨터 병리학(CPATH)에서 암 예후를 평가하는 데 중요한 도구야. 기존의 생존 분석(SA) 방법들이 흥미로운 발전을 이루긴 했지만, 보통은 매우 복잡한 구조와 대략적인 환자 레이블만 사용해서 기가픽셀 WSI에서 예후를 나타내는 시각적 표현을 배우는 데 한계가 있어. 이런 학습 방식은 현재 부족한 훈련 데이터와 CPATH의 표준 다중 인스턴스 학습(MIL) 프레임워크에 직면했을 때 성능에 큰 병목 현상이 생겨.

이걸 극복하기 위해, 이 논문에서는 처음으로 새로운 비전-언어 기반 생존 분석(VLSA) 패러다임을 제안해. 구체적으로 말하면, (1) VLSA는 병리학 VL 기반 모델에 의해 구동돼. 더 이상 고성능 네트워크에 의존하지 않고 데이터 효율성의 이점을 보여줘. (2) 비전 쪽에서는 VLSA가 예후 언어 정보를 인코딩하고, 이를 보조 신호로 사용해서 인스턴스 수준에서 예후 시각적 특징을 집합하는 데 도움을 줘. 이렇게 해서 MIL에서 약한 감독을 보완할 수 있어. 게다가 SA의 특성을 고려해서 i) 연속 생존 레이블을 텍스트 프롬프트로 변환하는 순서 생존 프롬프트 학습을 제안하고, ii) VL 기반 예측과 호환되도록 예측 목표로 순서 발생 함수도 제안해. VLSA의 예측은 우리 Shapley 값 기반 방법을 통해 직관적으로 해석할 수 있어. 다섯 개 데이터셋에 대한 광범위한 실험이 우리 방법의 효과성을 확인해줬어. 우리의 VLSA는 CPATH에서 SA를 위한 새로운 길을 열어줄 수 있을 것 같아, 기가픽셀 WSI에서 귀중한 예후 정보를 배우기 위해 약한 감독의 MIL에 효과적인 수단을 제공할 수 있으니까. 우리의 소스 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09386.pdf

Title: AMBER -- Advanced SegFormer for Multi-Band Image Segmentation: an application to Hyperspectral Imaging

Original Abstract:
Deep learning has revolutionized the field of hyperspectral image (HSI) analysis, enabling the extraction of complex and hierarchical features. While convolutional neural networks (CNNs) have been the backbone of HSI classification, their limitations in capturing global contextual features have led to the exploration of Vision Transformers (ViTs). This paper introduces AMBER, an advanced SegFormer specifically designed for multi-band image segmentation. AMBER enhances the original SegFormer by incorporating three-dimensional convolutions to handle hyperspectral data. Our experiments, conducted on the Indian Pines, Pavia University, and PRISMA datasets, show that AMBER outperforms traditional CNN-based methods in terms of Overall Accuracy, Kappa coefficient, and Average Accuracy on the first two datasets, and achieves state-of-the-art performance on the PRISMA dataset.

Translated Abstract:
딥러닝은 하이퍼스펙트럴 이미지(HSI) 분석 분야에 큰 변화를 가져왔고, 복잡하고 계층적인 특징을 추출하는 데 도움을 주고 있어. CNN(합성곱 신경망)이 HSI 분류의 기본이 되어왔지만, 전역적인 맥락 특징을 잡는 데 한계가 있어 비전 트랜스포머(ViT)의 가능성을 탐색하게 되었어.

이 논문에서는 다중 밴드 이미지 분할을 위해 특별히 설계된 AMBER라는 고급 SegFormer를 소개해. AMBER는 하이퍼스펙트럴 데이터를 다루기 위해 3차원 합성곱을 추가해서 원래 SegFormer를 개선했어. 

인디안 파인즈, 파비아 대학교, PRISMA 데이터셋을 사용한 실험 결과, AMBER는 첫 두 데이터셋에서 전체 정확도, 카파 계수, 평균 정확도 면에서 전통적인 CNN 기반 방법들보다 성능이 더 좋았고, PRISMA 데이터셋에서는 최신 성능을 기록했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09391.pdf

Title: Tran-GCN: A Transformer-Enhanced Graph Convolutional Network for Person Re-Identification in Monitoring Videos

Original Abstract:
Person Re-Identification (Re-ID) has gained popularity in computer vision, enabling cross-camera pedestrian recognition. Although the development of deep learning has provided a robust technical foundation for person Re-ID research, most existing person Re-ID methods overlook the potential relationships among local person features, failing to adequately address the impact of pedestrian pose variations and local body parts occlusion. Therefore, we propose a Transformer-enhanced Graph Convolutional Network (Tran-GCN) model to improve Person Re-Identification performance in monitoring videos. The model comprises four key components: (1) A Pose Estimation Learning branch is utilized to estimate pedestrian pose information and inherent skeletal structure data, extracting pedestrian key point information; (2) A Transformer learning branch learns the global dependencies between fine-grained and semantically meaningful local person features; (3) A Convolution learning branch uses the basic ResNet architecture to extract the person's fine-grained local features; (4) A Graph Convolutional Module (GCM) integrates local feature information, global feature information, and body information for more effective person identification after fusion. Quantitative and qualitative analysis experiments conducted on three different datasets (Market-1501, DukeMTMC-ReID, and MSMT17) demonstrate that the Tran-GCN model can more accurately capture discriminative person features in monitoring videos, significantly improving identification accuracy.

Translated Abstract:
사람 재식별(Person Re-Identification, Re-ID)은 컴퓨터 비전에서 인기가 높아졌고, 여러 카메라에서 보행자를 인식할 수 있게 해줍니다. 딥러닝의 발전 덕분에 사람 재식별 연구에 강력한 기술적 기반이 마련됐지만, 기존의 많은 방법들이 지역적인 사람 특징 간의 관계를 간과하고 있습니다. 이로 인해 보행자의 자세 변화나 신체 일부 가림 문제를 제대로 다루지 못하고 있죠. 

그래서 우리는 Transformer를 활용한 그래프 합성곱 신경망(Tran-GCN) 모델을 제안합니다. 이 모델은 모니터링 비디오에서 사람 재식별 성능을 향상시키기 위해 만들어졌습니다. 모델은 네 가지 주요 구성 요소로 이루어져 있습니다: 

1. 자세 추정 학습 브랜치는 보행자의 자세 정보와 본래의 골격 구조 데이터를 추정해 보행자의 주요 점 정보를 추출합니다.  
2. Transformer 학습 브랜치는 세밀하고 의미 있는 지역적인 사람 특징 간의 전반적인 의존성을 학습합니다.  
3. 합성곱 학습 브랜치는 기본 ResNet 구조를 사용해 사람의 세밀한 지역 특징을 추출합니다.  
4. 그래프 합성곱 모듈(GCM)은 지역 특징 정보, 전반적인 특징 정보, 그리고 신체 정보를 통합해 더 효과적인 사람 식별을 가능하게 합니다. 

세 가지 다른 데이터셋(마켓-1501, 듀크MTMC-ReID, MSMT17)에서 실시한 정량적 및 정성적 분석 실험 결과, Tran-GCN 모델이 모니터링 비디오에서 구별 가능한 사람 특징을 더 정확하게 포착할 수 있어 식별 정확도가 크게 향상됨을 보여주었습니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.09403.pdf

Title: AI-Driven Virtual Teacher for Enhanced Educational Efficiency: Leveraging Large Pretrain Models for Autonomous Error Analysis and Correction

Original Abstract:
Students frequently make mistakes while solving mathematical problems, and traditional error correction methods are both time-consuming and labor-intensive. This paper introduces an innovative \textbf{V}irtual \textbf{A}I \textbf{T}eacher system designed to autonomously analyze and correct student \textbf{E}rrors (VATE). Leveraging advanced large language models (LLMs), the system uses student drafts as a primary source for error analysis, which enhances understanding of the student's learning process. It incorporates sophisticated prompt engineering and maintains an error pool to reduce computational overhead. The AI-driven system also features a real-time dialogue component for efficient student interaction. Our approach demonstrates significant advantages over traditional and machine learning-based error correction methods, including reduced educational costs, high scalability, and superior generalizability. The system has been deployed on the Squirrel AI learning platform for elementary mathematics education, where it achieves 78.3\% accuracy in error analysis and shows a marked improvement in student learning efficiency. Satisfaction surveys indicate a strong positive reception, highlighting the system's potential to transform educational practices.

Translated Abstract:
학생들이 수학 문제를 풀 때 자주 실수를 하는데, 기존의 오류 수정 방법은 시간도 많이 들고 힘들어요. 이 논문에서는 학생의 실수를 자율적으로 분석하고 수정하는 \textbf{V}irtual \textbf{A}I \textbf{T}eacher 시스템(VATE)를 소개해요. 

이 시스템은 최신 대형 언어 모델(LLM)을 활용해서 학생의 초안을 주된 자료로 사용해 오류를 분석해요. 이렇게 하면 학생의 학습 과정을 더 잘 이해할 수 있어요. 그리고 더 효율적으로 오류를 처리하기 위해 고급 프롬프트 엔지니어링과 오류 풀을 유지하고 있어요. AI 기반 시스템은 학생과의 효율적인 상호작용을 위해 실시간 대화 기능도 포함되어 있어요.

우리의 접근 방식은 기존의 오류 수정 방법이나 머신러닝 기반 방법에 비해 여러 가지 장점이 있어요. 교육 비용을 줄이고, 높은 확장성을 가지며, 일반화 능력이 뛰어나요. 이 시스템은 초등 수학 교육을 위한 Squirrel AI 학습 플랫폼에 배포되었고, 오류 분석에서 78.3% 정확도를 달성하며 학생의 학습 효율성도 크게 향상되었어요. 만족도 조사에서도 긍정적인 반응을 얻었고, 이 시스템이 교육 방식을 혁신할 가능성이 크다는 것을 보여줍니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.09406.pdf

Title: Real-world Adversarial Defense against Patch Attacks based on Diffusion Model

Original Abstract:
Adversarial patches present significant challenges to the robustness of deep learning models, making the development of effective defenses become critical for real-world applications. This paper introduces DIFFender, a novel DIFfusion-based DeFender framework that leverages the power of a text-guided diffusion model to counter adversarial patch attacks. At the core of our approach is the discovery of the Adversarial Anomaly Perception (AAP) phenomenon, which enables the diffusion model to accurately detect and locate adversarial patches by analyzing distributional anomalies. DIFFender seamlessly integrates the tasks of patch localization and restoration within a unified diffusion model framework, enhancing defense efficacy through their close interaction. Additionally, DIFFender employs an efficient few-shot prompt-tuning algorithm, facilitating the adaptation of the pre-trained diffusion model to defense tasks without the need for extensive retraining. Our comprehensive evaluation, covering image classification and face recognition tasks, as well as real-world scenarios, demonstrates DIFFender's robust performance against adversarial attacks. The framework's versatility and generalizability across various settings, classifiers, and attack methodologies mark a significant advancement in adversarial patch defense strategies. Except for the popular visible domain, we have identified another advantage of DIFFender: its capability to easily expand into the infrared domain. Consequently, we demonstrate the good flexibility of DIFFender, which can defend against both infrared and visible adversarial patch attacks alternatively using a universal defense framework.

Translated Abstract:
적대적 패치는 딥러닝 모델의 강건성에 큰 도전을 주기 때문에, 실제 응용을 위해 효과적인 방어 기술 개발이 중요해졌어. 이 논문에서는 DIFFender라는 새로운 DIFusion 기반 방어 프레임워크를 소개해. 이건 텍스트 가이드 확산 모델의 힘을 활용해서 적대적 패치 공격에 맞서 싸우는 방법이야.

우리 접근 방식의 핵심은 Adversarial Anomaly Perception (AAP) 현상을 발견한 거야. 이 현상 덕분에 확산 모델이 분포의 이상을 분석해서 적대적 패치를 정확하게 감지하고 위치를 찾을 수 있어. DIFFender는 패치의 위치 찾기와 복원 작업을 하나의 통합된 확산 모델 프레임워크 안에서 자연스럽게 연결해. 이렇게 서로 밀접하게 상호작용함으로써 방어의 효율성을 높여.

또한, DIFFender는 효율적인 몇 개의 샷 프롬프트 튜닝 알고리즘을 사용해서, 사전 학습된 확산 모델을 방어 작업에 맞게 쉽게 조정할 수 있게 해. 광범위한 재학습 없이도 가능해. 우리의 종합적인 평가에서는 이미지 분류와 얼굴 인식 작업, 그리고 실제 상황을 포함해서 DIFFender가 적대적 공격에 대해 뛰어난 성과를 보였어.

이 프레임워크는 다양한 환경, 분류기, 공격 방법론에 걸쳐 다용성과 일반화를 보여줘. 이는 적대적 패치 방어 전략에서 중요한 발전이라고 할 수 있어. 또, 일반적으로 잘 알려진 가시 영역 외에도 DIFFender의 또 다른 장점은 적외선 영역으로 쉽게 확장할 수 있다는 거야. 그래서 우리는 DIFFender가 적외선과 가시적 적대적 패치 공격 모두에 대해 보편적인 방어 프레임워크를 사용해서 잘 방어할 수 있는 유연성을 보여주었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09412.pdf

Title: Label Convergence: Defining an Upper Performance Bound in Object Recognition through Contradictory Annotations

Original Abstract:
Annotation errors are a challenge not only during training of machine learning models, but also during their evaluation. Label variations and inaccuracies in datasets often manifest as contradictory examples that deviate from established labeling conventions. Such inconsistencies, when significant, prevent models from achieving optimal performance on metrics such as mean Average Precision (mAP). We introduce the notion of "label convergence" to describe the highest achievable performance under the constraint of contradictory test annotations, essentially defining an upper bound on model accuracy.
Recognizing that noise is an inherent characteristic of all data, our study analyzes five real-world datasets, including the LVIS dataset, to investigate the phenomenon of label convergence. We approximate that label convergence is between 62.63-67.52 mAP@[0.5:0.95:0.05] for LVIS with 95% confidence, attributing these bounds to the presence of real annotation errors. With current state-of-the-art (SOTA) models at the upper end of the label convergence interval for the well-studied LVIS dataset, we conclude that model capacity is sufficient to solve current object detection problems. Therefore, future efforts should focus on three key aspects: (1) updating the problem specification and adjusting evaluation practices to account for unavoidable label noise, (2) creating cleaner data, especially test data, and (3) including multi-annotated data to investigate annotation variation and make these issues visible from the outset.

Translated Abstract:
주석 오류는 머신 러닝 모델을 훈련할 때뿐만 아니라 평가할 때도 큰 문제야. 데이터셋에서 레이블의 차이나 부정확함이 종종 서로 모순되는 예시로 나타나는데, 이게 기존의 레이블 규칙에서 벗어나게 돼. 이러한 불일치가 심각하면 모델이 평균 평균 정밀도(mAP) 같은 성능 지표에서 최적의 성능을 내지 못하게 돼. 

우리는 "레이블 수렴"이라는 개념을 소개하는데, 이건 모순된 테스트 주석의 제약 아래에서 달성할 수 있는 최상의 성능을 설명해. 사실상 모델 정확도의 상한선을 정의하는 거야. 데이터의 노이즈가 모든 데이터의 본질적인 특성이라는 걸 인식하고, 우리는 LVIS 데이터셋을 포함한 다섯 개의 실제 데이터셋을 분석해서 레이블 수렴 현상을 조사했어. LVIS에 대해 95% 신뢰도로 레이블 수렴이 62.63-67.52 mAP@[0.5:0.95:0.05] 사이에 있을 것으로 추정했는데, 이는 실제 주석 오류의 존재 때문이야. 

현재의 최신 모델들이 잘 연구된 LVIS 데이터셋의 레이블 수렴 구간의 상단에 위치해 있다는 걸 보면, 모델의 능력이 현재의 객체 탐지 문제를 해결하기에 충분하다는 결론을 내렸어. 그래서 앞으로는 세 가지 주요 측면에 집중해야 해: (1) 불가피한 레이블 노이즈를 반영하기 위해 문제 정의와 평가 방식을 업데이트하고 조정하기, (2) 더 깨끗한 데이터, 특히 테스트 데이터를 만드는 것, (3) 주석 변화를 조사하고 이런 문제를 처음부터 드러내기 위해 다중 주석 데이터를 포함하는 것.

================================================================================

URL:
https://arxiv.org/pdf/2409.09424.pdf

Title: NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection

Original Abstract:
Data augmentation has seen significant advancements in computer vision to improve model performance over the years, particularly in scenarios with limited and insufficient data. Currently, most studies focus on adjusting the image or its features to expand the size, quality, and variety of samples during training in various tasks including object detection. However, we argue that it is necessary to investigate bounding box transformations as a model regularization technique rather than image-level transformations, especially in aerial imagery due to potentially inconsistent bounding box annotations. Hence, this letter presents a thorough investigation of bounding box transformation in terms of scaling, rotation, and translation for remote sensing object detection. We call this augmentation strategy NBBOX (Noise Injection into Bounding Box). We conduct extensive experiments on DOTA and DIOR-R, both well-known datasets that include a variety of rotated generic objects in aerial images. Experimental results show that our approach significantly improves remote sensing object detection without whistles and bells and it is more time-efficient than other state-of-the-art augmentation strategies.

Translated Abstract:
데이터 증강은 컴퓨터 비전 분야에서 모델 성능을 높이기 위해 많은 발전이 있었어. 특히 데이터가 부족한 경우에 효과적이야. 현재 대부분의 연구는 이미지나 이미지의 특징을 조정해서 훈련 샘플의 크기, 품질, 다양성을 늘리는 데 집중하고 있어. 주로 물체 탐지 같은 다양한 작업에서 말이지.

하지만 우리는 이미지 수준의 변환보다는 경계 상자 변환을 모델 규제 기법으로 연구하는 게 필요하다고 주장해. 특히 항공 이미지에서는 경계 상자 주석이 일관되지 않을 수 있으니까. 그래서 우리는 원격 감지 물체 탐지를 위해 스케일, 회전, 이동 측면에서 경계 상자 변환을 자세히 조사했어. 이 증강 전략을 NBBOX(Noise Injection into Bounding Box)라고 부르기로 했어.

DOTA와 DIOR-R라는 두 가지 잘 알려진 데이터셋에서 실험을 했어. 이 데이터셋에는 다양한 회전된 일반 물체가 항공 이미지에 포함되어 있어. 실험 결과, 우리의 접근법이 원격 감지 물체 탐지를 크게 개선했음을 보여줬어. 복잡한 장치 없이도 훨씬 시간 효율적이었고, 다른 최신 증강 전략들보다 더 나은 성과를 냈어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09430.pdf

Title: Evaluating Pre-trained Convolutional Neural Networks and Foundation Models as Feature Extractors for Content-based Medical Image Retrieval

Original Abstract:
Medical image retrieval refers to the task of finding similar images for given query images in a database, with applications such as diagnosis support, treatment planning, and educational tools for inexperienced medical practitioners. While traditional medical image retrieval was performed using clinical metadata, content-based medical image retrieval (CBMIR) relies on the characteristic features of the images, such as color, texture, shape, and spatial features. Many approaches have been proposed for CBMIR, and among them, using pre-trained convolutional neural networks (CNNs) is a widely utilized approach. However, considering the recent advances in the development of foundation models for various computer vision tasks, their application for CBMIR can be also investigated for its potentially superior performance.
In this study, we used several pre-trained feature extractors from well-known pre-trained CNNs (VGG19, ResNet-50, DenseNet121, and EfficientNetV2M) and pre-trained foundation models (MedCLIP, BioMedCLIP, OpenCLIP, CONCH and UNI) and investigated the CBMIR performance on a subset of the MedMNIST V2 dataset, including eight types of 2D and 3D medical images. Furthermore, we also investigated the effect of image size on the CBMIR performance.
Our results show that, overall, for the 2D datasets, foundation models deliver superior performance by a large margin compared to CNNs, with UNI providing the best overall performance across all datasets and image sizes. For 3D datasets, CNNs and foundation models deliver more competitive performance, with CONCH achieving the best overall performance. Moreover, our findings confirm that while using larger image sizes (especially for 2D datasets) yields slightly better performance, competitive CBMIR performance can still be achieved even with smaller image sizes. Our codes to generate and reproduce the results are available on GitHub.

Translated Abstract:
의료 이미지 검색은 데이터베이스에서 주어진 쿼리 이미지와 유사한 이미지를 찾는 작업을 말해. 이건 진단 지원, 치료 계획, 경험이 부족한 의료 전문가들을 위한 교육 도구 등 다양한 분야에 활용될 수 있어. 전통적인 의료 이미지 검색은 임상 메타데이터를 사용했지만, 내용 기반 의료 이미지 검색(CBMIR)은 이미지의 색상, 질감, 형태, 공간적 특징 같은 특성에 의존해.

CBMIR을 위한 여러 접근 방법이 제안됐고, 그 중에서도 사전 훈련된 컨볼루션 신경망(CNN)을 사용하는 방법이 많이 쓰여. 하지만 최근에 다양한 컴퓨터 비전 작업을 위한 기초 모델들이 발전하면서, CBMIR에 이 모델들을 적용해보는 것도 성능 면에서 더 나을 수 있어.

이 연구에서는 잘 알려진 사전 훈련된 CNN(VGG19, ResNet-50, DenseNet121, EfficientNetV2M)과 기초 모델(MedCLIP, BioMedCLIP, OpenCLIP, CONCH, UNI)에서 여러 개의 사전 훈련된 특징 추출기를 사용하고, MedMNIST V2 데이터셋의 일부에서 CBMIR 성능을 조사했어. 이 데이터셋에는 2D와 3D 의료 이미지 8종이 포함돼. 또한 이미지 크기가 CBMIR 성능에 미치는 영향도 살펴봤어.

우리의 결과에 따르면, 전반적으로 2D 데이터셋에서는 기초 모델이 CNN에 비해 성능이 훨씬 더 높아. UNI가 모든 데이터셋과 이미지 크기에서 가장 좋은 성능을 보였어. 3D 데이터셋의 경우, CNN과 기초 모델이 경쟁력 있는 성능을 내고, CONCH가 가장 좋은 성능을 기록했어. 게다가, 큰 이미지 크기를 사용할 때(특히 2D 데이터셋에서) 성능이 약간 좋아지긴 하지만, 작은 이미지 크기에서도 경쟁력 있는 CBMIR 성능을 여전히 달성할 수 있다는 걸 확인했어. 우리의 결과를 생성하고 재현할 수 있는 코드는 GitHub에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09432.pdf

Title: Detecting Looted Archaeological Sites from Satellite Image Time Series

Original Abstract:
Archaeological sites are the physical remains of past human activity and one of the main sources of information about past societies and cultures. However, they are also the target of malevolent human actions, especially in countries having experienced inner turmoil and conflicts. Because monitoring these sites from space is a key step towards their preservation, we introduce the DAFA Looted Sites dataset, \datasetname, a labeled multi-temporal remote sensing dataset containing 55,480 images acquired monthly over 8 years across 675 Afghan archaeological sites, including 135 sites looted during the acquisition period. \datasetname~is particularly challenging because of the limited number of training samples, the class imbalance, the weak binary annotations only available at the level of the time series, and the subtlety of relevant changes coupled with important irrelevant ones over a long time period. It is also an interesting playground to assess the performance of satellite image time series (SITS) classification methods on a real and important use case. We evaluate a large set of baselines, outline the substantial benefits of using foundation models and show the additional boost that can be provided by using complete time series instead of using a single image.

Translated Abstract:
고고학 유적지는 과거 인간 활동의 물리적 흔적이며, 과거 사회와 문화에 대한 중요한 정보의 출처야. 하지만 이런 유적지는 특히 내전이나 갈등을 겪은 나라에서 악의적인 인간 행동의 대상이 되기도 해. 

우리는 이러한 유적지를 보존하기 위해 우주에서 모니터링하는 것이 중요한 첫걸음이라고 생각해. 그래서 DAFA Looted Sites 데이터셋을 소개할게. 이 데이터셋은 8년 동안 매달 수집된 55,480장의 이미지로 구성되어 있고, 675개의 아프가니스탄 고고학 유적지를 포함하고 있어. 그중 135개는 이미지 수집 기간 동안 약탈당한 곳이야. 

이 데이터셋은 훈련 샘플이 적고, 클래스 불균형이 심하며, 시간 시계열 수준에서만 사용할 수 있는 약한 이진 주석이 있다는 점에서 특히 도전적이야. 또 오랜 시간 동안 관련된 변화와 중요하지 않은 변화가 섞여 있어서 구분하기가 어려워. 

이 데이터셋은 위성 이미지 시간 시계열(SITS) 분류 방법의 성능을 실제 사례에 적용해 평가해볼 수 있는 흥미로운 놀이터이기도 해. 우리는 다양한 기준선을 평가하고, 기초 모델을 사용할 때의 장점을 설명할 거야. 그리고 단일 이미지를 사용하는 대신 완전한 시간 시계열을 사용할 때 추가적인 성능 향상을 보여줄 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09444.pdf

Title: KAN-HyperpointNet for Point Cloud Sequence-Based 3D Human Action Recognition

Original Abstract:
Point cloud sequence-based 3D action recognition has achieved impressive performance and efficiency. However, existing point cloud sequence modeling methods cannot adequately balance the precision of limb micro-movements with the integrity of posture macro-structure, leading to the loss of crucial information cues in action inference. To overcome this limitation, we introduce D-Hyperpoint, a novel data type generated through a D-Hyperpoint Embedding module. D-Hyperpoint encapsulates both regional-momentary motion and global-static posture, effectively summarizing the unit human action at each moment. In addition, we present a D-Hyperpoint KANsMixer module, which is recursively applied to nested groupings of D-Hyperpoints to learn the action discrimination information and creatively integrates Kolmogorov-Arnold Networks (KAN) to enhance spatio-temporal interaction within D-Hyperpoints. Finally, we propose KAN-HyperpointNet, a spatio-temporal decoupled network architecture for 3D action recognition. Extensive experiments on two public datasets: MSR Action3D and NTU-RGB+D 60, demonstrate the state-of-the-art performance of our method.

Translated Abstract:
포인트 클라우드 시퀀스를 기반으로 한 3D 동작 인식이 꽤 인상적인 성능과 효율성을 보여주고 있어. 하지만 기존의 포인트 클라우드 시퀀스 모델링 방법들은 팔이나 다리의 미세한 움직임의 정확성과 자세의 큰 구조를 잘 조화시키지 못해. 이 때문에 동작 추론에서 중요한 정보가 사라지는 문제가 있어.

이런 한계를 극복하기 위해 우리는 D-Hyperpoint라는 새로운 데이터 타입을 도입했어. D-Hyperpoint는 D-Hyperpoint 임베딩 모듈을 통해 생성되며, 지역적인 순간적 움직임과 전반적인 정적인 자세를 모두 포괄해. 이렇게 해서 각 순간의 인간 동작을 효과적으로 요약할 수 있어.

또한, D-Hyperpoint KANsMixer 모듈을 제시하는데, 이 모듈은 D-Hyperpoint의 중첩된 그룹에 반복적으로 적용돼. 이 과정에서 동작 구별 정보를 배우고, Kolmogorov-Arnold Networks (KAN)을 창의적으로 통합해서 D-Hyperpoint 내의 시공간 상호작용을 강화해.

마지막으로, 우리는 3D 동작 인식을 위한 시공간 분리 네트워크 구조인 KAN-HyperpointNet을 제안해. MSR Action3D와 NTU-RGB+D 60이라는 두 개의 공공 데이터셋에서 광범위한 실험을 진행했는데, 우리 방법이 최첨단 성능을 보였어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09446.pdf

Title: MulCPred: Learning Multi-modal Concepts for Explainable Pedestrian Action Prediction

Original Abstract:
Pedestrian action prediction is of great significance for many applications such as autonomous driving. However, state-of-the-art methods lack explainability to make trustworthy predictions. In this paper, a novel framework called MulCPred is proposed that explains its predictions based on multi-modal concepts represented by training samples. Previous concept-based methods have limitations including: 1) they cannot directly apply to multi-modal cases; 2) they lack locality to attend to details in the inputs; 3) they suffer from mode collapse. These limitations are tackled accordingly through the following approaches: 1) a linear aggregator to integrate the activation results of the concepts into predictions, which associates concepts of different modalities and provides ante-hoc explanations of the relevance between the concepts and the predictions; 2) a channel-wise recalibration module that attends to local spatiotemporal regions, which enables the concepts with locality; 3) a feature regularization loss that encourages the concepts to learn diverse patterns. MulCPred is evaluated on multiple datasets and tasks. Both qualitative and quantitative results demonstrate that MulCPred is promising in improving the explainability of pedestrian action prediction without obvious performance degradation. Furthermore, by removing unrecognizable concepts from MulCPred, the cross-dataset prediction performance is improved, indicating the feasibility of further generalizability of MulCPred.

Translated Abstract:
보행자 행동 예측은 자율 주행 같은 여러 응용 프로그램에서 매우 중요해. 하지만 최신 방법들은 신뢰할 수 있는 예측을 위해 설명 가능성이 부족해. 이 논문에서는 MulCPred라는 새로운 프레임워크를 제안하는데, 이건 훈련 샘플로 표현된 다중 모달 개념을 기반으로 예측을 설명해.

이전의 개념 기반 방법들은 몇 가지 한계가 있어: 
1) 다중 모달 상황에 직접 적용할 수 없어; 
2) 입력의 세부 사항에 주의를 기울이는 지역성이 부족해; 
3) 모드 붕괴 문제도 있어. 

이런 한계는 다음과 같은 방법으로 해결했어: 
1) 선형 집계기를 사용해서 개념의 활성화 결과를 예측에 통합해. 이건 서로 다른 모달리티의 개념을 연결하고 개념과 예측 간의 관련성을 사전 설명해줘; 
2) 지역적 시공간 영역에 주의를 기울이는 채널별 재조정 모듈을 통해 개념에 지역성을 부여해; 
3) 다양한 패턴을 배우도록 유도하는 피처 정규화 손실을 추가했어.

MulCPred는 여러 데이터셋과 작업에서 평가됐어. 질적, 양적 결과 모두 MulCPred가 보행자 행동 예측의 설명 가능성을 개선하는 데 유망하다는 걸 보여줘, 성능 저하 없이 말이야. 게다가 MulCPred에서 인식할 수 없는 개념을 제거하면 크로스 데이터셋 예측 성능이 향상돼, 이는 MulCPred의 일반화 가능성을 더 높일 수 있다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09451.pdf

Title: On the Generalizability of Foundation Models for Crop Type Mapping

Original Abstract:
Foundation models pre-trained using self-supervised and weakly-supervised learning have shown powerful transfer learning capabilities on various downstream tasks, including language understanding, text generation, and image recognition. Recently, the Earth observation (EO) field has produced several foundation models pre-trained directly on multispectral satellite imagery (e.g., Sentinel-2) for applications like precision agriculture, wildfire and drought monitoring, and natural disaster response. However, few studies have investigated the ability of these models to generalize to new geographic locations, and potential concerns of geospatial bias -- models trained on data-rich developed countries not transferring well to data-scarce developing countries -- remain. We investigate the ability of popular EO foundation models to transfer to new geographic regions in the agricultural domain, where differences in farming practices and class imbalance make transfer learning particularly challenging. We first select six crop classification datasets across five continents, normalizing for dataset size and harmonizing classes to focus on four major cereal grains: maize, soybean, rice, and wheat. We then compare three popular foundation models, pre-trained on SSL4EO-S12, SatlasPretrain, and ImageNet, using in-distribution (ID) and out-of-distribution (OOD) evaluation. Experiments show that pre-trained weights designed explicitly for Sentinel-2, such as SSL4EO-S12, outperform general pre-trained weights like ImageNet. Furthermore, the benefits of pre-training on OOD data are the most significant when only 10--100 ID training samples are used. Transfer learning and pre-training with OOD and limited ID data show promising applications, as many developing regions have scarce crop type labels. All harmonized datasets and experimental code are open-source and available for download.

Translated Abstract:
기초 모델은 자기 지도 학습과 약한 지도 학습을 이용해 미리 훈련된 모델로, 언어 이해, 텍스트 생성, 이미지 인식 같은 다양한 작업에서 강력한 전이 학습 능력을 보여줬어. 최근에는 지구 관측(EO) 분야에서 멀티스펙트럼 위성 이미지(예: Sentinel-2)를 직접 사용해 미리 훈련된 몇 가지 기초 모델이 나왔고, 이 모델들은 정밀 농업, 산불 및 가뭄 모니터링, 자연 재해 대응 같은 응용 분야에 활용되고 있어.

하지만 이 모델들이 새로운 지리적 위치로 일반화할 수 있는 능력에 대한 연구는 거의 없고, 데이터가 풍부한 선진국에서 훈련된 모델이 데이터가 부족한 개발도상국에 잘 적용되지 않는 지리적 편향 문제도 여전히 남아 있어. 우리는 인기 있는 EO 기초 모델들이 농업 분야에서 새로운 지리적 지역으로 전이할 수 있는 능력을 조사했어. 농업 관행의 차이와 클래스 불균형 때문에 전이 학습이 특히 어려운 상황이야.

우리는 먼저 다섯 대륙에 걸쳐 여섯 개의 작물 분류 데이터셋을 선택하고, 데이터셋 크기를 정규화하고 클래스를 조화시켜서 주요 네 가지 곡물인 옥수수, 대두, 쌀, 밀에 집중했어. 그리고 SSL4EO-S12, SatlasPretrain, ImageNet에서 미리 훈련된 세 가지 인기 있는 기초 모델을 비교했어. 실험 결과, Sentinel-2에 맞게 특별히 설계된 미리 훈련된 가중치인 SSL4EO-S12가 일반적인 미리 훈련된 가중치인 ImageNet보다 더 좋은 성능을 보였어. 또한 OOD 데이터로 미리 훈련하는 것의 이점은 ID 훈련 샘플이 10~100개만 있을 때 가장 두드러지더라고.

전이 학습과 OOD 및 제한된 ID 데이터를 활용한 미리 훈련은 유망한 응용 가능성을 보여주고 있어. 많은 개발 지역에서 작물 유형 레이블이 부족한 상황이기 때문이야. 모든 조화된 데이터셋과 실험 코드는 오픈 소스로 제공되며, 다운로드할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09455.pdf

Title: Learning Keypoints for Multi-Agent Behavior Analysis using Self-Supervision

Original Abstract:
The study of social interactions and collective behaviors through multi-agent video analysis is crucial in biology. While self-supervised keypoint discovery has emerged as a promising solution to reduce the need for manual keypoint annotations, existing methods often struggle with videos containing multiple interacting agents, especially those of the same species and color. To address this, we introduce B-KinD-multi, a novel approach that leverages pre-trained video segmentation models to guide keypoint discovery in multi-agent scenarios. This eliminates the need for time-consuming manual annotations on new experimental settings and organisms. Extensive evaluations demonstrate improved keypoint regression and downstream behavioral classification in videos of flies, mice, and rats. Furthermore, our method generalizes well to other species, including ants, bees, and humans, highlighting its potential for broad applications in automated keypoint annotation for multi-agent behavior analysis. Code available under: this https URL

Translated Abstract:
다중 에이전트 비디오 분석을 통한 사회적 상호작용과 집단 행동 연구는 생물학에서 매우 중요해. 최근에 자기 지도 방식의 키포인트 발견 방법이 수동으로 키포인트 주석을 달 필요성을 줄여주는 유망한 해결책으로 떠올랐는데, 기존 방법들은 종종 같은 종이나 색상의 여러 상호작용 에이전트가 있는 비디오에서는 잘 작동하지 않아. 

이 문제를 해결하기 위해, 우리는 B-KinD-multi라는 새로운 방법을 소개해. 이 방법은 사전 훈련된 비디오 분할 모델을 활용해서 다중 에이전트 상황에서 키포인트 발견 과정을 도와줘. 이렇게 하면 새로운 실험 환경이나 생물체에 대해 시간을 많이 소모하는 수동 주석 작업이 필요 없어. 

광범위한 평가 결과, 파리, 쥐, 쥐의 비디오에서 키포인트 회귀와 이후 행동 분류가 개선된 걸 보여줬어. 게다가 우리 방법은 개미, 벌, 인간 같은 다른 종에도 잘 일반화되니까, 다중 에이전트 행동 분석을 위한 자동 키포인트 주석의 다양한 응용 가능성을 강조하고 있어. 

코드는 이 링크에서 확인할 수 있어: this https URL

================================================================================

URL:
https://arxiv.org/pdf/2409.09497.pdf

Title: Multi-Scale Grouped Prototypes for Interpretable Semantic Segmentation

Original Abstract:
Prototypical part learning is emerging as a promising approach for making semantic segmentation interpretable. The model selects real patches seen during training as prototypes and constructs the dense prediction map based on the similarity between parts of the test image and the prototypes. This improves interpretability since the user can inspect the link between the predicted output and the patterns learned by the model in terms of prototypical information. In this paper, we propose a method for interpretable semantic segmentation that leverages multi-scale image representation for prototypical part learning. First, we introduce a prototype layer that explicitly learns diverse prototypical parts at several scales, leading to multi-scale representations in the prototype activation output. Then, we propose a sparse grouping mechanism that produces multi-scale sparse groups of these scale-specific prototypical parts. This provides a deeper understanding of the interactions between multi-scale object representations while enhancing the interpretability of the segmentation model. The experiments conducted on Pascal VOC, Cityscapes, and ADE20K demonstrate that the proposed method increases model sparsity, improves interpretability over existing prototype-based methods, and narrows the performance gap with the non-interpretable counterpart models. Code is available at this http URL.

Translated Abstract:
프로토타입 부분 학습은 의미론적 분할을 더 이해하기 쉽게 만드는 유망한 접근법으로 떠오르고 있어. 이 모델은 훈련 중에 본 실제 패치들을 프로토타입으로 선택하고, 테스트 이미지의 부분과 프로토타입 간의 유사성을 바탕으로 밀집 예측 맵을 만들어. 이렇게 하면 사용자들이 예측된 결과와 모델이 학습한 패턴 간의 연결을 프로토타입 정보를 통해 확인할 수 있어서 해석 가능성이 높아져.

이 논문에서는 프로토타입 부분 학습을 위해 다중 스케일 이미지 표현을 활용하는 해석 가능한 의미론적 분할 방법을 제안해. 먼저, 여러 스케일에서 다양한 프로토타입 부분을 명시적으로 학습하는 프로토타입 레이어를 도입해서 프로토타입 활성화 출력에서 다중 스케일 표현을 만들어. 그 다음에는 이 스케일별 프로토타입 부분의 다중 스케일 희소 그룹을 생성하는 희소 그룹화 메커니즘을 제안해. 이렇게 하면 다중 스케일 객체 표현 간의 상호작용을 더 깊이 이해할 수 있고 분할 모델의 해석 가능성도 높아져.

Pascal VOC, Cityscapes, ADE20K에서 실시한 실험 결과, 제안한 방법이 모델의 희소성을 증가시키고 기존 프로토타입 기반 방법들에 비해 해석 가능성을 개선하며, 비해석 가능한 모델들과의 성능 차이를 줄이는 것을 보여줬어. 코드도 이 http URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09502.pdf

Title: One missing piece in Vision and Language: A Survey on Comics Understanding

Original Abstract:
Vision-language models have recently evolved into versatile systems capable of high performance across a range of tasks, such as document understanding, visual question answering, and grounding, often in zero-shot settings. Comics Understanding, a complex and multifaceted field, stands to greatly benefit from these advances. Comics, as a medium, combine rich visual and textual narratives, challenging AI models with tasks that span image classification, object detection, instance segmentation, and deeper narrative comprehension through sequential panels. However, the unique structure of comics -- characterized by creative variations in style, reading order, and non-linear storytelling -- presents a set of challenges distinct from those in other visual-language domains. In this survey, we present a comprehensive review of Comics Understanding from both dataset and task perspectives. Our contributions are fivefold: (1) We analyze the structure of the comics medium, detailing its distinctive compositional elements; (2) We survey the widely used datasets and tasks in comics research, emphasizing their role in advancing the field; (3) We introduce the Layer of Comics Understanding (LoCU) framework, a novel taxonomy that redefines vision-language tasks within comics and lays the foundation for future work; (4) We provide a detailed review and categorization of existing methods following the LoCU framework; (5) Finally, we highlight current research challenges and propose directions for future exploration, particularly in the context of vision-language models applied to comics. This survey is the first to propose a task-oriented framework for comics intelligence and aims to guide future research by addressing critical gaps in data availability and task definition. A project associated with this survey is available at this https URL.

Translated Abstract:
비전-언어 모델이 최근에 다양한 작업에서 높은 성능을 발휘할 수 있는 다재다능한 시스템으로 발전했어. 문서 이해, 시각적 질문 응답, 그리고 기반 설정에서의 작업들이 그런 예지. 만화 이해는 복잡하고 다면적인 분야여서 이런 발전으로 큰 혜택을 받을 수 있어. 만화는 시각적이고 텍스트적인 내러티브가 결합된 매체로, 이미지 분류, 객체 탐지, 인스턴스 분할, 그리고 순차적인 패널을 통한 깊은 내러티브 이해 같은 다양한 작업에서 AI 모델에 도전해.

하지만 만화의 독특한 구조는 스타일, 읽기 순서, 비선형 이야기 전개에서의 창의적인 변형으로 특징지어져서 다른 비주얼-언어 분야와는 다른 도전 과제를 제시해. 이 서베이에서는 데이터셋과 작업 관점에서 만화 이해에 대한 포괄적인 리뷰를 제공해. 우리의 기여는 다섯 가지야:

(1) 만화 매체의 구조를 분석하고, 그 특유의 구성 요소를 자세히 설명해;  
(2) 만화 연구에서 널리 사용되는 데이터셋과 작업을 조사하고, 이들이 분야 발전에 기여하는 역할을 강조해;  
(3) 만화 이해의 층(Layer of Comics Understanding, LoCU) 프레임워크를 소개해, 이는 만화 내에서 비전-언어 작업을 재정의하는 새로운 분류법이야;  
(4) LoCU 프레임워크에 따라 기존 방법들을 자세히 리뷰하고 분류해;  
(5) 마지막으로 현재 연구의 도전 과제를 강조하고, 만화에 적용된 비전-언어 모델의 맥락에서 미래 탐사를 위한 방향을 제안해.

이 서베이는 만화 지능을 위한 작업 지향 프레임워크를 처음 제안한 것이고, 데이터 가용성과 작업 정의에서의 중요한 격차를 다루며 미래 연구를 안내하는 걸 목표로 해. 이 서베이와 관련된 프로젝트는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09520.pdf

Title: Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment

Original Abstract:
Current AI-assisted skin image diagnosis has achieved dermatologist-level performance in classifying skin cancer, driven by rapid advancements in deep learning architectures. However, unlike traditional vision tasks, skin images in general present unique challenges due to the limited availability of well-annotated datasets, complex variations in conditions, and the necessity for detailed interpretations to ensure patient safety. Previous segmentation methods have sought to reduce image noise and enhance diagnostic performance, but these techniques require fine-grained, pixel-level ground truth masks for training. In contrast, with the rise of foundation models, the Segment Anything Model (SAM) has been introduced to facilitate promptable segmentation, enabling the automation of the segmentation process with simple yet effective prompts. Efforts applying SAM predominantly focus on dermatoscopy images, which present more easily identifiable lesion boundaries than clinical photos taken with smartphones. This limitation constrains the practicality of these approaches to real-world applications. To overcome the challenges posed by noisy clinical photos acquired via non-standardized protocols and to improve diagnostic accessibility, we propose a novel Cross-Attentive Fusion framework for interpretable skin lesion diagnosis. Our method leverages SAM to generate visual concepts for skin diseases using prompts, integrating local visual concepts with global image features to enhance model performance. Extensive evaluation on two skin disease datasets demonstrates our proposed method's effectiveness on lesion diagnosis and interpretability.

Translated Abstract:
현재 AI를 활용한 피부 이미지 진단은 피부암 분류에서 피부과 의사 수준의 성능을 달성했어. 이건 딥러닝 기술이 빠르게 발전하면서 가능해진 거야. 하지만 전통적인 비전 작업과는 달리, 피부 이미지는 잘 주석이 달린 데이터셋이 부족하고, 조건의 복잡한 변동성과 환자의 안전을 보장하기 위한 자세한 해석이 필요해서 독특한 도전과제를 가지고 있어.

이전의 분할 방법들은 이미지 노이즈를 줄이고 진단 성능을 높이려 했지만, 이 기술들은 훈련을 위해 세밀한 픽셀 단위의 정답 마스크가 필요해. 반면, 기초 모델의 발전과 함께 Segment Anything Model(SAM)이 도입되었어. 이 모델은 간단하면서도 효과적인 프롬프트로 분할 과정을 자동화할 수 있게 해줘.

SAM을 적용한 노력들은 주로 피부 병변의 경계가 더 쉽게 식별되는 피부경 검사 이미지를 중심으로 하고 있어. 하지만 이 제한은 스마트폰으로 찍은 임상 사진에 대한 실제 응용 가능성을 제한하고 있어. 그래서 우리는 표준화되지 않은 프로토콜로 얻은 노이즈가 많은 임상 사진이 주는 문제를 해결하고 진단 접근성을 높이기 위해 새로운 '크로스-어텐티브 퓨전' 프레임워크를 제안해.

우리 방법은 SAM을 활용해서 프롬프트를 통해 피부 질환에 대한 시각적 개념을 생성하고, 지역적 시각적 개념을 글로벌 이미지 특징과 통합해서 모델 성능을 향상시켜. 두 가지 피부 질환 데이터셋에 대한 광범위한 평가를 통해 우리 방법의 병변 진단과 해석 가능성에 대한 효과를 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09530.pdf

Title: An Augmentation-based Model Re-adaptation Framework for Robust Image Segmentation

Original Abstract:
Image segmentation is a crucial task in computer vision, with wide-ranging applications in industry. The Segment Anything Model (SAM) has recently attracted intensive attention; however, its application in industrial inspection, particularly for segmenting commercial anti-counterfeit codes, remains challenging. Unlike open-source datasets, industrial settings often face issues such as small sample sizes and complex textures. Additionally, computational cost is a key concern due to the varying number of trainable parameters. To address these challenges, we propose an Augmentation-based Model Re-adaptation Framework (AMRF). This framework leverages data augmentation techniques during training to enhance the generalisation of segmentation models, allowing them to adapt to newly released datasets with temporal disparity. By observing segmentation masks from conventional models (FCN and U-Net) and a pre-trained SAM model, we determine a minimal augmentation set that optimally balances training efficiency and model performance. Our results demonstrate that the fine-tuned FCN surpasses its baseline by 3.29% and 3.02% in cropping accuracy, and 5.27% and 4.04% in classification accuracy on two temporally continuous datasets. Similarly, the fine-tuned U-Net improves upon its baseline by 7.34% and 4.94% in cropping, and 8.02% and 5.52% in classification. Both models outperform the top-performing SAM models (ViT-Large and ViT-Base) by an average of 11.75% and 9.01% in cropping accuracy, and 2.93% and 4.83% in classification accuracy, respectively.

Translated Abstract:
이미지 분할은 컴퓨터 비전에서 중요한 작업으로, 산업에서 다양한 용도로 사용돼. 최근에 Segment Anything Model (SAM)이 큰 주목을 받고 있지만, 산업 검수에서 상업적인 위조 방지 코드를 분할하는 데는 여전히 어려움이 있어. 오픈 소스 데이터셋과는 달리, 산업 환경에서는 작은 샘플 사이즈와 복잡한 텍스처 같은 문제가 발생해. 게다가, 학습 가능한 파라미터 수에 따라 계산 비용도 중요한 문제야.

이런 문제를 해결하기 위해 우리는 Augmentation-based Model Re-adaptation Framework (AMRF)를 제안해. 이 프레임워크는 학습 중 데이터 증강 기법을 활용해서 분할 모델의 일반화를 높이도록 돕고, 새롭게 출시된 데이터셋에 적응할 수 있게 해. 기존 모델(FCN과 U-Net)과 사전 학습된 SAM 모델의 분할 마스크를 관찰하면서, 훈련 효율성과 모델 성능을 최적의 균형으로 맞추는 최소한의 증강 세트를 찾았어.

우리 결과에 따르면, 미세 조정된 FCN은 기본 성능보다 3.29%와 3.02% 향상된 자르기 정확도와 5.27%와 4.04% 향상된 분류 정확도를 보여줘. 비슷하게 미세 조정된 U-Net은 기본 성능보다 7.34%와 4.94% 향상된 자르기 정확도, 8.02%와 5.52% 향상된 분류 정확도를 기록했어. 두 모델 모두 최상위 SAM 모델(ViT-Large와 ViT-Base)보다 각각 평균 11.75%와 9.01% 향상된 자르기 정확도, 2.93%와 4.83% 향상된 분류 정확도를 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09560.pdf

Title: Evaluating authenticity and quality of image captions via sentiment and semantic analyses

Original Abstract:
The growth of deep learning (DL) relies heavily on huge amounts of labelled data for tasks such as natural language processing and computer vision. Specifically, in image-to-text or image-to-image pipelines, opinion (sentiment) may be inadvertently learned by a model from human-generated image captions. Additionally, learning may be affected by the variety and diversity of the provided captions. While labelling large datasets has largely relied on crowd-sourcing or data-worker pools, evaluating the quality of such training data is crucial.
This study proposes an evaluation method focused on sentiment and semantic richness. That method was applied to the COCO-MS dataset, comprising approximately 150K images with segmented objects and corresponding crowd-sourced captions. We employed pre-trained models (Twitter-RoBERTa-base and BERT-base) to extract sentiment scores and variability of semantic embeddings from captions. The relation of the sentiment score and semantic variability with object categories was examined using multiple linear regression. Results indicate that while most captions were neutral, about 6% of the captions exhibited strong sentiment influenced by specific object categories. Semantic variability of within-image captions remained low and uncorrelated with object categories. Model-generated captions showed less than 1.5% of strong sentiment which was not influenced by object categories and did not correlate with the sentiment of the respective human-generated captions. This research demonstrates an approach to assess the quality of crowd- or worker-sourced captions informed by image content.

Translated Abstract:
딥러닝(DL)의 발전은 자연어 처리와 컴퓨터 비전 같은 작업을 위해 많은 양의 라벨이 달린 데이터에 크게 의존해. 특히 이미지-텍스트나 이미지-이미지 파이프라인에서는, 모델이 사람의 이미지 캡션에서 감정(의견)을 무의식적으로 학습할 수 있어. 제공된 캡션의 다양성과 차별성도 학습에 영향을 줄 수 있어. 대량의 데이터셋에 라벨을 다는 건 주로 크라우드소싱이나 데이터 작업자 풀에 의존했지만, 이런 훈련 데이터의 품질을 평가하는 건 정말 중요해.

이 연구는 감정과 의미의 풍부함에 초점을 맞춘 평가 방법을 제안했어. 이 방법은 약 15만 개의 이미지와 그에 맞는 크라우드소싱 캡션으로 구성된 COCO-MS 데이터셋에 적용됐어. 우리는 사전 훈련된 모델(Twitter-RoBERTa-base와 BERT-base)을 사용해서 캡션에서 감정 점수와 의미 임베딩의 변동성을 추출했어. 감정 점수와 의미 변동성의 객체 카테고리 간 관계를 다중 선형 회귀를 통해 분석했어. 결과적으로 대부분의 캡션은 중립적이었고, 약 6%의 캡션은 특정 객체 카테고리에 의해 영향을 받은 강한 감정을 보였어. 이미지 내 캡션의 의미 변동성은 낮고 객체 카테고리와는 상관이 없었어. 모델이 생성한 캡션은 1.5% 미만의 강한 감정을 보여줬고, 객체 카테고리의 영향을 받지 않았으며, 사람 생성 캡션의 감정과도 상관이 없었어. 이 연구는 이미지 콘텐츠에 기반한 크라우드 또는 작업자 소싱 캡션의 품질을 평가하는 접근 방식을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09564.pdf

Title: TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings

Original Abstract:
Currently, inspired by the success of vision-language models (VLMs), an increasing number of researchers are focusing on improving VLMs and have achieved promising results. However, most existing methods concentrate on optimizing the connector and enhancing the language model component, while neglecting improvements to the vision encoder itself. In contrast, we propose Text Guided LLaVA (TG-LLaVA) in this paper, which optimizes VLMs by guiding the vision encoder with text, offering a new and orthogonal optimization direction. Specifically, inspired by the purpose-driven logic inherent in human behavior, we use learnable latent embeddings as a bridge to analyze textual instruction and add the analysis results to the vision encoder as guidance, refining it. Subsequently, another set of latent embeddings extracts additional detailed text-guided information from high-resolution local patches as auxiliary information. Finally, with the guidance of text, the vision encoder can extract text-related features, similar to how humans focus on the most relevant parts of an image when considering a question. This results in generating better answers. Experiments on various datasets validate the effectiveness of the proposed method. Remarkably, without the need for additional training data, our propsoed method can bring more benefits to the baseline (LLaVA-1.5) compared with other concurrent methods. Furthermore, the proposed method consistently brings improvement in different settings.

Translated Abstract:
현재, 비전-언어 모델(VLM)의 성공에 영감을 받아, 점점 더 많은 연구자들이 VLM을 개선하는 데 집중하고 있으며, 좋은 결과를 얻고 있어. 하지만 대부분의 기존 방법들은 커넥터를 최적화하거나 언어 모델 구성 요소를 강화하는 데만 집중하고, 비전 인코더 자체의 개선은 소홀히 하고 있어.

이와 달리, 우리는 이 논문에서 Text Guided LLaVA (TG-LLaVA)를 제안해. 이 방법은 비전 인코더를 텍스트로 안내하여 VLM을 최적화하는 새로운 방향을 제시해. 구체적으로, 인간 행동에 내재된 목적 중심의 논리에 영감을 받아, 학습 가능한 잠재 임베딩을 다리처럼 활용해 텍스트 지침을 분석하고, 그 결과를 비전 인코더에 안내로 추가해 개선하는 방식이야. 이후, 또 다른 잠재 임베딩 세트가 고해상도 지역 패치에서 추가적인 세부 텍스트 지침 정보를 보조 정보로 추출해.

결국 텍스트의 안내를 통해 비전 인코더는 질문을 고려할 때 인간이 이미지에서 가장 관련 있는 부분에 집중하는 것처럼 텍스트와 관련된 특징을 추출할 수 있어. 이로 인해 더 나은 답변을 생성할 수 있게 되지. 다양한 데이터셋에서의 실험 결과, 제안한 방법의 효과가 입증되었어. 특히, 추가적인 훈련 데이터 없이도 우리가 제안한 방법이 다른 경쟁 방법들에 비해 기준 모델(LLaVA-1.5)에게 더 많은 이점을 줄 수 있다는 점이 눈에 띄고, 이 방법은 다양한 환경에서도 꾸준히 개선 효과를 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09566.pdf

Title: Learning Transferable Features for Implicit Neural Representations

Original Abstract:
Implicit neural representations (INRs) have demonstrated success in a variety of applications, including inverse problems and neural rendering. An INR is typically trained to capture one signal of interest, resulting in learned neural features that are highly attuned to that signal. Assumed to be less generalizable, we explore the aspect of transferability of such learned neural features for fitting similar signals. We introduce a new INR training framework, STRAINER that learns transferrable features for fitting INRs to new signals from a given distribution, faster and with better reconstruction quality. Owing to the sequential layer-wise affine operations in an INR, we propose to learn transferable representations by sharing initial encoder layers across multiple INRs with independent decoder layers. At test time, the learned encoder representations are transferred as initialization for an otherwise randomly initialized INR. We find STRAINER to yield extremely powerful initialization for fitting images from the same domain and allow for $\approx +10dB$ gain in signal quality early on compared to an untrained INR itself. STRAINER also provides a simple way to encode data-driven priors in INRs. We evaluate STRAINER on multiple in-domain and out-of-domain signal fitting tasks and inverse problems and further provide detailed analysis and discussion on the transferability of STRAINER's features. Our demo can be accessed at this https URL .

Translated Abstract:
암묵적 신경 표현(Implicit Neural Representations, INRs)은 역문제와 신경 렌더링 등 다양한 응용 분야에서 성공을 거두었어. INR은 일반적으로 하나의 관심 신호를 포착하도록 훈련되며, 그 결과로 해당 신호에 매우 적합한 신경 특징을 배우게 돼. 이런 특징들이 일반화가 덜 된다고 가정했는데, 우리는 비슷한 신호를 맞추기 위한 학습된 신경 특징의 전이 가능성에 대해 탐구해봤어.

우리는 STRAINER라는 새로운 INR 훈련 프레임워크를 소개하는데, 이 프레임워크는 주어진 분포에서 새로운 신호에 맞추기 위해 전이 가능한 특징을 더 빠르고 더 좋은 재구성 품질로 학습해. INR에서의 순차적인 레이어별 선형 변환 덕분에, 우리는 여러 개의 INR에서 독립적인 디코더 레이어와 함께 초기 인코더 레이어를 공유함으로써 전이 가능한 표현을 학습하는 방안을 제안해. 테스트할 때, 학습된 인코더 표현은 무작위로 초기화된 INR의 초기값으로 전이돼.

우리는 STRAINER가 같은 도메인에서 이미지를 맞추는 데 매우 강력한 초기화를 제공하고, 훈련되지 않은 INR에 비해 신호 품질이 약 +10dB 향상된다는 것을 발견했어. STRAINER는 또한 INR에서 데이터 기반의 선험적 정보를 인코딩하는 간단한 방법을 제공해. 우리는 STRAINER를 여러 개의 도메인 내외 신호 맞추기 작업과 역문제에서 평가하고, STRAINER의 특징 전이 가능성에 대한 자세한 분석과 논의도 제공해. 우리의 데모는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09582.pdf

Title: NEVLP: Noise-Robust Framework for Efficient Vision-Language Pre-training

Original Abstract:
The success of Vision Language Models (VLMs) on various vision-language tasks heavily relies on pre-training with large scale web-crawled datasets. However, the noisy and incomplete nature of web data makes dataset scale crucial for performance, rendering end-to-end training increasingly prohibitive. In this paper, we propose NEVLP, a noise-robust framework for efficient vision-language pre-training that requires less pre-training data. Specifically, we bridge the modality gap between a frozen image encoder and a large language model with a transformer and introduce two innovative learning strategies: noise-adaptive learning and concept-enhanced learning to mitigate the impact of noise. In noise-adaptive learning, we estimate the noise probability of each image-text pair based on the transformer's memorization effect and employ noise-adaptive regularization on image-text contrastive learning to condition cross-modal alignment. In concept-enhanced learning, we enrich incomplete text by incorporating visual concepts (objects in the image) to provide prior information about existing objects for image-text matching and image-grounded text generation, thereby mitigating text incompletion. Our framework effectively utilizes noisy web data and achieves state-of-the-art performance with less pre-training data across a wide range of vision-language tasks, including image-text retrieval, image captioning, and visual question answering.

Translated Abstract:
비전 언어 모델(VLM)이 다양한 비전-언어 작업에서 성공하려면 대규모 웹 크롤링 데이터셋으로 미리 훈련하는 게 중요해. 하지만 웹 데이터가 시끄럽고 불완전하기 때문에 데이터셋의 크기가 성능에 큰 영향을 미쳐서, 끝에서 끝으로 훈련하는 게 점점 더 어려워지고 있어.

이 논문에서는 NEVLP라는 강한 노이즈 저항 프레임워크를 제안해. 이 프레임워크는 적은 양의 미리 훈련 데이터를 가지고도 효율적으로 비전-언어 미리 훈련을 할 수 있어. 구체적으로, 고정된 이미지 인코더와 대형 언어 모델 간의 차이를 해결하기 위해 트랜스포머를 사용하고, 노이즈 적응 학습과 개념 강화 학습이라는 두 가지 혁신적인 학습 전략을 도입해 노이즈의 영향을 줄여.

노이즈 적응 학습에서는 각 이미지-텍스트 쌍의 노이즈 확률을 트랜스포머의 기억 효과를 바탕으로 추정하고, 이미지-텍스트 대조 학습에 노이즈 적응 정규화를 적용해 서로 다른 모달리티의 정렬을 조정해. 개념 강화 학습에서는 불완전한 텍스트를 이미지 내의 시각적 개념(이미지 속 객체)으로 보강해서 이미지-텍스트 매칭과 이미지 기반 텍스트 생성을 위한 기존 객체에 대한 정보를 제공해 텍스트의 불완전성을 줄여.

우리의 프레임워크는 시끄러운 웹 데이터를 효과적으로 활용하고, 이미지-텍스트 검색, 이미지 캡셔닝, 시각적 질문 답변 등 다양한 비전-언어 작업에서 적은 미리 훈련 데이터로도 최첨단 성능을 달성해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09588.pdf

Title: GLCONet: Learning Multi-source Perception Representation for Camouflaged Object Detection

Original Abstract:
Recently, biological perception has been a powerful tool for handling the camouflaged object detection (COD) task. However, most existing methods are heavily dependent on the local spatial information of diverse scales from convolutional operations to optimize initial features. A commonly neglected point in these methods is the long-range dependencies between feature pixels from different scale spaces that can help the model build a global structure of the object, inducing a more precise image representation. In this paper, we propose a novel Global-Local Collaborative Optimization Network, called GLCONet. Technically, we first design a collaborative optimization strategy from the perspective of multi-source perception to simultaneously model the local details and global long-range relationships, which can provide features with abundant discriminative information to boost the accuracy in detecting camouflaged objects. Furthermore, we introduce an adjacent reverse decoder that contains cross-layer aggregation and reverse optimization to integrate complementary information from different levels for generating high-quality representations. Extensive experiments demonstrate that the proposed GLCONet method with different backbones can effectively activate potentially significant pixels in an image, outperforming twenty state-of-the-art methods on three public COD datasets. The source code is available at: \this https URL.

Translated Abstract:
최근 생물학적 인식이 위장된 물체 탐지(COD) 작업을 처리하는 데 강력한 도구로 자리잡고 있어. 하지만 기존 방법들은 대부분 합성곱 연산에서 다양한 스케일의 지역적 공간 정보에 많이 의존하고 있어. 이런 방법들에서 간과되는 점은 서로 다른 스케일 공간에서의 피처 픽셀들 간의 장거리 의존성인데, 이게 모델이 물체의 전반적인 구조를 구축하는 데 도움을 주고, 더 정밀한 이미지 표현을 유도할 수 있어.

이번 논문에서는 GLCONet이라는 새로운 글로벌-로컬 협업 최적화 네트워크를 제안해. 기술적으로는, 다중 소스 인식의 관점에서 협업 최적화 전략을 설계해서 지역 세부 정보와 글로벌 장거리 관계를 동시에 모델링할 수 있도록 했어. 이렇게 하면 탐지 정확도를 높이기 위해 풍부한 구별 정보를 가진 피처를 제공할 수 있어. 

게다가, 우리는 서로 다른 레벨의 보완 정보를 통합해 고품질 표현을 생성하기 위해 크로스 레이어 집계와 역 최적화를 포함한 인접 역 디코더를 도입했어. 광범위한 실험 결과, 다양한 백본을 사용하는 GLCONet 방법이 이미지에서 잠재적으로 중요한 픽셀을 효과적으로 활성화할 수 있으며, 세 가지 공공 COD 데이터셋에서 20개의 최첨단 방법을 초월하는 성능을 보여줬어. 소스 코드는 이 링크에서 확인할 수 있어: \this https URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.09593.pdf

Title: One-Shot Learning for Pose-Guided Person Image Synthesis in the Wild

Original Abstract:
Current Pose-Guided Person Image Synthesis (PGPIS) methods depend heavily on large amounts of labeled triplet data to train the generator in a supervised manner. However, they often falter when applied to in-the-wild samples, primarily due to the distribution gap between the training datasets and real-world test samples. While some researchers aim to enhance model generalizability through sophisticated training procedures, advanced architectures, or by creating more diverse datasets, we adopt the test-time fine-tuning paradigm to customize a pre-trained Text2Image (T2I) model. However, naively applying test-time tuning results in inconsistencies in facial identities and appearance attributes. To address this, we introduce a Visual Consistency Module (VCM), which enhances appearance consistency by combining the face, text, and image embedding. Our approach, named OnePoseTrans, requires only a single source image to generate high-quality pose transfer results, offering greater stability than state-of-the-art data-driven methods. For each test case, OnePoseTrans customizes a model in around 48 seconds with an NVIDIA V100 GPU.

Translated Abstract:
현재의 포즈 가이드 사람 이미지 합성(PGPIS) 방법은 생성기를 훈련하기 위해 많은 양의 라벨이 붙은 삼중 데이터에 의존하고 있어. 하지만 실제 환경에서 사용될 때는 훈련 데이터셋과 실제 테스트 샘플 간의 분포 차이 때문에 자주 문제가 생겨. 

몇몇 연구자들은 복잡한 훈련 절차나 고급 구조를 통해 모델의 일반화 능력을 향상시키려고 하거나 더 다양한 데이터셋을 만들려고 하지만, 우리는 테스트 타임 파인튜닝 방식을 사용해서 미리 훈련된 Text2Image(T2I) 모델을 맞춤 설정해. 하지만 단순히 테스트 타임 튜닝을 적용하면 얼굴 정체성과 외형 속성에서 일관성이 떨어지는 문제가 발생해.

이 문제를 해결하기 위해 우리는 비주얼 일관성 모듈(VCM)을 도입했어. 이 모듈은 얼굴, 텍스트, 이미지 임베딩을 결합해서 외형의 일관성을 높여줘. 우리의 방법, OnePoseTrans는 고품질의 포즈 전환 결과를 생성하기 위해 단 한 장의 소스 이미지만 필요로 해. 최신 데이터 기반 방법들보다 더 안정적이야. 각 테스트 케이스마다 OnePoseTrans는 약 48초 안에 NVIDIA V100 GPU를 사용해 모델을 맞춤 설정해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09605.pdf

Title: DreamMover: Leveraging the Prior of Diffusion Models for Image Interpolation with Large Motion

Original Abstract:
We study the problem of generating intermediate images from image pairs with large motion while maintaining semantic consistency. Due to the large motion, the intermediate semantic information may be absent in input images. Existing methods either limit to small motion or focus on topologically similar objects, leading to artifacts and inconsistency in the interpolation results. To overcome this challenge, we delve into pre-trained image diffusion models for their capabilities in semantic cognition and representations, ensuring consistent expression of the absent intermediate semantic representations with the input. To this end, we propose DreamMover, a novel image interpolation framework with three main components: 1) A natural flow estimator based on the diffusion model that can implicitly reason about the semantic correspondence between two images. 2) To avoid the loss of detailed information during fusion, our key insight is to fuse information in two parts, high-level space and low-level space. 3) To enhance the consistency between the generated images and input, we propose the self-attention concatenation and replacement approach. Lastly, we present a challenging benchmark dataset InterpBench to evaluate the semantic consistency of generated results. Extensive experiments demonstrate the effectiveness of our method. Our project is available at this https URL .

Translated Abstract:
우리는 큰 움직임이 있는 이미지 쌍으로부터 중간 이미지를 생성하는 문제를 연구하고 있어. 이 과정에서 의미적인 일관성을 유지하는 게 중요해. 큰 움직임 때문에 입력 이미지에서 중간 의미 정보가 없을 수 있어. 기존 방법들은 보통 작은 움직임에만 한정되거나 형태가 비슷한 객체에만 집중해서 아티팩트나 결과의 일관성 문제가 생기곤 해.

이 문제를 해결하기 위해 우리는 사전 훈련된 이미지 확산 모델을 활용해보자고 했어. 이 모델은 의미 인식과 표현이 뛰어나서, 입력 이미지와의 중간 의미 표현을 일관되게 유지할 수 있어. 그래서 우리는 DreamMover라는 새로운 이미지 보간 프레임워크를 제안해. 이 프레임워크는 세 가지 주요 요소로 구성되어 있어:

1) 두 이미지 간의 의미적 대응을 암묵적으로 추론할 수 있는 확산 모델 기반의 자연 흐름 추정기.
   
2) 정보 융합 시 세부 정보 손실을 피하기 위해, 고수준 공간과 저수준 공간으로 정보를 나누어 융합하는 방식.

3) 생성된 이미지와 입력 이미지 간의 일관성을 높이기 위해, 자기 주의력 연결 및 교체 접근 방식을 제안해.

마지막으로, 생성된 결과의 의미적 일관성을 평가하기 위해 InterpBench라는 도전적인 벤치마크 데이터셋을 소개할게. 여러 실험을 통해 우리의 방법이 효과적임을 입증했어. 우리 프로젝트는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09610.pdf

Title: TextureDiffusion: Target Prompt Disentangled Editing for Various Texture Transfer

Original Abstract:
Recently, text-guided image editing has achieved significant success. However, existing methods can only apply simple textures like wood or gold when changing the texture of an object. Complex textures such as cloud or fire pose a challenge. This limitation stems from that the target prompt needs to contain both the input image content and <texture>, restricting the texture representation. In this paper, we propose TextureDiffusion, a tuning-free image editing method applied to various texture transfer. Initially, the target prompt is directly set to "<texture>", making the texture disentangled from the input image content to enhance texture representation. Subsequently, query features in self-attention and features in residual blocks are utilized to preserve the structure of the input image. Finally, to maintain the background, we introduce an edit localization technique which blends the self-attention results and the intermediate latents. Comprehensive experiments demonstrate that TextureDiffusion can harmoniously transfer various textures with excellent structure and background preservation.

Translated Abstract:
최근 텍스트 기반 이미지 편집이 큰 성공을 거두었어. 하지만 기존 방법들은 나무나 금 같은 간단한 질감만 적용할 수 있어. 구름이나 불 같은 복잡한 질감은 어려운 문제야. 이런 한계는 목표 프롬프트가 입력 이미지의 내용과 <질감>을 모두 포함해야 해서 질감 표현이 제한되기 때문이야.

이 논문에서는 TextureDiffusion이라는 조정이 필요 없는 이미지 편집 방법을 제안해. 처음에 목표 프롬프트를 "<질감>"으로 설정해서 질감을 입력 이미지 내용과 분리시켜 질감 표현을 향상시켜. 다음으로는 자기 주의(self-attention)에서 쿼리 특징과 잔여 블록의 특징을 활용해 입력 이미지의 구조를 유지해.

마지막으로 배경을 유지하기 위해 편집 로컬라이제이션 기법을 도입해서 자기 주의 결과와 중간 레이턴트를 혼합해. 여러 실험을 통해 TextureDiffusion이 다양한 질감을 우수한 구조와 배경 보존으로 조화롭게 전이할 수 있음을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09611.pdf

Title: Integrating Audio Narrations to Strengthen Domain Generalization in Multimodal First-Person Action Recognition

Original Abstract:
First-person activity recognition is rapidly growing due to the widespread use of wearable cameras but faces challenges from domain shifts across different environments, such as varying objects or background scenes. We propose a multimodal framework that improves domain generalization by integrating motion, audio, and appearance features. Key contributions include analyzing the resilience of audio and motion features to domain shifts, using audio narrations for enhanced audio-text alignment, and applying consistency ratings between audio and visual narrations to optimize the impact of audio in recognition during training. Our approach achieves state-of-the-art performance on the ARGO1M dataset, effectively generalizing across unseen scenarios and locations.

Translated Abstract:
1인칭 활동 인식이 요즘 웨어러블 카메라의 사용이 늘면서 급격히 발전하고 있어. 하지만 다양한 환경에서 물체나 배경 장면이 달라지면 어려움이 생겨. 우리는 동작, 오디오, 외관 특성을 통합한 다중 모드 프레임워크를 제안해. 이걸 통해 도메인 일반화를 개선할 수 있어.

주요 기여는 도메인 변화에 대한 오디오와 동작 특성의 강인성을 분석하고, 오디오 내레이션을 사용해 오디오-텍스트 정렬을 향상시키는 거야. 그리고 오디오와 비주얼 내레이션 간의 일관성 평가를 적용해 훈련 중 인식에서 오디오의 영향을 최적화했어.

우리 방법은 ARGO1M 데이터셋에서 최첨단 성능을 달성했으며, 보지 못한 상황과 장소에서도 효과적으로 일반화할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09616.pdf

Title: Enhancing Weakly-Supervised Object Detection on Static Images through (Hallucinated) Motion

Original Abstract:
While motion has garnered attention in various tasks, its potential as a modality for weakly-supervised object detection (WSOD) in static images remains unexplored. Our study introduces an approach to enhance WSOD methods by integrating motion information. This method involves leveraging hallucinated motion from static images to improve WSOD on image datasets, utilizing a Siamese network for enhanced representation learning with motion, addressing camera motion through motion normalization, and selectively training images based on object motion. Experimental validation on the COCO and YouTube-BB datasets demonstrates improvements over a state-of-the-art method.

Translated Abstract:
움직임은 여러 작업에서 주목받고 있지만, 정적인 이미지에서 약한 감독 객체 탐지(WSOD) 방법으로서의 가능성은 아직 탐구되지 않았어. 

우리 연구는 움직임 정보를 통합해서 WSOD 방법을 개선하는 접근 방식을 소개해. 이 방법은 정적인 이미지에서 환각된 움직임을 활용해서 이미지 데이터셋의 WSOD를 향상시키는 거야. 시암 네트워크를 사용해서 움직임과 함께 더 나은 표현 학습을 하고, 카메라 움직임을 처리하기 위해 움직임 정규화를 적용해. 그리고 객체의 움직임에 따라 이미지를 선택적으로 훈련해.

COCO와 YouTube-BB 데이터셋에서 실험한 결과, 최첨단 방법보다 개선된 성과를 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09628.pdf

Title: Can Large Language Models Grasp Event Signals? Exploring Pure Zero-Shot Event-based Recognition

Original Abstract:
Recent advancements in event-based zero-shot object recognition have demonstrated promising results. However, these methods heavily depend on extensive training and are inherently constrained by the characteristics of CLIP. To the best of our knowledge, this research is the first study to explore the understanding capabilities of large language models (LLMs) for event-based visual content. We demonstrate that LLMs can achieve event-based object recognition without additional training or fine-tuning in conjunction with CLIP, effectively enabling pure zero-shot event-based recognition. Particularly, we evaluate the ability of GPT-4o / 4turbo and two other open-source LLMs to directly recognize event-based visual content. Extensive experiments are conducted across three benchmark datasets, systematically assessing the recognition accuracy of these models. The results show that LLMs, especially when enhanced with well-designed prompts, significantly improve event-based zero-shot recognition performance. Notably, GPT-4o outperforms the compared models and exceeds the recognition accuracy of state-of-the-art event-based zero-shot methods on N-ImageNet by five orders of magnitude. The implementation of this paper is available at \url{this https URL}.

Translated Abstract:
최근 이벤트 기반 제로샷 객체 인식에서 좋은 결과가 나오고 있어. 하지만 이런 방법들은 많은 훈련에 의존하고, CLIP의 특성 때문에 한계가 있어. 우리가 아는 바로는, 이 연구가 대형 언어 모델(LLMs)이 이벤트 기반 시각 콘텐츠를 이해할 수 있는 능력을 탐구한 첫 번째 연구야.

우리는 LLMs가 CLIP과 함께 추가적인 훈련이나 미세 조정 없이도 이벤트 기반 객체 인식을 할 수 있다는 걸 보여줘. 그래서 순수 제로샷 이벤트 기반 인식을 가능하게 해. 특히, 우리는 GPT-4o / 4turbo와 두 개의 오픈 소스 LLM이 이벤트 기반 시각 콘텐츠를 직접 인식하는 능력을 평가했어.

세 개의 벤치마크 데이터셋을 통해 다양한 실험을 진행하면서 이 모델들의 인식 정확도를 체계적으로 평가했어. 결과는 LLMs가 잘 설계된 프롬프트와 함께 사용할 때 이벤트 기반 제로샷 인식 성능을 크게 향상시킨다는 걸 보여줘. 특히, GPT-4o는 비교한 모델들보다 더 뛰어나고, N-ImageNet에서 최신 이벤트 기반 제로샷 방법들을 다섯 배 더 초월하는 인식 정확도를 기록했어. 이 논문의 구현은 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09635.pdf

Title: A Novel Framework For Text Detection From Natural Scene Images With Complex Background

Original Abstract:
Recognizing texts from camera images is a known hard problem because of the difficulties in text detection from the varied and complicated background. In this paper we propose a novel and efficient method to detect text region from images with complex background using Wavelet Transforms. The framework uses Wavelet Transformation of the original image in its grayscale form followed by Sub-band filtering. Then Region clustering technique is applied using centroids of the regions, further Bounding box is fitted to each region thus identifying the text regions. This method is much sophisticated and efficient than the previous methods as it doesn't stick to a particular font size of the text thus, making it generalized. The sample set used for experimental purpose consists of 50 images with varying backgrounds. Images with edge prominence are considered. Furthermore, our method can be easily customized for applications with different scopes.

Translated Abstract:
카메라 이미지에서 텍스트를 인식하는 건 어려운 문제야. 복잡한 배경에서 텍스트를 감지하는 게 힘들기 때문이지. 이 논문에서는 복잡한 배경을 가진 이미지에서 텍스트 영역을 감지하는 새로운 방법을 제안해. 이 방법은 웨이블릿 변환(Wavelet Transforms)을 사용해.

우선, 원본 이미지를 그레이스케일 형태로 변환한 후, 서브밴드 필터링(Sub-band filtering)을 해. 그 다음에는 지역 클러스터링 기술을 사용해서 각 영역의 중심점을 기준으로 군집을 형성해. 그리고 각 지역에 바운딩 박스를 맞춰서 텍스트 영역을 식별해.

이 방법은 이전의 방법들보다 훨씬 정교하고 효율적이야. 특정 폰트 크기에 얽매이지 않아서 일반화가 가능해. 실험에 사용된 샘플 세트는 배경이 다양한 50개의 이미지로 구성되어 있어. 가장자리가 뚜렷한 이미지를 고려했지. 게다가, 우리 방법은 다양한 용도에 맞게 쉽게 커스터마이즈할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09649.pdf

Title: SparX: A Sparse Cross-Layer Connection Mechanism for Hierarchical Vision Mamba and Transformer Networks

Original Abstract:
Due to the capability of dynamic state space models (SSMs) in capturing long-range dependencies with near-linear computational complexity, Mamba has shown notable performance in NLP tasks. This has inspired the rapid development of Mamba-based vision models, resulting in promising results in visual recognition tasks. However, such models are not capable of distilling features across layers through feature aggregation, interaction, and selection. Moreover, existing cross-layer feature aggregation methods designed for CNNs or ViTs are not practical in Mamba-based models due to high computational costs. Therefore, this paper aims to introduce an efficient cross-layer feature aggregation mechanism for Mamba-based vision backbone networks. Inspired by the Retinal Ganglion Cells (RGCs) in the human visual system, we propose a new sparse cross-layer connection mechanism termed SparX to effectively improve cross-layer feature interaction and reuse. Specifically, we build two different types of network layers: ganglion layers and normal layers. The former has higher connectivity and complexity, enabling multi-layer feature aggregation and interaction in an input-dependent manner. In contrast, the latter has lower connectivity and complexity. By interleaving these two types of layers, we design a new vision backbone network with sparsely cross-connected layers, achieving an excellent trade-off among model size, computational cost, memory cost, and accuracy in comparison to its counterparts. For instance, with fewer parameters, SparX-Mamba-T improves the top-1 accuracy of VMamba-T from 82.5% to 83.5%, while SparX-Swin-T achieves a 1.3% increase in top-1 accuracy compared to Swin-T. Extensive experimental results demonstrate that our new connection mechanism possesses both superior performance and generalization capabilities on various vision tasks.

Translated Abstract:
Mamba는 동적 상태 공간 모델(SSM)이 긴 의존성을 잘 포착하면서도 거의 선형 계산 복잡도로 작동할 수 있기 때문에 NLP 작업에서 뛰어난 성능을 보여줬어. 이 덕분에 Mamba 기반의 비전 모델들이 빠르게 발전했고, 시각 인식 작업에서도 좋은 결과를 내고 있어.

하지만 이런 모델들은 레이어 간의 특징을 집합, 상호작용, 선택을 통해 잘 추출하지 못해. 또한, CNN이나 ViT를 위해 설계된 기존의 레이어 간 특징 집합 방법은 Mamba 기반 모델에서는 계산 비용이 너무 높아서 실용적이지 않아. 그래서 이 논문에서는 Mamba 기반 비전 백본 네트워크를 위한 효율적인 레이어 간 특징 집합 메커니즘을 소개하려고 해.

인간 시각 시스템의 망막 신경세포(RGC)에 영감을 받아, 우리는 SparX라는 새로운 희소 레이어 간 연결 메커니즘을 제안해. 이 메커니즘은 레이어 간 특징 상호작용과 재사용을 효과적으로 개선할 수 있어. 구체적으로, 우리는 두 가지 종류의 네트워크 레이어를 만들어: 신경세포 레이어와 일반 레이어. 신경세포 레이어는 연결성이 높고 복잡성이 커서, 입력에 따라 여러 레이어의 특징을 집합하고 상호작용할 수 있어. 반면에 일반 레이어는 연결성이 낮고 복잡성이 적어.

이 두 종류의 레이어를 교차시키면서, 우리는 희소하게 서로 연결된 레이어를 가진 새로운 비전 백본 네트워크를 설계했어. 이 네트워크는 모델 크기, 계산 비용, 메모리 비용, 정확도 간의 균형을 잘 이뤘어. 예를 들어, SparX-Mamba-T는 VMamba-T의 top-1 정확도를 82.5%에서 83.5%로 개선했고, SparX-Swin-T는 Swin-T에 비해 top-1 정확도가 1.3% 증가했어. 다양한 시각 작업에서 우리의 새로운 연결 메커니즘이 뛰어난 성능과 일반화 능력을 가지고 있다는 것을 실험 결과로 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09668.pdf

Title: EditBoard: Towards A Comprehensive Evaluation Benchmark for Text-based Video Editing Models

Original Abstract:
The rapid development of diffusion models has significantly advanced AI-generated content (AIGC), particularly in Text-to-Image (T2I) and Text-to-Video (T2V) generation. Text-based video editing, leveraging these generative capabilities, has emerged as a promising field, enabling precise modifications to videos based on text prompts. Despite the proliferation of innovative video editing models, there is a conspicuous lack of comprehensive evaluation benchmarks that holistically assess these models' performance across various dimensions. Existing evaluations are limited and inconsistent, typically summarizing overall performance with a single score, which obscures models' effectiveness on individual editing tasks. To address this gap, we propose EditBoard, the first comprehensive evaluation benchmark for text-based video editing models. EditBoard encompasses nine automatic metrics across four dimensions, evaluating models on four task categories and introducing three new metrics to assess fidelity. This task-oriented benchmark facilitates objective evaluation by detailing model performance and providing insights into each model's strengths and weaknesses. By open-sourcing EditBoard, we aim to standardize evaluation and advance the development of robust video editing models.

Translated Abstract:
확산 모델의 빠른 발전 덕분에 AI로 생성된 콘텐츠, 특히 텍스트를 이미지로 변환(T2I)하고 텍스트를 비디오로 변환(T2V)하는 기술이 크게 발전했어. 텍스트 기반 비디오 편집은 이런 생성 능력을 활용해서 텍스트 프롬프트를 바탕으로 비디오를 정확하게 수정할 수 있는 유망한 분야로 떠올랐어.

하지만 혁신적인 비디오 편집 모델이 많아졌음에도 불구하고, 이 모델들의 성능을 전반적으로 평가할 수 있는 종합적인 기준이 부족해. 기존의 평가는 한정적이고 일관성이 없어서, 보통 하나의 점수로 전체 성과를 요약하는데 그쳐. 이러면 각 모델이 개별 편집 작업에서 얼마나 효과적인지를 잘 알기 어려워.

이런 문제를 해결하기 위해 우리는 EditBoard를 제안해. EditBoard는 텍스트 기반 비디오 편집 모델을 위한 첫 번째 종합 평가 기준이야. 이 기준은 네 가지 차원에서 아홉 개의 자동 평가 지표를 포함하고, 네 가지 작업 카테고리에서 모델을 평가해. 또, 충실도를 평가하기 위한 세 가지 새로운 지표도 추가했어. 이 작업 중심의 기준은 모델 성능을 자세히 평가하고 각 모델의 강점과 약점에 대한 통찰을 제공해서 객관적인 평가를 가능하게 해.

EditBoard를 오픈 소스화함으로써 우리는 평가 기준을 표준화하고, 강력한 비디오 편집 모델 개발을 촉진하려고 해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09670.pdf

Title: Unsupervised Hyperspectral and Multispectral Image Blind Fusion Based on Deep Tucker Decomposition Network with Spatial-Spectral Manifold Learning

Original Abstract:
Hyperspectral and multispectral image fusion aims to generate high spectral and spatial resolution hyperspectral images (HR-HSI) by fusing high-resolution multispectral images (HR-MSI) and low-resolution hyperspectral images (LR-HSI). However, existing fusion methods encounter challenges such as unknown degradation parameters, incomplete exploitation of the correlation between high-dimensional structures and deep image features. To overcome these issues, in this article, an unsupervised blind fusion method for hyperspectral and multispectral images based on Tucker decomposition and spatial spectral manifold learning (DTDNML) is proposed. We design a novel deep Tucker decomposition network that maps LR-HSI and HR-MSI into a consistent feature space, achieving reconstruction through decoders with shared parameter. To better exploit and fuse spatial-spectral features in the data, we design a core tensor fusion network that incorporates a spatial spectral attention mechanism for aligning and fusing features at different scales. Furthermore, to enhance the capacity in capturing global information, a Laplacian-based spatial-spectral manifold constraints is introduced in shared-decoders. Sufficient experiments have validated that this method enhances the accuracy and efficiency of hyperspectral and multispectral fusion on different remote sensing datasets. The source code is available at this https URL.

Translated Abstract:
하이퍼스펙트럼 이미지와 멀티스펙트럼 이미지 융합은 고해상도 멀티스펙트럼 이미지(HR-MSI)와 저해상도 하이퍼스펙트럼 이미지(LR-HSI)를 합쳐서 고스펙트럼 및 고공간 해상도의 하이퍼스펙트럼 이미지(HR-HSI)를 만드는 거야. 그런데 기존의 융합 방법들은 잘 모르는 열화 파라미터나 고차원 구조와 깊은 이미지 특징 간의 상관관계를 제대로 활용하지 못하는 문제점이 있어.

이런 문제를 해결하기 위해, 이 논문에서는 터커 분해와 공간 스펙트럴 매니폴드 학습을 기반으로 한 비지도 블라인드 융합 방법(DTDNML)을 제안해. 우리는 LR-HSI와 HR-MSI를 일관된 특징 공간으로 맵핑하는 새로운 딥 터커 분해 네트워크를 설계했고, 공유 파라미터를 가진 디코더를 통해 재구성을 이뤄냈어. 데이터에서 공간-스펙트럴 특징을 더 잘 활용하고 융합하기 위해, 서로 다른 스케일에서 특징을 정렬하고 융합하는 공간 스펙트럴 주의 메커니즘을 포함한 코어 텐서 융합 네트워크를 설계했어.

게다가, 전역 정보를 더 잘 포착할 수 있도록 공유 디코더에 라플라시안 기반의 공간-스펙트럴 매니폴드 제약을 도입했어. 충분한 실험을 통해 이 방법이 다양한 원격 탐사 데이터셋에서 하이퍼스펙트럼과 멀티스펙트럼 융합의 정확성과 효율성을 높인다는 것을 확인했어. 소스 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09673.pdf

Title: SITSMamba for Crop Classification based on Satellite Image Time Series

Original Abstract:
Satellite image time series (SITS) data provides continuous observations over time, allowing for the tracking of vegetation changes and growth patterns throughout the seasons and years. Numerous deep learning (DL) approaches using SITS for crop classification have emerged recently, with the latest approaches adopting Transformer for SITS classification. However, the quadratic complexity of self-attention in Transformer poses challenges for classifying long time series. While the cutting-edge Mamba architecture has demonstrated strength in various domains, including remote sensing image interpretation, its capacity to learn temporal representations in SITS data remains unexplored. Moreover, the existing SITS classification methods often depend solely on crop labels as supervision signals, which fails to fully exploit the temporal information. In this paper, we proposed a Satellite Image Time Series Mamba (SITSMamba) method for crop classification based on remote sensing time series data. The proposed SITSMamba contains a spatial encoder based on Convolutional Neural Networks (CNN) and a Mamba-based temporal encoder. To exploit richer temporal information from SITS, we design two branches of decoder used for different tasks. The first branch is a crop Classification Branch (CBranch), which includes a ConvBlock to decode the feature to a crop map. The second branch is a SITS Reconstruction Branch that uses a Linear layer to transform the encoded feature to predict the original input values. Furthermore, we design a Positional Weight (PW) applied to the RBranch to help the model learn rich latent knowledge from SITS. We also design two weighting factors to control the balance of the two branches during training. The code of SITSMamba is available at: this https URL.

Translated Abstract:
위성 이미지 시계열(SITS) 데이터는 시간에 따라 지속적으로 관찰할 수 있게 해줘서, 계절과 연도에 걸쳐 식물의 변화와 성장 패턴을 추적할 수 있어. 최근에는 SITS를 이용한 작물 분류를 위한 여러 딥러닝(DL) 방법들이 등장했는데, 최신 방법들은 Transformer를 활용해서 SITS 분류를 하고 있어. 하지만 Transformer의 자기 주의(self-attention)에서 발생하는 제곱 복잡성 때문에 긴 시계열 데이터를 분류하는 데 어려움이 있어.

최신 Mamba 아키텍처는 원격 감지 이미지 해석 등 여러 분야에서 강력한 성능을 보였지만, SITS 데이터에서 시간적 표현을 학습하는 능력은 아직 연구되지 않았어. 게다가 기존의 SITS 분류 방법들은 주로 작물 레이블만을 감독 신호로 사용해서 시간 정보를 충분히 활용하지 못하고 있어.

이 논문에서는 원격 감지 시계열 데이터를 기반으로 한 작물 분류를 위한 새로운 방법인 위성 이미지 시계열 맘바(SITSMamba)를 제안했어. SITSMamba는 CNN 기반의 공간 인코더와 Mamba 기반의 시간 인코더로 구성돼. SITS의 시간 정보를 더 잘 활용하기 위해, 우리는 다른 작업을 위해 사용되는 두 개의 디코더 가지를 설계했어. 첫 번째 가지는 작물 분류 가지(CBranch)로, 특성을 작물 지도(crop map)로 디코딩하는 ConvBlock을 포함하고 있어. 두 번째 가지는 SITS 재구성 가지로, 선형 계층을 사용해서 인코딩된 특성을 원본 입력 값으로 예측하도록 변환해.

또한, 우리는 RBranch에 적용되는 위치 가중치(PW)를 설계해서 모델이 SITS에서 더 풍부한 잠재 지식을 학습할 수 있도록 도와줘. 훈련 중 두 가지 가지의 균형을 조절하기 위해 두 가지 가중치 요소도 설계했어. SITSMamba의 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09678.pdf

Title: A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities

Original Abstract:
Human Activity Recognition (HAR) systems aim to understand human behaviour and assign a label to each action, attracting significant attention in computer vision due to their wide range of applications. HAR can leverage various data modalities, such as RGB images and video, skeleton, depth, infrared, point cloud, event stream, audio, acceleration, and radar signals. Each modality provides unique and complementary information suited to different application scenarios. Consequently, numerous studies have investigated diverse approaches for HAR using these modalities. This paper presents a comprehensive survey of the latest advancements in HAR from 2014 to 2024, focusing on machine learning (ML) and deep learning (DL) approaches categorized by input data modalities. We review both single-modality and multi-modality techniques, highlighting fusion-based and co-learning frameworks. Additionally, we cover advancements in hand-crafted action features, methods for recognizing human-object interactions, and activity detection. Our survey includes a detailed dataset description for each modality and a summary of the latest HAR systems, offering comparative results on benchmark datasets. Finally, we provide insightful observations and propose effective future research directions in HAR.

Translated Abstract:
인간 활동 인식(HAR) 시스템은 사람의 행동을 이해하고 각 행동에 라벨을 붙이는 걸 목표로 해. 이 분야는 다양한 응용 프로그램 덕분에 컴퓨터 비전에서 많은 관심을 받고 있어. HAR은 RGB 이미지, 비디오, 스켈레톤, 깊이 정보, 적외선, 포인트 클라우드, 이벤트 스트림, 오디오, 가속도, 레이더 신호 같은 여러 데이터를 활용할 수 있어. 각 데이터는 서로 다른 상황에 맞는 독특하고 보완적인 정보를 제공해.

그래서 많은 연구들이 이런 데이터 모달리티를 활용한 다양한 HAR 접근법을 조사해왔어. 이 논문에서는 2014년부터 2024년까지 HAR의 최신 발전을 포괄적으로 정리했어. 머신 러닝(ML)과 딥 러닝(DL) 접근법을 입력 데이터 모달리티에 따라 분류해서 다뤘어. 단일 모달리티와 다중 모달리티 기술을 리뷰하면서, 융합 기반과 공동 학습 프레임워크를 강조했어.

또한, 수작업으로 만든 행동 특징, 인간-물체 상호작용 인식 방법, 활동 탐지의 발전도 다뤘어. 각 모달리티에 대한 자세한 데이터셋 설명과 최신 HAR 시스템의 요약도 포함되어 있어서, 벤치마크 데이터셋에 대한 비교 결과를 제공해. 마지막으로, HAR에 대한 통찰력 있는 관찰과 효과적인 미래 연구 방향도 제안했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09681.pdf

Title: E-Commerce Inpainting with Mask Guidance in Controlnet for Reducing Overcompletion

Original Abstract:
E-commerce image generation has always been one of the core demands in the e-commerce field. The goal is to restore the missing background that matches the main product given. In the post-AIGC era, diffusion models are primarily used to generate product images, achieving impressive results. This paper systematically analyzes and addresses a core pain point in diffusion model generation: overcompletion, which refers to the difficulty in maintaining product features. We propose two solutions: 1. Using an instance mask fine-tuned inpainting model to mitigate this phenomenon; 2. Adopting a train-free mask guidance approach, which incorporates refined product masks as constraints when combining ControlNet and UNet to generate the main product, thereby avoiding overcompletion of the product. Our method has achieved promising results in practical applications and we hope it can serve as an inspiring technical report in this field.

Translated Abstract:
전자상거래 이미지 생성은 항상 전자상거래 분야에서 중요한 요구사항 중 하나야. 목표는 주어진 주요 제품에 맞는 배경을 복원하는 거야. AIGC 시대 이후로는 주로 확산 모델을 사용해서 제품 이미지를 생성하고, 꽤 인상적인 결과를 내고 있어.

이 논문은 확산 모델 생성에서의 핵심 문제인 '과도한 완성(overcompletion)'을 체계적으로 분석하고 다루고 있어. 과도한 완성이란 제품의 특징을 유지하는 데 어려움이 있다는 걸 의미해. 우리는 두 가지 해결책을 제안해: 

1. 인스턴스 마스크를 사용한 미세 조정된 인페인팅 모델을 이용해서 이 현상을 완화해.
2. 트레인 프리 마스크 가이드를 채택해서, ControlNet과 UNet을 결합할 때 정제된 제품 마스크를 제약으로 사용하는 방법이야. 이로 인해 제품의 과도한 완성을 피할 수 있어.

우리 방법은 실제 응용에서 유망한 결과를 얻었고, 이 분야에서 영감을 줄 수 있는 기술 보고서가 되길 바래.

================================================================================

URL:
https://arxiv.org/pdf/2409.09707.pdf

Title: Synergistic Spotting and Recognition of Micro-Expression via Temporal State Transition

Original Abstract:
Micro-expressions are involuntary facial movements that cannot be consciously controlled, conveying subtle cues with substantial real-world applications. The analysis of micro-expressions generally involves two main tasks: spotting micro-expression intervals in long videos and recognizing the emotions associated with these intervals. Previous deep learning methods have primarily relied on classification networks utilizing sliding windows. However, fixed window sizes and window-level hard classification introduce numerous constraints. Additionally, these methods have not fully exploited the potential of complementary pathways for spotting and recognition. In this paper, we present a novel temporal state transition architecture grounded in the state space model, which replaces conventional window-level classification with video-level regression. Furthermore, by leveraging the inherent connections between spotting and recognition tasks, we propose a synergistic strategy that enhances overall analysis performance. Extensive experiments demonstrate that our method achieves state-of-the-art performance. The codes and pre-trained models are available at this https URL.

Translated Abstract:
마이크로 표정은 우리가 의식적으로 조절할 수 없는 얼굴 움직임으로, 미세한 신호를 전달해 실제 세계에서 많은 활용이 있어. 마이크로 표정을 분석할 때는 주로 두 가지 작업이 필요해: 긴 비디오에서 마이크로 표정 구간을 찾는 것과 이 구간과 관련된 감정을 인식하는 거지. 

이전의 딥러닝 방법들은 주로 슬라이딩 윈도를 사용하는 분류 네트워크에 의존했어. 하지만 고정된 윈도 크기와 윈도 수준의 하드 분류는 여러 제약을 줘. 게다가, 이런 방법들은 표정 탐지와 인식의 보완 경로를 충분히 활용하지 못했어. 

이 논문에서는 상태 공간 모델에 기반한 새로운 시간 상태 전이 아키텍처를 제안해. 이 아키텍처는 기존의 윈도 수준 분류를 비디오 수준 회귀로 대체해. 또한, 탐지와 인식 작업 간의 본질적인 연결을 활용해서 전체 분석 성능을 향상시키는 시너지 전략을 제안해. 

광범위한 실험 결과, 우리의 방법이 최첨단 성능을 달성했음을 보여줘. 코드와 사전 훈련된 모델은 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09708.pdf

Title: ELSA: Exploiting Layer-wise N:M Sparsity for Vision Transformer Acceleration

Original Abstract:
$N{:}M$ sparsity is an emerging model compression method supported by more and more accelerators to speed up sparse matrix multiplication in deep neural networks. Most existing $N{:}M$ sparsity methods compress neural networks with a uniform setting for all layers in a network or heuristically determine the layer-wise configuration by considering the number of parameters in each layer. However, very few methods have been designed for obtaining a layer-wise customized $N{:}M$ sparse configuration for vision transformers (ViTs), which usually consist of transformer blocks involving the same number of parameters. In this work, to address the challenge of selecting suitable sparse configuration for ViTs on $N{:}M$ sparsity-supporting accelerators, we propose ELSA, Exploiting Layer-wise $N{:}M$ Sparsity for ViTs. Considering not only all $N{:}M$ sparsity levels supported by a given accelerator but also the expected throughput improvement, our methodology can reap the benefits of accelerators supporting mixed sparsity by trading off negligible accuracy loss with both memory usage and inference time reduction for ViT models. For instance, our approach achieves a noteworthy 2.9$\times$ reduction in FLOPs for both Swin-B and DeiT-B with only a marginal degradation of accuracy on ImageNet. Our code will be released upon paper acceptance.

Translated Abstract:
$N{:}M$ 희소성은 딥 뉴럴 네트워크에서 희소 행렬 곱셈을 가속화하기 위해 점점 더 많은 가속기에 의해 지원되는 새로운 모델 압축 방법이야. 대부분의 기존 $N{:}M$ 희소성 방법들은 네트워크의 모든 층에 대해 같은 설정으로 신경망을 압축하거나, 각 층의 파라미터 수를 고려해서 휴리스틱하게 층별 구성을 결정해. 그런데, 비전 트랜스포머(ViTs)를 위한 층별 맞춤형 $N{:}M$ 희소 구성을 얻기 위한 방법은 거의 없어. ViTs는 보통 같은 수의 파라미터를 가진 트랜스포머 블록으로 구성되거든.

이번 연구에서는 ViTs에 적합한 희소 구성을 선택하는 문제를 해결하기 위해 ELSA, 즉 층별 $N{:}M$ 희소성을 활용하는 방법을 제안해. 주어진 가속기가 지원하는 모든 $N{:}M$ 희소성 수준을 고려할 뿐만 아니라, 기대되는 처리량 개선도 생각해봐. 우리의 방법론은 ViT 모델의 메모리 사용량과 추론 시간을 줄이면서도 정확도 손실은 거의 없애는 방식으로 혼합 희소성을 지원하는 가속기의 장점을 활용할 수 있어. 예를 들어, 우리의 접근 방식은 Swin-B와 DeiT-B 모두에서 ImageNet에서 정확도가 약간 떨어지는 것만으로도 FLOPs를 2.9배 줄이는 성과를 거두었어. 코드도 논문이 수락되면 공개할 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09714.pdf

Title: Pre-Training for 3D Hand Pose Estimation with Contrastive Learning on Large-Scale Hand Images in the Wild

Original Abstract:
We present a contrastive learning framework based on in-the-wild hand images tailored for pre-training 3D hand pose estimators, dubbed HandCLR. Pre-training on large-scale images achieves promising results in various tasks, but prior 3D hand pose pre-training methods have not fully utilized the potential of diverse hand images accessible from in-the-wild videos. To facilitate scalable pre-training, we first prepare an extensive pool of hand images from in-the-wild videos and design our method with contrastive learning. Specifically, we collected over 2.0M hand images from recent human-centric videos, such as 100DOH and Ego4D. To extract discriminative information from these images, we focus on the similarity of hands; pairs of similar hand poses originating from different samples, and propose a novel contrastive learning method that embeds similar hand pairs closer in the latent space. Our experiments demonstrate that our method outperforms conventional contrastive learning approaches that produce positive pairs sorely from a single image with data augmentation. We achieve significant improvements over the state-of-the-art method in various datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% on AssemblyHands.

Translated Abstract:
우리는 손 이미지를 활용한 대비 학습 프레임워크인 HandCLR을 소개해. 이 프레임워크는 3D 손 자세 추정을 위한 사전 학습에 맞춰져 있어. 대규모 이미지로 사전 학습을 하면 여러 작업에서 좋은 결과를 얻을 수 있지만, 기존의 3D 손 자세 사전 학습 방법들은 야외 비디오에서 얻은 다양한 손 이미지의 잠재력을 충분히 활용하지 못했어.

사전 학습을 쉽게 할 수 있도록, 우리는 먼저 야외 비디오에서 손 이미지를 많이 모았어. 그리고 대비 학습을 기반으로 방법을 설계했지. 구체적으로, 우리는 100DOH와 Ego4D 같은 최근의 사람 중심 비디오에서 200만 개 이상의 손 이미지를 수집했어.

이 이미지들에서 구별할 수 있는 정보를 추출하기 위해, 손의 유사성에 초점을 맞췄어. 서로 다른 샘플에서 나온 유사한 손 자세 쌍을 가지고, 이 유사한 손 쌍을 잠재 공간에서 가깝게 임베딩하는 새로운 대비 학습 방법을 제안했어. 실험 결과, 우리의 방법이 단일 이미지에서 데이터 증강만으로 긍정적 쌍을 만드는 기존의 대비 학습 방식보다 더 나은 성능을 보여줬어. 여러 데이터셋에서 최신 방법에 비해 큰 개선을 이뤘고, FreiHand에서 15%, DexYCB에서 10%, AssemblyHands에서 4%의 향상을 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09716.pdf

Title: Disentangling Visual Priors: Unsupervised Learning of Scene Interpretations with Compositional Autoencoder

Original Abstract:
Contemporary deep learning architectures lack principled means for capturing and handling fundamental visual concepts, like objects, shapes, geometric transforms, and other higher-level structures. We propose a neurosymbolic architecture that uses a domain-specific language to capture selected priors of image formation, including object shape, appearance, categorization, and geometric transforms. We express template programs in that language and learn their parameterization with features extracted from the scene by a convolutional neural network. When executed, the parameterized program produces geometric primitives which are rendered and assessed for correspondence with the scene content and trained via auto-association with gradient. We confront our approach with a baseline method on a synthetic benchmark and demonstrate its capacity to disentangle selected aspects of the image formation process, learn from small data, correct inference in the presence of noise, and out-of-sample generalization.

Translated Abstract:
현재의 딥러닝 구조는 객체, 형태, 기하학적 변환 같은 기본적인 시각 개념을 잘 포착하고 처리하는 방법이 부족해. 우리는 특정 도메인 언어를 사용하는 신경 상징적 아키텍처를 제안해. 이 아키텍처는 이미지 형성의 선택된 선험적인 정보, 예를 들어 객체의 형태, 외관, 분류, 기하학적 변환 등을 포착해.

우리는 그 언어로 템플릿 프로그램을 표현하고, 장면에서 추출한 특징을 사용해 파라미터를 학습해. 실행할 때, 파라미터화된 프로그램은 기하학적 기본 요소를 생성하고, 이를 장면 내용과의 일치성을 평가하며, 그래디언트를 통해 자가 연관으로 학습해.

우리는 이 방법을 합성 벤치마크와 비교하고, 이미지 형성 과정의 특정 측면을 분리하는 능력, 작은 데이터로부터 학습하는 능력, 노이즈가 있을 때 추론을 수정하는 능력, 그리고 샘플 외 일반화 능력을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09724.pdf

Title: MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face Forgery Detection

Original Abstract:
The rapid development of photo-realistic face generation methods has raised significant concerns in society and academia, highlighting the urgent need for robust and generalizable face forgery detection (FFD) techniques. Although existing approaches mainly capture face forgery patterns using image modality, other modalities like fine-grained noises and texts are not fully explored, which limits the generalization capability of the model. In addition, most FFD methods tend to identify facial images generated by GAN, but struggle to detect unseen diffusion-synthesized ones. To address the limitations, we aim to leverage the cutting-edge foundation model, contrastive language-image pre-training (CLIP), to achieve generalizable diffusion face forgery detection (DFFD). In this paper, we propose a novel multi-modal fine-grained CLIP (MFCLIP) model, which mines comprehensive and fine-grained forgery traces across image-noise modalities via language-guided face forgery representation learning, to facilitate the advancement of DFFD. Specifically, we devise a fine-grained language encoder (FLE) that extracts fine global language features from hierarchical text prompts. We design a multi-modal vision encoder (MVE) to capture global image forgery embeddings as well as fine-grained noise forgery patterns extracted from the richest patch, and integrate them to mine general visual forgery traces. Moreover, we build an innovative plug-and-play sample pair attention (SPA) method to emphasize relevant negative pairs and suppress irrelevant ones, allowing cross-modality sample pairs to conduct more flexible alignment. Extensive experiments and visualizations show that our model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations.

Translated Abstract:
사진처럼 실제같은 얼굴 생성 기술이 빠르게 발전하면서, 사회와 학계에서 얼굴 위조 탐지 기술(FFD)에 대한 우려가 커지고 있어. 그래서 강력하고 일반화 가능한 FFD 기술이 필요하다는 목소리가 높아지고 있어. 

기존의 방법들은 주로 이미지 모달리티를 이용해서 얼굴 위조 패턴을 포착하고 있지만, 세밀한 노이즈나 텍스트 같은 다른 모달리티는 충분히 탐구되지 않았어. 이로 인해 모델의 일반화 능력이 제한되고 있지. 게다가 대부분의 FFD 방법들은 GAN으로 생성된 얼굴 이미지를 식별하는 데는 성공하지만, 새로운 확산 합성 이미지 탐지에는 어려움을 겪고 있어.

이런 한계를 극복하기 위해, 우리는 최신 기초 모델인 대조적 언어-이미지 사전 학습(CLIP)을 활용해서 일반화 가능한 확산 얼굴 위조 탐지(DFFD)를 목표로 하고 있어. 이 논문에서는 새로운 다중 모달 세밀한 CLIP(MFCLIP) 모델을 제안해. 이 모델은 언어 기반의 얼굴 위조 표현 학습을 통해 이미지-노이즈 모달리티 전반에 걸쳐 포괄적이고 세밀한 위조 흔적을 찾아내는 데 도움을 줘.

특히, 우리는 계층적 텍스트 프롬프트에서 세밀한 전역 언어 특징을 추출하는 세밀한 언어 인코더(FLE)를 개발했어. 그리고 전역 이미지 위조 임베딩과 가장 풍부한 패치에서 추출한 세밀한 노이즈 위조 패턴을 캡처하는 다중 모달 비전 인코더(MVE)를 설계했어. 이 두 가지를 통합해서 일반적인 시각적 위조 흔적을 찾아내고 있어. 

더 나아가, 관련 없는 부정적인 쌍을 억제하고 관련 있는 부정적인 쌍을 강조하기 위해 혁신적인 플러그 앤 플레이 샘플 쌍 주의(SPA) 방법을 만들었어. 이걸 통해 서로 다른 모달리티의 샘플 쌍이 더 유연하게 정렬될 수 있게 해. 다양한 실험과 시각화 결과를 보면, 우리의 모델이 교차 생성기, 교차 위조, 교차 데이터셋 평가 등 여러 설정에서 최첨단 기술보다 더 뛰어난 성능을 보였어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09740.pdf

Title: VGG-Tex: A Vivid Geometry-Guided Facial Texture Estimation Model for High Fidelity Monocular 3D Face Reconstruction

Original Abstract:
3D face reconstruction from monocular images has promoted the development of various applications such as augmented reality. Though existing methods have made remarkable progress, most of them emphasize geometric reconstruction, while overlooking the importance of texture prediction. To address this issue, we propose VGG-Tex, a novel Vivid Geometry-Guided Facial Texture Estimation model designed for High Fidelity Monocular 3D Face Reconstruction. The core of this approach is leveraging 3D parametric priors to enhance the outcomes of 2D UV texture estimation. Specifically, VGG-Tex includes a Facial Attributes Encoding Module, a Geometry-Guided Texture Generator, and a Visibility-Enhanced Texture Completion Module. These components are responsible for extracting parametric priors, generating initial textures, and refining texture details, respectively. Based on the geometry-texture complementarity principle, VGG-Tex also introduces a Texture-guided Geometry Refinement Module to further balance the overall fidelity of the reconstructed 3D faces, along with corresponding losses. Comprehensive experiments demonstrate that our method significantly improves texture reconstruction performance compared to existing state-of-the-art methods.

Translated Abstract:
단안 이미지로부터 3D 얼굴을 재구성하는 기술은 증강 현실 같은 다양한 응용 프로그램의 발전에 기여하고 있어. 기존 방법들이 꽤 발전했지만, 대부분은 기하학적 재구성에 집중하고 텍스처 예측의 중요성을 간과하고 있어. 이 문제를 해결하기 위해 우리는 VGG-Tex라는 새로운 모델을 제안해. 이 모델은 고품질 단안 3D 얼굴 재구성을 위해 설계된 생생한 기하학 기반 얼굴 텍스처 추정 모델이야.

이 방법의 핵심은 3D 파라메트릭 프라이어를 활용해서 2D UV 텍스처 추정 결과를 향상시키는 거야. VGG-Tex는 얼굴 속성 인코딩 모듈, 기하학 기반 텍스처 생성기, 가시성 향상 텍스처 완성 모듈을 포함해. 이 모듈들은 각각 파라메트릭 프라이어를 추출하고, 초기 텍스처를 생성하며, 텍스처 세부 사항을 다듬는 역할을 해.

또한 VGG-Tex는 기하학과 텍스처의 상호 보완성 원칙에 따라 텍스처 기반 기하학 정제 모듈을 도입해. 이 모듈은 재구성된 3D 얼굴의 전반적인 품질을 더욱 균형 있게 조정해 주고, 그에 맞는 손실도 설정해. 종합적인 실험 결과, 우리의 방법이 기존의 최첨단 방법들에 비해 텍스처 재구성 성능을 크게 향상시킨다는 것을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09748.pdf

Title: Explore the Hallucination on Low-level Perception for MLLMs

Original Abstract:
The rapid development of Multi-modality Large Language Models (MLLMs) has significantly influenced various aspects of industry and daily life, showcasing impressive capabilities in visual perception and understanding. However, these models also exhibit hallucinations, which limit their reliability as AI systems, especially in tasks involving low-level visual perception and understanding. We believe that hallucinations stem from a lack of explicit self-awareness in these models, which directly impacts their overall performance. In this paper, we aim to define and evaluate the self-awareness of MLLMs in low-level visual perception and understanding tasks. To this end, we present QL-Bench, a benchmark settings to simulate human responses to low-level vision, investigating self-awareness in low-level visual perception through visual question answering related to low-level attributes such as clarity and lighting. Specifically, we construct the LLSAVisionQA dataset, comprising 2,990 single images and 1,999 image pairs, each accompanied by an open-ended question about its low-level features. Through the evaluation of 15 MLLMs, we demonstrate that while some models exhibit robust low-level visual capabilities, their self-awareness remains relatively underdeveloped. Notably, for the same model, simpler questions are often answered more accurately than complex ones. However, self-awareness appears to improve when addressing more challenging questions. We hope that our benchmark will motivate further research, particularly focused on enhancing the self-awareness of MLLMs in tasks involving low-level visual perception and understanding.

Translated Abstract:
다양한 산업과 일상생활에서 멀티모달 대형 언어 모델(MLLMs)의 빠른 발전이 많은 영향을 미치고 있어. 이 모델들은 시각 인지와 이해에서 인상적인 능력을 보여주고 있어. 하지만 이 모델들은 환각 현상을 보이기도 해서, 특히 저수준 시각 인지와 이해를 해야 하는 작업에서는 신뢰성이 떨어져. 우리는 이런 환각이 모델의 명확한 자기 인식 부족에서 비롯된다고 생각해, 이게 전반적인 성능에 영향을 미쳐.

이 논문에서는 저수준 시각 인지와 이해 작업에서 MLLMs의 자기 인식을 정의하고 평가하는 게 목표야. 이를 위해 QL-Bench라는 벤치마크를 제시하는데, 이는 저수준 비전을 위한 인간의 반응을 시뮬레이션해. 저수준 속성인 선명도와 조명 같은 것과 관련된 시각 질문 응답을 통해 자기 인식을 조사하는 거야. 구체적으로, LLSAVisionQA 데이터셋을 만들어서 2,990개의 단일 이미지와 1,999개의 이미지 쌍을 포함하고, 각 이미지에 저수준 특징에 대한 개방형 질문을 달았어.

15개의 MLLMs를 평가해본 결과, 몇몇 모델은 저수준 시각 능력이 뛰어난 반면, 자기 인식은 상대적으로 부족하다는 걸 보여줬어. 흥미롭게도 같은 모델에서 더 간단한 질문이 복잡한 질문보다 더 정확하게 답변되는 경우가 많아. 하지만 더 어려운 질문을 다룰 때 자기 인식이 개선되는 경향이 있어. 우리의 벤치마크가 저수준 시각 인지와 이해 작업에서 MLLMs의 자기 인식을 향상시키는 연구를 촉진하길 바래.

================================================================================

URL:
https://arxiv.org/pdf/2409.09753.pdf

Title: DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation

Original Abstract:
Test Time Adaptation (TTA) has emerged as a practical solution to mitigate the performance degradation of Deep Neural Networks (DNNs) in the presence of corruption/ noise affecting inputs. Existing approaches in TTA continuously adapt the DNN, leading to excessive resource consumption and performance degradation due to accumulation of error stemming from lack of supervision. In this work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to address such issues. Our key approach is to proactively learn latent representations of some corruption types, each one associated with a sub-network state tailored to correctly classify inputs affected by that corruption. After deployment, DARDA adapts the DNN to previously unseen corruptions in an unsupervised fashion by (i) estimating the latent representation of the ongoing corruption; (ii) selecting the sub-network whose associated corruption is the closest in the latent space to the ongoing corruption; and (iii) adapting DNN state, so that its representation matches the ongoing corruption. This way, DARDA is more resource efficient and can swiftly adapt to new distributions caused by different corruptions without requiring a large variety of input data. Through experiments with two popular mobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA reduces energy consumption and average cache memory footprint respectively by 1.74x and 2.64x with respect to the state of the art, while increasing the performance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.

Translated Abstract:
테스트 시간 적응(Test Time Adaptation, TTA)은 입력에 영향을 미치는 손상이나 노이즈로 인해 딥 뉴럴 네트워크(Deep Neural Networks, DNN)의 성능 저하를 줄이는 실용적인 방법으로 떠올랐어. 기존의 TTA 방법들은 DNN을 지속적으로 조정하는데, 이게 자원을 과도하게 소모하게 하고 감독이 부족해서 오류가 쌓이면서 성능이 떨어지는 문제가 있어.

이 연구에서는 이런 문제를 해결하기 위해 도메인 인식 실시간 동적 적응(Domain-Aware Real-Time Dynamic Adaptation, DARDA)을 제안해. 우리의 핵심 방법은 여러 종류의 손상에 대한 잠재적 표현(latent representation)을 능동적으로 학습하는 건데, 각각의 표현은 그 손상에 영향을 받은 입력을 정확히 분류하기 위해 맞춤형 서브 네트워크 상태와 연결돼.

배포 후, DARDA는 (i) 현재 진행 중인 손상의 잠재적 표현을 추정하고; (ii) 잠재 공간에서 현재 손상과 가장 가까운 서브 네트워크를 선택하고; (iii) DNN 상태를 조정해서 그 표현이 현재 손상과 맞도록 해. 이렇게 해서 DARDA는 자원 효율성이 더 높고, 다양한 입력 데이터 없이도 다른 손상으로 인해 발생하는 새로운 분포에 빠르게 적응할 수 있어.

라즈베리 파이와 NVIDIA 제트슨 나노 같은 두 가지 인기 있는 모바일 엣지 디바이스로 실험한 결과, DARDA는 최신 기술에 비해 에너지 소비를 1.74배 줄이고 평균 캐시 메모리 사용량을 2.64배 줄이면서, CIFAR-10, CIFAR-100, TinyImagenet에서 각각 10.4%, 5.7%, 4.4% 성능을 향상시켰어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09754.pdf

Title: Towards Single-Lens Controllable Depth-of-Field Imaging via All-in-Focus Aberration Correction and Monocular Depth Estimation

Original Abstract:
Controllable Depth-of-Field (DoF) imaging commonly produces amazing visual effects based on heavy and expensive high-end lenses. However, confronted with the increasing demand for mobile scenarios, it is desirable to achieve a lightweight solution with Minimalist Optical Systems (MOS). This work centers around two major limitations of MOS, i.e., the severe optical aberrations and uncontrollable DoF, for achieving single-lens controllable DoF imaging via computational methods. A Depth-aware Controllable DoF Imaging (DCDI) framework is proposed equipped with All-in-Focus (AiF) aberration correction and monocular depth estimation, where the recovered image and corresponding depth map are utilized to produce imaging results under diverse DoFs of any high-end lens via patch-wise convolution. To address the depth-varying optical degradation, we introduce a Depth-aware Degradation-adaptive Training (DA2T) scheme. At the dataset level, a Depth-aware Aberration MOS (DAMOS) dataset is established based on the simulation of Point Spread Functions (PSFs) under different object distances. Additionally, we design two plug-and-play depth-aware mechanisms to embed depth information into the aberration image recovery for better tackling depth-aware degradation. Furthermore, we propose a storage-efficient Omni-Lens-Field model to represent the 4D PSF library of various lenses. With the predicted depth map, recovered image, and depth-aware PSF map inferred by Omni-Lens-Field, single-lens controllable DoF imaging is achieved. Comprehensive experimental results demonstrate that the proposed framework enhances the recovery performance, and attains impressive single-lens controllable DoF imaging results, providing a seminal baseline for this field. The source code and the established dataset will be publicly available at this https URL.

Translated Abstract:
제어 가능한 심도(DoF) 이미징은 고급 렌즈를 이용해 멋진 시각 효과를 만들어내는 게 일반적이야. 하지만 모바일 환경에서의 수요가 증가하면서, 간단한 광학 시스템(MOS)으로 가벼운 해결책을 찾는 게 필요해. 이 연구는 MOS의 두 가지 주요 한계, 즉 심각한 광학 왜곡과 제어 불가능한 DoF를 해결하는 데 초점을 맞추고 있어. 

우리는 Depth-aware Controllable DoF Imaging (DCDI) 프레임워크를 제안해. 이 프레임워크는 모든 초점(AiF) 왜곡 보정과 단안 깊이 추정을 포함하고, 복원된 이미지와 관련된 깊이 맵을 사용해 다양한 DoF에서 이미지를 생성해. 이를 위해 패치 단위로 컨볼루션을 사용해. 깊이에 따라 변하는 광학 저하를 해결하기 위해 Depth-aware Degradation-adaptive Training (DA2T) 방식을 도입했어. 데이터셋 차원에서는, 서로 다른 물체 거리에서의 점 확산 함수(PSF)를 시뮬레이션하여 Depth-aware Aberration MOS (DAMOS) 데이터셋을 만들었어. 

또한, 깊이 정보를 왜곡 이미지 복원에 통합할 수 있는 두 가지 플러그 앤 플레이 깊이 인식 메커니즘도 설계했어. 그리고 다양한 렌즈의 4D PSF 라이브러리를 나타내기 위해 저장 공간이 효율적인 Omni-Lens-Field 모델을 제안했어. 예측된 깊이 맵, 복원된 이미지, 그리고 Omni-Lens-Field를 통해 추론된 깊이 인식 PSF 맵을 활용해 단일 렌즈 제어 가능한 DoF 이미징을 달성했어. 

실험 결과를 보면, 제안된 프레임워크가 복원 성능을 향상시키고, 인상적인 단일 렌즈 제어 가능한 DoF 이미징 결과를 얻었다는 걸 보여줘. 이 분야의 중요한 기준점을 제공하는 거지. 소스 코드와 데이터셋은 이 URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09756.pdf

Title: MesonGS: Post-training Compression of 3D Gaussians via Efficient Attribute Transformation

Original Abstract:
3D Gaussian Splatting demonstrates excellent quality and speed in novel view synthesis. Nevertheless, the huge file size of the 3D Gaussians presents challenges for transmission and storage. Current works design compact models to replace the substantial volume and attributes of 3D Gaussians, along with intensive training to distill information. These endeavors demand considerable training time, presenting formidable hurdles for practical deployment. To this end, we propose MesonGS, a codec for post-training compression of 3D Gaussians. Initially, we introduce a measurement criterion that considers both view-dependent and view-independent factors to assess the impact of each Gaussian point on the rendering output, enabling the removal of insignificant points. Subsequently, we decrease the entropy of attributes through two transformations that complement subsequent entropy coding techniques to enhance the file compression rate. More specifically, we first replace rotation quaternions with Euler angles; then, we apply region adaptive hierarchical transform to key attributes to reduce entropy. Lastly, we adopt finer-grained quantization to avoid excessive information loss. Moreover, a well-crafted finetune scheme is devised to restore quality. Extensive experiments demonstrate that MesonGS significantly reduces the size of 3D Gaussians while preserving competitive quality.

Translated Abstract:
3D 가우시안 스플래팅은 새로운 시점 합성에서 뛰어난 품질과 속도를 보여줘. 하지만 3D 가우시안의 파일 크기가 너무 커서 전송과 저장에 문제가 있어. 현재 연구들은 3D 가우시안의 큰 부피와 속성을 대신할 수 있는 컴팩트 모델을 만들고, 정보를 정제하기 위해 많은 훈련을 요구하고 있어. 이런 노력들은 상당한 훈련 시간을 필요로 해서 실제로 사용하기엔 어려움이 있어.

그래서 우리는 MesonGS라는 코덱을 제안해. 이건 3D 가우시안의 후처리 압축을 위한 거야. 먼저, 각 가우시안 포인트가 렌더링 결과에 미치는 영향을 평가하기 위해 시점 의존적 및 비의존적 요소를 모두 고려하는 측정 기준을 도입해. 이렇게 해서 중요하지 않은 포인트를 제거할 수 있어.

그 다음, 두 가지 변환을 통해 속성의 엔트로피를 줄여서 이후의 엔트로피 코딩 기술과 잘 맞춰서 파일 압축률을 높여. 구체적으로, 먼저 회전 쿼터니온을 오일러 각으로 바꾸고, 그 다음에는 주요 속성에 지역 적응형 계층 변환을 적용해서 엔트로피를 줄여. 마지막으로, 지나치게 많은 정보 손실을 피하기 위해 세밀한 양자화를 도입해. 게다가, 품질을 복원하기 위한 잘 설계된 미세 조정 계획도 마련했어.

많은 실험을 통해 MesonGS가 3D 가우시안의 크기를 크게 줄이면서도 경쟁력 있는 품질을 유지한다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09766.pdf

Title: Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting

Original Abstract:
This study explores a workflow for automated segmentation of lesions in FDG and PSMA PET/CT images. Due to the substantial differences in image characteristics between FDG and PSMA, specialized preprocessing steps are required. Utilizing YOLOv8 for data classification, the FDG and PSMA images are preprocessed separately before feeding them into the segmentation models, aiming to improve lesion segmentation accuracy. The study focuses on evaluating the performance of automated segmentation workflow for multitracer PET images. The findings are expected to provide critical insights for enhancing diagnostic workflows and patient-specific treatment plans. Our code will be open-sourced and available at this https URL.

Translated Abstract:
이 연구는 FDG와 PSMA PET/CT 이미지에서 병변을 자동으로 구분하는 워크플로우를 탐구해. FDG와 PSMA의 이미지 특성이 많이 달라서, 특별한 전처리 단계가 필요해. 

YOLOv8을 사용해서 데이터를 분류하고, FDG와 PSMA 이미지를 각각 전처리한 다음에 분할 모델에 넣어. 이렇게 하면 병변 분할 정확도를 높이는 게 목표야. 

연구는 다중 추적 PET 이미지에 대한 자동 분할 워크플로우의 성능을 평가하는 데 초점을 맞추고 있어. 결과는 진단 워크플로우와 환자 맞춤형 치료 계획을 개선하는 데 중요한 통찰력을 제공할 것으로 기대돼. 

우리 코드는 오픈 소스로 제공될 예정이고, 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09774.pdf

Title: Generalizing Alignment Paradigm of Text-to-Image Generation with Preferences through $f$-divergence Minimization

Original Abstract:
Direct Preference Optimization (DPO) has recently expanded its successful application from aligning large language models (LLMs) to aligning text-to-image models with human preferences, which has generated considerable interest within the community. However, we have observed that these approaches rely solely on minimizing the reverse Kullback-Leibler divergence during alignment process between the fine-tuned model and the reference model, neglecting the incorporation of other divergence constraints. In this study, we focus on extending reverse Kullback-Leibler divergence in the alignment paradigm of text-to-image models to $f$-divergence, which aims to garner better alignment performance as well as good generation diversity. We provide the generalized formula of the alignment paradigm under the $f$-divergence condition and thoroughly analyze the impact of different divergence constraints on alignment process from the perspective of gradient fields. We conduct comprehensive evaluation on image-text alignment performance, human value alignment performance and generation diversity performance under different divergence constraints, and the results indicate that alignment based on Jensen-Shannon divergence achieves the best trade-off among them. The option of divergence employed for aligning text-to-image models significantly impacts the trade-off between alignment performance (especially human value alignment) and generation diversity, which highlights the necessity of selecting an appropriate divergence for practical applications.

Translated Abstract:
Direct Preference Optimization (DPO)는 최근에 큰 언어 모델(LLM)에서 텍스트-이미지 모델을 사람의 선호에 맞게 조정하는 데 성공적으로 적용되면서 큰 관심을 받고 있어. 그런데 이 방법들이 정교하게 조정된 모델과 기준 모델 간의 정렬 과정에서 오직 역 쿨백-라이블러 발산을 최소화하는 데만 의존하고 있다는 점을 발견했어. 다른 발산 제약 조건을 포함하는 건 무시되고 있어.

이번 연구에서는 텍스트-이미지 모델의 정렬 패러다임에서 역 쿨백-라이블러 발산을 $f$-발산으로 확장하는 데 초점을 맞췄어. 이게 더 나은 정렬 성능과 좋은 생성 다양성을 얻는 데 목표가 있어. 우리는 $f$-발산 조건 하의 정렬 패러다임을 위한 일반화된 공식을 제공하고, 다양한 발산 제약이 정렬 과정에 미치는 영향을 기울기 필드 관점에서 철저히 분석했어.

이미지-텍스트 정렬 성능, 인간 가치 정렬 성능, 그리고 생성 다양성 성능에 대해 다양한 발산 제약 하에서 종합적인 평가를 진행했어. 그 결과, 젠센-샤논 발산을 기반으로 한 정렬이 이들 간의 최상의 균형을 이룬다는 걸 보여줬어. 텍스트-이미지 모델을 정렬하는 데 사용하는 발산 옵션이 정렬 성능(특히 인간 가치 정렬)과 생성 다양성 간의 균형에 크게 영향을 미친다는 점에서, 실용적인 응용을 위해 적절한 발산을 선택하는 것이 필요하다는 걸 강조해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09777.pdf

Title: DiFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and Iterative Refinement for Efficient End-to-End Autonomous Driving

Original Abstract:
Current end-to-end autonomous driving methods resort to unifying modular designs for various tasks (e.g. perception, prediction and planning). Although optimized in a planning-oriented spirit with a fully differentiable framework, existing end-to-end driving systems without ego-centric designs still suffer from unsatisfactory performance and inferior efficiency, owing to the rasterized scene representation learning and redundant information transmission. In this paper, we revisit the human driving behavior and propose an ego-centric fully sparse paradigm, named DiFSD, for end-to-end self-driving. Specifically, DiFSD mainly consists of sparse perception, hierarchical interaction and iterative motion planner. The sparse perception module performs detection, tracking and online mapping based on sparse representation of the driving scene. The hierarchical interaction module aims to select the Closest In-Path Vehicle / Stationary (CIPV / CIPS) from coarse to fine, benefiting from an additional geometric prior. As for the iterative motion planner, both selected interactive agents and ego-vehicle are considered for joint motion prediction, where the output multi-modal ego-trajectories are optimized in an iterative fashion. Besides, both position-level motion diffusion and trajectory-level planning denoising are introduced for uncertainty modeling, thus facilitating the training stability and convergence of the whole framework. Extensive experiments conducted on nuScenes dataset demonstrate the superior planning performance and great efficiency of DiFSD, which significantly reduces the average L2 error by \textbf{66\%} and collision rate by \textbf{77\%} than UniAD while achieves \textbf{8.2$\times$} faster running efficiency.

Translated Abstract:
현재의 완전 자율주행 방법들은 여러 작업(예: 인식, 예측, 계획)을 위해 모듈 디자인을 통합하는 방식으로 작업하고 있어. 완전히 차별화된 프레임워크로 최적화는 되어 있지만, 기존의 자율주행 시스템은 여전히 만족스럽지 않은 성능과 비효율적인 문제를 겪고 있어. 이는 장면 표현 학습이 rasterized 형태로 되어 있고, 중복된 정보 전송 때문이야.

이 논문에서는 인간의 운전 행동을 다시 살펴보고, 자율주행을 위한 'DiFSD'라는 자아 중심의 완전 희소 패러다임을 제안해. DiFSD는 주로 희소 인식, 계층적 상호작용, 반복적인 동작 계획자로 구성돼. 희소 인식 모듈은 운전 장면의 희소 표현을 기반으로 탐지, 추적, 온라인 맵핑을 수행해.

계층적 상호작용 모듈은 추가적인 기하학적 정보를 활용해서 대략적인 것에서 구체적인 것까지 '가장 가까운 경로 내 차량/정지 물체(CIPV/CIPS)'를 선택하는 역할을 해. 반복적인 동작 계획자는 선택된 상호작용 객체와 자아 차량을 함께 고려해 동작 예측을 하는데, 이때 출력되는 다중 모달 자아 궤적은 반복적으로 최적화돼.

또한, 위치 수준의 동작 확산과 궤적 수준의 계획 노이즈 제거가 불확실성 모델링을 위해 도입돼, 전체 프레임워크의 훈련 안정성과 수렴성을 돕는 역할을 해. nuScenes 데이터셋에서 수행된 광범위한 실험 결과, DiFSD는 계획 성능이 우수하고 효율성이 뛰어난 것으로 나타났어. 평균 L2 오차를 UniAD보다 66% 줄이고, 충돌률을 77% 감소시키면서 실행 효율성은 8.2배 빨라졌어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09779.pdf

Title: Underwater Image Enhancement via Dehazing and Color Restoration

Original Abstract:
With the rapid development of marine engineering projects such as marine resource extraction and oceanic surveys, underwater visual imaging and analysis has become a critical technology. Unfortunately, due to the inevitable non-linear attenuation of light in underwater environments, underwater images and videos often suffer from low contrast, blurriness, and color degradation, which significantly complicate the subsequent research. Existing underwater image enhancement methods often treat the haze and color cast as a unified degradation process and disregard their independence and interdependence, which limits the performance improvement. Here, we propose a Vision Transformer (ViT)-based network (referred to as WaterFormer) to improve the underwater image quality. WaterFormer contains three major components: a dehazing block (DehazeFormer Block) to capture the self-correlated haze features and extract deep-level features, a Color Restoration Block (CRB) to capture self-correlated color cast features, and a Channel Fusion Block (CFB) to capture fusion features within the network. To ensure authenticity, a soft reconstruction layer based on the underwater imaging physics model is included. To improve the quality of the enhanced images, we introduce the Chromatic Consistency Loss and Sobel Color Loss to train the network. Comprehensive experimental results demonstrate that WaterFormer outperforms other state-of-the-art methods in enhancing underwater images.

Translated Abstract:
해양 자원 추출이나 해양 조사 같은 해양 공학 프로젝트가 빠르게 발전하면서, 수중 시각화와 분석 기술이 매우 중요해졌어. 하지만 수중 환경에서는 빛이 비선형적으로 감쇠되기 때문에, 수중 이미지와 비디오가 종종 낮은 대비, 흐림, 색상 저하 같은 문제를 겪어. 이걸 해결하기가 쉽지 않지. 기존의 수중 이미지 개선 방법들은 안개와 색상 왜곡을 하나의 문제로 보는데, 이게 두 가지가 독립적이고 서로 영향을 주는 관계라는 걸 무시해서 성능 개선에 한계가 있어.

그래서 우리는 비전 트랜스포머(ViT) 기반의 네트워크인 WaterFormer를 제안해. WaterFormer는 세 가지 주요 구성 요소로 이루어져 있어: 

1. **DehazeFormer Block**: 안개와 관련된 특징을 잡고, 깊은 수준의 특징을 추출해.
2. **Color Restoration Block (CRB)**: 색상 왜곡과 관련된 특징을 잡아.
3. **Channel Fusion Block (CFB)**: 네트워크 내에서 특징을 융합하는 역할을 해.

이 네트워크의 진정성을 보장하기 위해 수중 이미징 물리 모델을 기반으로 한 소프트 재구성 레이어도 포함했어. 개선된 이미지의 품질을 높이기 위해 크로마틱 일관성 손실과 소벨 색상 손실을 도입해서 네트워크를 훈련시켜. 

종합적인 실험 결과에 따르면, WaterFormer는 다른 최신 방법들보다 수중 이미지 개선에서 더 뛰어난 성능을 보여.

================================================================================

URL:
https://arxiv.org/pdf/2409.09784.pdf

Title: Enhancing Lesion Segmentation in PET/CT Imaging with Deep Learning and Advanced Data Preprocessing Techniques

Original Abstract:
The escalating global cancer burden underscores the critical need for precise diagnostic tools in oncology. This research employs deep learning to enhance lesion segmentation in PET/CT imaging, utilizing a dataset of 900 whole-body FDG-PET/CT and 600 PSMA-PET/CT studies from the AutoPET challenge III. Our methodical approach includes robust preprocessing and data augmentation techniques to ensure model robustness and generalizability. We investigate the influence of non-zero normalization and modifications to the data augmentation pipeline, such as the introduction of RandGaussianSharpen and adjustments to the Gamma transform parameter. This study aims to contribute to the standardization of preprocessing and augmentation strategies in PET/CT imaging, potentially improving the diagnostic accuracy and the personalized management of cancer patients. Our code will be open-sourced and available at this https URL.

Translated Abstract:
전 세계적으로 암 환자가 늘어나고 있어서, 정확한 진단 도구가 정말 필요해. 이 연구는 딥러닝을 활용해서 PET/CT 영상에서 병변을 더 잘 분할하는 방법을 제안해. 데이터셋은 AutoPET Challenge III에서 가져온 900개의 전체 신체 FDG-PET/CT와 600개의 PSMA-PET/CT 연구 자료야.

우리는 모델의 강건성과 일반화를 보장하기 위해 철저한 전처리와 데이터 증강 기법을 사용했어. 그리고 비제로 정규화의 영향과 데이터 증강 파이프라인을 수정하는 방법, 예를 들어 RandGaussianSharpen의 도입과 감마 변환 파라미터 조정 등을 조사했어.

이 연구의 목표는 PET/CT 영상에서 전처리와 증강 전략의 표준화를 돕는 거야. 이렇게 되면 암 환자의 진단 정확도와 개인 맞춤형 관리가 개선될 수 있을 거라고 생각해. 우리 코드는 오픈소스로 제공될 예정이니, 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09788.pdf

Title: Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models

Original Abstract:
Despite recent advances demonstrating vision-language models' (VLMs) abilities to describe complex relationships in images using natural language, their capability to quantitatively reason about object sizes and distances remains underexplored. In this work, we introduce a manually annotated benchmark, Q-Spatial Bench, with 271 questions across five categories designed for quantitative spatial reasoning and systematically investigate the performance of state-of-the-art VLMs on this task. Our analysis reveals that reasoning about distances between objects is particularly challenging for SoTA VLMs; however, some VLMs significantly outperform others, with an over 40-point gap between the two best performing models. We also make the surprising observation that the success rate of the top-performing VLM increases by 19 points when a reasoning path using a reference object emerges naturally in the response. Inspired by this observation, we develop a zero-shot prompting technique, SpatialPrompt, that encourages VLMs to answer quantitative spatial questions using reference objects as visual cues. By instructing VLMs to use reference objects in their reasoning paths via SpatialPrompt, Gemini 1.5 Pro, Gemini 1.5 Flash, and GPT-4V improve their success rates by over 40, 20, and 30 points, respectively. We emphasize that these significant improvements are obtained without needing more data, model architectural modifications, or fine-tuning.

Translated Abstract:
최근 비전-언어 모델(VLMs)이 이미지의 복잡한 관계를 자연어로 설명하는 능력을 보여주는 발전이 있었지만, 물체의 크기와 거리에 대해 정량적으로 추론하는 능력은 아직 충분히 연구되지 않았어. 이 연구에서는 정량적 공간 추론을 위해 설계된 271개의 질문이 포함된 수동 주석 기준인 Q-Spatial Bench를 소개해. 그리고 최신 VLM들이 이 작업에서 어떻게 수행되는지 체계적으로 조사했어.

우리 분석 결과, 물체 간의 거리 추론이 최신 VLM들에게 특히 어려운 과제라는 걸 알게 되었어. 하지만 일부 VLM들은 다른 모델들보다 훨씬 잘 수행되는데, 가장 잘하는 두 모델 간에는 40점 이상의 차이가 나. 또, 상관 물체를 사용하는 추론 경로가 자연스럽게 나오면, 가장 성능이 좋은 VLM의 성공률이 19점 증가하는 놀라운 결과도 발견했어.

이 관찰에 영감을 받아, 우리는 VLM들이 참조 물체를 시각적 단서로 사용해 정량적 공간 질문에 답할 수 있도록 유도하는 제로샷 프롬프팅 기술인 SpatialPrompt를 개발했어. SpatialPrompt를 사용해서 VLM들이 추론 경로에 참조 물체를 포함하도록 지시하니, Gemini 1.5 Pro는 40점, Gemini 1.5 Flash는 20점, GPT-4V는 30점 이상 성공률이 향상됐어. 우리가 강조하고 싶은 건, 이러한 큰 개선이 추가 데이터나 모델 아키텍처 수정, 세부 조정 없이 이루어졌다는 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09790.pdf

Title: Multiple Rotation Averaging with Constrained Reweighting Deep Matrix Factorization

Original Abstract:
Multiple rotation averaging plays a crucial role in computer vision and robotics domains. The conventional optimization-based methods optimize a nonlinear cost function based on certain noise assumptions, while most previous learning-based methods require ground truth labels in the supervised training process. Recognizing the handcrafted noise assumption may not be reasonable in all real-world scenarios, this paper proposes an effective rotation averaging method for mining data patterns in a learning manner while avoiding the requirement of labels. Specifically, we apply deep matrix factorization to directly solve the multiple rotation averaging problem in unconstrained linear space. For deep matrix factorization, we design a neural network model, which is explicitly low-rank and symmetric to better suit the background of multiple rotation averaging. Meanwhile, we utilize a spanning tree-based edge filtering to suppress the influence of rotation outliers. What's more, we also adopt a reweighting scheme and dynamic depth selection strategy to further improve the robustness. Our method synthesizes the merit of both optimization-based and learning-based methods. Experimental results on various datasets validate the effectiveness of our proposed method.

Translated Abstract:
다중 회전 평균화는 컴퓨터 비전과 로봇 분야에서 중요한 역할을 해. 기존의 최적화 기반 방법들은 특정한 노이즈 가정을 바탕으로 비선형 비용 함수를 최적화하고, 대부분의 이전 학습 기반 방법들은 감독된 훈련 과정에서 실제 레이블이 필요해. 하지만 모든 실제 상황에서 손으로 만든 노이즈 가정이 합리적이지 않을 수도 있다는 점을 인식하고, 이 논문에서는 라벨 없이 데이터 패턴을 학습하는 방식으로 다중 회전 평균화 방법을 제안해.

구체적으로, 우리는 심층 행렬 분해를 적용해서 제약 없는 선형 공간에서 다중 회전 평균화 문제를 직접 해결해. 심층 행렬 분해를 위해서, 우리는 다중 회전 평균화의 배경에 더 잘 맞도록 명시적으로 저계수 및 대칭인 신경망 모델을 설계했어. 동시에, 회전 이상치의 영향을 줄이기 위해 스패닝 트리 기반의 엣지 필터링을 사용해. 게다가, 우리는 재가중치 기법과 동적 깊이 선택 전략을 채택해서 강인성을 더욱 향상시켜.

우리 방법은 최적화 기반과 학습 기반 방법의 장점을 합쳤어. 여러 데이터셋에서 실험한 결과, 제안한 방법의 효과를 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09804.pdf

Title: Abnormal Event Detection In Videos Using Deep Embedding

Original Abstract:
Abnormal event detection or anomaly detection in surveillance videos is currently a challenge because of the diversity of possible events. Due to the lack of anomalous events at training time, anomaly detection requires the design of learning methods without supervision. In this work we propose an unsupervised approach for video anomaly detection with the aim to jointly optimize the objectives of the deep neural network and the anomaly detection task using a hybrid architecture. Initially, a convolutional autoencoder is pre-trained in an unsupervised manner with a fusion of depth, motion and appearance features. In the second step, we utilize the encoder part of the pre-trained autoencoder and extract the embeddings of the fused input. Now, we jointly train/ fine tune the encoder to map the embeddings to a hypercenter. Thus, embeddings of normal data fall near the hypercenter, whereas embeddings of anomalous data fall far away from the hypercenter.

Translated Abstract:
감시 비디오에서 비정상 사건 탐지, 즉 이상 탐지는 다양한 사건들이 존재하기 때문에 현재 도전 과제가 되고 있어. 훈련할 때 비정상 사건이 부족하기 때문에, 이상 탐지를 위해서는 감독 없이 학습 방법을 설계해야 해.

이 연구에서는 비디오 이상 탐지를 위한 비감독 접근 방식을 제안해. 여기서 목표는 딥 뉴럴 네트워크의 목표와 이상 탐지 작업을 함께 최적화하는 거야. 처음에는 합성된 깊이, 움직임, 외관 특징들을 이용해 컨볼루션 오토인코더를 비감독 방식으로 사전 훈련해.

두 번째 단계에서는 사전 훈련된 오토인코더의 인코더 부분을 이용해서 합성 입력의 임베딩을 추출해. 이제 인코더를 함께 훈련하고 미세 조정해서 임베딩을 하이퍼센터에 매핑해. 이렇게 하면 정상 데이터의 임베딩은 하이퍼센터 근처에 위치하고, 비정상 데이터의 임베딩은 하이퍼센터에서 멀리 떨어지게 돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.09808.pdf

Title: Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion

Original Abstract:
Mamba and Vision Mamba (Vim) models have shown their potential as an alternative to methods based on Transformer architecture. This work introduces Fast Mamba for Vision (Famba-V), a cross-layer token fusion technique to enhance the training efficiency of Vim models. The key idea of Famba-V is to identify and fuse similar tokens across different Vim layers based on a suit of cross-layer strategies instead of simply applying token fusion uniformly across all the layers that existing works propose. We evaluate the performance of Famba-V on CIFAR-100. Our results show that Famba-V is able to enhance the training efficiency of Vim models by reducing both training time and peak memory usage during training. Moreover, the proposed cross-layer strategies allow Famba-V to deliver superior accuracy-efficiency trade-offs. These results all together demonstrate Famba-V as a promising efficiency enhancement technique for Vim models.

Translated Abstract:
Mamba와 Vision Mamba (Vim) 모델은 Transformer 구조를 기반으로 한 방법의 대안으로서 가능성을 보여줬어. 이번 연구에서는 Fast Mamba for Vision (Famba-V)을 소개하는데, 이는 Vim 모델의 훈련 효율을 높이는 크로스 레이어 토큰 융합 기술이야. 

Famba-V의 핵심 아이디어는 기존 연구들이 제안한 것처럼 모든 레이어에서 토큰 융합을 고르게 적용하는 게 아니라, 다양한 Vim 레이어에서 비슷한 토큰을 찾아서 융합하는 거야. 이 과정은 여러 레이어에 걸친 전략을 기반으로 해. 

우리는 CIFAR-100에서 Famba-V의 성능을 평가했어. 결과를 보면, Famba-V가 Vim 모델의 훈련 효율을 높여주는 걸 알 수 있어. 훈련 시간이 줄어들고 훈련 중에 사용하는 최대 메모리도 감소했어. 게다가 제안한 크로스 레이어 전략 덕분에 Famba-V는 정확도와 효율성의 균형을 잘 맞출 수 있어. 

이 모든 결과는 Famba-V가 Vim 모델의 효율성을 높이는 데 유망한 기술이라는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09832.pdf

Title: Template-based Multi-Domain Face Recognition

Original Abstract:
Despite the remarkable performance of deep neural networks for face detection and recognition tasks in the visible spectrum, their performance on more challenging non-visible domains is comparatively still lacking. While significant research has been done in the fields of domain adaptation and domain generalization, in this paper we tackle scenarios in which these methods have limited applicability owing to the lack of training data from target domains. We focus on the problem of single-source (visible) and multi-target (SWIR, long-range/remote, surveillance, and body-worn) face recognition task. We show through experiments that a good template generation algorithm becomes crucial as the complexity of the target domain increases. In this context, we introduce a template generation algorithm called Norm Pooling (and a variant known as Sparse Pooling) and show that it outperforms average pooling across different domains and networks, on the IARPA JANUS Benchmark Multi-domain Face (IJB-MDF) dataset.

Translated Abstract:
딥 뉴럴 네트워크가 가시광선에서 얼굴 탐지와 인식 작업에 뛰어난 성능을 보이긴 하지만, 비가시광선 같은 더 어려운 분야에서는 성능이 여전히 부족해. 도메인 적응과 도메인 일반화에 대한 연구가 많이 진행되었지만, 이 논문에서는 목표 도메인에서 훈련 데이터가 부족 때문에 이 방법들이 한계가 있는 상황을 다뤄.

우리는 단일 소스(가시광선)와 다중 타겟(SWIR, 장거리/원거리, 감시, 몸에 착용하는) 얼굴 인식 문제에 집중해. 실험을 통해 목표 도메인의 복잡성이 증가할수록 좋은 템플릿 생성 알고리즘의 중요성이 커진다는 걸 보여줄 거야. 이 맥락에서 우리는 Norm Pooling이라는 템플릿 생성 알고리즘(변형인 Sparse Pooling도 포함)을 소개하고, IARPA JANUS Benchmark Multi-domain Face (IJB-MDF) 데이터셋에서 다양한 도메인과 네트워크에서 평균 풀링보다 성능이 뛰어남을 입증할 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09841.pdf

Title: Tracking Virtual Meetings in the Wild: Re-identification in Multi-Participant Virtual Meetings

Original Abstract:
In recent years, workplaces and educational institutes have widely adopted virtual meeting platforms. This has led to a growing interest in analyzing and extracting insights from these meetings, which requires effective detection and tracking of unique individuals. In practice, there is no standardization in video meetings recording layout, and how they are captured across the different platforms and services. This, in turn, creates a challenge in acquiring this data stream and analyzing it in a uniform fashion. Our approach provides a solution to the most general form of video recording, usually consisting of a grid of participants (\cref{fig:videomeeting}) from a single video source with no metadata on participant locations, while using the least amount of constraints and assumptions as to how the data was acquired. Conventional approaches often use YOLO models coupled with tracking algorithms, assuming linear motion trajectories akin to that observed in CCTV footage. However, such assumptions fall short in virtual meetings, where participant video feed window can abruptly change location across the grid. In an organic video meeting setting, participants frequently join and leave, leading to sudden, non-linear movements on the video grid. This disrupts optical flow-based tracking methods that depend on linear motion. Consequently, standard object detection and tracking methods might mistakenly assign multiple participants to the same tracker. In this paper, we introduce a novel approach to track and re-identify participants in remote video meetings, by utilizing the spatio-temporal priors arising from the data in our domain. This, in turn, increases tracking capabilities compared to the use of general object tracking. Our approach reduces the error rate by 95% on average compared to YOLO-based tracking methods as a baseline.

Translated Abstract:
최근 몇 년 동안, 직장과 교육 기관에서 가상 회의 플랫폼을 많이 사용하게 되었어. 그래서 이 회의들을 분석하고 인사이트를 추출하는 것에 대한 관심이 커지고 있는데, 이를 위해서는 개별 참가자를 효과적으로 감지하고 추적해야 해. 

하지만 실제로는 비디오 회의의 녹화 형식이 표준화되어 있지 않고, 각 플랫폼과 서비스마다 캡처 방식이 달라. 이 때문에 데이터 스트림을 수집하고 균일하게 분석하는 데 어려움이 생겨. 우리의 접근 방식은 일반적인 비디오 녹화 형태를 해결해 주는데, 보통은 참가자들이 그리드 형태로 배치된 단일 비디오 소스에서 촬영되고 참가자 위치에 대한 메타데이터가 없어. 데이터가 어떻게 수집되었는지에 대한 제약이나 가정을 최소화하면서 말이야.

전통적인 방법은 보통 YOLO 모델과 추적 알고리즘을 함께 사용해서, CCTV 영상에서 보이는 것처럼 선형 움직임 경로를 가정해. 하지만 가상 회의에서는 참가자의 비디오 피드 창이 그리드에서 갑자기 위치를 바꿀 수 있기 때문에 이런 가정이 맞지 않아. 실제 비디오 회의에서는 참가자들이 자주 들어오고 나가서 비디오 그리드에서 갑작스럽고 비선형적인 움직임이 발생해. 이게 선형 움직임에 의존하는 광학 흐름 기반 추적 방법을 방해하게 돼. 그래서 일반적인 물체 감지와 추적 방법이 여러 참가자를 같은 추적기로 잘못 지정할 수 있어.

이 논문에서는 원격 비디오 회의에서 참가자를 추적하고 재식별하는 새로운 접근 방식을 소개해. 우리의 분야에서 데이터에서 발생하는 시공간적 선행 정보를 활용하는 거야. 이를 통해 일반 물체 추적을 사용할 때보다 추적 능력을 높일 수 있어. 우리의 방법은 YOLO 기반 추적 방법에 비해 평균적으로 95%의 오류율을 줄였어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09867.pdf

Title: Towards Kinetic Manipulation of the Latent Space

Original Abstract:
The latent space of many generative models are rich in unexplored valleys and mountains. The majority of tools used for exploring them are so far limited to Graphical User Interfaces (GUIs). While specialized hardware can be used for this task, we show that a simple feature extraction of pre-trained Convolutional Neural Networks (CNNs) from a live RGB camera feed does a very good job at manipulating the latent space with simple changes in the scene, with vast room for improvement. We name this new paradigm Visual-reactive Interpolation, and the full code can be found at this https URL.

Translated Abstract:
많은 생성 모델의 잠재 공간에는 탐험되지 않은 계곡과 산이 많이 있어. 지금까지 이들을 탐색하는 도구는 대부분 그래픽 사용자 인터페이스(GUI)로 제한되어 있었어. 전문 하드웨어를 사용할 수도 있지만, 우리는 라이브 RGB 카메라 피드에서 사전 훈련된 합성 곱 신경망(CNN)으로 간단한 특징 추출을 하는 것이 장면의 작은 변화로도 잠재 공간을 잘 조작할 수 있다는 걸 보여줬어. 

이 방법은 개선의 여지가 많고, 우리는 이 새로운 패러다임을 '시각 반응 인터폴레이션(Visual-reactive Interpolation)'이라고 불러. 전체 코드는 이 https URL에서 찾을 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09877.pdf

Title: REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models

Original Abstract:
This paper introduces a novel framework for detecting and segmenting critical road assets on Thai highways using an advanced Refined Generalized Focal Loss (REG) formulation. Integrated into state-of-the-art vision-based detection and segmentation models, the proposed method effectively addresses class imbalance and the challenges of localizing small, underrepresented road elements, including pavilions, pedestrian bridges, information signs, single-arm poles, bus stops, warning signs, and concrete guardrails. To improve both detection and segmentation accuracy, a multi-task learning strategy is adopted, optimizing REG across multiple tasks. REG is further enhanced by incorporating a spatial-contextual adjustment term, which accounts for the spatial distribution of road assets, and a probabilistic refinement that captures prediction uncertainty in complex environments, such as varying lighting conditions and cluttered backgrounds. Our rigorous mathematical formulation demonstrates that REG minimizes localization and classification errors by applying adaptive weighting to hard-to-detect instances while down-weighting easier examples. Experimental results show a substantial performance improvement, achieving a mAP50 of 80.34 and an F1-score of 77.87, significantly outperforming conventional methods. This research underscores the capability of advanced loss function refinements to enhance the robustness and accuracy of road asset detection and segmentation, thereby contributing to improved road safety and infrastructure management. For an in-depth discussion of the mathematical background and related methods, please refer to previous work available at \url{this https URL}.

Translated Abstract:
이 논문은 태국 고속도로에서 중요한 도로 자산을 탐지하고 분할하는 새로운 프레임워크를 소개해. 이 방법은 정교하게 발전된 일반화된 포컬 손실(Refined Generalized Focal Loss, REG) 공식을 사용해. 최첨단 비전 기반 탐지 및 분할 모델에 통합되어, 클래스 불균형 문제와 작은 도로 요소(예: 정자, 보행자 다리, 정보 표지판, 단일 기둥, 버스 정류장, 경고 표지판, 콘크리트 가드레일 등)의 로컬화 도전 과제를 효과적으로 해결해.

탐지와 분할 정확도를 높이기 위해 다중 과제 학습 전략을 채택했어. REG는 여러 작업에서 최적화되며, 공간적 맥락 조정 항을 추가해 도로 자산의 공간 분포를 고려하고, 복잡한 환경에서의 예측 불확실성을 포착하는 확률적 정제를 포함해 더 향상돼. 우리의 엄격한 수학적 공식화는 REG가 탐지가 어려운 사례에 적응형 가중치를 적용하고, 쉽게 탐지되는 사례의 가중치를 줄여서 로컬화와 분류 오류를 최소화한다는 걸 보여줘.

실험 결과는 상당한 성능 향상을 보여주며, mAP50이 80.34, F1-score가 77.87에 달해 기존 방법들보다 훨씬 뛰어난 결과를 냈어. 이 연구는 고급 손실 함수 개선이 도로 자산 탐지와 분할의 강건성과 정확성을 높이는 데 기여할 수 있음을 강조하고, 도로 안전과 인프라 관리에 도움이 되도록 해. 수학적 배경과 관련된 방법에 대한 자세한 논의는 이전 작업을 참고해봐.

================================================================================

URL:
https://arxiv.org/pdf/2409.09893.pdf

Title: Resolving Inconsistent Semantics in Multi-Dataset Image Segmentation

Original Abstract:
Leveraging multiple training datasets to scale up image segmentation models is beneficial for increasing robustness and semantic understanding. Individual datasets have well-defined ground truth with non-overlapping mask layouts and mutually exclusive semantics. However, merging them for multi-dataset training disrupts this harmony and leads to semantic inconsistencies; for example, the class "person" in one dataset and class "face" in another will require multilabel handling for certain pixels. Existing methods struggle with this setting, particularly when evaluated on label spaces mixed from the individual training sets. To overcome these issues, we introduce a simple yet effective multi-dataset training approach by integrating language-based embeddings of class names and label space-specific query embeddings. Our method maintains high performance regardless of the underlying inconsistencies between training datasets. Notably, on four benchmark datasets with label space inconsistencies during inference, we outperform previous methods by 1.6% mIoU for semantic segmentation, 9.1% PQ for panoptic segmentation, 12.1% AP for instance segmentation, and 3.0% in the newly proposed PIQ metric.

Translated Abstract:
이미지 세분화 모델을 확장하기 위해 여러 개의 훈련 데이터셋을 활용하는 것은 모델의 안정성과 의미 이해를 높이는 데 도움이 돼. 각 데이터셋은 겹치지 않는 마스크 레이아웃과 서로 다른 의미를 가진 명확한 기준을 가지고 있어. 하지만 이 데이터셋들을 합쳐서 다중 데이터셋 훈련을 하면 이런 조화가 깨지고 의미 불일치가 생겨. 예를 들어, 한 데이터셋에서 "사람" 클래스와 다른 데이터셋에서 "얼굴" 클래스가 있으면 특정 픽셀에 대해 다중 레이블 처리가 필요해.

기존 방법들은 이런 상황에서 잘 작동하지 않는데, 특히 개별 훈련 세트에서 혼합된 레이블 공간으로 평가할 때 더 그렇지. 이 문제를 해결하기 위해 우리는 클래스 이름의 언어 기반 임베딩과 레이블 공간에 특화된 쿼리 임베딩을 통합한 간단하면서도 효과적인 다중 데이터셋 훈련 방법을 제안해. 우리의 방법은 훈련 데이터셋 간의 불일치가 있더라도 높은 성능을 유지해.

특히, 추론 시 레이블 공간 불일치가 있는 네 개의 벤치마크 데이터셋에서 우리는 이전 방법보다 의미 세분화에서 1.6% mIoU, 판옵틱 세분화에서 9.1% PQ, 인스턴스 세분화에서 12.1% AP, 그리고 새로 제안한 PIQ 메트릭에서 3.0% 더 높은 성능을 기록했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09896.pdf

Title: GRIN: Zero-Shot Metric Depth with Pixel-Level Diffusion

Original Abstract:
3D reconstruction from a single image is a long-standing problem in computer vision. Learning-based methods address its inherent scale ambiguity by leveraging increasingly large labeled and unlabeled datasets, to produce geometric priors capable of generating accurate predictions across domains. As a result, state of the art approaches show impressive performance in zero-shot relative and metric depth estimation. Recently, diffusion models have exhibited remarkable scalability and generalizable properties in their learned representations. However, because these models repurpose tools originally designed for image generation, they can only operate on dense ground-truth, which is not available for most depth labels, especially in real-world settings. In this paper we present GRIN, an efficient diffusion model designed to ingest sparse unstructured training data. We use image features with 3D geometric positional encodings to condition the diffusion process both globally and locally, generating depth predictions at a pixel-level. With comprehensive experiments across eight indoor and outdoor datasets, we show that GRIN establishes a new state of the art in zero-shot metric monocular depth estimation even when trained from scratch.

Translated Abstract:
단일 이미지로 3D 재구성하는 건 컴퓨터 비전에서 오랫동안 해결되지 않은 문제야. 학습 기반 방법들은 큰 라벨이 붙은 데이터셋과 라벨이 없는 데이터셋을 활용해서 이 문제의 고유한 스케일 모호성을 해결하려고 해. 그 덕분에 다양한 분야에서 정확한 예측을 할 수 있는 기하학적 우선순위를 만들어내고 있어. 그래서 최신 기법들은 제로샷 상대 깊이와 메트릭 깊이 추정에서 좋은 성능을 보이고 있어.

최근에 확산 모델들이 그들의 학습된 표현에서 놀라운 확장성과 일반화 가능한 특성을 보여줬어. 하지만 이 모델들은 원래 이미지 생성을 위해 설계된 도구를 재활용하기 때문에, 밀집된 실제 깊이 정보가 필요해. 이 정보는 대부분의 깊이 라벨에선 구할 수 없고, 특히 현실 세계에서는 더더욱 그렇지. 

이 논문에서는 GRIN이라는 효율적인 확산 모델을 소개해. 이 모델은 드문 비구조적 학습 데이터를 사용할 수 있도록 설계됐어. 우리는 3D 기하학적 위치 부호화를 가진 이미지 특성을 사용해서 확산 과정을 전역적이고 지역적으로 조절해, 픽셀 수준에서 깊이 예측을 생성해. 여덟 개의 실내외 데이터셋에서 종합적인 실험을 통해, GRIN이 제로샷 메트릭 단안 깊이 추정에서 새로운 최첨단 성능을 확립했다는 걸 보여줘. 심지어 처음부터 학습했을 때도 말이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09907.pdf

Title: Rapid Adaptation of Earth Observation Foundation Models for Segmentation

Original Abstract:
This study investigates the efficacy of Low-Rank Adaptation (LoRA) in fine-tuning Earth Observation (EO) foundation models for flood segmentation. We hypothesize that LoRA, a parameter-efficient technique, can significantly accelerate the adaptation of large-scale EO models to this critical task while maintaining high performance. We apply LoRA to fine-tune a state-of-the-art EO foundation model pre-trained on diverse satellite imagery, using a curated dataset of flood events. Our results demonstrate that LoRA-based fine-tuning (r-256) improves F1 score by 6.66 points and IoU by 0.11 compared to a frozen encoder baseline, while significantly reducing computational costs. Notably, LoRA outperforms full fine-tuning, which proves computationally infeasible on our hardware. We further assess generalization through out-of-distribution (OOD) testing on a geographically distinct flood event. While LoRA configurations show improved OOD performance over the baseline. This work contributes to research on efficient adaptation of foundation models for specialized EO tasks, with implications for rapid response systems in disaster management. Our findings demonstrate LoRA's potential for enabling faster deployment of accurate flood segmentation models in resource-constrained, time-critical scenarios.

Translated Abstract:
이 연구는 저랭크 적응(Low-Rank Adaptation, LoRA)이 홍수 세분화를 위해 지구 관측(Earth Observation, EO) 기본 모델을 미세 조정하는 데 얼마나 효과적인지를 조사해. 우리는 LoRA가 파라미터 효율적인 기술이라서 대규모 EO 모델을 이 중요한 작업에 빠르게 적응시킬 수 있다고 생각해. 

우리는 다양한 위성 이미지를 기반으로 미리 훈련된 최신 EO 기본 모델에 LoRA를 적용해, 홍수 사건에 대한 정제된 데이터셋을 사용했어. 결과를 보니, LoRA 기반 미세 조정(r-256)이 고정된 인코더 기준선에 비해 F1 점수를 6.66점 높이고 IoU를 0.11 개선했어. 게다가 컴퓨팅 비용도 대폭 줄였지. 특히, LoRA는 전체 미세 조정보다 성능이 좋았고, 전체 미세 조정은 우리 하드웨어에서 실현하기 어려웠어.

우리는 또 다른 지역의 홍수 사건에 대해 분포 외(OOD) 테스트를 통해 일반화 능력을 평가했어. LoRA 설정이 기준선보다 OOD 성능이 개선된 것도 확인했어. 이 연구는 특수한 EO 작업을 위한 기본 모델의 효율적인 적응에 대한 연구에 기여하고, 재난 관리에서 신속 대응 시스템에 대한 함의가 있어. 

우리의 발견은 LoRA가 자원이 제한되고 시간이 중요한 상황에서 정확한 홍수 세분화 모델을 더 빠르게 배포할 수 있는 잠재력을 보여준다는 점이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09915.pdf

Title: Forearm Ultrasound based Gesture Recognition on Edge

Original Abstract:
Ultrasound imaging of the forearm has demonstrated significant potential for accurate hand gesture classification. Despite this progress, there has been limited focus on developing a stand-alone end- to-end gesture recognition system which makes it mobile, real-time and more user friendly. To bridge this gap, this paper explores the deployment of deep neural networks for forearm ultrasound-based hand gesture recognition on edge devices. Utilizing quantization techniques, we achieve substantial reductions in model size while maintaining high accuracy and low latency. Our best model, with Float16 quantization, achieves a test accuracy of 92% and an inference time of 0.31 seconds on a Raspberry Pi. These results demonstrate the feasibility of efficient, real-time gesture recognition on resource-limited edge devices, paving the way for wearable ultrasound-based systems.

Translated Abstract:
팔꿈치의 초음파 이미징은 손 제스처 분류에 큰 가능성을 보여줬어. 하지만 독립적인 엔드-투-엔드 제스처 인식 시스템 개발에는 아직 많이 신경 쓰이지 않았어. 이 시스템은 이동 가능하고 실시간으로 사용할 수 있으며, 더 사용자 친화적이야. 

이 논문은 엣지 디바이스에서 팔꿈치 초음파 기반 손 제스처 인식을 위해 딥 뉴럴 네트워크를 적용하는 방법을 다뤄. 양자화 기법을 사용해서 모델 크기를 크게 줄이면서도 높은 정확도와 낮은 지연 시간을 유지했어. 

우리의 최상의 모델은 Float16 양자화를 통해 Raspberry Pi에서 92%의 테스트 정확도와 0.31초의 추론 시간을 기록했어. 이 결과는 자원이 제한된 엣지 디바이스에서 효율적이고 실시간으로 제스처 인식이 가능하다는 걸 보여주고, 착용 가능한 초음파 기반 시스템의 길을 열어주고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09953.pdf

Title: Uncertainty-Guided Appearance-Motion Association Network for Out-of-Distribution Action Detection

Original Abstract:
Out-of-distribution (OOD) detection targets to detect and reject test samples with semantic shifts, to prevent models trained on in-distribution (ID) dataset from producing unreliable predictions. Existing works only extract the appearance features on image datasets, and cannot handle dynamic multimedia scenarios with much motion information. Therefore, we target a more realistic and challenging OOD detection task: OOD action detection (ODAD). Given an untrimmed video, ODAD first classifies the ID actions and recognizes the OOD actions, and then localizes ID and OOD actions. To this end, in this paper, we propose a novel Uncertainty-Guided Appearance-Motion Association Network (UAAN), which explores both appearance features and motion contexts to reason spatial-temporal inter-object interaction for ODAD.Firstly, we design separate appearance and motion branches to extract corresponding appearance-oriented and motion-aspect object representations. In each branch, we construct a spatial-temporal graph to reason appearance-guided and motion-driven inter-object interaction. Then, we design an appearance-motion attention module to fuse the appearance and motion features for final action detection. Experimental results on two challenging datasets show that UAAN beats state-of-the-art methods by a significant margin, illustrating its effectiveness.

Translated Abstract:
Out-of-distribution (OOD) 탐지는 의미가 바뀐 테스트 샘플을 찾아내고 거부하는 걸 목표로 해. 이렇게 해서 in-distribution (ID) 데이터셋으로 훈련된 모델들이 믿을 수 없는 예측을 하지 않도록 방지할 수 있어. 기존 연구들은 이미지 데이터셋에서의 외형 특징만 뽑아내고, 많은 움직임 정보가 있는 동적 멀티미디어 상황은 다룰 수 없었어.

그래서 우리는 더 현실적이고 도전적인 OOD 탐지 작업, 즉 OOD 행동 탐지(ODAD)를 목표로 해. 잘라내지 않은 비디오가 주어졌을 때, ODAD는 먼저 ID 행동을 분류하고 OOD 행동을 인식한 다음, ID와 OOD 행동을 지역화해. 이를 위해 우리는 새로운 불확실성 기반 외형-움직임 연결 네트워크(UAAN)를 제안해. UAAN은 외형 특징과 움직임 맥락을 모두 탐색해서 ODAD를 위한 공간-시간 상호작용을 이해해.

먼저, 우리는 외형과 움직임을 각각 따로 추출하는 가지를 설계했어. 각 가지에서는 외형 유도와 움직임 기반의 객체 간 상호작용을 이해하기 위해 공간-시간 그래프를 만들어. 그리고 나서, 외형과 움직임 특징을 결합해서 최종 행동 탐지를 위한 외형-움직임 주의 모듈을 설계했어. 두 개의 도전적인 데이터셋에서 실험한 결과, UAAN이 최신 방법들보다 상당히 높은 성능을 보여서 그 효과성을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09968.pdf

Title: Artificial Intelligence-Based Opportunistic Coronary Calcium Screening in the Veterans Affairs National Healthcare System

Original Abstract:
Coronary artery calcium (CAC) is highly predictive of cardiovascular events. While millions of chest CT scans are performed annually in the United States, CAC is not routinely quantified from scans done for non-cardiac purposes. A deep learning algorithm was developed using 446 expert segmentations to automatically quantify CAC on non-contrast, non-gated CT scans (AI-CAC). Our study differs from prior works as we leverage imaging data across the Veterans Affairs national healthcare system, from 98 medical centers, capturing extensive heterogeneity in imaging protocols, scanners, and patients. AI-CAC performance on non-gated scans was compared against clinical standard ECG-gated CAC scoring. Non-gated AI-CAC differentiated zero vs. non-zero and less than 100 vs. 100 or greater Agatston scores with accuracies of 89.4% (F1 0.93) and 87.3% (F1 0.89), respectively, in 795 patients with paired gated scans within a year of a non-gated CT scan. Non-gated AI-CAC was predictive of 10-year all-cause mortality (CAC 0 vs. >400 group: 25.4% vs. 60.2%, Cox HR 3.49, p < 0.005), and composite first-time stroke, MI, or death (CAC 0 vs. >400 group: 33.5% vs. 63.8%, Cox HR 3.00, p < 0.005). In a screening dataset of 8,052 patients with low-dose lung cancer-screening CTs (LDCT), 3,091/8,052 (38.4%) individuals had AI-CAC >400. Four cardiologists qualitatively reviewed LDCT images from a random sample of >400 AI-CAC patients and verified that 527/531 (99.2%) would benefit from lipid-lowering therapy. To the best of our knowledge, this is the first non-gated CT CAC algorithm developed across a national healthcare system, on multiple imaging protocols, without filtering intra-cardiac hardware, and compared against a strong gated CT reference. We report superior performance relative to previous CAC algorithms evaluated against paired gated scans that included patients with intra-cardiac hardware.

Translated Abstract:
관상동맥 칼슘(CAC)은 심혈관 사건을 예측하는 데 매우 중요한 지표야. 미국에서는 매년 수백만 건의 흉부 CT 스캔이 이루어지는데, 비심장 목적의 스캔에서 CAC를 정기적으로 측정하지는 않아. 그래서 우리는 446개의 전문가 분할 데이터를 이용해 비대조, 비게이트 CT 스캔에서 CAC를 자동으로 정량화하는 딥러닝 알고리즘(AI-CAC)을 개발했어.

이 연구는 이전 연구들과 다르게, 98개의 의료 센터에서 수집된 데이터를 활용해 다양한 촬영 프로토콜, 스캐너, 환자들의 이질성을 포괄하고 있어. 비게이트 스캔에서 AI-CAC의 성능을 임상 표준인 ECG 게이트 CAC 스코어와 비교했어. 비게이트 AI-CAC는 0과 비0, 그리고 100 미만과 100 이상 Agatston 점수를 각각 89.4%(F1 0.93)와 87.3%(F1 0.89)의 정확도로 구분했어. 

비게이트 AI-CAC는 10년 전체 사망률 예측에서도 유의미한 결과를 보였어 (CAC 0 vs. >400 그룹: 25.4% vs. 60.2%, Cox HR 3.49, p < 0.005) 그리고 첫 번째 뇌졸중, 심근경색, 또는 사망을 예측했어 (CAC 0 vs. >400 그룹: 33.5% vs. 63.8%, Cox HR 3.00, p < 0.005). 

8,052명의 저선량 폐암 스크리닝 CT( LDCT) 환자 중 3,091명(38.4%)이 AI-CAC가 >400으로 나타났어. 4명의 심장 전문의가 랜덤 샘플로 AI-CAC가 >400인 환자들의 LDCT 이미지를 검토했는데, 531명 중 527명(99.2%)이 지질 저하 요법의 혜택을 받을 것이라고 확인했어.

우리 연구는 국가 의료 시스템에서 여러 촬영 프로토콜을 기반으로 개발된 첫 비게이트 CT CAC 알고리즘으로, 심장 내부 하드웨어를 필터링하지 않고 강력한 게이트 CT 기준과 비교했어. 이전의 CAC 알고리즘보다 성능이 우수한 결과를 보고해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09969.pdf

Title: 2S-ODIS: Two-Stage Omni-Directional Image Synthesis by Geometric Distortion Correction

Original Abstract:
Omni-directional images have been increasingly used in various applications, including virtual reality and SNS (Social Networking Services). However, their availability is comparatively limited in contrast to normal field of view (NFoV) images, since specialized cameras are required to take omni-directional images. Consequently, several methods have been proposed based on generative adversarial networks (GAN) to synthesize omni-directional images, but these approaches have shown difficulties in training of the models, due to instability and/or significant time consumption in the training. To address these problems, this paper proposes a novel omni-directional image synthesis method, 2S-ODIS (Two-Stage Omni-Directional Image Synthesis), which generated high-quality omni-directional images but drastically reduced the training time. This was realized by utilizing the VQGAN (Vector Quantized GAN) model pre-trained on a large-scale NFoV image database such as ImageNet without fine-tuning. Since this pre-trained model does not represent distortions of omni-directional images in the equi-rectangular projection (ERP), it cannot be applied directly to the omni-directional image synthesis in ERP. Therefore, two-stage structure was adopted to first create a global coarse image in ERP and then refine the image by integrating multiple local NFoV images in the higher resolution to compensate the distortions in ERP, both of which are based on the pre-trained VQGAN model. As a result, the proposed method, 2S-ODIS, achieved the reduction of the training time from 14 days in OmniDreamer to four days in higher image quality.

Translated Abstract:
오므니 방향 이미지가 가상 현실과 SNS(소셜 네트워킹 서비스) 같은 다양한 분야에서 점점 더 많이 사용되고 있어. 하지만 오므니 방향 이미지를 찍으려면 특별한 카메라가 필요해서 일반 시야(NFoV) 이미지에 비해 사용 가능성이 많이 제한돼. 그래서 생성적 적대 신경망(GAN)을 이용해서 오므니 방향 이미지를 합성하는 방법들이 여러 개 제안됐지만, 모델 훈련이 불안정하거나 시간이 많이 걸려서 어려움을 겪고 있어.

이 문제를 해결하기 위해, 이 논문에서는 새로운 오므니 방향 이미지 합성 방법인 2S-ODIS(2단계 오므니 방향 이미지 합성)를 제안해. 이 방법은 훈련 시간을 대폭 줄이면서도 고품질의 오므니 방향 이미지를 만들어낼 수 있어. 이건 ImageNet 같은 대규모 NFoV 이미지 데이터베이스에서 미세 조정 없이 미리 훈련된 VQGAN(벡터 양자화 GAN) 모델을 활용해서 가능했어.

근데 이 미리 훈련된 모델은 등각 직사각형 투영(ERP)에서 오므니 방향 이미지의 왜곡을 잘 표현하지 못해서, ERP에서 오므니 방향 이미지 합성에 직접 적용할 수는 없어. 그래서 먼저 ERP에서 전반적인 거친 이미지를 만들고, 그 다음에 여러 개의 고해상도 NFoV 이미지를 통합해서 이미지의 왜곡을 보완하는 2단계 구조를 채택했어. 이 두 단계 모두 미리 훈련된 VQGAN 모델을 기반으로 해.

결과적으로, 제안된 2S-ODIS 방법은 OmniDreamer에서 14일 걸리던 훈련 시간을 4일로 줄이면서 더 높은 이미지 품질을 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10021.pdf

Title: LithoHoD: A Litho Simulator-Powered Framework for IC Layout Hotspot Detection

Original Abstract:
Recent advances in VLSI fabrication technology have led to die shrinkage and increased layout density, creating an urgent demand for advanced hotspot detection techniques. However, by taking an object detection network as the backbone, recent learning-based hotspot detectors learn to recognize only the problematic layout patterns in the training data. This fact makes these hotspot detectors difficult to generalize to real-world scenarios. We propose a novel lithography simulator-powered hotspot detection framework to overcome this difficulty. Our framework integrates a lithography simulator with an object detection backbone, merging the extracted latent features from both the simulator and the object detector via well-designed cross-attention blocks. Consequently, the proposed framework can be used to detect potential hotspot regions based on I) the variation of possible circuit shape deformation estimated by the lithography simulator, and ii) the problematic layout patterns already known. To this end, we utilize RetinaNet with a feature pyramid network as the object detection backbone and leverage LithoNet as the lithography simulator. Extensive experiments demonstrate that our proposed simulator-guided hotspot detection framework outperforms previous state-of-the-art methods on real-world data.

Translated Abstract:
최근 VLSI 제작 기술의 발전 덕분에 반도체 칩이 더 작아지고 레이아웃 밀도가 증가하면서, 고급 핫스팟 탐지 기술에 대한 수요가 급증하고 있어. 하지만, 기존의 학습 기반 핫스팟 탐지기들은 객체 탐지 네트워크를 사용해서 훈련 데이터에서 문제 있는 레이아웃 패턴만 인식하도록 학습해. 이 때문에 실제 상황에서는 잘 일반화되지 않는 문제가 있어.

우리는 이 문제를 해결하기 위해 새로운 리소그래피 시뮬레이터 기반의 핫스팟 탐지 프레임워크를 제안해. 이 프레임워크는 리소그래피 시뮬레이터와 객체 탐지 네트워크를 통합해서, 두 시스템에서 추출된 특징을 잘 설계된 크로스 어텐션 블록을 통해 결합해. 그래서 이 프레임워크는 I) 리소그래피 시뮬레이터가 추정한 회로 형태 변형의 변화를 바탕으로 핫스팟 지역을 탐지할 수 있고, II) 이미 알려진 문제 있는 레이아웃 패턴을 기반으로도 탐지가 가능해.

이런 목적을 위해 우리는 객체 탐지 네트워크로 RetinaNet과 피처 피라미드 네트워크를 사용하고, 리소그래피 시뮬레이터로 LithoNet을 활용했어. 다양한 실험 결과, 우리가 제안한 시뮬레이터 기반의 핫스팟 탐지 프레임워크가 실제 데이터에서 이전의 최신 방법들보다 더 뛰어난 성능을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10028.pdf

Title: AttnMod: Attention-Based New Art Styles

Original Abstract:
Imagine a human artist looking at the generated photo of a diffusion model, and hoping to create a painting out of it. There could be some feature of the object in the photo that the artist wants to emphasize, some color to disperse, some silhouette to twist, or some part of the scene to be materialized. These intentions can be viewed as the modification of the cross attention from the text prompt onto UNet, during the desoising diffusion. This work presents AttnMod, to modify attention for creating new unpromptable art styles out of existing diffusion models. The style-creating behavior is studied across different setups.

Translated Abstract:
한 인간 예술가가 확산 모델로 생성된 사진을 보고 그걸 바탕으로 그림을 그리려고 하는 상황을 상상해봐. 사진 속 물체의 어떤 특징을 강조하고 싶거나, 색상을 퍼뜨리거나, 실루엣을 변형시키거나, 장면의 어떤 부분을 구체화하고 싶을 수도 있어. 이런 의도는 텍스트 프롬프트에서 UNet으로 가는 크로스 어텐션의 수정으로 볼 수 있어. 

이 연구는 AttnMod를 소개해, 기존의 확산 모델을 기반으로 새롭고 독창적인 예술 스타일을 만들기 위해 어텐션을 수정하는 방법을 다루고 있어. 스타일 생성 행동은 다양한 설정에서 연구됐어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10041.pdf

Title: DENSER: 3D Gaussians Splatting for Scene Reconstruction of Dynamic Urban Environments

Original Abstract:
This paper presents DENSER, an efficient and effective approach leveraging 3D Gaussian splatting (3DGS) for the reconstruction of dynamic urban environments. While several methods for photorealistic scene representations, both implicitly using neural radiance fields (NeRF) and explicitly using 3DGS have shown promising results in scene reconstruction of relatively complex dynamic scenes, modeling the dynamic appearance of foreground objects tend to be challenging, limiting the applicability of these methods to capture subtleties and details of the scenes, especially far dynamic objects. To this end, we propose DENSER, a framework that significantly enhances the representation of dynamic objects and accurately models the appearance of dynamic objects in the driving scene. Instead of directly using Spherical Harmonics (SH) to model the appearance of dynamic objects, we introduce and integrate a new method aiming at dynamically estimating SH bases using wavelets, resulting in better representation of dynamic objects appearance in both space and time. Besides object appearance, DENSER enhances object shape representation through densification of its point cloud across multiple scene frames, resulting in faster convergence of model training. Extensive evaluations on KITTI dataset show that the proposed approach significantly outperforms state-of-the-art methods by a wide margin. Source codes and models will be uploaded to this repository this https URL

Translated Abstract:
이 논문에서는 DENSER라는 효율적이고 효과적인 방법을 소개해. 이 방법은 3D 가우시안 스플래팅(3DGS)을 활용해서 동적인 도시 환경을 재구성하는 거야. 

사진처럼 사실적인 장면 표현을 위한 여러 방법들이 있는데, 신경 방사 필드(NeRF)를 사용한 방법과 3DGS를 사용한 방법이 상대적으로 복잡한 동적 장면을 재구성하는 데 좋은 결과를 보여줬어. 하지만 전경 물체의 동적인 모습을 모델링하는 건 여전히 어려워서, 이런 방법들이 장면의 미세한 부분이나 세부사항을 포착하는 데 한계가 있어, 특히 멀리 있는 동적 물체를 다루는 데는 더 그렇고.

그래서 우리는 DENSER라는 프레임워크를 제안해. 이 프레임워크는 동적 물체의 표현을 크게 향상시키고, 운전 장면에서 동적 물체의 모습을 정확하게 모델링할 수 있도록 해. 동적 물체의 모습을 모델링할 때 구형 조화 함수(SH)를 직접 사용하는 대신, 웨이브렛을 사용해서 SH 기초를 동적으로 추정하는 새로운 방법을 도입하고 통합했어. 이 덕분에 동적 물체의 모습을 공간과 시간에서 더 잘 표현할 수 있게 됐어.

물체의 모습뿐만 아니라, DENSER는 여러 장면 프레임에 걸쳐 포인트 클라우드의 밀도를 높여서 물체의 형태 표현도 개선해. 이렇게 하면 모델 훈련이 더 빨리 수렴하게 돼. KITTI 데이터셋에 대한 광범위한 평가 결과, 제안한 방법이 최신 기술보다 훨씬 더 우수한 성능을 보였어. 소스 코드와 모델은 이 저장소에 업로드될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10063.pdf

Title: GlobalMapNet: An Online Framework for Vectorized Global HD Map Construction

Original Abstract:
High-definition (HD) maps are essential for autonomous driving systems. Traditionally, an expensive and labor-intensive pipeline is implemented to construct HD maps, which is limited in scalability. In recent years, crowdsourcing and online mapping have emerged as two alternative methods, but they have limitations respectively. In this paper, we provide a novel methodology, namely global map construction, to perform direct generation of vectorized global maps, combining the benefits of crowdsourcing and online mapping. We introduce GlobalMapNet, the first online framework for vectorized global HD map construction, which updates and utilizes a global map on the ego vehicle. To generate the global map from scratch, we propose GlobalMapBuilder to match and merge local maps continuously. We design a new algorithm, Map NMS, to remove duplicate map elements and produce a clean map. We also propose GlobalMapFusion to aggregate historical map information, improving consistency of prediction. We examine GlobalMapNet on two widely recognized datasets, Argoverse2 and nuScenes, showing that our framework is capable of generating globally consistent results.

Translated Abstract:
고해상도(HD) 지도는 자율주행 시스템에 필수적이야. 전통적으로 HD 지도를 만드는 과정은 비싸고 많은 시간과 노력이 들기 때문에 확장성이 제한적이었어. 최근 몇 년 동안, 크라우드소싱과 온라인 매핑이 두 가지 대안 방법으로 떠올랐지만 각각 한계가 있어. 

이 논문에서는 크라우드소싱과 온라인 매핑의 장점을 결합한 새로운 방법론인 글로벌 지도 구축을 제안해. 우리는 GlobalMapNet을 소개하는데, 이건 벡터화된 글로벌 HD 지도를 만드는 첫 번째 온라인 프레임워크야. 이 프레임워크는 자차에 있는 글로벌 지도를 업데이트하고 활용해. 

글로벌 지도를 처음부터 생성하기 위해, 우리는 GlobalMapBuilder를 제안해. 이건 지역 지도를 지속적으로 맞추고 병합하는 역할을 해. 또, Map NMS라는 새로운 알고리즘을 설계해서 중복된 지도 요소를 제거하고 깔끔한 지도를 만들어. 마지막으로, GlobalMapFusion을 통해 역사적인 지도 정보를 집계해 예측의 일관성을 높여. 

우리는 GlobalMapNet을 두 가지 잘 알려진 데이터셋인 Argoverse2와 nuScenes에서 실험해봤고, 이 프레임워크가 전 세계적으로 일관된 결과를 생성할 수 있다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10071.pdf

Title: Towards Physically-Realizable Adversarial Attacks in Embodied Vision Navigation

Original Abstract:
The deployment of embodied navigation agents in safety-critical environments raises concerns about their vulnerability to adversarial attacks on deep neural networks. However, current attack methods often lack practicality due to challenges in transitioning from the digital to the physical world, while existing physical attacks for object detection fail to achieve both multi-view effectiveness and naturalness. To address this, we propose a practical attack method for embodied navigation by attaching adversarial patches with learnable textures and opacity to objects. Specifically, to ensure effectiveness across varying viewpoints, we employ a multi-view optimization strategy based on object-aware sampling, which uses feedback from the navigation model to optimize the patch's texture. To make the patch inconspicuous to human observers, we introduce a two-stage opacity optimization mechanism, where opacity is refined after texture optimization. Experimental results show our adversarial patches reduce navigation success rates by about 40%, outperforming previous methods in practicality, effectiveness, and naturalness. Code is available at: [this https URL].

Translated Abstract:
안전이 중요한 환경에서 로봇 내비게이션 에이전트를 사용할 때, 딥 뉴럴 네트워크에 대한 적대적 공격에 취약하다는 우려가 있어. 하지만 현재의 공격 방법들은 디지털 세계에서 물리적 세계로 넘어가는 데 어려움이 있어서 실용성이 떨어져. 게다가 기존의 물체 탐지용 물리적 공격들은 여러 시점에서 효과적이거나 자연스럽지 못해.

그래서 우리는 물체에 학습 가능한 질감과 불투명도를 가진 적대적 패치를 붙이는 실용적인 공격 방법을 제안해. 특히, 다양한 시점에서도 효과적이도록 물체 인식 샘플링을 기반으로 한 다중 시점 최적화 전략을 사용해서 내비게이션 모델의 피드백을 통해 패치의 질감을 최적화해. 그리고 이 패치가 사람에게 눈에 띄지 않도록 하는 두 단계의 불투명도 최적화 메커니즘을 도입했어. 이 과정에서 질감 최적화 후에 불투명도를 조정해.

실험 결과, 우리의 적대적 패치가 내비게이션 성공률을 약 40% 줄이는 데 성공했어. 이전 방법들보다 실용성, 효과성, 자연스러움에서 더 나은 결과를 보여줬고, 코드도 [이 링크]에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10080.pdf

Title: DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality Image Fusion

Original Abstract:
Multi-modality image fusion aims to integrate complementary data information from different imaging modalities into a single image. Existing methods often generate either blurry fused images that lose fine-grained semantic information or unnatural fused images that appear perceptually cropped from the inputs. In this work, we propose a novel two-phase discriminative autoencoder framework, termed DAE-Fuse, that generates sharp and natural fused images. In the adversarial feature extraction phase, we introduce two discriminative blocks into the encoder-decoder architecture, providing an additional adversarial loss to better guide feature extraction by reconstructing the source images. While the two discriminative blocks are adapted in the attention-guided cross-modality fusion phase to distinguish the structural differences between the fused output and the source inputs, injecting more naturalness into the results. Extensive experiments on public infrared-visible, medical image fusion, and downstream object detection datasets demonstrate our method's superiority and generalizability in both quantitative and qualitative evaluations.

Translated Abstract:
멀티모달 이미지 융합은 서로 다른 이미징 모달리티에서 보완적인 데이터 정보를 하나의 이미지로 통합하는 걸 목표로 해. 기존 방법들은 종종 세밀한 의미 정보를 잃어버린 흐릿한 융합 이미지를 만들거나, 입력 이미지에서 잘린 것처럼 보이는 부자연스러운 융합 이미지를 만들어.

이 연구에서는 DAE-Fuse라는 새로운 두 단계의 차별화된 오토인코더 프레임워크를 제안해. 이 방법은 선명하고 자연스러운 융합 이미지를 생성해. 첫 번째 단계인 적대적 특징 추출 단계에서는 인코더-디코더 구조에 두 개의 차별화된 블록을 추가해. 이 블록들은 원본 이미지를 재구성함으로써 특징 추출을 더 잘 안내하는 추가적인 적대적 손실을 제공해.

두 번째 단계인 주의 기반 크로스 모달리티 융합에서는 두 개의 차별화된 블록을 조정해서 융합된 출력과 원본 입력 간의 구조적 차이를 구별하게 해. 이렇게 해서 결과에 더 자연스러움을 주지. 

공공 적외선-가시광선, 의료 이미지 융합, 그리고 하위 객체 탐지 데이터셋에서의 광범위한 실험 결과는 우리의 방법이 정량적, 정성적 평가 모두에서 우수하고 일반화 가능하다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.10090.pdf

Title: MotionCom: Automatic and Motion-Aware Image Composition with LLM and Video Diffusion Prior

Original Abstract:
This work presents MotionCom, a training-free motion-aware diffusion based image composition, enabling automatic and seamless integration of target objects into new scenes with dynamically coherent results without finetuning or optimization. Traditional approaches in this area suffer from two significant limitations: they require manual planning for object placement and often generate static compositions lacking motion realism. MotionCom addresses these issues by utilizing a Large Vision Language Model (LVLM) for intelligent planning, and a Video Diffusion prior for motion-infused image synthesis, streamlining the composition process. Our multi-modal Chain-of-Thought (CoT) prompting with LVLM automates the strategic placement planning of foreground objects, considering their potential motion and interaction within the scenes. Complementing this, we propose a novel method MotionPaint to distill motion-aware information from pretrained video diffusion models in the generation phase, ensuring that these objects are not only seamlessly integrated but also endowed with realistic motion. Extensive quantitative and qualitative results highlight MotionCom's superiority, showcasing its efficiency in streamlining the planning process and its capability to produce compositions that authentically depict motion and interaction.

Translated Abstract:
이 연구는 MotionCom이라는 새로운 방법을 소개해. 이건 훈련 없이도 움직임을 인식하는 확산 기반 이미지 합성을 가능하게 해주고, 목표 물체들을 새로운 장면에 자동으로 통합할 수 있게 해줘. 게다가 결과가 동적으로 일관성이 있어서 추가적인 조정이나 최적화가 필요 없어.

전통적인 방법들은 두 가지 큰 한계가 있어. 첫째, 물체 배치를 위해 수동으로 계획해야 하고, 둘째, 종종 정적인 구성을 만들어서 움직임이 사실적이지 않아. MotionCom은 이러한 문제를 해결하기 위해 대형 비전 언어 모델(LVLM)을 활용해 똑똑한 계획을 하고, 비디오 확산 모델을 사용해 움직임이 포함된 이미지를 생성해. 이로 인해 합성 과정이 간소화돼.

우리는 LVLM을 이용해 다중 모달 체인 오브 생각(CoT) 프롬프트를 사용해서 전경 물체들의 전략적 배치를 자동화해. 이 과정에서 물체들이 장면 내에서 어떤 움직임과 상호작용을 할지를 고려해. 여기에 더해서, 우리는 MotionPaint라는 새로운 방법을 제안해. 이 방법은 사전 훈련된 비디오 확산 모델에서 움직임 인식 정보를 추출해서 생성 단계에서 활용해. 이렇게 하면 물체들이 매끄럽게 통합될 뿐만 아니라 사실적인 움직임도 가질 수 있어.

다양한 정량적 및 정성적 결과가 MotionCom의 우수성을 보여주고 있어. 계획 과정을 간소화하는 효율성과 움직임과 상호작용을 진정으로 묘사하는 구성을 만들어내는 능력이 드러나고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10094.pdf

Title: DDoS: Diffusion Distribution Similarity for Out-of-Distribution Detection

Original Abstract:
Out-of-Distribution (OoD) detection determines whether the given samples are from the training distribution of the classifier-under-protection, i.e., the In-Distribution (InD), or from a different OoD. Latest researches introduce diffusion models pre-trained on InD data to advocate OoD detection by transferring an OoD image into a generated one that is close to InD, so that one could capture the distribution disparities between original and generated images to detect OoD data. Existing diffusion-based detectors adopt perceptual metrics on the two images to measure such disparities, but ignore a fundamental fact: Perceptual metrics are devised essentially for human-perceived similarities of low-level image patterns, e.g., textures and colors, and are not advisable in evaluating distribution disparities, since images with different low-level patterns could possibly come from the same distribution. To address this issue, we formulate a diffusion-based detection framework that considers the distribution similarity between a tested image and its generated counterpart via a novel proper similarity metric in the informative feature space and probability space learned by the classifier-under-protection. An anomaly-removal strategy is further presented to enlarge such distribution disparities by removing abnormal OoD information in the feature space to facilitate the detection. Extensive empirical results unveil the insufficiency of perceptual metrics and the effectiveness of our distribution similarity framework with new state-of-the-art detection performance.

Translated Abstract:
Out-of-Distribution (OoD) 탐지는 주어진 샘플이 보호받는 분류기의 훈련 분포에서 왔는지, 즉 In-Distribution (InD)인지 아니면 다른 OoD에서 왔는지를 판단하는 과정이야. 최신 연구들은 InD 데이터로 사전 훈련된 확산 모델을 사용해서 OoD 탐지를 개선하려고 해. 이 모델은 OoD 이미지를 InD에 가까운 생성된 이미지로 변환하는데, 이렇게 하면 원본 이미지와 생성된 이미지 사이의 분포 차이를 포착해서 OoD 데이터를 탐지할 수 있어.

기존의 확산 기반 탐지기는 두 이미지 간의 차이를 측정하기 위해 지각적 메트릭을 사용하지만, 이건 근본적인 사실을 무시하고 있어. 지각적 메트릭은 기본적으로 인간이 인지하는 저수준 이미지 패턴, 예를 들어 질감이나 색상에 대한 유사성을 평가하는 데 만들어졌고, 분포 차이를 평가하는 데는 적합하지 않아. 왜냐하면 서로 다른 저수준 패턴을 가진 이미지들이 같은 분포에서 올 수도 있기 때문이야.

이 문제를 해결하기 위해, 우리는 테스트 이미지와 생성된 이미지 간의 분포 유사성을 고려하는 확산 기반 탐지 프레임워크를 만들었어. 이 프레임워크는 보호받는 분류기가 학습한 정보 특성 공간과 확률 공간에서 새로운 적절한 유사성 메트릭을 사용해. 그리고 탐지를 쉽게 하기 위해 특성 공간에서 비정상적인 OoD 정보를 제거하는 이상치 제거 전략도 제시했어. 

많은 실험 결과는 지각적 메트릭의 한계를 드러내고, 우리의 분포 유사성 프레임워크가 새로운 최첨단 탐지 성능을 보여준다는 걸 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10095.pdf

Title: Human Insights Driven Latent Space for Different Driving Perspectives: A Unified Encoder for Efficient Multi-Task Inference

Original Abstract:
Autonomous driving holds great potential to transform road safety and traffic efficiency by minimizing human error and reducing congestion. A key challenge in realizing this potential is the accurate estimation of steering angles, which is essential for effective vehicle navigation and control. Recent breakthroughs in deep learning have made it possible to estimate steering angles directly from raw camera inputs. However, the limited available navigation data can hinder optimal feature learning, impacting the system's performance in complex driving scenarios. In this paper, we propose a shared encoder trained on multiple computer vision tasks critical for urban navigation, such as depth, pose, and 3D scene flow estimation, as well as semantic, instance, panoptic, and motion segmentation. By incorporating diverse visual information used by humans during navigation, this unified encoder might enhance steering angle estimation. To achieve effective multi-task learning within a single encoder, we introduce a multi-scale feature network for pose estimation to improve depth learning. Additionally, we employ knowledge distillation from a multi-backbone model pretrained on these navigation tasks to stabilize training and boost performance. Our findings demonstrate that a shared backbone trained on diverse visual tasks is capable of providing overall perception capabilities. While our performance in steering angle estimation is comparable to existing methods, the integration of human-like perception through multi-task learning holds significant potential for advancing autonomous driving systems. More details and the pretrained model are available at this https URL.

Translated Abstract:
자율주행은 인간의 실수를 줄이고 교통 혼잡을 완화함으로써 도로 안전과 교통 효율성을 크게 개선할 수 있는 가능성이 있어. 하지만 이 가능성을 실현하려면 정확한 조향 각도를 추정하는 게 중요한데, 이는 차량 내비게이션과 제어에 필수적이야. 최근 딥러닝의 발전 덕분에 원시 카메라 입력으로부터 직접 조향 각도를 추정할 수 있게 되었어. 하지만 내비게이션 데이터가 제한적이면 최적의 특징 학습이 어려워지고, 복잡한 주행 상황에서 시스템 성능에 영향을 줄 수 있어.

이 논문에서는 도시 내비게이션에 중요한 여러 컴퓨터 비전 작업, 예를 들어 깊이, 자세, 3D 장면 흐름 추정, 그리고 의미, 인스턴스, 팬옵틱, 모션 분할 등을 위해 훈련된 공유 인코더를 제안해. 사람의 내비게이션 시 사용되는 다양한 시각 정보를 통합함으로써 이 통합 인코더가 조향 각도 추정을 향상시킬 수 있을 거야. 하나의 인코더 내에서 효과적인 다중 작업 학습을 달성하기 위해, 깊이 학습을 개선하기 위한 자세 추정을 위한 다중 스케일 특징 네트워크를 도입했어. 또한, 이러한 내비게이션 작업에 대해 사전 훈련된 다중 백본 모델로부터 지식 증류를 사용하여 훈련을 안정화하고 성능을 높였어.

우리의 연구 결과는 다양한 시각 작업에 대해 훈련된 공유 백본이 전체적인 인식 능력을 제공할 수 있음을 보여줘. 조향 각도 추정 성능은 기존 방법과 비슷하지만, 다중 작업 학습을 통한 인간 같은 인식 통합이 자율주행 시스템을 발전시킬 큰 잠재력을 가지고 있어. 더 많은 세부사항과 사전 훈련된 모델은 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10101.pdf

Title: Adaptive Segmentation-Based Initialization for Steered Mixture of Experts Image Regression

Original Abstract:
Kernel image regression methods have shown to provide excellent efficiency in many image processing task, such as image and light-field compression, Gaussian Splatting, denoising and super-resolution. The estimation of parameters for these methods frequently employ gradient descent iterative optimization, which poses significant computational burden for many applications. In this paper, we introduce a novel adaptive segmentation-based initialization method targeted for optimizing Steered-Mixture-of Experts (SMoE) gating networks and Radial-Basis-Function (RBF) networks with steering kernels. The novel initialization method allocates kernels into pre-calculated image segments. The optimal number of kernels, kernel positions, and steering parameters are derived per segment in an iterative optimization and kernel sparsification procedure. The kernel information from "local" segments is then transferred into a "global" initialization, ready for use in iterative optimization of SMoE, RBF, and related kernel image regression methods. Results show that drastic objective and subjective quality improvements are achievable compared to widely used regular grid initialization, "state-of-the-art" K-Means initialization and previously introduced segmentation-based initialization methods, while also drastically improving the sparsity of the regression models. For same quality, the novel initialization results in models with around 50% reduction of kernels. In addition, a significant reduction of convergence time is achieved, with overall run-time savings of up to 50%. The segmentation-based initialization strategy itself admits heavy parallel computation; in theory, it may be divided into as many tasks as there are segments in the images. By accessing only four parallel GPUs, run-time savings of already 50% for initialization are achievable.

Translated Abstract:
커널 이미지 회귀 방법들은 이미지 처리 작업에서 매우 뛰어난 효율성을 보여줬어. 예를 들어 이미지와 라이트 필드 압축, 가우시안 스플래팅, 노이즈 제거, 초해상도 같은 작업들이야. 이런 방법들의 파라미터 추정은 보통 경량화된 최적화를 위해 경량 경량 하강법을 사용하는데, 이는 많은 애플리케이션에서 계산 부담이 커.

이 논문에서는 Steered-Mixture-of Experts (SMoE) 게이팅 네트워크와 Radial-Basis-Function (RBF) 네트워크를 최적화하기 위한 새로운 적응형 세분화 기반 초기화 방법을 소개해. 이 새로운 초기화 방법은 미리 계산된 이미지 세그먼트에 커널을 배정해. 최적의 커널 수, 커널 위치, 스티어링 파라미터는 각 세그먼트마다 반복 최적화와 커널 희소화 과정을 통해 도출돼. 그리고 "로컬" 세그먼트에서 얻은 커널 정보는 "글로벌" 초기화로 전이돼, SMoE, RBF, 그리고 관련된 커널 이미지 회귀 방법의 반복 최적화에 사용될 준비가 돼.

결과적으로, 일반적으로 많이 쓰이는 정규 격자 초기화, "최첨단" K-Means 초기화, 그리고 이전에 소개된 세분화 기반 초기화 방법들에 비해 매우 큰 객관적 및 주관적 품질 개선이 가능하다는 걸 보여줬어. 또한 회귀 모델의 희소성도 크게 개선됐어. 같은 품질을 유지하면서, 새로운 초기화 방법은 약 50% 정도 커널 수를 줄일 수 있었어. 그리고 수렴 시간도 많이 줄어들었고, 전체 실행 시간도 최대 50%까지 절약할 수 있었어. 세분화 기반 초기화 전략 자체는 병렬 계산이 가능하고, 이론상으로는 이미지의 세그먼트 수만큼 작업을 나눌 수 있어. 네 개의 병렬 GPU만 사용해도 초기화에서 이미 50%의 실행 시간 절약이 가능해.

================================================================================

URL:
https://arxiv.org/pdf/2409.10104.pdf

Title: A Comparative Study of Open Source Computer Vision Models for Application on Small Data: The Case of CFRP Tape Laying

Original Abstract:
In the realm of industrial manufacturing, Artificial Intelligence (AI) is playing an increasing role, from automating existing processes to aiding in the development of new materials and techniques. However, a significant challenge arises in smaller, experimental processes characterized by limited training data availability, questioning the possibility to train AI models in such small data contexts. In this work, we explore the potential of Transfer Learning to address this challenge, specifically investigating the minimum amount of data required to develop a functional AI model. For this purpose, we consider the use case of quality control of Carbon Fiber Reinforced Polymer (CFRP) tape laying in aerospace manufacturing using optical sensors. We investigate the behavior of different open-source computer vision models with a continuous reduction of the training data. Our results show that the amount of data required to successfully train an AI model can be drastically reduced, and the use of smaller models does not necessarily lead to a loss of performance.

Translated Abstract:
산업 제조 분야에서 인공지능(AI)이 점점 더 중요한 역할을 하고 있어. 기존 프로세스를 자동화하는 것부터 새로운 재료와 기술 개발을 도와주는 것까지 다양해. 그런데 작은 실험적인 프로세스에서는 훈련 데이터가 부족한 경우가 많아서, 이런 작은 데이터로 AI 모델을 훈련할 수 있을지에 대한 의문이 생겨. 

이 연구에서는 이런 문제를 해결할 수 있는 전이 학습(Transfer Learning)의 가능성을 살펴보았어. 특히, 기능적인 AI 모델을 개발하는 데 필요한 최소 데이터 양을 조사했지. 이를 위해 항공 제조에서 탄소 섬유 강화 폴리머(CFRP) 테이프를 깔 때 품질 관리를 하는 사례를 고려했어. 우리는 훈련 데이터를 점점 줄여가며 다양한 오픈소스 컴퓨터 비전 모델의 동작을 살펴봤어.

결과적으로, AI 모델을 성공적으로 훈련하는 데 필요한 데이터 양을 크게 줄일 수 있다는 걸 확인했어. 그리고 작은 모델을 사용하는 게 성능 저하로 이어지지 않는다는 것도 알게 되었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10141.pdf

Title: PSHuman: Photorealistic Single-view Human Reconstruction using Cross-Scale Diffusion

Original Abstract:
Detailed and photorealistic 3D human modeling is essential for various applications and has seen tremendous progress. However, full-body reconstruction from a monocular RGB image remains challenging due to the ill-posed nature of the problem and sophisticated clothing topology with self-occlusions. In this paper, we propose PSHuman, a novel framework that explicitly reconstructs human meshes utilizing priors from the multiview diffusion model. It is found that directly applying multiview diffusion on single-view human images leads to severe geometric distortions, especially on generated faces. To address it, we propose a cross-scale diffusion that models the joint probability distribution of global full-body shape and local facial characteristics, enabling detailed and identity-preserved novel-view generation without any geometric distortion. Moreover, to enhance cross-view body shape consistency of varied human poses, we condition the generative model on parametric models like SMPL-X, which provide body priors and prevent unnatural views inconsistent with human anatomy. Leveraging the generated multi-view normal and color images, we present SMPLX-initialized explicit human carving to recover realistic textured human meshes efficiently. Extensive experimental results and quantitative evaluations on CAPE and THuman2.1 datasets demonstrate PSHumans superiority in geometry details, texture fidelity, and generalization capability.

Translated Abstract:
자세하고 사진처럼 사실적인 3D 인간 모델링은 다양한 응용 프로그램에 필수적이며, 많은 발전이 있었어. 하지만 단일 RGB 이미지로부터 전체 몸을 재구성하는 건 여전히 어려운 일이야. 왜냐하면 이 문제는 잘 정의되지 않았고, 복잡한 옷의 형태와 자기 가림 때문에 그렇지.

이 논문에서는 PSHuman이라는 새로운 프레임워크를 제안해. 이건 다중 뷰 확산 모델에서 얻은 정보를 활용해서 인간의 메쉬를 명확하게 재구성해. 그런데 단일 뷰 인간 이미지에 다중 뷰 확산을 직접 적용하면 심각한 기하학적 왜곡이 발생해, 특히 생성된 얼굴에서 말이야. 그래서 우리는 전체 몸의 형태와 얼굴의 특성을 함께 모델링하는 크로스 스케일 확산을 제안해. 이 방법을 통해 기하학적 왜곡 없이 세부적이고 정체성을 유지한 새로운 뷰를 생성할 수 있어.

게다가 다양한 인간 자세의 크로스 뷰 몸 형태 일관성을 높이기 위해, SMPL-X 같은 파라메트릭 모델을 기반으로 생성 모델을 조건화해. 이렇게 하면 몸의 우선 정보를 제공하고, 인간 해부학과 일치하지 않는 불自然한 뷰를 방지할 수 있어. 생성된 다중 뷰 노멀 및 컬러 이미지를 활용해서, 현실적인 텍스처가 있는 인간 메쉬를 효율적으로 복구하는 SMPLX 초기화 명시적 인간 조각을 제시해.

CAPE와 THuman2.1 데이터셋에 대한 광범위한 실험 결과와 정량적 평가를 통해 PSHuman이 기하학적 세부사항, 텍스처 충실도, 일반화 능력에서 우수하다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.10151.pdf

Title: AutoPET Challenge III: Testing the Robustness of Generalized Dice Focal Loss trained 3D Residual UNet for FDG and PSMA Lesion Segmentation from Whole-Body PET/CT Images

Original Abstract:
Automated segmentation of cancerous lesions in PET/CT scans is a crucial first step in quantitative image analysis. However, training deep learning models for segmentation with high accuracy is particularly challenging due to the variations in lesion size, shape, and radiotracer uptake. These lesions can appear in different parts of the body, often near healthy organs that also exhibit considerable uptake, making the task even more complex. As a result, creating an effective segmentation model for routine PET/CT image analysis is challenging. In this study, we utilized a 3D Residual UNet model and employed the Generalized Dice Focal Loss function to train the model on the AutoPET Challenge 2024 dataset. We conducted a 5-fold cross-validation and used an average ensembling technique using the models from the five folds. In the preliminary test phase for Task-1, the average ensemble achieved a mean Dice Similarity Coefficient (DSC) of 0.6687, mean false negative volume (FNV) of 10.9522 ml and mean false positive volume (FPV) 2.9684 ml. More details about the algorithm can be found on our GitHub repository: this https URL. The training code has been shared via the repository: this https URL.

Translated Abstract:
암 병변을 PET/CT 스캔에서 자동으로 분할하는 것은 정량적 이미지 분석의 중요한 첫 단계야. 하지만 깊은 학습 모델을 정확하게 훈련시키는 건 어려운 일이야. 병변의 크기, 형태, 방사성 추적자 흡수량이 다양해서 그렇지. 이 병변은 몸의 여러 부위에 나타날 수 있는데, 종종 건강한 장기 근처에 있어 건강한 장기도 많은 흡수를 보여줘서 작업을 더 복잡하게 만들어.

그래서 일상적인 PET/CT 이미지 분석을 위한 효과적인 분할 모델을 만드는 게 쉽지 않아. 이번 연구에서는 3D Residual UNet 모델을 사용하고, Generalized Dice Focal Loss 함수를 적용해서 AutoPET Challenge 2024 데이터셋으로 모델을 훈련했어. 5배 교차 검증을 진행했고, 다섯 개의 폴드에서 나온 모델을 평균 앙상블 기법으로 사용했어.

Task-1의 초기 테스트 단계에서 평균 앙상블은 Dice Similarity Coefficient(DSC) 0.6687, 평균 거짓 음성 부피(FNV) 10.9522 ml, 평균 거짓 양성 부피(FPV) 2.9684 ml를 기록했어. 알고리즘에 대한 더 자세한 내용은 우리의 GitHub 리포지토리에서 확인할 수 있어: 이 https URL. 훈련 코드는 리포지토리를 통해 공유했어: 이 https URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.10156.pdf

Title: Contrastive Learning for Character Detection in Ancient Greek Papyri

Original Abstract:
This thesis investigates the effectiveness of SimCLR, a contrastive learning technique, in Greek letter recognition, focusing on the impact of various augmentation techniques. We pretrain the SimCLR backbone using the Alpub dataset (pretraining dataset) and fine-tune it on a smaller ICDAR dataset (finetuning dataset) to compare SimCLR's performance against traditional baseline models, which use cross-entropy and triplet loss functions. Additionally, we explore the role of different data augmentation strategies, essential for the SimCLR training process. Methodologically, we examine three primary approaches: (1) a baseline model using cross-entropy loss, (2) a triplet embedding model with a classification layer, and (3) a SimCLR pretrained model with a classification layer. Initially, we train the baseline, triplet, and SimCLR models using 93 augmentations on ResNet-18 and ResNet-50 networks with the ICDAR dataset. From these, the top four augmentations are selected using a statistical t-test. Pretraining of SimCLR is conducted on the Alpub dataset, followed by fine-tuning on the ICDAR dataset. The triplet loss model undergoes a similar process, being pretrained on the top four augmentations before fine-tuning on ICDAR. Our experiments show that SimCLR does not outperform the baselines in letter recognition tasks. The baseline model with cross-entropy loss demonstrates better performance than both SimCLR and the triplet loss model. This study provides a detailed evaluation of contrastive learning for letter recognition, highlighting SimCLR's limitations while emphasizing the strengths of traditional supervised learning models in this task. We believe SimCLR's cropping strategies may cause a semantic shift in the input image, reducing training effectiveness despite the large pretraining dataset. Our code is available at this https URL.

Translated Abstract:
이 논문은 그리스 문자 인식에서 대비 학습 기법인 SimCLR의 효과를 조사하고, 다양한 데이터 증강 기법의 영향을 중점적으로 살펴봐. 우리는 Alpub 데이터셋(프리트레이닝 데이터셋)을 사용해 SimCLR 백본을 사전 훈련하고, 더 작은 ICDAR 데이터셋(파인튠 데이터셋)에서 미세 조정을 해서 SimCLR의 성능을 전통적인 기준 모델인 교차 엔트로피와 트리플렛 손실 함수를 사용하는 모델들과 비교해.

또한, SimCLR 훈련 과정에서 중요한 역할을 하는 다양한 데이터 증강 전략도 살펴봐. 방법론적으로는 세 가지 주요 접근 방식을 검토했어: (1) 교차 엔트로피 손실을 사용하는 기준 모델, (2) 분류 계층이 있는 트리플렛 임베딩 모델, (3) 분류 계층이 있는 SimCLR 사전 훈련 모델이야. 처음에는 ICDAR 데이터셋으로 ResNet-18과 ResNet-50 네트워크에서 93가지 증강 기법을 사용해 기준, 트리플렛, SimCLR 모델을 훈련했어. 그 중에서 통계적 t-테스트를 통해 상위 네 가지 증강 기법을 선택했지.

SimCLR의 사전 훈련은 Alpub 데이터셋에서 진행하고, 그 다음 ICDAR 데이터셋에서 미세 조정을 해. 트리플렛 손실 모델도 비슷한 과정을 거치고, 상위 네 가지 증강 기법으로 사전 훈련 후 ICDAR에서 미세 조정을 해. 우리의 실험 결과, SimCLR이 문자 인식 작업에서 기준 모델들보다 성능이 떨어진다는 걸 보여줘. 교차 엔트로피 손실을 사용하는 기준 모델이 SimCLR과 트리플렛 손실 모델보다 더 나은 성능을 보여.

이 연구는 문자 인식에 대한 대비 학습의 자세한 평가를 제공하며, SimCLR의 한계를 강조하고 전통적인 지도 학습 모델의 강점을 부각시켜. 우리는 SimCLR의 크롭 전략이 입력 이미지의 의미를 바꿔 훈련 효율성을 떨어뜨릴 수 있다고 생각해, 비록 대규모 사전 훈련 데이터셋이 있어도 말이야. 우리의 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10175.pdf

Title: VideoRun2D: Cost-Effective Markerless Motion Capture for Sprint Biomechanics

Original Abstract:
Sprinting is a determinant ability, especially in team sports. The kinematics of the sprint have been studied in the past using different methods specially developed considering human biomechanics and, among those methods, markerless systems stand out as very cost-effective. On the other hand, we have now multiple general methods for pixel and body tracking based on recent machine learning breakthroughs with excellent performance in body tracking, but these excellent trackers do not generally consider realistic human biomechanics. This investigation first adapts two of these general trackers (MoveNet and CoTracker) for realistic biomechanical analysis and then evaluate them in comparison to manual tracking (with key points manually marked using the software Kinovea).
Our best resulting markerless body tracker particularly adapted for sprint biomechanics is termed VideoRun2D. The experimental development and assessment of VideoRun2D is reported on forty sprints recorded with a video camera from 5 different subjects, focusing our analysis in 3 key angles in sprint biomechanics: inclination of the trunk, flex extension of the hip and the knee. The CoTracker method showed huge differences compared to the manual labeling approach. However, the angle curves were correctly estimated by the MoveNet method, finding errors between 3.2° and 5.5°.
In conclusion, our proposed VideoRun2D based on MoveNet core seems to be a helpful tool for evaluating sprint kinematics in some scenarios. On the other hand, the observed precision of this first version of VideoRun2D as a markerless sprint analysis system may not be yet enough for highly demanding applications. Future research lines towards that purpose are also discussed at the end: better tracking post-processing and user- and time-dependent adaptation.

Translated Abstract:
스프린트는 특히 팀 스포츠에서 중요한 능력이야. 스프린트의 운동학에 대한 연구는 인간 생체역학을 고려한 다양한 방법으로 진행됐는데, 그 중에서도 마커가 없는 시스템이 특히 비용 효율적이야. 요즘은 최근의 머신러닝 기술 덕분에 픽셀과 신체 추적을 위한 여러 일반적인 방법이 생겼는데, 이런 뛰어난 추적기들은 보통 현실적인 인간 생체역학을 고려하지 않아.

이 연구는 먼저 두 가지 일반 추적기(MoveNet과 CoTracker)를 현실적인 생체역학 분석에 맞게 조정하고, 그 결과를 수동 추적(소프트웨어 Kinovea를 사용해 주요 지점을 수동으로 표시)과 비교해서 평가했어. 우리가 개발한 마커가 없는 신체 추적기 중에서 스프린트 생체역학에 특히 적합한 것은 VideoRun2D라고 불려. VideoRun2D의 실험 개발과 평가는 5명의 다른 피실험자로부터 촬영한 40번의 스프린트를 기반으로 하고, 분석은 스프린트 생체역학의 3가지 주요 각도: 몸통의 기울기, 엉덩이의 굴곡과 신전, 무릎에 초점을 맞췄어.

CoTracker 방법은 수동 레이블링 방식에 비해 큰 차이를 보였어. 하지만 MoveNet 방법은 각도 곡선을 잘 추정했고, 오차는 3.2°에서 5.5° 사이였어. 

결론적으로, MoveNet 코어를 기반으로 한 우리의 VideoRun2D는 특정 상황에서 스프린트 운동학을 평가하는 데 유용한 도구로 보이지만, 이 첫 번째 버전의 VideoRun2D가 마커 없는 스프린트 분석 시스템으로서 높은 요구를 충족하기에는 아직 부족할 수 있어. 마지막으로 향후 연구 방향에 대해서도 논의했는데, 더 나은 추적 후처리와 사용자 및 시간에 따라 적응할 수 있는 방법이 포함돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.10178.pdf

Title: ExelMap: Explainable Element-based HD-Map Change Detection and Update

Original Abstract:
Acquisition and maintenance are central problems in deploying high-definition (HD) maps for autonomous driving, with two lines of research prevalent in current literature: Online HD map generation and HD map change detection. However, the generated map's quality is currently insufficient for safe deployment, and many change detection approaches fail to precisely localize and extract the changed map elements, hence lacking explainability and hindering a potential fleet-based cooperative HD map update. In this paper, we propose the novel task of explainable element-based HD map change detection and update. In extending recent approaches that use online mapping techniques informed with an outdated map prior for HD map updating, we present ExelMap, an explainable element-based map updating strategy that specifically identifies changed map elements. In this context, we discuss how currently used metrics fail to capture change detection performance, while allowing for unfair comparison between prior-less and prior-informed map generation methods. Finally, we present an experimental study on real-world changes related to pedestrian crossings of the Argoverse 2 Map Change Dataset. To the best of our knowledge, this is the first comprehensive problem investigation of real-world end-to-end element-based HD map change detection and update, and ExelMap the first proposed solution.

Translated Abstract:
고해상도(HD) 지도는 자율주행에 필수적인데, 지도 획득과 유지 관리가 큰 문제야. 현재 연구에서는 온라인 HD 지도 생성과 HD 지도 변화 탐지 두 가지 방향이 주로 다뤄지고 있어. 하지만 생성된 지도의 품질은 안전하게 사용하기에는 부족하고, 많은 변화 탐지 방법들이 변경된 지도 요소를 정확하게 찾아내지 못해. 이로 인해 설명 가능성이 떨어지고, 협력 기반 HD 지도 업데이트에 방해가 되고 있어. 

이 논문에서는 설명 가능한 요소 기반 HD 지도 변화 탐지와 업데이트라는 새로운 작업을 제안해. 구식 지도를 활용해 온라인 매핑 기술을 사용하는 최근 접근법을 확장하여, 변경된 지도 요소를 정확히 식별하는 ExelMap이라는 설명 가능한 지도 업데이트 전략을 소개할 거야. 이 과정에서 현재 사용되는 지표들이 변화 탐지 성능을 제대로 반영하지 못하고, 기존 지도 없이 생성한 것과 기존 지도를 활용한 것 간의 불공평한 비교를 허용한다는 문제점을 논의해.

마지막으로, Argoverse 2 지도 변화 데이터셋의 보행자 횡단 보도와 관련된 실제 변화에 대한 실험 연구를 제시할 거야. 우리가 아는 한, 이 연구는 실제 상황에서의 끝에서 끝으로 이어지는 요소 기반 HD 지도 변화 탐지와 업데이트에 대한 첫 번째 종합적인 문제 조사이며, ExelMap은 제안된 첫 번째 해결책이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10180.pdf

Title: RealDiff: Real-world 3D Shape Completion using Self-Supervised Diffusion Models

Original Abstract:
Point cloud completion aims to recover the complete 3D shape of an object from partial observations. While approaches relying on synthetic shape priors achieved promising results in this domain, their applicability and generalizability to real-world data are still limited. To tackle this problem, we propose a self-supervised framework, namely RealDiff, that formulates point cloud completion as a conditional generation problem directly on real-world measurements. To better deal with noisy observations without resorting to training on synthetic data, we leverage additional geometric cues. Specifically, RealDiff simulates a diffusion process at the missing object parts while conditioning the generation on the partial input to address the multimodal nature of the task. We further regularize the training by matching object silhouettes and depth maps, predicted by our method, with the externally estimated ones. Experimental results show that our method consistently outperforms state-of-the-art methods in real-world point cloud completion.

Translated Abstract:
포인트 클라우드 완성은 부분적인 관측값에서 객체의 완전한 3D 형태를 복구하는 걸 목표로 해. 합성 형태 사전 정보를 사용하는 방법들이 이 분야에서 좋은 결과를 냈지만, 실제 데이터에 적용하는 건 아직 한계가 있어. 

이 문제를 해결하기 위해, 우리는 RealDiff라는 자기 지도 학습 프레임워크를 제안해. 이 방법은 포인트 클라우드 완성을 실제 관측값을 기반으로 한 조건부 생성 문제로 설정해. 

노이즈가 많은 관측값을 더 잘 처리하기 위해 합성 데이터로 학습하는 대신 추가적인 기하학적 단서를 활용해. 구체적으로, RealDiff는 누락된 객체 부분에서 확산 과정을 시뮬레이션하고, 부분 입력에 따라 생성을 조정해 이 작업의 다중 모드 특성을 해결해. 

또한, 우리의 방법으로 예측한 객체 실루엣과 깊이 맵을 외부에서 추정한 것과 맞추는 방식으로 훈련을 규제해. 실험 결과, 우리의 방법이 실제 포인트 클라우드 완성에서 최첨단 방법들보다 일관되게 더 나은 성능을 보였어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10197.pdf

Title: Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models

Original Abstract:
Recent progress in Multimodal Large Language Models(MLLMs) often use large image tokens to compensate the visual shortcoming of MLLMs, which not only exhibits obvious redundancy but also greatly exacerbates the already high computation. Token pruning is an effective solution for speeding up MLLMs, but when and how to drop tokens still remains a challenge. In this paper, we propose a novel and training-free approach for the effective visual token pruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning recipe for MLLMs according to a pre-defined budget. Specifically, FitPrune considers token pruning as a statistical problem of MLLM and its objective is to find out an optimal pruning scheme that can minimize the divergence of the attention distributions before and after pruning. In practice, FitPrune can be quickly accomplished based on the attention statistics from a small batch of inference data, avoiding the expensive trials of MLLMs. According to the pruning recipe, an MLLM can directly remove the redundant visual tokens of different examples during inference. To validate FitPrune, we apply it to a set of recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct extensive experiments on a set of benchmarks. The experimental results show that our FitPrune can not only reduce the computational complexity to a large extent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT with only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in about 5 minutes. Our code is available at this https URL.

Translated Abstract:
최근 다중 모달 대형 언어 모델(MLLMs)의 발전은 시각적 단점을 보완하기 위해 큰 이미지 토큰을 사용하는 경우가 많아. 그런데 이 방식은 불필요한 중복이 생기고, 이미 높은 계산량을 더 늘려버려.

토큰 프루닝은 MLLMs의 속도를 높이는 효과적인 방법이지만, 언제 어떻게 토큰을 제거할지가 여전히 도전 과제로 남아 있어. 그래서 우리는 FitPrune이라고 하는 새로운 방법을 제안해. 이 방법은 MLLMs의 시각적 토큰을 효과적으로 프루닝할 수 있게 해주고, 훈련 없이도 사용할 수 있어. 미리 정해진 예산에 따라 MLLMs를 위한 완전한 프루닝 레시피를 빠르게 만들어낼 수 있어.

구체적으로, FitPrune은 토큰 프루닝을 MLLM의 통계적 문제로 보고, 프루닝 전후의 주의 분포 차이를 최소화하는 최적의 프루닝 계획을 찾는 게 목표야. 실제로 FitPrune은 작은 배치의 추론 데이터를 기반으로 주의 통계 정보를 활용해 빠르게 실행될 수 있어서, MLLMs의 비싼 시험을 피할 수 있어.

프루닝 레시피에 따라 MLLM은 추론 중에 서로 다른 예제의 중복된 시각 토큰을 바로 제거할 수 있어. FitPrune을 검증하기 위해, 우리는 최신 MLLMs인 LLaVA-1.5, LLaVA-HR, LLaVA-NEXT에 적용하고 여러 벤치마크에서 광범위한 실험을 했어. 실험 결과에 따르면, FitPrune은 높은 성능을 유지하면서도 계산 복잡성을 크게 줄일 수 있었어. 예를 들어, LLaVA-NEXT의 경우 0.5%의 정확도 감소로 -54.9%의 FLOPs를 줄일 수 있었어. 특히, 프루닝 레시피는 약 5분 안에 얻을 수 있어. 코드도 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10206.pdf

Title: Garment Attribute Manipulation with Multi-level Attention

Original Abstract:
In the rapidly evolving field of online fashion shopping, the need for more personalized and interactive image retrieval systems has become paramount. Existing methods often struggle with precisely manipulating specific garment attributes without inadvertently affecting others. To address this challenge, we propose GAMMA (Garment Attribute Manipulation with Multi-level Attention), a novel framework that integrates attribute-disentangled representations with a multi-stage attention-based architecture. GAMMA enables targeted manipulation of fashion image attributes, allowing users to refine their searches with high accuracy. By leveraging a dual-encoder Transformer and memory block, our model achieves state-of-the-art performance on popular datasets like Shopping100k and DeepFashion.

Translated Abstract:
온라인 패션 쇼핑이 빠르게 발전하면서, 더 개인화되고 상호작용적인 이미지 검색 시스템의 필요성이 커지고 있어. 기존 방법들은 특정 의류 속성을 정확하게 조작하는 데 어려움을 겪고, 다른 속성에 영향을 주는 경우가 많아. 

이런 문제를 해결하기 위해, 우리는 GAMMA(다단계 주의 기반 의류 속성 조작)라는 새로운 프레임워크를 제안해. GAMMA는 속성 분리 표현을 다단계 주의 구조와 결합해, 패션 이미지 속성을 목표로 조작할 수 있게 해줘. 이를 통해 사용자들은 검색을 더 정확하게 조정할 수 있어. 

우리 모델은 이중 인코더 트랜스포머와 메모리 블록을 활용해서, Shopping100k와 DeepFashion 같은 인기 데이터셋에서 최첨단 성능을 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10213.pdf

Title: Neuromorphic Facial Analysis with Cross-Modal Supervision

Original Abstract:
Traditional approaches for analyzing RGB frames are capable of providing a fine-grained understanding of a face from different angles by inferring emotions, poses, shapes, landmarks. However, when it comes to subtle movements standard RGB cameras might fall behind due to their latency, making it hard to detect micro-movements that carry highly informative cues to infer the true emotions of a subject. To address this issue, the usage of event cameras to analyze faces is gaining increasing interest. Nonetheless, all the expertise matured for RGB processing is not directly transferrable to neuromorphic data due to a strong domain shift and intrinsic differences in how data is represented. The lack of labeled data can be considered one of the main causes of this gap, yet gathering data is harder in the event domain since it cannot be crawled from the web and labeling frames should take into account event aggregation rates and the fact that static parts might not be visible in certain frames. In this paper, we first present FACEMORPHIC, a multimodal temporally synchronized face dataset comprising both RGB videos and event streams. The data is labeled at a video level with facial Action Units and also contains streams collected with a variety of applications in mind, ranging from 3D shape estimation to lip-reading. We then show how temporal synchronization can allow effective neuromorphic face analysis without the need to manually annotate videos: we instead leverage cross-modal supervision bridging the domain gap by representing face shapes in a 3D space.

Translated Abstract:
전통적인 RGB 프레임 분석 방법은 다양한 각도에서 얼굴을 세밀하게 이해할 수 있도록 감정, 자세, 모양, 랜드마크 등을 추론할 수 있어. 하지만 미세한 움직임을 감지하는 데는 한계가 있어. 표준 RGB 카메라는 지연이 있어 미세 움직임을 놓치기 쉬운데, 이런 미세 움직임은 개인의 진짜 감정을 추론하는 데 중요한 힌트를 줄 수 있어. 그래서 얼굴 분석에 이벤트 카메라를 사용하는 것에 대한 관심이 높아지고 있어.

그런데 RGB 처리에 대한 모든 전문 지식이 뉴로모픽 데이터에 직접 적용되지는 않아. 데이터 표현 방식이 다르고 도메인 차이가 크기 때문이야. 라벨이 붙은 데이터가 부족한 것도 이 격차의 주요 원인 중 하나인데, 이벤트 도메인에서는 웹에서 데이터를 수집할 수 없고, 프레임에 라벨을 붙일 때 이벤트 집계 속도와 정적 부분이 특정 프레임에서 보이지 않을 수 있다는 점을 고려해야 해서 데이터 수집이 더 힘들어.

이 논문에서는 먼저 FACEMORPHIC이라는 멀티모달 동기화된 얼굴 데이터셋을 소개해. 이 데이터셋은 RGB 비디오와 이벤트 스트림을 모두 포함하고 있어. 데이터는 얼굴 행동 단위(Facial Action Units)로 비디오 수준에서 라벨링되어 있고, 3D 형태 추정부터 입술 읽기까지 다양한 응용 프로그램을 염두에 두고 수집된 스트림도 포함되어 있어. 

그 다음으로는 시간 동기화가 효과적인 뉴로모픽 얼굴 분석을 가능하게 한다는 걸 보여줘. 비디오를 수동으로 주석 달 필요 없이, 대신 얼굴 모양을 3D 공간에서 표현함으로써 도메인 격차를 연결하는 교차 모달 감독(cross-modal supervision)을 활용해.

================================================================================

URL:
https://arxiv.org/pdf/2409.10228.pdf

Title: Robust Bird's Eye View Segmentation by Adapting DINOv2

Original Abstract:
Extracting a Bird's Eye View (BEV) representation from multiple camera images offers a cost-effective, scalable alternative to LIDAR-based solutions in autonomous driving. However, the performance of the existing BEV methods drops significantly under various corruptions such as brightness and weather changes or camera failures. To improve the robustness of BEV perception, we propose to adapt a large vision foundational model, DINOv2, to BEV estimation using Low Rank Adaptation (LoRA). Our approach builds on the strong representation space of DINOv2 by adapting it to the BEV task in a state-of-the-art framework, SimpleBEV. Our experiments show increased robustness of BEV perception under various corruptions, with increasing gains from scaling up the model and the input resolution. We also showcase the effectiveness of the adapted representations in terms of fewer learnable parameters and faster convergence during training.

Translated Abstract:
여러 카메라 이미지에서 새의 눈높이(BEV) 표현을 추출하는 것은 자율주행에서 LIDAR 기반 솔루션보다 비용 효율적이고 확장 가능한 방법이야. 하지만 기존 BEV 방법은 밝기 변화, 날씨 변화, 카메라 고장 같은 여러 손상 상황에서 성능이 많이 떨어져. 

BEV 인식의 강인성을 높이기 위해, 우리는 DINOv2라는 큰 비전 기초 모델을 Low Rank Adaptation(LoRA) 기법을 사용해서 BEV 추정에 맞게 조정할 것을 제안해. 우리의 접근 방식은 DINOv2의 강력한 표현 공간을 활용해서, 최신 프레임워크인 SimpleBEV에 BEV 작업에 맞게 조정하는 거야. 

실험 결과, 다양한 손상 상황에서도 BEV 인식의 강인성이 증가하는 걸 보여줬고, 모델과 입력 해상도를 키우면 이득이 더 커지는 걸 확인했어. 또한, 조정된 표현이 학습 가능한 파라미터 수가 적고 훈련 중 더 빠르게 수렴한다는 점에서도 효과적이라는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10247.pdf

Title: SOLVR: Submap Oriented LiDAR-Visual Re-Localisation

Original Abstract:
This paper proposes SOLVR, a unified pipeline for learning based LiDAR-Visual re-localisation which performs place recognition and 6-DoF registration across sensor modalities. We propose a strategy to align the input sensor modalities by leveraging stereo image streams to produce metric depth predictions with pose information, followed by fusing multiple scene views from a local window using a probabilistic occupancy framework to expand the limited field-of-view of the camera. Additionally, SOLVR adopts a flexible definition of what constitutes positive examples for different training losses, allowing us to simultaneously optimise place recognition and registration performance. Furthermore, we replace RANSAC with a registration function that weights a simple least-squares fitting with the estimated inlier likelihood of sparse keypoint correspondences, improving performance in scenarios with a low inlier ratio between the query and retrieved place. Our experiments on the KITTI and KITTI360 datasets show that SOLVR achieves state-of-the-art performance for LiDAR-Visual place recognition and registration, particularly improving registration accuracy over larger distances between the query and retrieved place.

Translated Abstract:
이 논문은 SOLVR이라는 통합 파이프라인을 제안해. 이 시스템은 LiDAR와 시각 정보를 활용해서 장소 인식과 6자유도 등록을 동시에 수행해. 입력되는 센서 모달리티를 정렬하기 위해 스테레오 이미지 스트림을 활용해서 메트릭 깊이 예측과 자세 정보를 만들어내고, 그런 다음 지역 창에서 여러 장면 뷰를 융합해 카메라의 한정된 시야를 확장하는 확률적 점유 프레임워크를 사용해.

게다가, SOLVR은 훈련 손실에 대한 긍정적인 예시를 유연하게 정의해. 이 덕분에 장소 인식과 등록 성능을 동시에 최적화할 수 있어. 또한, RANSAC 대신 희소 키포인트 대응의 추정된 인라이어 가능성을 고려한 간단한 최소제곱 피팅을 가중하는 등록 함수를 사용해. 이렇게 하면 쿼리와 검색된 장소 간의 인라이어 비율이 낮은 상황에서도 성능이 향상돼.

KITTI와 KITTI360 데이터셋에서 실험한 결과, SOLVR은 LiDAR-Visual 장소 인식과 등록에서 최신 성능을 달성했어. 특히 쿼리와 검색된 장소 사이의 거리가 클 때 등록 정확도가 크게 개선됐어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10262.pdf

Title: Hydra-SGG: Hybrid Relation Assignment for One-stage Scene Graph Generation

Original Abstract:
DETR introduces a simplified one-stage framework for scene graph generation (SGG). However, DETR-based SGG models face two challenges: i) Sparse supervision, as each image typically contains fewer than 10 relation annotations, while the models employ over 100 relation queries. This sparsity arises because each ground truth relation is assigned to only one single query during training. ii) False negative samples, since one ground truth relation may have multiple queries with similar matching scores. These suboptimally matched queries are simply treated as negative samples, causing the loss of valuable supervisory signals. As a response, we devise Hydra-SGG, a one-stage SGG method that adopts a new Hybrid Relation Assignment. This assignment combines a One-to-One Relation Assignment with a newly introduced IoU-based One-to-Many Relation Assignment. Specifically, each ground truth is assigned to multiple relation queries with high IoU subject-object boxes. This Hybrid Relation Assignment increases the number of positive training samples, alleviating sparse supervision. Moreover, we, for the first time, empirically show that self-attention over relation queries helps reduce duplicated relation predictions. We, therefore, propose Hydra Branch, a parameter-sharing auxiliary decoder without a self-attention layer. This design promotes One-to-Many Relation Assignment by enabling different queries to predict the same relation. Hydra-SGG achieves state-of-the-art performance with 10.6 mR@20 and 16.0 mR@50 on VG150, while only requiring 12 training epochs. It also sets a new state-of-the-art on Open Images V6 and and GQA.

Translated Abstract:
DETR은 장면 그래프 생성(SGG)을 위한 간단한 일단계 프레임워크를 소개해. 하지만 DETR 기반의 SGG 모델은 두 가지 문제에 직면해 있어. 

첫 번째는 희소한 감독 신호야. 대부분의 이미지에는 10개 미만의 관계 주석이 있는데, 모델은 100개 이상의 관계 쿼리를 사용해. 이 희소성은 각 실제 관계가 훈련 중에 단 하나의 쿼리만 할당받기 때문이야. 

두 번째는 잘못된 음성 샘플 문제야. 하나의 실제 관계가 비슷한 매칭 점수를 가진 여러 쿼리와 연결될 수 있는데, 이렇게 잘못 매칭된 쿼리는 그냥 음성 샘플로 처리돼서 중요한 감독 신호를 잃게 돼. 

이 문제를 해결하기 위해 우리는 Hydra-SGG라는 새로운 일단계 SGG 방법을 만들었어. 이 방법은 새로운 혼합 관계 할당 방식(Hybrid Relation Assignment)을 채택해. 이 방식은 1:1 관계 할당과 새롭게 도입된 IoU 기반의 1:다 관계 할당을 결합해. 구체적으로 말하면, 각 실제 관계는 높은 IoU를 가진 주체-객체 박스와 함께 여러 관계 쿼리에 할당돼. 이렇게 혼합 관계 할당을 통해 긍정적인 훈련 샘플 수를 늘려서 희소한 감독 신호 문제를 완화해. 

또한, 우리는 처음으로 관계 쿼리 간의 자기 주의(self-attention)가 중복된 관계 예측을 줄이는 데 도움이 된다는 걸 실증적으로 보여줬어. 그래서 Hydra Branch라는 파라미터 공유 보조 디코더를 제안했어. 이 디자인은 서로 다른 쿼리가 같은 관계를 예측할 수 있게 해주면서 1:다 관계 할당을 촉진해. 

Hydra-SGG는 VG150에서 10.6 mR@20과 16.0 mR@50으로 최신 성능을 달성했고, 훈련 에폭은 단 12개만 필요해. 또 Open Images V6와 GQA에서도 새로운 최첨단 성능을 기록했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10269.pdf

Title: BAFNet: Bilateral Attention Fusion Network for Lightweight Semantic Segmentation of Urban Remote Sensing Images

Original Abstract:
Large-scale semantic segmentation networks often achieve high performance, while their application can be challenging when faced with limited sample sizes and computational resources. In scenarios with restricted network size and computational complexity, models encounter significant challenges in capturing long-range dependencies and recovering detailed information in images. We propose a lightweight bilateral semantic segmentation network called bilateral attention fusion network (BAFNet) to efficiently segment high-resolution urban remote sensing images. The model consists of two paths, namely dependency path and remote-local path. The dependency path utilizes large kernel attention to acquire long-range dependencies in the image. Besides, multi-scale local attention and efficient remote attention are designed to construct remote-local path. Finally, a feature aggregation module is designed to effectively utilize the different features of the two paths. Our proposed method was tested on public high-resolution urban remote sensing datasets Vaihingen and Potsdam, with mIoU reaching 83.20% and 86.53%, respectively. As a lightweight semantic segmentation model, BAFNet not only outperforms advanced lightweight models in accuracy but also demonstrates comparable performance to non-lightweight state-of-the-art methods on two datasets, despite a tenfold variance in floating-point operations and a fifteenfold difference in network parameters.

Translated Abstract:
대규모 의미 분할 네트워크는 보통 높은 성능을 보이지만, 샘플 수가 적고 계산 자원이 제한된 상황에서는 적용하기가 어려워. 네트워크 크기와 계산 복잡성이 제한된 경우, 모델은 이미지에서 장거리 의존성을 포착하고 세부 정보를 복구하는 데 큰 어려움을 겪어. 

우리는 고해상도 도시 원격 감지 이미지를 효율적으로 분할하기 위해 경량화된 양방향 의미 분할 네트워크인 BAFNet을 제안해. 이 모델은 의존 경로와 원격-로컬 경로라는 두 가지 경로로 구성돼. 의존 경로는 큰 커널 주의를 사용해서 이미지의 장거리 의존성을 얻고, 원격-로컬 경로는 다중 스케일 로컬 주의와 효율적인 원격 주의를 설계해서 구성해. 마지막으로, 두 경로의 다양한 특징을 효과적으로 활용하기 위해 특징 집계 모듈을 설계했어. 

우리가 제안한 방법은 공개된 고해상도 도시 원격 감지 데이터셋인 Vaihingen과 Potsdam에서 테스트했는데, mIoU가 각각 83.20%와 86.53%에 도달했어. BAFNet은 경량화된 의미 분할 모델로서 정확도 면에서 고급 경량 모델보다 뛰어난 성능을 보이고, 두 데이터셋에서 비경량 최신 방법들과 비슷한 성능을 보여줬어. 이 과정에서 부동 소수점 연산은 10배 차이, 네트워크 파라미터는 15배 차이가 나지만 말이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10272.pdf

Title: Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data

Original Abstract:
This study introduces a laboratory experiment designed to assess the influence of annotation strategies, levels of imbalanced data, and prior experience, on the performance of human annotators. The experiment focuses on labeling aerial imagery, using ArcGIS Pro tools, to detect and segment small-scale photovoltaic solar panels, selected as a case study for rectangular objects. The experiment is conducted using images with a pixel size of 0.15\textbf{$m$}, involving both expert and non-expert participants, across different setup strategies and target-background ratio datasets. Our findings indicate that human annotators generally perform more effectively in object detection than in segmentation tasks. A marked tendency to commit more Type II errors (False Negatives, i.e., undetected objects) than Type I errors (False Positives, i.e. falsely detecting objects that do not exist) was observed across all experimental setups and conditions, suggesting a consistent bias in detection and segmentation processes. Performance was better in tasks with higher target-background ratios (i.e., more objects per unit area). Prior experience did not significantly impact performance and may, in some cases, even lead to overestimation in segmentation. These results provide evidence that human annotators are relatively cautious and tend to identify objects only when they are confident about them, prioritizing underestimation over overestimation. Annotators' performance is also influenced by object scarcity, showing a decline in areas with extremely imbalanced datasets and a low ratio of target-to-background. These findings may enhance annotation strategies for remote sensing research while efficient human annotators are crucial in an era characterized by growing demands for high-quality training data to improve segmentation and detection models.

Translated Abstract:
이 연구는 주석 전략, 불균형 데이터의 수준, 그리고 이전 경험이 인간 주석자의 성능에 미치는 영향을 평가하기 위해 설계된 실험을 소개해. 실험은 항공 이미지를 라벨링하는 데 초점을 맞추고, ArcGIS Pro 도구를 사용해서 작은 규모의 태양광 패널을 탐지하고 분할하는 사례 연구를 진행했어. 실험에서는 0.15m 픽셀 크기의 이미지를 사용하고, 전문가와 비전문가 참가자들이 다양한 설정 전략과 목표-배경 비율 데이터셋을 가지고 참여했어.

우리의 발견은 인간 주석자가 일반적으로 물체 탐지 작업에서 분할 작업보다 더 효과적으로 수행한다는 거야. 모든 실험 설정과 조건에서 Type II 오류(즉, 탐지되지 않은 물체)가 Type I 오류(즉, 존재하지 않는 물체를 잘못 탐지하는 경우)보다 더 많이 발생하는 경향이 관찰되었어. 이는 탐지와 분할 과정에서 일관된 편향이 있음을 시사해. 목표-배경 비율이 높은 작업(즉, 단위 면적당 물체가 더 많은 경우)에서 성능이 더 좋았어. 이전 경험은 성능에 큰 영향을 미치지 않았고, 어떤 경우에는 분할에서 과대평가로 이어지기도 했어.

이 결과는 인간 주석자가 상대적으로 조심스러워서 자신이 확신할 수 있을 때만 물체를 식별하는 경향이 있음을 보여줘. 주석자의 성능은 물체의 희소성에도 영향을 받아서, 극단적으로 불균형한 데이터셋과 낮은 목표-배경 비율의 영역에서는 성능이 떨어졌어. 이러한 발견은 원격 감지 연구를 위한 주석 전략을 개선하는 데 도움이 될 수 있고, 고품질 훈련 데이터의 수요가 증가하는 시대에서 효율적인 인간 주석자가 중요하다는 점을 강조해.

================================================================================

URL:
https://arxiv.org/pdf/2409.10286.pdf

Title: Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation

Original Abstract:
Accurate and robust medical image classification is a challenging task, especially in application domains where available annotated datasets are small and present high imbalance between target classes. Considering that data acquisition is not always feasible, especially for underrepresented classes, our approach introduces a novel synthetic augmentation strategy using class-specific Variational Autoencoders (VAEs) and latent space interpolation to improve discrimination capabilities.
By generating realistic, varied synthetic data that fills feature space gaps, we address issues of data scarcity and class imbalance. The method presented in this paper relies on the interpolation of latent representations within each class, thus enriching the training set and improving the model's generalizability and diagnostic accuracy. The proposed strategy was tested in a small dataset of 321 images created to train and validate an automatic method for assessing the quality of cleanliness of esophagogastroduodenoscopy images. By combining real and synthetic data, an increase of over 18\% in the accuracy of the most challenging underrepresented class was observed. The proposed strategy not only benefited the underrepresented class but also led to a general improvement in other metrics, including a 6\% increase in global accuracy and precision.

Translated Abstract:
정확하고 강력한 의료 이미지 분류는 어려운 작업이야. 특히 주어진 데이터셋이 작고, 타겟 클래스 간의 불균형이 큰 경우엔 더 그렇지. 데이터 수집이 항상 가능하지 않다는 점을 고려했을 때, 우리는 클래스별 변분 오토인코더(VAE)와 잠재 공간 보간을 이용한 새로운 합성 증강 전략을 도입했어. 이 방법은 식별 능력을 향상시키기 위해서야.

우리는 현실적이고 다양한 합성 데이터를 생성해서 특성 공간의 빈틈을 메워. 이렇게 해서 데이터 부족과 클래스 불균형 문제를 해결할 수 있어. 이 논문에서 제안하는 방법은 각 클래스 내에서 잠재 표현을 보간하는 데 의존하고, 그래서 훈련 세트를 풍부하게 만들어 모델의 일반화 능력과 진단 정확성을 높여.

제안한 전략은 321개의 이미지를 사용하는 작은 데이터셋에서 테스트되었고, 이는 식도위십이지장내시경 이미지의 청결도를 평가하는 자동화된 방법을 훈련하고 검증하기 위해 만들어졌어. 실제 데이터와 합성 데이터를 결합하여, 가장 어려운 저대표 클래스의 정확도가 18\% 이상 상승했어. 제안된 전략은 저대표 클래스에만 이익을 준 게 아니라, 전반적인 정확도와 정밀도에서도 6\% 증가라는 일반적인 개선을 가져왔어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10291.pdf

Title: Anatomical Positional Embeddings

Original Abstract:
We propose a self-supervised model producing 3D anatomical positional embeddings (APE) of individual medical image voxels. APE encodes voxels' anatomical closeness, i.e., voxels of the same organ or nearby organs always have closer positional embeddings than the voxels of more distant body parts. In contrast to the existing models of anatomical positional embeddings, our method is able to efficiently produce a map of voxel-wise embeddings for a whole volumetric input image, which makes it an optimal choice for different downstream applications. We train our APE model on 8400 publicly available CT images of abdomen and chest regions. We demonstrate its superior performance compared with the existing models on anatomical landmark retrieval and weakly-supervised few-shot localization of 13 abdominal organs. As a practical application, we show how to cheaply train APE to crop raw CT images to different anatomical regions of interest with 0.99 recall, while reducing the image volume by 10-100 times. The code and the pre-trained APE model are available at this https URL .

Translated Abstract:
우리는 개별 의료 이미지의 볼륨 픽셀에 대한 3D 해부학적 위치 임베딩(APE)을 생성하는 자기 지도 모델을 제안해. APE는 픽셀 간의 해부학적 근접성을 인코딩해. 즉, 같은 장기나 인접한 장기의 픽셀은 더 먼 신체 부위의 픽셀보다 항상 더 가까운 위치 임베딩을 가져.

기존의 해부학적 위치 임베딩 모델과 달리, 우리의 방법은 전체 볼륨 입력 이미지에 대한 픽셀 단위 임베딩 맵을 효율적으로 생성할 수 있어. 이 덕분에 다양한 하위 응용 프로그램에 최적의 선택이 돼.

우리는 8400개의 공개 CT 이미지를 사용해 APE 모델을 훈련했어. 그리고 기존 모델들과 비교했을 때 해부학적 랜드마크 검색과 13개의 복부 장기에 대한 약한 지도 몇 샷 로컬라이제이션에서 뛰어난 성능을 보여줬어.

실용적인 응용으로, APE를 사용해 원시 CT 이미지를 다양한 해부학적 관심 영역으로 잘라내는 방법을 보여줬어. 이 과정에서 0.99의 재현율을 달성하면서 이미지 볼륨을 10배에서 100배까지 줄일 수 있었어. 코드와 사전 훈련된 APE 모델은 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10297.pdf

Title: On Synthetic Texture Datasets: Challenges, Creation, and Curation

Original Abstract:
The influence of textures on machine learning models has been an ongoing investigation, specifically in texture bias/learning, interpretability, and robustness. However, due to the lack of large and diverse texture data available, the findings in these works have been limited, as more comprehensive evaluations have not been feasible. Image generative models are able to provide data creation at scale, but utilizing these models for texture synthesis has been unexplored and poses additional challenges both in creating accurate texture images and validating those images. In this work, we introduce an extensible methodology and corresponding new dataset for generating high-quality, diverse texture images capable of supporting a broad set of texture-based tasks. Our pipeline consists of: (1) developing prompts from a range of descriptors to serve as input to text-to-image models, (2) adopting and adapting Stable Diffusion pipelines to generate and filter the corresponding images, and (3) further filtering down to the highest quality images. Through this, we create the Prompted Textures Dataset (PTD), a dataset of 362,880 texture images that span 56 textures. During the process of generating images, we find that NSFW safety filters in image generation pipelines are highly sensitive to texture (and flag up to 60\% of our texture images), uncovering a potential bias in these models and presenting unique challenges when working with texture data. Through both standard metrics and a human evaluation, we find that our dataset is high quality and diverse.

Translated Abstract:
텍스처가 머신러닝 모델에 미치는 영향은 계속 연구되고 있어. 특히 텍스처 편향, 학습, 해석 가능성, 강인성 같은 부분에서 말이지. 그런데 대규모의 다양한 텍스처 데이터가 부족해서 이런 연구 결과들이 한계가 있었어. 더 포괄적인 평가를 하기 어려웠으니까.

이미지 생성 모델들은 대량의 데이터를 만들 수 있지만, 텍스처 합성에 이 모델들을 활용하는 건 아직 잘 알려지지 않았고, 정확한 텍스처 이미지를 만드는 것과 그 이미지를 검증하는 데 추가적인 어려움이 있어. 

이번 연구에서는 고품질의 다양한 텍스처 이미지를 생성할 수 있는 확장 가능한 방법론과 새로운 데이터셋을 소개해. 이 데이터셋은 다양한 텍스처 기반 작업을 지원할 수 있어. 우리의 파이프라인은 다음과 같아: (1) 여러 설명자로부터 프롬프트를 개발해 텍스트-투-이미지 모델에 입력으로 사용하고, (2) 안정적인 확산(Stable Diffusion) 파이프라인을 적용하고 수정해서 해당 이미지를 생성하고 필터링하며, (3) 가장 고품질의 이미지로 추가 필터링을 해.

이 과정을 통해 우리는 362,880개의 텍스처 이미지로 구성된 Prompted Textures Dataset(PTD)을 만들었어. 이 데이터셋은 56가지 텍스처를 포함하고 있어. 이미지를 생성하는 과정에서, 이미지 생성 파이프라인의 NSFW 안전 필터가 텍스처에 매우 민감하다는 걸 발견했어. 이 필터는 우리 텍스처 이미지의 60%까지 차단하더라고. 이로 인해 모델에서 잠재적인 편향이 드러났고, 텍스처 데이터를 다룰 때 독특한 도전 과제가 생겼어.

표준 지표와 인간 평가를 통해, 우리의 데이터셋이 고품질이면서도 다양하다는 걸 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10327.pdf

Title: Baking Relightable NeRF for Real-time Direct/Indirect Illumination Rendering

Original Abstract:
Relighting, which synthesizes a novel view under a given lighting condition (unseen in training time), is a must feature for immersive photo-realistic experience. However, real-time relighting is challenging due to high computation cost of the rendering equation which requires shape and material decomposition and visibility test to model shadow. Additionally, for indirect illumination, additional computation of rendering equation on each secondary surface point (where reflection occurs) is required rendering real-time relighting challenging. We propose a novel method that executes a CNN renderer to compute primary surface points and rendering parameters, required for direct illumination. We also present a lightweight hash grid-based renderer, for indirect illumination, which is recursively executed to perform the secondary ray tracing process. Both renderers are trained in a distillation from a pre-trained teacher model and provide real-time physically-based rendering under unseen lighting condition at a negligible loss of rendering quality.

Translated Abstract:
리라이트닝은 주어진 조명 조건에서 새로운 시점을 합성하는 과정인데, 이 조명 조건은 훈련할 때 본 적이 없는 거야. 이 기능은 몰입감 있는 사실적인 경험을 위해 꼭 필요해. 그런데 실시간 리라이트닝은 렌더링 방정식의 계산 비용이 높아서 어려워. 이 방정식은 그림자를 모델링하기 위해 형태와 재료 분해, 가시성 테스트를 요구해. 게다가 간접 조명을 위해서는 반사가 일어나는 각 이차 표면 지점에서 렌더링 방정식을 추가로 계산해야 해서 실시간 리라이트닝이 더 어려워.

우리는 CNN 렌더러를 사용해서 직접 조명에 필요한 주요 표면 지점과 렌더링 파라미터를 계산하는 새로운 방법을 제안해. 그리고 간접 조명용으로는 가벼운 해시 그리드 기반 렌더러를 만들었어. 이 렌더러는 재귀적으로 실행돼서 이차 광선 추적 과정을 수행해. 두 렌더러는 미리 훈련된 교사 모델에서 증류 훈련을 통해 학습되며, 보지 못한 조명 조건에서도 실시간으로 물리 기반 렌더링을 제공하면서 렌더링 품질의 손실이 거의 없어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10328.pdf

Title: Fuse4Seg: Image-Level Fusion Based Multi-Modality Medical Image Segmentation

Original Abstract:
Although multi-modality medical image segmentation holds significant potential for enhancing the diagnosis and understanding of complex diseases by integrating diverse imaging modalities, existing methods predominantly rely on feature-level fusion strategies. We argue the current feature-level fusion strategy is prone to semantic inconsistencies and misalignments across various imaging modalities because it merges features at intermediate layers in a neural network without evaluative control. To mitigate this, we introduce a novel image-level fusion based multi-modality medical image segmentation method, Fuse4Seg, which is a bi-level learning framework designed to model the intertwined dependencies between medical image segmentation and medical image fusion. The image-level fusion process is seamlessly employed to guide and enhance the segmentation results through a layered optimization approach. Besides, the knowledge gained from the segmentation module can effectively enhance the fusion module. This ensures that the resultant fused image is a coherent representation that accurately amalgamates information from all modalities. Moreover, we construct a BraTS-Fuse benchmark based on BraTS dataset, which includes 2040 paired original images, multi-modal fusion images, and ground truth. This benchmark not only serves image-level medical segmentation but is also the largest dataset for medical image fusion to date. Extensive experiments on several public datasets and our benchmark demonstrate the superiority of our approach over prior state-of-the-art (SOTA) methodologies.

Translated Abstract:
다중 모달리티 의료 이미지 분할은 다양한 이미징 기법을 결합해 복잡한 질병을 진단하고 이해하는 데 큰 잠재력을 가지고 있어. 하지만 기존 방법들은 주로 특징 수준의 융합 전략에 의존하고 있어. 우리는 현재의 특징 수준 융합 전략이 다양한 이미징 모달리티 사이에서 의미적 일관성이 부족하고 정렬이 잘 안 되는 문제를 겪고 있다고 주장해. 이 방법은 신경망의 중간 레이어에서 특징을 합치기 때문에 평가적 제어가 없거든.

이 문제를 해결하기 위해 우리는 Fuse4Seg라는 새로운 이미지 수준의 융합 기반 다중 모달리티 의료 이미지 분할 방법을 소개해. 이건 의료 이미지 분할과 의료 이미지 융합 사이의 복잡한 의존성을 모델링하기 위한 이중 학습 프레임워크야. 이미지 수준의 융합 과정이 분할 결과를 안내하고 향상시키는 데 매끄럽게 사용돼서, 계층적 최적화 접근 방식을 통해 이루어져.

또한, 분할 모듈에서 얻은 지식이 융합 모듈을 효과적으로 향상시킬 수 있어. 이렇게 해서 최종적으로 생성된 융합 이미지는 모든 모달리티의 정보를 정확하게 통합한 일관된 표현이 되는 거야. 게다가, 우리는 BraTS 데이터셋을 기반으로 한 BraTS-Fuse 벤치마크를 구축했어. 이 벤치마크는 2040개의 원본 이미지 쌍, 다중 모달 융합 이미지, 그리고 정답을 포함하고 있어. 이 벤치마크는 이미지 수준의 의료 분할을 위한 것일 뿐만 아니라, 현재까지 의료 이미지 융합을 위한 가장 큰 데이터셋이기도 해.

여러 공공 데이터셋과 우리의 벤치마크를 통한 광범위한 실험 결과, 우리의 접근 방식이 이전의 최신 기술보다 우수하다는 것을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10329.pdf

Title: InfoDisent: Explainability of Image Classification Models by Information Disentanglement

Original Abstract:
Understanding the decisions made by image classification networks is a critical area of research in deep learning. This task is traditionally divided into two distinct approaches: post-hoc methods and intrinsic methods. Post-hoc methods, such as GradCam, aim to interpret the decisions of pre-trained models by identifying regions of the image where the network focuses its attention. However, these methods provide only a high-level overview, making it difficult to fully understand the network's decision-making process. Conversely, intrinsic methods, like prototypical parts models, offer a more detailed understanding of network predictions but are constrained by specific architectures, training methods, and datasets.
In this paper, we introduce InfoDisent, a hybrid model that combines the advantages of both approaches. By utilizing an information bottleneck, InfoDisent disentangles the information in the final layer of a pre-trained deep network, enabling the breakdown of classification decisions into basic, understandable atomic components. Unlike standard prototypical parts approaches, InfoDisent can interpret the decisions of pre-trained classification networks and be used for making classification decisions, similar to intrinsic models. We validate the effectiveness of InfoDisent on benchmark datasets such as ImageNet, CUB-200-2011, Stanford Cars, and Stanford Dogs for both convolutional and transformer backbones.

Translated Abstract:
이미지 분류 네트워크가 내리는 결정을 이해하는 것은 딥러닝에서 중요한 연구 분야야. 이 작업은 전통적으로 두 가지 접근법으로 나뉘어져 있어: 사후 방법과 내재적 방법이야. 

사후 방법인 GradCam 같은 건, 사전 훈련된 모델의 결정을 해석하려고 이미지를 네트워크가 집중하는 부분을 찾아내. 하지만 이런 방법은 고수준의 개요만 제공해서 네트워크의 결정 과정 전체를 이해하기는 어려워. 반면에, 내재적 방법인 프로토타입 부분 모델 같은 건 네트워크의 예측을 더 자세히 이해할 수 있게 해주지만, 특정 아키텍처, 훈련 방법, 데이터셋에 제한을 받아.

이 논문에서는 두 접근법의 장점을 결합한 하이브리드 모델인 InfoDisent를 소개해. 정보 병목 현상을 이용해서, InfoDisent는 사전 훈련된 딥 네트워크의 마지막 층에서 정보를 분리해, 분류 결정을 기본적이고 이해하기 쉬운 원자적 요소로 나눌 수 있도록 해. 일반적인 프로토타입 부분 접근법과 달리, InfoDisent는 사전 훈련된 분류 네트워크의 결정을 해석할 수 있고, 내재적 모델처럼 분류 결정을 내리는 데 사용할 수 있어. 

우리는 InfoDisent의 효과를 ImageNet, CUB-200-2011, Stanford Cars, Stanford Dogs 같은 벤치마크 데이터셋에서 검증했어. 컨볼루션 백본과 트랜스포머 백본 모두에서 말이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10353.pdf

Title: Taming Diffusion Models for Image Restoration: A Review

Original Abstract:
Diffusion models have achieved remarkable progress in generative modelling, particularly in enhancing image quality to conform to human preferences. Recently, these models have also been applied to low-level computer vision for photo-realistic image restoration (IR) in tasks such as image denoising, deblurring, dehazing, etc. In this review paper, we introduce key constructions in diffusion models and survey contemporary techniques that make use of diffusion models in solving general IR tasks. Furthermore, we point out the main challenges and limitations of existing diffusion-based IR frameworks and provide potential directions for future work.

Translated Abstract:
확산 모델은 생성 모델링에서 놀라운 발전을 이루었고, 특히 이미지 품질을 인간의 취향에 맞게 향상시키는 데 유용해. 최근에는 이런 모델들이 저수준 컴퓨터 비전에서도 사용되고 있어. 예를 들어, 사진처럼 사실적인 이미지 복원(복원 작업)에서 이미지 노이즈 제거, 블러 제거, 안개 제거 같은 작업에 적용되고 있지.

이 리뷰 논문에서는 확산 모델의 주요 구조를 소개하고, 이러한 모델을 사용해서 일반적인 복원 작업을 해결하는 최신 기술들을 살펴볼 거야. 또한, 기존의 확산 기반 복원 프레임워크가 직면한 주요 도전 과제와 한계도 짚어보고, 앞으로의 연구 방향에 대해서도 제안할 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10357.pdf

Title: 2D or not 2D: How Does the Dimensionality of Gesture Representation Affect 3D Co-Speech Gesture Generation?

Original Abstract:
Co-speech gestures are fundamental for communication. The advent of recent deep learning techniques has facilitated the creation of lifelike, synchronous co-speech gestures for Embodied Conversational Agents. "In-the-wild" datasets, aggregating video content from platforms like YouTube via human pose detection technologies, provide a feasible solution by offering 2D skeletal sequences aligned with speech. Concurrent developments in lifting models enable the conversion of these 2D sequences into 3D gesture databases. However, it is important to note that the 3D poses estimated from the 2D extracted poses are, in essence, approximations of the ground-truth, which remains in the 2D domain. This distinction raises questions about the impact of gesture representation dimensionality on the quality of generated motions - a topic that, to our knowledge, remains largely unexplored. Our study examines the effect of using either 2D or 3D joint coordinates as training data on the performance of speech-to-gesture deep generative models. We employ a lifting model for converting generated 2D pose sequences into 3D and assess how gestures created directly in 3D stack up against those initially generated in 2D and then converted to 3D. We perform an objective evaluation using widely used metrics in the gesture generation field as well as a user study to qualitatively evaluate the different approaches.

Translated Abstract:
공동 언어 제스처는 의사소통에 아주 중요해. 최근의 딥 러닝 기술 덕분에, 현실감 있는 동기화된 공동 언어 제스처를 만들어낼 수 있게 됐어. "야외" 데이터셋은 유튜브 같은 플랫폼에서 사람의 자세를 감지하는 기술을 활용해 비디오 내용을 모아 만든 건데, 이게 2D 뼈대 시퀀스를 음성과 맞춰서 제공해줘.

그와 동시에, 리프팅 모델의 발전 덕분에 이 2D 시퀀스를 3D 제스처 데이터베이스로 변환할 수 있게 되었어. 하지만 2D에서 추출한 자세로부터 추정된 3D 자세는 사실상 실제 3D 자세의 근사치라는 점을 알아두는 게 중요해. 실제 자세는 여전히 2D 영역에 남아있거든. 이런 차이는 제스처 표현의 차원이 생성된 동작의 품질에 어떤 영향을 미치는지에 대한 질문을 불러일으켜. 이 부분은 우리가 아는 한, 아직 크게 연구되지 않은 주제야.

우리 연구는 훈련 데이터로 2D 또는 3D 관절 좌표를 사용할 때의 성능 차이를 확인해. 우리는 생성된 2D 자세 시퀀스를 3D로 변환하기 위해 리프팅 모델을 사용하고, 3D로 직접 생성한 제스처와 2D에서 시작해 3D로 변환한 제스처를 비교해봐. 제스처 생성 분야에서 널리 사용되는 지표를 이용해 객관적인 평가를 하고, 사용자 연구를 통해 다양한 접근 방식의 질적인 평가도 진행해.

================================================================================

URL:
https://arxiv.org/pdf/2409.10362.pdf

Title: Frequency-Guided Masking for Enhanced Vision Self-Supervised Learning

Original Abstract:
We present a novel frequency-based Self-Supervised Learning (SSL) approach that significantly enhances its efficacy for pre-training. Prior work in this direction masks out pre-defined frequencies in the input image and employs a reconstruction loss to pre-train the model. While achieving promising results, such an implementation has two fundamental limitations as identified in our paper. First, using pre-defined frequencies overlooks the variability of image frequency responses. Second, pre-trained with frequency-filtered images, the resulting model needs relatively more data to adapt to naturally looking images during fine-tuning. To address these drawbacks, we propose FOurier transform compression with seLf-Knowledge distillation (FOLK), integrating two dedicated ideas. First, inspired by image compression, we adaptively select the masked-out frequencies based on image frequency responses, creating more suitable SSL tasks for pre-training. Second, we employ a two-branch framework empowered by knowledge distillation, enabling the model to take both the filtered and original images as input, largely reducing the burden of downstream tasks. Our experimental results demonstrate the effectiveness of FOLK in achieving competitive performance to many state-of-the-art SSL methods across various downstream tasks, including image classification, few-shot learning, and semantic segmentation.

Translated Abstract:
우리는 새로운 주파수 기반의 자기 지도 학습(SSL) 접근 방식을 제안해, 모델의 사전 학습 효과를 크게 향상시켰어. 이전 연구들은 입력 이미지에서 미리 정의된 주파수를 가리고 재구성 손실을 사용해 모델을 사전 학습했지. 이 방법은 괜찮은 결과를 얻긴 했지만, 우리 논문에서 지적한 두 가지 기본적인 한계가 있어.

첫째, 미리 정의된 주파수를 사용하는 건 이미지 주파수 반응의 변동성을 무시해. 둘째, 주파수 필터링된 이미지로 사전 학습한 모델은, 미세 조정할 때 자연스러운 이미지에 적응하기 위해 상대적으로 더 많은 데이터가 필요해. 이런 단점을 해결하기 위해, 우리는 FOurier 변환 압축과 자기 지식 증류(FOLK)를 제안해. 이건 두 가지 아이디어를 통합한 거야.

첫 번째는, 이미지 압축에서 영감을 받아서 이미지 주파수 반응에 따라 가려질 주파수를 적절히 선택해. 이렇게 하면 사전 학습을 위한 더 적합한 SSL 작업을 만들 수 있어. 두 번째는, 지식 증류를 이용한 두 가지 가지 프레임워크를 사용해, 모델이 필터링된 이미지와 원본 이미지를 모두 입력으로 받을 수 있게 해. 이렇게 하면 다운스트림 작업의 부담을 크게 줄일 수 있어.

우리 실험 결과는 FOLK가 이미지 분류, 소수 샷 학습, 의미론적 분할 등 다양한 다운스트림 작업에서 여러 최신 SSL 방법들과 경쟁력 있는 성능을 보여준다는 걸 입증해.

================================================================================

URL:
https://arxiv.org/pdf/2409.10365.pdf

Title: Robust image representations with counterfactual contrastive learning

Original Abstract:
Contrastive pretraining can substantially increase model generalisation and downstream performance. However, the quality of the learned representations is highly dependent on the data augmentation strategy applied to generate positive pairs. Positive contrastive pairs should preserve semantic meaning while discarding unwanted variations related to the data acquisition domain. Traditional contrastive pipelines attempt to simulate domain shifts through pre-defined generic image transformations. However, these do not always mimic realistic and relevant domain variations for medical imaging such as scanner differences. To tackle this issue, we herein introduce counterfactual contrastive learning, a novel framework leveraging recent advances in causal image synthesis to create contrastive positive pairs that faithfully capture relevant domain variations. Our method, evaluated across five datasets encompassing both chest radiography and mammography data, for two established contrastive objectives (SimCLR and DINO-v2), outperforms standard contrastive learning in terms of robustness to acquisition shift. Notably, counterfactual contrastive learning achieves superior downstream performance on both in-distribution and on external datasets, especially for images acquired with scanners under-represented in the training set. Further experiments show that the proposed framework extends beyond acquisition shifts, with models trained with counterfactual contrastive learning substantially improving subgroup performance across biological sex.

Translated Abstract:
대조적 사전 훈련은 모델의 일반화 능력과 후속 성능을 크게 향상시킬 수 있어. 하지만 학습된 표현의 품질은 긍정적인 쌍을 생성하는 데이터 증강 전략에 많이 의존해. 긍정적인 대조 쌍은 의미를 유지하면서 데이터 수집과 관련된 원치 않는 변화를 없애야 해.

전통적인 대조 파이프라인은 미리 정해진 일반적인 이미지 변환을 통해 도메인 변화를 시뮬레이션하려고 해. 하지만 이런 방식은 의료 이미징에서 스캐너 차이 같은 현실적이고 관련성 있는 도메인 변화를 항상 잘 모방하지는 못해. 

이 문제를 해결하기 위해 우리는 반사실적 대조 학습이라는 새로운 프레임워크를 소개해. 이 방법은 최근의 인과 이미지 합성 기술을 활용해 대조적인 긍정 쌍을 만들어 관련 도메인 변화를 잘 캡처해. 우리는 가슴 엑스레이와 유방 촬영 데이터를 포함한 다섯 개의 데이터셋에서 SimCLR과 DINO-v2라는 두 가지 확립된 대조 목표에 대해 평가했어. 그 결과, 기존의 대조 학습 방식에 비해 취득 변화에 대한 강인성이 더 뛰어난 것을 알 수 있었어.

특히 반사실적 대조 학습은 훈련 세트에서 잘 나타나지 않는 스캐너로 촬영된 이미지들을 포함한 외부 데이터셋에서도 성능이 우수했어. 추가 실험에서도 제안된 프레임워크는 취득 변화에 그치지 않고, 반사실적 대조 학습으로 훈련된 모델이 생물학적 성별에 따른 하위 그룹 성능을 크게 향상시키는 것을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10385.pdf

Title: Mamba-ST: State Space Model for Efficient Style Transfer

Original Abstract:
The goal of style transfer is, given a content image and a style source, generating a new image preserving the content but with the artistic representation of the style source. Most of the state-of-the-art architectures use transformers or diffusion-based models to perform this task, despite the heavy computational burden that they require. In particular, transformers use self- and cross-attention layers which have large memory footprint, while diffusion models require high inference time. To overcome the above, this paper explores a novel design of Mamba, an emergent State-Space Model (SSM), called Mamba-ST, to perform style transfer. To do so, we adapt Mamba linear equation to simulate the behavior of cross-attention layers, which are able to combine two separate embeddings into a single output, but drastically reducing memory usage and time complexity. We modified the Mamba's inner equations so to accept inputs from, and combine, two separate data streams. To the best of our knowledge, this is the first attempt to adapt the equations of SSMs to a vision task like style transfer without requiring any other module like cross-attention or custom normalization layers. An extensive set of experiments demonstrates the superiority and efficiency of our method in performing style transfer compared to transformers and diffusion models. Results show improved quality in terms of both ArtFID and FID metrics. Code is available at this https URL.

Translated Abstract:
스타일 전송의 목표는 콘텐츠 이미지와 스타일 소스를 주었을 때, 콘텐츠는 그대로 유지하면서 스타일 소스의 예술적 표현을 담은 새로운 이미지를 생성하는 거야. 최신 기술들은 보통 변환기(transformers)나 확산 모델(diffusion models)을 사용해서 이 작업을 수행하는데, 이 과정이 많은 계산 자원을 요구해. 특히, 변환기는 자가 주의(self-attention)와 교차 주의(cross-attention) 레이어를 사용해서 메모리 사용량이 많고, 확산 모델은 추론 시간이 길어.

이 문제를 해결하기 위해, 이 논문은 Mamba라는 새로운 상태 공간 모델(State-Space Model, SSM)의 디자인을 탐구해. 이 모델은 Mamba-ST라고 불리는데, 스타일 전송을 수행하기 위해 만들어졌어. 우리는 Mamba의 선형 방정식을 조정해서 교차 주의 레이어의 동작을 시뮬레이션하도록 했어. 이 방식은 두 개의 다른 임베딩을 하나의 출력으로 합치는 동시에 메모리 사용량과 시간 복잡성을 크게 줄일 수 있어.

Mamba의 내부 방정식을 수정해서 두 개의 서로 다른 데이터 스트림에서 입력을 받고 결합할 수 있게 만들었어. 우리가 아는 한, SSM의 방정식을 스타일 전송 같은 비전 작업에 적용한 첫 번째 시도야. 교차 주의나 맞춤형 정규화 레이어 같은 다른 모듈이 필요하지 않아.

광범위한 실험 결과, 우리의 방법이 변환기와 확산 모델에 비해 스타일 전송을 수행하는 데 있어서 우수성과 효율성을 보여줬어. 결과는 ArtFID와 FID 메트릭 모두에서 품질이 향상된 걸 보여줘. 코드도 이 URL에서 이용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10389.pdf

Title: Prompt-and-Transfer: Dynamic Class-aware Enhancement for Few-shot Segmentation

Original Abstract:
For more efficient generalization to unseen domains (classes), most Few-shot Segmentation (FSS) would directly exploit pre-trained encoders and only fine-tune the decoder, especially in the current era of large models. However, such fixed feature encoders tend to be class-agnostic, inevitably activating objects that are irrelevant to the target class. In contrast, humans can effortlessly focus on specific objects in the line of sight. This paper mimics the visual perception pattern of human beings and proposes a novel and powerful prompt-driven scheme, called ``Prompt and Transfer" (PAT), which constructs a dynamic class-aware prompting paradigm to tune the encoder for focusing on the interested object (target class) in the current task. Three key points are elaborated to enhance the prompting: 1) Cross-modal linguistic information is introduced to initialize prompts for each task. 2) Semantic Prompt Transfer (SPT) that precisely transfers the class-specific semantics within the images to prompts. 3) Part Mask Generator (PMG) that works in conjunction with SPT to adaptively generate different but complementary part prompts for different individuals. Surprisingly, PAT achieves competitive performance on 4 different tasks including standard FSS, Cross-domain FSS (e.g., CV, medical, and remote sensing domains), Weak-label FSS, and Zero-shot Segmentation, setting new state-of-the-arts on 11 benchmarks.

Translated Abstract:
보다 효율적으로 보지 못한 도메인(클래스)에 일반화하려면, 대부분의 Few-shot Segmentation(FSS) 방식이 사전 훈련된 인코더를 직접 활용하고 디코더만 미세 조정하는 방법을 사용해. 특히 요즘처럼 큰 모델이 있는 시대에는 더 그렇지. 하지만 이렇게 고정된 특징 인코더는 클래스에 상관없이 물체를 활성화시키는 경향이 있어서, 목표 클래스와 관련 없는 물체도 반응하게 돼. 반면에 사람은 시야에 있는 특정 물체에 쉽게 집중할 수 있어.

이 논문은 인간의 시각 인식 패턴을 모방해서 "Prompt and Transfer"(PAT)라는 새로운 강력한 프롬프트 기반 방안을 제안해. 이 방법은 현재 작업에서 관심 있는 물체(목표 클래스)에 집중하도록 인코더를 조정할 수 있는 동적 클래스 인식 프롬프트 패러다임을 구축해. 여기서 강조하는 세 가지 주요 포인트가 있어:

1) 각 작업에 대한 프롬프트를 초기화하기 위해 교차 모달 언어 정보를 도입해.
2) 이미지 내 클래스 특유의 의미를 프롬프트로 정확하게 전이하는 Semantic Prompt Transfer(SPT)를 사용해.
3) SPT와 함께 작동하여 서로 다른 개인을 위해 다르게 하지만 보완적인 부분 프롬프트를 적응적으로 생성하는 Part Mask Generator(PMG)가 있어.

놀랍게도, PAT는 표준 FSS, 교차 도메인 FSS(예: CV, 의료, 원격 탐지 도메인), 약한 레이블 FSS, 제로샷 세그멘테이션을 포함한 4가지 작업에서 경쟁력 있는 성능을 달성하며, 11개의 벤치마크에서 새로운 최첨단 결과를 세웠어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10422.pdf

Title: Learning Semi-Supervised Medical Image Segmentation from Spatial Registration

Original Abstract:
Semi-supervised medical image segmentation has shown promise in training models with limited labeled data and abundant unlabeled data. However, state-of-the-art methods ignore a potentially valuable source of unsupervised semantic information -- spatial registration transforms between image volumes. To address this, we propose CCT-R, a contrastive cross-teaching framework incorporating registration information. To leverage the semantic information available in registrations between volume pairs, CCT-R incorporates two proposed modules: Registration Supervision Loss (RSL) and Registration-Enhanced Positive Sampling (REPS). The RSL leverages segmentation knowledge derived from transforms between labeled and unlabeled volume pairs, providing an additional source of pseudo-labels. REPS enhances contrastive learning by identifying anatomically-corresponding positives across volumes using registration transforms. Experimental results on two challenging medical segmentation benchmarks demonstrate the effectiveness and superiority of CCT-R across various semi-supervised settings, with as few as one labeled case. Our code is available at this https URL.

Translated Abstract:
반지도 의료 이미지 분할은 제한된 라벨 데이터와 풍부한 비라벨 데이터로 모델을 훈련하는 데 좋은 가능성을 보였어. 하지만 최신 방법들은 이미지 볼륨 간의 공간 정합 변환 같은 중요한 비지도 의미 정보를 무시하고 있어. 

이 문제를 해결하기 위해 우리는 CCT-R이라는, 정합 정보를 포함한 대조적 교차 교육 프레임워크를 제안해. 볼륨 쌍 간의 정합에서 얻을 수 있는 의미 정보를 활용하기 위해 CCT-R은 두 가지 모듈을 포함하고 있어: 정합 감독 손실(RSL)과 정합 강화 긍정 샘플링(REPS). RSL은 라벨이 있는 볼륨 쌍과 비라벨 볼륨 쌍 간의 변환에서 얻은 분할 지식을 활용해 추가적인 의사 라벨을 제공해. REPS는 정합 변환을 사용해 볼륨 간에 해부학적으로 일치하는 긍정 샘플을 찾아 대조 학습을 강화해.

두 가지 어려운 의료 분할 벤치마크에서의 실험 결과는 다양한 반지도 설정에서 CCT-R의 효과와 우수성을 보여줘. 라벨이 하나만 있는 경우에도 가능하더라. 우리 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10445.pdf

Title: Deep-Wide Learning Assistance for Insect Pest Classification

Original Abstract:
Accurate insect pest recognition plays a critical role in agriculture. It is a challenging problem due to the intricate characteristics of insects. In this paper, we present DeWi, novel learning assistance for insect pest classification. With a one-stage and alternating training strategy, DeWi simultaneously improves several Convolutional Neural Networks in two perspectives: discrimination (by optimizing a triplet margin loss in a supervised training manner) and generalization (via data augmentation). From that, DeWi can learn discriminative and in-depth features of insect pests (deep) yet still generalize well to a large number of insect categories (wide). Experimental results show that DeWi achieves the highest performances on two insect pest classification benchmarks (76.44\% accuracy on the IP102 dataset and 99.79\% accuracy on the D0 dataset, respectively). In addition, extensive evaluations and ablation studies are conducted to thoroughly investigate our DeWi and demonstrate its superiority. Our source code is available at this https URL.

Translated Abstract:
정확한 곤충 해충 인식은 농업에서 정말 중요해. 하지만 곤충의 복잡한 특성 때문에 이 문제는 쉽지 않아. 이 논문에서는 곤충 해충 분류를 위한 새로운 학습 보조 도구인 DeWi를 소개해. 

DeWi는 한 단계의 교대 훈련 전략을 사용해서 여러 개의 합성곱 신경망을 동시에 개선해. 여기서 두 가지 관점이 있어: 하나는 구별력을 높이는 거고(감독 훈련 방식으로 트리플렛 마진 손실을 최적화함), 다른 하나는 일반화를 잘하는 거야(데이터 증대를 통해). 이렇게 해서 DeWi는 곤충 해충의 구별되고 깊이 있는 특징을 잘 배우면서도 많은 곤충 카테고리에 잘 일반화할 수 있어.

실험 결과, DeWi는 두 개의 곤충 해충 분류 벤치마크에서 가장 높은 성능을 보여줬어(IP102 데이터셋에서 76.44% 정확도, D0 데이터셋에서 99.79% 정확도). 게다가, 우리의 DeWi를 자세히 조사하고 그 우수성을 입증하기 위해 많은 평가와 소거 연구도 진행했어. 우리의 소스 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10473.pdf

Title: MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion

Original Abstract:
Self-supervised learning has proved effective for skeleton-based human action understanding. However, previous works either rely on contrastive learning that suffers false negative problems or are based on reconstruction that learns too much unessential low-level clues, leading to limited representations for downstream tasks. Recently, great advances have been made in generative learning, which is naturally a challenging yet meaningful pretext task to model the general underlying data distributions. However, the representation learning capacity of generative models is under-explored, especially for the skeletons with spacial sparsity and temporal redundancy. To this end, we propose Masked Conditional Diffusion (MacDiff) as a unified framework for human skeleton modeling. For the first time, we leverage diffusion models as effective skeleton representation learners. Specifically, we train a diffusion decoder conditioned on the representations extracted by a semantic encoder. Random masking is applied to encoder inputs to introduce a information bottleneck and remove redundancy of skeletons. Furthermore, we theoretically demonstrate that our generative objective involves the contrastive learning objective which aligns the masked and noisy views. Meanwhile, it also enforces the representation to complement for the noisy view, leading to better generalization performance. MacDiff achieves state-of-the-art performance on representation learning benchmarks while maintaining the competence for generative tasks. Moreover, we leverage the diffusion model for data augmentation, significantly enhancing the fine-tuning performance in scenarios with scarce labeled data. Our project is available at this https URL.

Translated Abstract:
자기 지도 학습은 인체 동작 이해에 있어 뼈대 기반 접근 방식에 효과적인 것으로 입증되었습니다. 하지만 이전 연구들은 대조 학습에 의존하거나, 너무 많은 불필요한 저수준의 단서를 학습하는 재구성 기반 접근 방식을 사용했습니다. 이로 인해 후속 작업에 대한 표현력이 제한적이었습니다.

최근에는 일반적인 데이터 분포를 모델링하기 위한 도전적이지만 의미 있는 전제 작업으로서 생성 학습이 크게 발전했습니다. 하지만 생성 모델의 표현 학습 능력은 충분히 탐구되지 않았습니다. 특히 공간적 희소성과 시간적 중복성이 있는 뼈대에 대해서는 더욱 그렇습니다.

이 문제를 해결하기 위해 우리는 인간 뼈대 모델링을 위한 통합 프레임워크인 Masked Conditional Diffusion (MacDiff)를 제안합니다. 처음으로, 우리는 확산 모델을 효과적인 뼈대 표현 학습기로 활용합니다. 구체적으로, 우리는 의미 인코더가 추출한 표현에 조건을 둔 확산 디코더를 학습시킵니다. 인코더 입력에 무작위 마스킹을 적용해 정보 병목 현상을 도입하고 뼈대의 중복성을 제거합니다.

게다가, 우리의 생성 목표가 마스킹된 노이즈 뷰와 정렬되는 대조 학습 목표를 포함한다는 것을 이론적으로 보여줍니다. 동시에, 이는 표현이 노이즈 뷰를 보완하도록 강제하여 더 나은 일반화 성능을 이끌어냅니다. MacDiff는 표현 학습 벤치마크에서 최첨단 성능을 달성하면서 생성 작업에 대한 능력도 유지합니다.

더불어, 우리는 데이터 증강을 위해 확산 모델을 활용하여 레이블이 부족한 상황에서 미세 조정 성능을 크게 향상시킵니다. 우리의 프로젝트는 해당 URL에서 확인할 수 있습니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.10476.pdf

Title: SimInversion: A Simple Framework for Inversion-Based Text-to-Image Editing

Original Abstract:
Diffusion models demonstrate impressive image generation performance with text guidance. Inspired by the learning process of diffusion, existing images can be edited according to text by DDIM inversion. However, the vanilla DDIM inversion is not optimized for classifier-free guidance and the accumulated error will result in the undesired performance. While many algorithms are developed to improve the framework of DDIM inversion for editing, in this work, we investigate the approximation error in DDIM inversion and propose to disentangle the guidance scale for the source and target branches to reduce the error while keeping the original framework. Moreover, a better guidance scale (i.e., 0.5) than default settings can be derived theoretically. Experiments on PIE-Bench show that our proposal can improve the performance of DDIM inversion dramatically without sacrificing efficiency.

Translated Abstract:
확산 모델은 텍스트를 이용해 인상적인 이미지를 생성하는 능력을 보여줘. 확산 과정에서 영감을 받아, 기존 이미지를 텍스트에 맞게 DDIM 역전환을 통해 편집할 수 있어. 하지만 일반적인 DDIM 역전환은 분류기 없는 가이드를 최적화하지 않아서 누적된 오류가 발생해. 이로 인해 원하는 성능을 내지 못하게 돼.

DDIM 역전환을 개선하기 위한 여러 알고리즘이 개발됐지만, 이 연구에서는 DDIM 역전환에서의 근사 오류를 조사하고, 오류를 줄이기 위해 소스와 타겟 브랜치의 가이드를 분리하는 방법을 제안해. 이렇게 해도 원래의 프레임워크는 유지할 수 있어. 게다가, 이론적으로 기본 설정보다 더 나은 가이드 스케일(예: 0.5)을 도출할 수 있어.

PIE-Bench 실험 결과, 우리의 제안이 DDIM 역전환의 성능을 크게 개선하면서도 효율성을 희생하지 않는다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.10481.pdf

Title: Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance

Original Abstract:
3D face reconstruction (3DFR) algorithms are based on specific assumptions tailored to distinct application scenarios. These assumptions limit their use when acquisition conditions, such as the subject's distance from the camera or the camera's characteristics, are different than expected, as typically happens in video surveillance. Additionally, 3DFR algorithms follow various strategies to address the reconstruction of a 3D shape from 2D data, such as statistical model fitting, photometric stereo, or deep learning. In the present study, we explore the application of three 3DFR algorithms representative of the SOTA, employing each one as the template set generator for a face verification system. The scores provided by each system are combined by score-level fusion. We show that the complementarity induced by different 3DFR algorithms improves performance when tests are conducted at never-seen-before distances from the camera and camera characteristics (cross-distance and cross-camera settings), thus encouraging further investigations on multiple 3DFR-based approaches.

Translated Abstract:
3D 얼굴 재구성(3DFR) 알고리즘은 특정 가정에 기반해서 만들어져 있어, 각각 다른 용도에 맞게 조정되어 있어. 이런 가정 덕분에 카메라와의 거리나 카메라의 특성 같은 조건이 예상과 다를 때 사용이 제한될 수 있어. 이건 보통 비디오 감시에서 자주 발생하는 문제야.

또한, 3DFR 알고리즘은 2D 데이터에서 3D 형태를 재구성하기 위해 다양한 전략을 사용해. 예를 들어 통계 모델 적합, 광학 스테레오, 딥러닝 같은 방법들이 있어. 이번 연구에서는 최신 기술을 대표하는 세 가지 3DFR 알고리즘을 살펴보고, 각 알고리즘을 얼굴 검증 시스템의 템플릿 세트 생성기로 사용했어.

각 시스템이 제공하는 점수는 점수 수준에서 융합돼. 우리는 서로 다른 3DFR 알고리즘이 서로 보완적인 역할을 하면서 카메라와의 거리나 카메라의 특성이 전혀 다른 상황에서도 성능을 개선할 수 있다는 걸 보여줬어. 그래서 앞으로 여러 3DFR 기반 접근법에 대한 더 많은 연구가 필요하다는 걸 강조해.

================================================================================

URL:
https://arxiv.org/pdf/2409.10488.pdf

Title: Do Pre-trained Vision-Language Models Encode Object States?

Original Abstract:
For a vision-language model (VLM) to understand the physical world, such as cause and effect, a first step is to capture the temporal dynamics of the visual world, for example how the physical states of objects evolve over time (e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs pre-trained on web-scale data learn to encode object states, which can be extracted with zero-shot text prompts. We curate an object state recognition dataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models trained with contrastive and generative objectives. We observe that while these state-of-the-art vision-language models can reliably perform object recognition, they consistently fail to accurately distinguish the objects' physical states. Through extensive experiments, we identify three areas for improvements for VLMs to better encode object states, namely the quality of object localization, the architecture to bind concepts to objects, and the objective to learn discriminative visual and language encoders on object states. Data and code are released.

Translated Abstract:
비전-언어 모델(VLM)이 물리적 세계, 예를 들어 원인과 결과를 이해하려면, 첫 번째 단계로 시각적 세계의 시간적 변화를 포착하는 게 필요해. 예를 들어, 전체 사과가 썰린 사과로 변하는 과정을 말하는 거야. 우리 논문은 웹 규모의 데이터로 미리 훈련된 VLM이 객체 상태를 인코딩하는 방법을 배웠는지 알아보려고 해. 이 객체 상태는 제로샷 텍스트 프롬프트로 추출할 수 있어.

우리는 ChangeIt-Frames라는 객체 상태 인식 데이터셋을 만들었고, 대조적 및 생성적 목표로 훈련된 모델을 포함해 아홉 개의 오픈 소스 VLM을 평가했어. 이 최첨단 비전-언어 모델들이 객체 인식을 신뢰성 있게 수행할 수 있지만, 객체의 물리적 상태를 정확하게 구별하는 데에는 지속적으로 실패한다는 걸 관찰했어.

우리는 광범위한 실험을 통해 VLM이 객체 상태를 더 잘 인코딩하기 위해 개선해야 할 세 가지 분야를 찾았어. 첫째는 객체 위치 인식의 질, 둘째는 개념을 객체와 연결하는 아키텍처, 셋째는 객체 상태에 대한 차별적 시각 및 언어 인코더를 배우는 목표야. 데이터와 코드는 공개했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09039.pdf

Title: AutoGeo: Automating Geometric Image Dataset Creation for Enhanced Geometry Understanding

Original Abstract:
With the rapid advancement of large language models, there has been a growing interest in their capabilities in mathematical reasoning. However, existing research has primarily focused on text-based algebra problems, neglecting the study of geometry due to the lack of high-quality geometric datasets. To address this gap, this paper introduces AutoGeo, a novel approach for automatically generating mathematical geometric images to fulfill the demand for large-scale and diverse geometric datasets. AutoGeo facilitates the creation of AutoGeo-100k, an extensive repository comprising 100k high-quality geometry image-text pairs. By leveraging precisely defined geometric clauses, AutoGeo-100k contains a wide variety of geometric shapes, including lines, polygons, circles, and complex spatial relationships, etc. Furthermore, this paper demonstrates the efficacy of AutoGeo-100k in enhancing the performance of multimodal large language models through fine-tuning. Experimental results indicate significant improvements in the model's ability in handling geometric images, as evidenced by enhanced accuracy in tasks such as geometric captioning and mathematical reasoning. This research not only fills a critical gap in the availability of geometric datasets but also paves the way for the advancement of sophisticated AI-driven tools in education and research. Project page: this https URL.

Translated Abstract:
최근 대형 언어 모델이 빠르게 발전하면서, 수학적 추론 능력에 대한 관심이 높아지고 있어. 그런데 지금까지 연구들은 주로 텍스트 기반의 대수 문제에 집중했지, 기하학에 대한 연구는 고품질 기하학 데이터셋이 부족해서 잘 이루어지지 않았어. 

이런 문제를 해결하기 위해 이 논문에서는 AutoGeo라는 새로운 방법을 소개해. AutoGeo는 대규모의 다양한 기하학 데이터셋을 만들기 위해 수학적 기하학 이미지를 자동으로 생성하는 시스템이야. 이를 통해 AutoGeo-100k라는 10만 개의 고품질 기하학 이미지-텍스트 쌍으로 이루어진 방대한 저장소를 만들었어. AutoGeo-100k는 정확히 정의된 기하학 문구를 활용해서 선, 다각형, 원, 그리고 복잡한 공간 관계 등 다양한 기하학적 형태를 포함하고 있어.

또한, 이 논문에서는 AutoGeo-100k가 다중 모달 대형 언어 모델의 성능을 향상시키는 데 효과적이라는 것을 보여주고 있어. 실험 결과는 기하학 이미지를 처리하는 모델의 능력이 상당히 개선되었다는 걸 나타내고, 기하학 캡셔닝이나 수학적 추론 같은 작업에서 정확도가 높아졌어. 

이 연구는 기하학 데이터셋의 부족 문제를 해결할 뿐 아니라, 교육과 연구에서 복잡한 AI 도구의 발전을 위한 길을 열어줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09052.pdf

Title: OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography

Original Abstract:
Multimodal large language models (MLLMs) have achieved significant success in the general field of image processing. Their emerging task generalization and freeform conversational capabilities can greatly facilitate medical diagnostic assistance, helping patients better understand their conditions and enhancing doctor-patient trust. Computed Tomography (CT) is a non-invasive imaging technique used to capture the internal mechanisms of a patient's condition and is widely utilized. However, in past research, the complex textural features of this imaging data have made accurate interpretation by algorithms challenging, impeding the performance of general LLMs in diagnostic assistance. To address this, we developed OrthoDoc, a MLLM designed for CT diagnostics. OrthoDoc is trained on 120,000 CT images and diagnostic reports and includes a Retrieval-Augmented Generation (RAG) module capable of effectively mitigating model hallucinations. This module is informed by extensive medical literature, textbooks, and explanatory data. Thus, OrthoDoc not only processes complex CT images but also stores, understands, and reasons over medical knowledge and language. In extensive experiments, OrthoDoc outperforms commercial models led by GPT-4, demonstrating superior diagnostic capabilities and accuracy. Specifically, OrthoDoc significantly surpasses existing models in the diagnosis of common orthopedic conditions such as fractures, arthritis, and tumors. Additionally, OrthoDoc exhibits robust generalization and stability when handling rare and complex cases.

Translated Abstract:
다중 모달 대형 언어 모델(MLLMs)은 이미지 처리 분야에서 큰 성공을 거두었어. 이 모델들은 다양한 작업을 잘 수행하고 자연스러운 대화를 할 수 있어서, 의료 진단을 도와주고 환자들이 자신의 상태를 더 잘 이해할 수 있도록 도와줘. 이로 인해 의사와 환자 간의 신뢰도 향상될 수 있어.

CT(Computed Tomography)는 환자의 내부 상태를 비침습적으로 촬영하는 기술로, 널리 사용되고 있어. 하지만 이전 연구에서는 CT 이미지의 복잡한 텍스처 특성 때문에 알고리즘이 이를 정확하게 해석하기 어려웠고, 그로 인해 일반 LLM들의 진단 보조 성능이 저하됐어. 그래서 우리는 CT 진단을 위해 OrthoDoc라는 MLLM을 개발했어. 

OrthoDoc는 120,000개의 CT 이미지와 진단 보고서를 바탕으로 훈련되었고, 모델의 환각 현상을 효과적으로 줄일 수 있는 Retrieval-Augmented Generation(RAG) 모듈을 포함하고 있어. 이 모듈은 방대한 의료 문헌, 교과서, 설명 데이터를 참고해. 따라서 OrthoDoc는 복잡한 CT 이미지를 처리할 뿐만 아니라, 의료 지식과 언어를 저장하고 이해하며 논리적으로 추론할 수 있어.

광범위한 실험 결과, OrthoDoc는 GPT-4 같은 상업적인 모델보다 뛰어난 진단 능력과 정확성을 보여줬어. 특히, 골절, 관절염, 종양 같은 일반적인 정형외과 질환 진단에서 기존 모델들을 크게 초월했어. 또한 OrthoDoc는 드물고 복잡한 사례를 다룰 때도 강력한 일반화 능력과 안정성을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09053.pdf

Title: Deep learning-based classification of breast cancer molecular subtypes from H&E whole-slide images

Original Abstract:
Classifying breast cancer molecular subtypes is crucial for tailoring treatment strategies. While immunohistochemistry (IHC) and gene expression profiling are standard methods for molecular subtyping, IHC can be subjective, and gene profiling is costly and not widely accessible in many regions. Previous approaches have highlighted the potential application of deep learning models on H&E-stained whole slide images (WSI) for molecular subtyping, but these efforts vary in their methods, datasets, and reported performance. In this work, we investigated whether H&E-stained WSIs could be solely leveraged to predict breast cancer molecular subtypes (luminal A, B, HER2-enriched, and Basal). We used 1,433 WSIs of breast cancer in a two-step pipeline: first, classifying tumor and non-tumor tiles to use only the tumor regions for molecular subtyping; and second, employing a One-vs-Rest (OvR) strategy to train four binary OvR classifiers and aggregating their results using an eXtreme Gradient Boosting (XGBoost) model. The pipeline was tested on 221 hold-out WSIs, achieving an overall macro F1 score of 0.95 for tumor detection and 0.73 for molecular subtyping. Our findings suggest that, with further validation, supervised deep learning models could serve as supportive tools for molecular subtyping in breast cancer. Our codes are made available to facilitate ongoing research and development.

Translated Abstract:
유방암의 분자 아형을 분류하는 것은 맞춤형 치료 전략을 세우는 데 매우 중요해. 일반적으로 면역조직화학(IHC)과 유전자 발현 프로파일링이 분자 아형 분류에 사용되는데, IHC는 주관적일 수 있고, 유전자 프로파일링은 비용이 많이 들고 많은 지역에서 접근하기 어려워. 

이전 연구들은 H&E 염색된 전체 슬라이드 이미지(WSI)를 이용한 딥러닝 모델의 가능성을 강조했지만, 방법, 데이터셋, 성능 보고에서 차이가 있었어. 이번 연구에서는 H&E 염색된 WSI만을 이용해 유방암 분자 아형(루미널 A, B, HER2-풍부형, 기저 세포형)을 예측할 수 있는지 조사했어. 

우리는 1,433개의 유방암 WSI를 두 단계로 나눠서 처리했어. 첫 번째 단계에서는 종양과 비종양 타일을 분류해서 종양 영역만 사용했어. 두 번째 단계에서는 One-vs-Rest (OvR) 전략을 사용해 네 개의 이진 OvR 분류기를 훈련시키고, 그 결과를 eXtreme Gradient Boosting (XGBoost) 모델로 통합했어. 이 파이프라인은 221개의 홀드아웃 WSI에서 테스트했는데, 종양 탐지에 대해 전체 매크로 F1 점수가 0.95, 분자 아형 분류에 대해 0.73을 기록했어. 

우리의 연구 결과는 추가 검증을 통해 감독된 딥러닝 모델이 유방암의 분자 아형 분류에 도움이 될 수 있다는 걸 시사해. 연구와 개발을 계속할 수 있도록 우리의 코드를 공개했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09085.pdf

Title: HESSO: Towards Automatic Efficient and User Friendly Any Neural Network Training and Pruning

Original Abstract:
Structured pruning is one of the most popular approaches to effectively compress the heavy deep neural networks (DNNs) into compact sub-networks while retaining performance. The existing methods suffer from multi-stage procedures along with significant engineering efforts and human expertise. The Only-Train-Once (OTO) series has been recently proposed to resolve the many pain points by streamlining the workflow by automatically conducting (i) search space generation, (ii) structured sparse optimization, and (iii) sub-network construction. However, the built-in sparse optimizers in the OTO series, i.e., the Half-Space Projected Gradient (HSPG) family, have limitations that require hyper-parameter tuning and the implicit controls of the sparsity exploration, consequently requires intervening by human expertise. To address such limitations, we propose a Hybrid Efficient Structured Sparse Optimizer (HESSO). HESSO could automatically and efficiently train a DNN to produce a high-performing subnetwork. Meanwhile, it is almost tuning-free and enjoys user-friendly integration for generic training applications. To address another common issue of irreversible performance collapse observed in pruning DNNs, we further propose a Corrective Redundant Identification Cycle (CRIC) for reliably identifying indispensable structures. We numerically demonstrate the efficacy of HESSO and its enhanced version HESSO-CRIC on a variety of applications ranging from computer vision to natural language processing, including large language model. The numerical results showcase that HESSO can achieve competitive even superior performance to varying state-of-the-arts and support most DNN architectures. Meanwhile, CRIC can effectively prevent the irreversible performance collapse and further enhance the performance of HESSO on certain applications. The code is available at this https URL.

Translated Abstract:
구조적 프루닝은 무거운 딥 뉴럴 네트워크(DNN)를 성능을 유지하면서 컴팩트한 서브 네트워크로 압축하는 데 가장 인기 있는 방법 중 하나야. 기존 방법들은 여러 단계의 절차와 많은 엔지니어링 노력, 그리고 사람의 전문 지식을 필요로 해서 좀 불편해. 최근에 제안된 Only-Train-Once(OTO) 시리즈는 이 문제를 해결하기 위해 워크플로우를 간소화했어. 이건 (i) 탐색 공간 생성, (ii) 구조적 희소 최적화, (iii) 서브 네트워크 구축을 자동으로 진행해.

하지만 OTO 시리즈에 내장된 희소 최적화기인 Half-Space Projected Gradient(HSPG) 계열은 하이퍼 파라미터 조정과 희소성 탐색의 암묵적 제어가 필요해. 그래서 결국 사람의 개입이 필요해. 이런 한계를 해결하기 위해 우리는 하이브리드 효율 구조적 희소 최적화기(HESSO)를 제안해. HESSO는 DNN을 자동으로 그리고 효율적으로 훈련시켜서 높은 성능의 서브 네트워크를 만들어낼 수 있어. 게다가 거의 조정이 필요 없고 일반적인 훈련 애플리케이션에 쉽게 통합할 수 있어.

DNN을 프루닝할 때 발생하는 또 다른 일반적인 문제인 성능 붕괴를 방지하기 위해, 우리는 필수 구조를 신뢰성 있게 식별하기 위한 수정된 잉여 식별 주기(CRIC)를 추가로 제안해. 우리는 HESSO와 그 향상된 버전인 HESSO-CRIC의 효과를 컴퓨터 비전부터 자연어 처리, 그리고 대형 언어 모델에 이르기까지 다양한 애플리케이션에서 수치적으로 보여줬어. 수치 결과는 HESSO가 여러 최첨단 기술과 경쟁할 수 있는 성능을 달성할 수 있음을 보여주고, 대부분의 DNN 아키텍처를 지원해. 한편, CRIC는 성능 붕괴를 효과적으로 방지하고 특정 애플리케이션에서 HESSO의 성능을 더욱 향상시킬 수 있어. 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09086.pdf

Title: Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language Models on a Single GPU

Original Abstract:
Multimodal Large Language Models (MLLMs) are distinguished by their multimodal comprehensive ability and widely used in many real-world applications including GPT-4o, autonomous driving and robotics. Despite their impressive performance, the multimodal inputs always incur long context. The inference under long context requires caching massive Key and Value states (KV cache) of previous tokens, which introduces high latency and excessive memory consumption. Due to this reason, it is challenging to deploy streaming inference of MLLMs on edge devices, which largely constrains the power and usage of MLLMs in real-world applications. In this paper, we introduce Inf-MLLM, an efficient inference framework for MLLMs, which enable streaming inference of MLLM on a single GPU with infinite context. Inf-MLLM is based on our key observation of the attention pattern in both LLMs and MLLMs called "attention saddles". Thanks to the newly discovered attention pattern, Inf-MLLM maintains a size-constrained KV cache by dynamically caching recent tokens and relevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel approach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM enables multiple LLMs and MLLMs to achieve stable performance over 4M-token long texts and multi-round conversations with 1-hour-long videos on a single GPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than existing methods such as StreamingLLM and 2x speedup than H2O.

Translated Abstract:
멀티모달 대형 언어 모델(MLLMs)은 여러 가지 입력을 다룰 수 있는 능력이 뛰어나고, GPT-4o, 자율주행, 로봇공학 등 다양한 실제 응용 분야에서 널리 사용되고 있어. 하지만 이 모델들은 긴 컨텍스트를 요구하기 때문에, 이전 토큰의 많은 키와 값 상태(KV 캐시)를 저장해야 해. 이 과정에서 높은 지연 시간과 과도한 메모리 소모가 발생하지. 이런 이유로, MLLMs를 엣지 디바이스에서 스트리밍 추론으로 구현하는 게 어려워서 실제 응용 분야에서 MLLMs의 활용이 제한돼.

이 논문에서는 Inf-MLLM이라는 효율적인 추론 프레임워크를 소개해. 이 프레임워크는 무한한 컨텍스트를 가지고 단일 GPU에서 MLLM의 스트리밍 추론을 가능하게 해. Inf-MLLM은 LLM과 MLLM에서 관찰된 "어텐션 새들"이라는 어텐션 패턴에 기반하고 있어. 새롭게 발견된 이 어텐션 패턴 덕분에, Inf-MLLM은 최근 토큰과 관련 토큰을 동적으로 캐싱하면서 크기가 제한된 KV 캐시를 유지할 수 있어.

더 나아가, Inf-MLLM은 어텐션 바이어스라는 새로운 접근 방식을 제안해 MLLM이 장기적인 의존성을 포착할 수 있도록 도와. 우리는 Inf-MLLM이 여러 LLM과 MLLM이 4M 토큰 길이의 텍스트와 1시간 길이의 비디오에서 다중 라운드 대화를 안정적으로 수행할 수 있게 해준다는 것을 보여줬어. 또한, Inf-MLLM은 기존의 방법들인 StreamingLLM보다 스트리밍 추론 품질이 뛰어나고, H2O보다 2배 빠른 성능을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09108.pdf

Title: Trimming the Risk: Towards Reliable Continuous Training for Deep Learning Inspection Systems

Original Abstract:
The industry increasingly relies on deep learning (DL) technology for manufacturing inspections, which are challenging to automate with rule-based machine vision algorithms. DL-powered inspection systems derive defect patterns from labeled images, combining human-like agility with the consistency of a computerized system. However, finite labeled datasets often fail to encompass all natural variations necessitating Continuous Training (CT) to regularly adjust their models with recent data. Effective CT requires fresh labeled samples from the original distribution; otherwise, selfgenerated labels can lead to silent performance degradation. To mitigate this risk, we develop a robust CT-based maintenance approach that updates DL models using reliable data selections through a two-stage filtering process. The initial stage filters out low-confidence predictions, as the model inherently discredits them. The second stage uses variational auto-encoders and histograms to generate image embeddings that capture latent and pixel characteristics, then rejects the inputs of substantially shifted embeddings as drifted data with erroneous overconfidence. Then, a fine-tuning of the original DL model is executed on the filtered inputs while validating on a mixture of recent production and original datasets. This strategy mitigates catastrophic forgetting and ensures the model adapts effectively to new operational conditions. Evaluations on industrial inspection systems for popsicle stick prints and glass bottles using critical real-world datasets showed less than 9% of erroneous self-labeled data are retained after filtering and used for fine-tuning, improving model performance on production data by up to 14% without compromising its results on original validation data.

Translated Abstract:
산업계에서는 제조 검사에 딥러닝(DL) 기술을 점점 더 많이 사용하고 있어. 기존의 규칙 기반 기계 비전 알고리즘으로 자동화하기 어려운 작업이거든. DL 기반 검사 시스템은 라벨이 붙은 이미지에서 결함 패턴을 찾아내고, 사람처럼 빠르게 움직이면서도 컴퓨터 시스템의 일관성을 갖고 있어.

하지만 한정된 라벨 데이터셋만으로는 자연에서 발생할 수 있는 모든 변화를 포괄하기 어려워서, 계속해서 모델을 최신 데이터로 조정하는 Continuous Training(CT)이 필요해. 효과적인 CT를 위해서는 원래 배포에서 나오는 새로운 라벨 샘플이 필요해. 그렇지 않으면 자체 생성된 라벨이 성능 저하를 초래할 수 있어.

이런 위험을 줄이기 위해, 우리는 신뢰할 수 있는 데이터 선택을 통해 DL 모델을 업데이트하는 강력한 CT 기반 유지보수 방법을 개발했어. 첫 번째 단계에서는 확신이 적은 예측을 걸러내고, 모델이 본래 그것들을 신뢰하지 않거든. 두 번째 단계에서는 변분 오토인코더와 히스토그램을 사용해서 이미지 임베딩을 생성하고, 잠재적인 특성과 픽셀 특성을 포착해. 그런 다음, 상당히 변화된 임베딩을 드리프트된 데이터로 간주하고 제거해.

그 후, 필터링된 입력에 대해 원래 DL 모델을 미세 조정하고, 최근 생산 데이터와 원래 데이터셋을 섞어서 검증해. 이 전략은 재앙적인 망각을 줄이고 모델이 새로운 운영 조건에 효과적으로 적응하도록 해. 실제 산업 검사 시스템인 아이스크림 막대 인쇄와 유리병을 대상으로 한 평가에서는 필터링 후 9% 미만의 잘못된 자체 라벨 데이터가 남았고, 이 데이터를 사용해서 미세 조정을 했어. 그 결과, 원래 검증 데이터의 성능을 해치지 않으면서 생산 데이터에서 모델 성능이 최대 14% 향상됐어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09173.pdf

Title: Phikon-v2, A large and public feature extractor for biomarker prediction

Original Abstract:
Gathering histopathology slides from over 100 publicly available cohorts, we compile a diverse dataset of 460 million pathology tiles covering more than 30 cancer sites. Using this dataset, we train a large self-supervised vision transformer using DINOv2 and publicly release one iteration of this model for further experimentation, coined Phikon-v2. While trained on publicly available histology slides, Phikon-v2 surpasses our previously released model (Phikon) and performs on par with other histopathology foundation models (FM) trained on proprietary data. Our benchmarks include eight slide-level tasks with results reported on external validation cohorts avoiding any data contamination between pre-training and evaluation datasets. Our downstream training procedure follows a simple yet robust ensembling strategy yielding a +1.75 AUC increase across tasks and models compared to one-shot retraining (p<0.001). We compare Phikon (ViT-B) and Phikon-v2 (ViT-L) against 14 different histology feature extractors, making our evaluation the most comprehensive to date. Our result support evidences that DINOv2 handles joint model and data scaling better than iBOT. Also, we show that recent scaling efforts are overall beneficial to downstream performance in the context of biomarker prediction with GigaPath and H-Optimus-0 (two ViT-g with 1.1B parameters each) standing out. However, the statistical margins between the latest top-performing FMs remain mostly non-significant; some even underperform on specific indications or tasks such as MSI prediction - deposed by a 13x smaller model developed internally. While latest foundation models may exhibit limitations for clinical deployment, they nonetheless offer excellent grounds for the development of more specialized and cost-efficient histology encoders fueling AI-guided diagnostic tools.

Translated Abstract:
100개 이상의 공개된 데이터셋에서 병리학 슬라이드를 모아서, 30개 이상의 암 유형을 포함한 4억 6천만 개의 다양한 병리 타일로 구성된 데이터셋을 만들었어. 이 데이터셋을 사용해서 DINOv2를 활용한 큰 자기 지도 비전 변환기 모델을 훈련시켰고, 이 모델의 한 버전을 Phikon-v2라는 이름으로 공개했어. 

Phikon-v2는 공개된 조직 슬라이드로 훈련되었지만, 이전에 발표한 모델인 Phikon보다 성능이 더 뛰어나고, 비공식 데이터로 훈련된 다른 병리학 기반 모델들과 비슷한 성능을 보여. 우리의 벤치마크는 8개의 슬라이드 수준 작업을 포함하고, 사전 훈련 데이터와 평가 데이터 간의 오염을 피하면서 외부 검증 데이터셋에서 결과를 보고했어. 

후속 훈련 과정은 간단하지만 강력한 앙상블 전략을 따르며, 이로 인해 작업과 모델에 따라 +1.75 AUC가 증가했어. Phikon (ViT-B)과 Phikon-v2 (ViT-L)를 14가지 다른 병리학 특징 추출기와 비교했는데, 이 평가가 지금까지 가장 포괄적이었어. 

우리 결과는 DINOv2가 iBOT보다 모델과 데이터 스케일링을 더 잘 처리한다는 것을 보여줘. 또, 최근 스케일링 노력들이 바이오마커 예측과 관련해서는 전반적으로 유익하다는 것을 입증했어. GigaPath와 H-Optimus-0(각각 11억 개의 파라미터를 가진 두 개의 ViT-g 모델)이 특히 두드러져. 하지만 최신의 상위 성능을 가진 모델들 간의 통계적 차이는 대부분 유의미하지 않았고, 일부는 MSI 예측 같은 특정 지표나 작업에서 오히려 작은 모델보다 성능이 떨어졌어. 

최신의 기반 모델들이 임상 배치에서 한계가 있을 수 있지만, 여전히 AI 기반 진단 도구를 발전시키는 데 필요한 더 전문화되고 비용 효율적인 병리학 인코더 개발의 훌륭한 기초를 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09188.pdf

Title: FiAt-Net: Detecting Fibroatheroma Plaque Cap in 3D Intravascular OCT Images

Original Abstract:
The key manifestation of coronary artery disease (CAD) is development of fibroatheromatous plaque, the cap of which may rupture and subsequently lead to coronary artery blocking and heart attack. As such, quantitative analysis of coronary plaque, its plaque cap, and consequently the cap's likelihood to rupture are of critical importance when assessing a risk of cardiovascular events. This paper reports a new deep learning based approach, called FiAt-Net, for detecting angular extent of fibroatheroma (FA) and segmenting its cap in 3D intravascular optical coherence tomography (IVOCT) images. IVOCT 2D image frames are first associated with distinct clusters and data from each cluster are used for model training. As plaque is typically focal and thus unevenly distributed, a binary partitioning method is employed to identify FA plaque areas to focus on to mitigate the data imbalance issue. Additional image representations (called auxiliary images) are generated to capture IVOCT intensity changes to help distinguish FA and non-FA areas on the coronary wall. Information in varying scales is derived from the original IVOCT and auxiliary images, and a multi-head self-attention mechanism is employed to fuse such information. Our FiAt-Net achieved high performance on a 3D IVOCT coronary image dataset, demonstrating its effectiveness in accurately detecting FA cap in IVOCT images.

Translated Abstract:
관상동맥질환(CAD)의 주요 증상은 섬유성 죽종(plaque) 형성인데, 이 죽종의 덮개가 찢어지면 관상동맥이 막히고 심장마비가 발생할 수 있어. 그래서 관상 죽종, 그 덮개, 그리고 덮개가 찢어질 확률을 정량적으로 분석하는 게 심혈관 사건의 위험을 평가할 때 정말 중요해. 

이 논문에서는 FiAt-Net이라는 새로운 딥러닝 기반 접근법을 소개할 건데, 이 방법은 3D 혈관 내 광학 단층촬영(IVOCT) 이미지에서 섬유성 죽종(FA)의 각도 범위를 감지하고, 그 덮개를 분할하는 데 사용돼. 먼저, IVOCT의 2D 이미지 프레임을 서로 다른 클러스터와 연결하고, 각 클러스터의 데이터를 모델 훈련에 활용해. 죽종은 보통 국소적이고 고르지 않게 분포되어 있어서, 데이터 불균형 문제를 줄이기 위해 이진 분할 방법을 사용해 FA 죽종 영역을 식별해.

또한, IVOCT의 강도 변화를 포착하기 위해 추가 이미지(보조 이미지)를 만들어 FA와 비FA 영역을 구분하는 데 도움을 줘. 원본 IVOCT 이미지와 보조 이미지에서 다양한 스케일의 정보를 추출하고, 다중 헤드 자기 주의 메커니즘을 활용해 이런 정보를 융합해. 우리 FiAt-Net은 3D IVOCT 관상 이미지 데이터셋에서 높은 성능을 보여줬고, IVOCT 이미지에서 FA 덮개를 정확하게 감지하는 데 효과적임을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09194.pdf

Title: Hierarchical Hypercomplex Network for Multimodal Emotion Recognition

Original Abstract:
Emotion recognition is relevant in various domains, ranging from healthcare to human-computer interaction. Physiological signals, being beyond voluntary control, offer reliable information for this purpose, unlike speech and facial expressions which can be controlled at will. They reflect genuine emotional responses, devoid of conscious manipulation, thereby enhancing the credibility of emotion recognition systems. Nonetheless, multimodal emotion recognition with deep learning models remains a relatively unexplored field. In this paper, we introduce a fully hypercomplex network with a hierarchical learning structure to fully capture correlations. Specifically, at the encoder level, the model learns intra-modal relations among the different channels of each input signal. Then, a hypercomplex fusion module learns inter-modal relations among the embeddings of the different modalities. The main novelty is in exploiting intra-modal relations by endowing the encoders with parameterized hypercomplex convolutions (PHCs) that thanks to hypercomplex algebra can capture inter-channel interactions within single modalities. Instead, the fusion module comprises parameterized hypercomplex multiplications (PHMs) that can model inter-modal correlations. The proposed architecture surpasses state-of-the-art models on the MAHNOB-HCI dataset for emotion recognition, specifically in classifying valence and arousal from electroencephalograms (EEGs) and peripheral physiological signals. The code of this study is available at this https URL.

Translated Abstract:
감정 인식은 헬스케어부터 인간-컴퓨터 상호작용까지 다양한 분야에서 중요해. 생리학적 신호는 자발적으로 제어할 수 없는 신호라서, 말이나 표정처럼 마음대로 조작할 수 없는 정보로 신뢰할 수 있어. 이런 신호는 진짜 감정 반응을 반영하니까 감정 인식 시스템의 신뢰성을 높여줘. 

하지만 딥러닝 모델을 활용한 다중 모드 감정 인식은 아직 많이 연구되지 않았어. 이 논문에서는 상관관계를 완전히 포착하기 위해 계층적 학습 구조를 가진 완전 하이퍼복소 네트워크를 소개해. 구체적으로, 인코더 단계에서 모델은 각 입력 신호의 서로 다른 채널 간의 내부 모드 관계를 학습해. 그 다음, 하이퍼복소 융합 모듈은 서로 다른 모드의 임베딩 간의 외부 모드 관계를 학습해. 

가장 새로운 점은 인코더에 매개변수화된 하이퍼복소 컨볼루션(PHC)을 도입해서 내부 모드 관계를 활용하는 거야. 하이퍼복소 대수 덕분에 단일 모드 내의 채널 간 상호작용을 포착할 수 있어. 반면에, 융합 모듈은 외부 모드 상관관계를 모델링할 수 있는 매개변수화된 하이퍼복소 곱셈(PHM)으로 구성돼. 

제안한 구조는 MAHNOB-HCI 데이터셋에서 감정 인식에 관한 최신 모델들을 초월해. 특히, 뇌파(EEG)와 주변 생리 신호에서 감정의 발양(valence)과 각성(arousal)을 분류하는 데 성능이 뛰어나. 이 연구의 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09216.pdf

Title: Spectral U-Net: Enhancing Medical Image Segmentation via Spectral Decomposition

Original Abstract:
This paper introduces Spectral U-Net, a novel deep learning network based on spectral decomposition, by exploiting Dual Tree Complex Wavelet Transform (DTCWT) for down-sampling and inverse Dual Tree Complex Wavelet Transform (iDTCWT) for up-sampling. We devise the corresponding Wave-Block and iWave-Block, integrated into the U-Net architecture, aiming at mitigating information loss during down-sampling and enhancing detail reconstruction during up-sampling. In the encoder, we first decompose the feature map into high and low-frequency components using DTCWT, enabling down-sampling while mitigating information loss. In the decoder, we utilize iDTCWT to reconstruct higher-resolution feature maps from down-sampled features. Evaluations on the Retina Fluid, Brain Tumor, and Liver Tumor segmentation datasets with the nnU-Net framework demonstrate the superiority of the proposed Spectral U-Net.

Translated Abstract:
이 논문에서는 스펙트럴 U-Net이라는 새로운 딥러닝 네트워크를 소개해. 이 네트워크는 스펙트럴 분해를 기반으로 하고, 다운샘플링을 위해 이중 트리 복소 웨이브렛 변환(DTCWT)을 활용하고, 업샘플링을 위해 역 이중 트리 복소 웨이브렛 변환(iDTCWT)을 사용해.

우리는 U-Net 구조에 통합된 Wave-Block과 iWave-Block을 개발했어. 이건 다운샘플링할 때 정보 손실을 줄이고, 업샘플링할 때 세부 정보를 더 잘 복원하는 데 목표가 있어. 인코더에서는 먼저 DTCWT를 사용해서 피처 맵을 고주파와 저주파 성분으로 분해해. 이렇게 하면 다운샘플링하면서도 정보 손실을 줄일 수 있어.

디코더에서는 iDTCWT를 이용해서 다운샘플링된 피처에서 더 높은 해상도의 피처 맵을 다시 만들어. Retina Fluid, Brain Tumor, Liver Tumor 세그멘테이션 데이터셋에서 nnU-Net 프레임워크로 평가한 결과, 제안한 스펙트럴 U-Net이 더 우수하다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09245.pdf

Title: Robust Training of Neural Networks at Arbitrary Precision and Sparsity

Original Abstract:
The discontinuous operations inherent in quantization and sparsification introduce obstacles to backpropagation. This is particularly challenging when training deep neural networks in ultra-low precision and sparse regimes. We propose a novel, robust, and universal solution: a denoising affine transform that stabilizes training under these challenging conditions. By formulating quantization and sparsification as perturbations during training, we derive a perturbation-resilient approach based on ridge regression. Our solution employs a piecewise constant backbone model to ensure a performance lower bound and features an inherent noise reduction mechanism to mitigate perturbation-induced corruption. This formulation allows existing models to be trained at arbitrarily low precision and sparsity levels with off-the-shelf recipes. Furthermore, our method provides a novel perspective on training temporal binary neural networks, contributing to ongoing efforts to narrow the gap between artificial and biological neural networks.

Translated Abstract:
양자화와 희소화에서 발생하는 불연속적인 작업은 역전파에 장애물을 만들어. 특히 초저 정밀도와 희소한 환경에서 딥 뉴럴 네트워크를 훈련할 때 더 어려워. 

우리는 새로운 강력하고 보편적인 해결책을 제안해: 훈련을 안정화하는 노이즈 감소 아핀 변환이야. 양자화와 희소화를 훈련 중의 변동으로 설정함으로써, 우리는 능선 회귀에 기반한 변동 저항 접근법을 도출했어. 

우리의 해결책은 성능 하한을 보장하는 조각별 상수 백본 모델을 사용하고, 변동으로 인한 손상을 줄이는 내재적 노이즈 감소 메커니즘을 갖추고 있어. 이 방식은 기존 모델들이 임의의 저정밀도와 희소성 수준에서 손쉽게 훈련될 수 있게 해. 

게다가 우리의 방법은 시간 이진 뉴럴 네트워크 훈련에 대한 새로운 시각을 제공하며, 인공지능 네트워크와 생물학적 네트워크 간의 격차를 좁히기 위한 노력에 기여하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09309.pdf

Title: Real-Time Stochastic Terrain Mapping and Processing for Autonomous Safe Landing

Original Abstract:
Onboard terrain sensing and mapping for safe planetary landings often suffer from missed hazardous features, e.g., small rocks, due to the large observational range and the limited resolution of the obtained terrain data. To this end, this paper develops a novel real-time stochastic terrain mapping algorithm that accounts for topographic uncertainty between the sampled points, or the uncertainty due to the sparse 3D terrain measurements. We introduce a Gaussian digital elevation map that is efficiently constructed using the combination of Delauney triangulation and local Gaussian process regression. The geometric investigation of the lander-terrain interaction is exploited to efficiently evaluate the marginally conservative local slope and roughness while avoiding the costly computation of the local plane. The conservativeness is proved in the paper. The developed real-time uncertainty quantification pipeline enables stochastic landing safety evaluation under challenging operational conditions, such as a large observational range or limited sensor capability, which is a critical stepping stone for the development of predictive guidance algorithms for safe autonomous planetary landing. Detailed reviews on background and related works are also presented.

Translated Abstract:
우주선이 안전하게 행성에 착륙하기 위해 필요한 지형 감지와 맵핑은 종종 작은 바위 같은 위험한 요소를 놓치는 경우가 많아. 그 이유는 관측 범위가 넓고 지형 데이터의 해상도가 제한적이기 때문이야. 

이 논문에서는 샘플링된 지점 사이의 지형 불확실성을 고려한 새로운 실시간 확률적 지형 맵핑 알고리즘을 개발했어. 여기서는 희소한 3D 지형 측정으로 인한 불확실성도 포함돼. 우리가 제안하는 것은 가우시안 디지털 고도 맵인데, 이건 Delauney 삼각분할과 지역 가우시안 프로세스 회귀를 결합해 효율적으로 구축돼.

착륙선과 지형 간의 상호작용을 기하학적으로 조사해서, 지역 경사와 거칠기를 효율적으로 평가할 수 있어. 이 과정에서 지역 평면을 계산하는 비용이 많이 드는 문제를 피할 수 있어. 논문에서는 이런 conservativeness가 어떻게 증명되는지도 다뤘어. 

이렇게 개발된 실시간 불확실성 정량화 파이프라인은, 넓은 관측 범위나 제한된 센서 능력 같은 어려운 상황에서도 안전한 착륙 평가를 가능하게 해. 이는 안전한 자율 행성 착륙을 위한 예측 안내 알고리즘 개발에 중요한 초석이 돼. 배경과 관련된 연구에 대한 자세한 리뷰도 포함되어 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09318.pdf

Title: ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models

Original Abstract:
Hallucination poses a significant challenge for multimodal large language models (MLLMs). However, existing benchmarks for evaluating hallucinations are static, which can lead to potential data contamination. This paper introduces ODE, an open-set, dynamic protocol for evaluating object existence hallucinations in MLLMs. Our framework employs graph structures to model associations between real-word concepts and generates novel samples for both general and domain-specific scenarios. The dynamic combination of concepts, along with various combination principles, ensures a broad sample distribution. Experimental results show that MLLMs exhibit higher hallucination rates with ODE-generated samples, effectively avoiding data contamination. Moreover, these samples can also be used for fine-tuning to improve MLLM performance on existing benchmarks.

Translated Abstract:
환각은 다중 모드 대형 언어 모델(MLLMs)에서 큰 도전 과제가 되고 있어. 하지만 현재 환각을 평가하는 기준은 정적이라서 데이터 오염이 발생할 수 있어. 이 논문에서는 ODE라는 새로운 프로토콜을 소개하는데, 이건 열린 세트로 동적인 방식으로 MLLMs에서 객체 존재 환각을 평가하는 거야.

우리의 프레임워크는 그래프 구조를 사용해서 실제 세계 개념 간의 관계를 모델링하고, 일반적인 상황과 특정 도메인에 맞는 새로운 샘플을 생성해. 개념의 동적인 조합과 다양한 조합 원칙 덕분에 샘플 분포가 넓어져. 실험 결과를 보면 ODE로 생성된 샘플을 사용할 때 MLLMs의 환각 비율이 더 높아지고, 데이터 오염을 잘 피할 수 있어.

게다가, 이런 샘플은 기존 기준에서 MLLM 성능을 개선하기 위해 파인튜닝에도 사용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09330.pdf

Title: VOMTC: Vision Objects for Millimeter and Terahertz Communications

Original Abstract:
Recent advances in sensing and computer vision (CV) technologies have opened the door for the application of deep learning (DL)-based CV technologies in the realm of 6G wireless communications. For the successful application of this emerging technology, it is crucial to have a qualified vision dataset tailored for wireless applications (e.g., RGB images containing wireless devices such as laptops and cell phones). An aim of this paper is to propose a large-scale vision dataset referred to as Vision Objects for Millimeter and Terahertz Communications (VOMTC). The VOMTC dataset consists of 20,232 pairs of RGB and depth images obtained from a camera attached to the base station (BS), with each pair labeled with three representative object categories (person, cell phone, and laptop) and bounding boxes of the objects. Through experimental studies of the VOMTC datasets, we show that the beamforming technique exploiting the VOMTC-trained object detector outperforms conventional beamforming techniques.

Translated Abstract:
최근 센싱과 컴퓨터 비전(CV) 기술의 발전 덕분에 6G 무선 통신 분야에서 딥러닝(DL) 기반 CV 기술을 적용할 수 있는 가능성이 열렸어. 이 새로운 기술을 성공적으로 활용하려면, 무선 애플리케이션에 맞춘 적절한 비전 데이터셋이 필요해. 예를 들어, 노트북이나 휴대폰 같은 무선 기기가 포함된 RGB 이미지 같은 것들이지.

이 논문의 목표는 '밀리미터 및 테라헤르츠 통신을 위한 비전 오브젝트(VOMTC)'라는 대규모 비전 데이터셋을 제안하는 거야. VOMTC 데이터셋은 기지국(BS)에 부착된 카메라로 얻은 20,232 쌍의 RGB 이미지와 깊이 이미지로 구성되어 있어. 각 쌍은 세 가지 대표적인 객체 카테고리(사람, 휴대폰, 노트북)와 객체의 경계 상자로 라벨링 되어 있어.

VOMTC 데이터셋을 활용한 실험 연구를 통해, VOMTC로 훈련된 객체 탐지기를 이용한 빔포밍 기술이 기존의 빔포밍 기술보다 성능이 뛰어나다는 것을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09361.pdf

Title: Beta-Sigma VAE: Separating beta and decoder variance in Gaussian variational autoencoder

Original Abstract:
Variational autoencoder (VAE) is an established generative model but is notorious for its blurriness. In this work, we investigate the blurry output problem of VAE and resolve it, exploiting the variance of Gaussian decoder and $\beta$ of beta-VAE. Specifically, we reveal that the indistinguishability of decoder variance and $\beta$ hinders appropriate analysis of the model by random likelihood value, and limits performance improvement by omitting the gain from $\beta$. To address the problem, we propose Beta-Sigma VAE (BS-VAE) that explicitly separates $\beta$ and decoder variance $\sigma^2_x$ in the model. Our method demonstrates not only superior performance in natural image synthesis but also controllable parameters and predictable analysis compared to conventional VAE. In our experimental evaluation, we employ the analysis of rate-distortion curve and proxy metrics on computer vision datasets. The code is available on this https URL

Translated Abstract:
변분 오토인코더(Variational Autoencoder, VAE)는 잘 알려진 생성 모델이지만, 출력 결과가 흐릿하다는 문제로 악명이 높아. 이번 연구에서는 VAE의 흐릿한 출력 문제를 조사하고 해결했어. 우리는 가우시안 디코더의 분산과 베타-VAE의 $\beta$를 활용했어.

특히, 디코더의 분산과 $\beta$의 구별이 모델의 적절한 분석을 방해하고, $\beta$에서 얻을 수 있는 이익을 무시함으로써 성능 향상을 제한한다는 것을 밝혔어. 이 문제를 해결하기 위해 우리는 베타-시그마 VAE(Beta-Sigma VAE, BS-VAE)를 제안했어. 이 모델은 $\beta$와 디코더의 분산 $\sigma^2_x$를 명확히 분리해.

우리의 방법은 자연 이미지 합성에서 뛰어난 성능을 보여줄 뿐만 아니라, 기존 VAE에 비해 조절 가능한 파라미터와 예측 가능한 분석을 제공해. 실험 평가에서는 비율-왜곡 곡선(rate-distortion curve) 분석과 컴퓨터 비전 데이터셋에 대한 대리 지표(proxy metrics)를 사용했어. 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09370.pdf

Title: MotionTTT: 2D Test-Time-Training Motion Estimation for 3D Motion Corrected MRI

Original Abstract:
A major challenge of the long measurement times in magnetic resonance imaging (MRI), an important medical imaging technology, is that patients may move during data acquisition. This leads to severe motion artifacts in the reconstructed images and volumes. In this paper, we propose a deep learning-based test-time-training method for accurate motion estimation. The key idea is that a neural network trained for motion-free reconstruction has a small loss if there is no motion, thus optimizing over motion parameters passed through the reconstruction network enables accurate estimation of motion. The estimated motion parameters enable to correct for the motion and to reconstruct accurate motion-corrected images. Our method uses 2D reconstruction networks to estimate rigid motion in 3D, and constitutes the first deep learning based method for 3D rigid motion estimation towards 3D-motion-corrected MRI. We show that our method can provably reconstruct motion parameters for a simple signal and neural network model. We demonstrate the effectiveness of our method for both retrospectively simulated motion and prospectively collected real motion-corrupted data.

Translated Abstract:
MRI(자기공명영상)는 중요한 의료 영상 기술인데, 측정 시간이 길어지면 환자가 움직일 수 있는 큰 문제가 있어. 이렇게 되면 재구성된 이미지와 볼륨에서 심각한 움직임 아티팩트가 생겨. 

이 논문에서는 정확한 움직임 추정을 위해 딥러닝 기반의 테스트 타임 트레이닝 방법을 제안해. 핵심 아이디어는, 움직임 없는 재구성을 위해 훈련된 신경망은 움직임이 없을 때 손실이 작다는 거야. 그래서 재구성 네트워크를 통해 전달된 움직임 매개변수를 최적화하면 정확한 움직임 추정이 가능해. 

추정된 움직임 매개변수는 움직임을 보정하고, 정확한 움직임 보정된 이미지를 재구성하는 데 도움이 돼. 우리 방법은 2D 재구성 네트워크를 사용해서 3D에서 강체 움직임을 추정하고, 3D 움직임 보정을 위한 첫 번째 딥러닝 기반 방법이야. 

우리는 이 방법이 간단한 신호와 신경망 모델에 대해 움직임 매개변수를 재구성할 수 있다는 것을 보여줬어. 또한, 회고적으로 시뮬레이션한 움직임과 실제로 수집된 움직임이 손상된 데이터 모두에서 이 방법의 효과를 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09387.pdf

Title: Estimating Neural Orientation Distribution Fields on High Resolution Diffusion MRI Scans

Original Abstract:
The Orientation Distribution Function (ODF) characterizes key brain microstructural properties and plays an important role in understanding brain structural connectivity. Recent works introduced Implicit Neural Representation (INR) based approaches to form a spatially aware continuous estimate of the ODF field and demonstrated promising results in key tasks of interest when compared to conventional discrete approaches. However, traditional INR methods face difficulties when scaling to large-scale images, such as modern ultra-high-resolution MRI scans, posing challenges in learning fine structures as well as inefficiencies in training and inference speed. In this work, we propose HashEnc, a grid-hash-encoding-based estimation of the ODF field and demonstrate its effectiveness in retaining structural and textural features. We show that HashEnc achieves a 10% enhancement in image quality while requiring 3x less computational resources than current methods. Our code can be found at this https URL.

Translated Abstract:
Orientation Distribution Function (ODF)는 뇌의 중요한 미세구조 특성을 설명하고, 뇌의 구조적 연결성을 이해하는 데 중요한 역할을 해. 최근 연구에서는 ODF 필드를 공간적으로 인식할 수 있는 연속적인 추정을 하기 위해 Implicit Neural Representation (INR) 기반의 접근법을 도입했어. 전통적인 방법에 비해 주요 작업에서 괜찮은 결과를 보여줬지.

하지만 기존의 INR 방법은 현대의 초고해상도 MRI 스캔 같은 대규모 이미지에 적용할 때 어려움이 있어. 이 때문에 세부 구조를 배우는 데 힘들고 훈련과 추론 속도가 비효율적이야. 

이 연구에서는 HashEnc라는 새로운 방법을 제안해. 이건 그리드 해시 인코딩을 기반으로 ODF 필드를 추정하는 방법인데, 구조적 및 텍스처 특성을 잘 유지하는 효과를 보여줬어. HashEnc는 기존 방법보다 3배 적은 계산 자원으로 이미지 품질을 10% 향상시켰다는 걸 보여줬어. 코드도 이 URL에서 찾아볼 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09478.pdf

Title: From FDG to PSMA: A Hitchhiker's Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging

Original Abstract:
Automated lesion segmentation in PET/CT scans is crucial for improving clinical workflows and advancing cancer diagnostics. However, the task is challenging due to physiological variability, different tracers used in PET imaging, and diverse imaging protocols across medical centers. To address this, the autoPET series was created to challenge researchers to develop algorithms that generalize across diverse PET/CT environments. This paper presents our solution for the autoPET III challenge, targeting multitracer, multicenter generalization using the nnU-Net framework with the ResEncL architecture. Key techniques include misalignment data augmentation and multi-modal pretraining across CT, MR, and PET datasets to provide an initial anatomical understanding. We incorporate organ supervision as a multitask approach, enabling the model to distinguish between physiological uptake and tracer-specific patterns, which is particularly beneficial in cases where no lesions are present. Compared to the default nnU-Net, which achieved a Dice score of 57.61, or the larger ResEncL (65.31) our model significantly improved performance with a Dice score of 68.40, alongside a reduction in false positive (FPvol: 7.82) and false negative (FNvol: 10.35) volumes. These results underscore the effectiveness of combining advanced network design, augmentation, pretraining, and multitask learning for PET/CT lesion segmentation. Code is publicly available at this https URL.

Translated Abstract:
PET/CT 스캔에서 병변 분할을 자동화하는 건 임상 작업 흐름을 개선하고 암 진단을 발전시키는 데 굉장히 중요해. 하지만 이 작업은 생리적 변동성, PET 이미징에 사용되는 다양한 트레이서, 그리고 의료 센터마다 다른 이미징 프로토콜 때문에 어려워. 

이런 문제를 해결하기 위해 autoPET 시리즈가 만들어졌고, 연구자들이 다양한 PET/CT 환경에서 일반화할 수 있는 알고리즘을 개발하도록 도전하고 있어. 이 논문에서는 autoPET III 챌린지를 위한 우리의 솔루션을 제시하는데, 멀티 트레이서와 멀티 센터 일반화를 목표로 nnU-Net 프레임워크와 ResEncL 아키텍처를 사용했어.

주요 기술로는 잘못 정렬된 데이터 증강과 CT, MR, PET 데이터셋을 통한 멀티 모달 프리트레이닝이 포함돼, 이를 통해 초기 해부학적 이해를 제공해. 우리는 장기 감독을 멀티태스크 접근 방식으로 포함시켜, 모델이 생리적 흡수와 트레이서 특정 패턴을 구분할 수 있게 했어. 이건 병변이 없는 경우에 특히 유익해.

기본 nnU-Net은 Dice 점수 57.61을 얻었고, 더 큰 ResEncL은 65.31을 기록했지만, 우리 모델은 Dice 점수 68.40으로 성능을 크게 향상시켰어. 그리고 잘못된 양성(FPvol: 7.82)과 잘못된 음성(FNvol: 10.35) 볼륨도 줄였어. 이 결과는 PET/CT 병변 분할을 위해 고급 네트워크 설계, 증강, 프리트레이닝, 그리고 멀티태스크 학습을 결합하는 게 효과적임을 보여줘. 코드는 이 링크에서 공개되어 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09479.pdf

Title: MAC-VO: Metrics-aware Covariance for Learning-based Stereo Visual Odometry

Original Abstract:
We propose the MAC-VO, a novel learning-based stereo VO that leverages the learned metrics-aware matching uncertainty for dual purposes: selecting keypoint and weighing the residual in pose graph optimization. Compared to traditional geometric methods prioritizing texture-affluent features like edges, our keypoint selector employs the learned uncertainty to filter out the low-quality features based on global inconsistency. In contrast to the learning-based algorithms that model the scale-agnostic diagonal weight matrix for covariance, we design a metrics-aware covariance model to capture the spatial error during keypoint registration and the correlations between different axes. Integrating this covariance model into pose graph optimization enhances the robustness and reliability of pose estimation, particularly in challenging environments with varying illumination, feature density, and motion patterns. On public benchmark datasets, MAC-VO outperforms existing VO algorithms and even some SLAM algorithms in challenging environments. The covariance map also provides valuable information about the reliability of the estimated poses, which can benefit decision-making for autonomous systems.

Translated Abstract:
우리는 MAC-VO라는 새로운 학습 기반의 스테레오 VO를 제안해. 이건 배운 메트릭스 인식 매칭 불확실성을 활용해서 두 가지 목적을 달성하는 방법이야: 키포인트 선택하고 포즈 그래프 최적화에서 잔여값의 가중치를 조정하는 거지.

전통적인 기하학적 방법이 엣지 같은 질감이 풍부한 특징을 우선시하는 것과 달리, 우리의 키포인트 선택기는 학습된 불확실성을 사용해서 글로벌 불일치에 따라 저품질 특징을 걸러내. 학습 기반 알고리즘들이 공분산을 위해 스케일에 무관한 대각선 가중치 행렬을 모델링하는 것과는 다르게, 우리는 키포인트 등록 중에 공간적 오류와 서로 다른 축 간의 상관관계를 포착하는 메트릭스 인식 공분산 모델을 설계했어.

이 공분산 모델을 포즈 그래프 최적화에 통합하면 포즈 추정의 강건성과 신뢰성이 높아져, 특히 조명, 특징 밀도, 그리고 움직임 패턴이 다양한 어려운 환경에서 더 효과적이야. 공개 벤치마크 데이터셋에서 MAC-VO는 기존 VO 알고리즘은 물론, 어려운 환경에서 일부 SLAM 알고리즘보다도 더 뛰어난 성능을 보여줬어. 공분산 맵은 추정된 포즈의 신뢰성에 대한 귀중한 정보를 제공해서 자율 시스템의 의사결정에도 도움이 될 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09484.pdf

Title: Self-Prompting Polyp Segmentation in Colonoscopy using Hybrid Yolo-SAM 2 Model

Original Abstract:
Early diagnosis and treatment of polyps during colonoscopy are essential for reducing the incidence and mortality of Colorectal Cancer (CRC). However, the variability in polyp characteristics and the presence of artifacts in colonoscopy images and videos pose significant challenges for accurate and efficient polyp detection and segmentation. This paper presents a novel approach to polyp segmentation by integrating the Segment Anything Model (SAM 2) with the YOLOv8 model. Our method leverages YOLOv8's bounding box predictions to autonomously generate input prompts for SAM 2, thereby reducing the need for manual annotations. We conducted exhaustive tests on five benchmark colonoscopy image datasets and two colonoscopy video datasets, demonstrating that our method exceeds state-of-the-art models in both image and video segmentation tasks. Notably, our approach achieves high segmentation accuracy using only bounding box annotations, significantly reducing annotation time and effort. This advancement holds promise for enhancing the efficiency and scalability of polyp detection in clinical settings this https URL.

Translated Abstract:
대장내시경에서 용종을 조기에 진단하고 치료하는 건 대장암(CRC)의 발생률과 사망률을 줄이는 데 정말 중요해. 하지만 용종의 특성이 다양하고, 내시경 이미지나 비디오에 잡음이 많아서 정확하고 효율적으로 용종을 찾아내고 분할하는 데 큰 어려움이 있어.

이 논문은 Segment Anything Model(SAM 2)과 YOLOv8 모델을 결합한 새로운 용종 분할 방법을 제시해. 우리의 방법은 YOLOv8의 바운딩 박스 예측을 활용해 SAM 2를 위한 입력 프롬프트를 자동으로 생성해서 수동 주석 작업의 필요성을 줄여.

우리는 다섯 개의 기준 대장내시경 이미지 데이터셋과 두 개의 대장내시경 비디오 데이터셋에서 철저한 테스트를 진행했어. 그 결과, 우리의 방법이 이미지와 비디오 분할 작업 모두에서 최신 모델보다 더 뛰어난 성능을 보였어. 특히, 바운딩 박스 주석만 사용해도 높은 분할 정확도를 유지하면서 주석 작업의 시간과 노력을 크게 줄일 수 있었어. 이 발전은 임상 환경에서 용종 탐지의 효율성과 확장성을 향상시킬 가능성이 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09542.pdf

Title: MANGO: Disentangled Image Transformation Manifolds with Grouped Operators

Original Abstract:
Learning semantically meaningful image transformations (i.e. rotation, thickness, blur) directly from examples can be a challenging task. Recently, the Manifold Autoencoder (MAE) proposed using a set of Lie group operators to learn image transformations directly from examples. However, this approach has limitations, as the learned operators are not guaranteed to be disentangled and the training routine is prohibitively expensive when scaling up the model. To address these limitations, we propose MANGO (transformation Manifolds with Grouped Operators) for learning disentangled operators that describe image transformations in distinct latent subspaces. Moreover, our approach allows practitioners the ability to define which transformations they aim to model, thus improving the semantic meaning of the learned operators. Through our experiments, we demonstrate that MANGO enables composition of image transformations and introduces a one-phase training routine that leads to a 100x speedup over prior works.

Translated Abstract:
이미지 변환(예: 회전, 두께, 흐림)을 예제에서 직접 배우는 건 꽤 어려운 일이야. 최근에 Manifold Autoencoder (MAE)라는 방법이 리 군(Group) 연산자를 사용해서 예제로부터 이미지 변환을 배우는 방식을 제안했어. 하지만 이 방법에는 한계가 있어. 배운 연산자들이 서로 분리되어 있지 않거나, 모델을 키울 때 훈련 과정이 너무 비쌉니다.

이런 한계를 해결하기 위해 우리는 MANGO(Grouped Operators를 이용한 변환 다양체)를 제안해. 이 방법은 이미지 변환을 서로 다른 잠재적 하위 공간에서 설명하는 분리된 연산자들을 배울 수 있게 해줘. 게다가, 실무자들이 어떤 변환을 모델링할지 정의할 수 있게 해주기 때문에 배운 연산자의 의미를 더 향상시킬 수 있어.

실험을 통해 우리는 MANGO가 이미지 변환의 조합을 가능하게 하고, 이전 연구보다 100배 빠른 단일 훈련 과정을 도입한다는 것을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09555.pdf

Title: Enhancing Printed Circuit Board Defect Detection through Ensemble Learning

Original Abstract:
The quality control of printed circuit boards (PCBs) is paramount in advancing electronic device technology. While numerous machine learning methodologies have been utilized to augment defect detection efficiency and accuracy, previous studies have predominantly focused on optimizing individual models for specific defect types, often overlooking the potential synergies between different approaches. This paper introduces a comprehensive inspection framework leveraging an ensemble learning strategy to address this gap. Initially, we utilize four distinct PCB defect detection models utilizing state-of-the-art methods: EfficientDet, MobileNet SSDv2, Faster RCNN, and YOLOv5. Each method is capable of identifying PCB defects independently. Subsequently, we integrate these models into an ensemble learning framework to enhance detection performance. A comparative analysis reveals that our ensemble learning framework significantly outperforms individual methods, achieving a 95% accuracy in detecting diverse PCB defects. These findings underscore the efficacy of our proposed ensemble learning framework in enhancing PCB quality control processes.

Translated Abstract:
인쇄 회로 기판(PCB)의 품질 관리는 전자 기기 기술 발전에 정말 중요해. 많은 머신러닝 방법들이 결함 탐지의 효율성과 정확성을 높이기 위해 사용되었지만, 이전 연구들은 주로 특정 결함 유형에 맞춰 개별 모델을 최적화하는 데 집중했어. 그래서 서로 다른 접근법 간의 시너지를 간과하는 경우가 많았지. 

이 논문은 이런 문제를 해결하기 위해 앙상블 학습 전략을 활용한 종합 검사 프레임워크를 소개해. 처음에는 최신 방법인 EfficientDet, MobileNet SSDv2, Faster RCNN, YOLOv5를 사용해서 네 가지 서로 다른 PCB 결함 탐지 모델을 만들어. 각 방법은 독립적으로 PCB 결함을 찾아낼 수 있어. 

그 다음에 이 모델들을 앙상블 학습 프레임워크에 통합해서 탐지 성능을 향상시켜. 비교 분석을 해보니, 우리 앙상블 학습 프레임워크가 개별 방법들보다 훨씬 뛰어난 성능을 보였고, 다양한 PCB 결함을 탐지하는 데 95%의 정확도를 달성했어. 이 결과는 PCB 품질 관리 과정을 강화하는 데 우리 앙상블 학습 프레임워크가 효과적이라는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09569.pdf

Title: Bias Begets Bias: The Impact of Biased Embeddings on Diffusion Models

Original Abstract:
With the growing adoption of Text-to-Image (TTI) systems, the social biases of these models have come under increased scrutiny. Herein we conduct a systematic investigation of one such source of bias for diffusion models: embedding spaces. First, because traditional classifier-based fairness definitions require true labels not present in generative modeling, we propose statistical group fairness criteria based on a model's internal representation of the world. Using these definitions, we demonstrate theoretically and empirically that an unbiased text embedding space for input prompts is a necessary condition for representationally balanced diffusion models, meaning the distribution of generated images satisfy diversity requirements with respect to protected attributes. Next, we investigate the impact of biased embeddings on evaluating the alignment between generated images and prompts, a process which is commonly used to assess diffusion models. We find that biased multimodal embeddings like CLIP can result in lower alignment scores for representationally balanced TTI models, thus rewarding unfair behavior. Finally, we develop a theoretical framework through which biases in alignment evaluation can be studied and propose bias mitigation methods. By specifically adapting the perspective of embedding spaces, we establish new fairness conditions for diffusion model development and evaluation.

Translated Abstract:
텍스트-이미지(TTI) 시스템의 사용이 늘어나면서, 이 모델들의 사회적 편견이 주목받고 있어. 이번 연구에서는 확산 모델의 한 편견 원천인 임베딩 공간에 대해 체계적으로 조사해봤어.

먼저, 전통적인 분류기 기반의 공정성 정의는 생성 모델링에서는 실제 레이블이 없어 필요하지 않기 때문에, 우리는 모델이 세상을 어떻게 내포하는지를 바탕으로 한 통계적 그룹 공정성 기준을 제안해. 이 기준을 사용해서, 우리는 편향되지 않은 텍스트 임베딩 공간이 생성된 이미지의 다양성을 보호 속성과 관련해 충족해야 한다는 것을 이론적으로와 실험적으로 보여줬어.

다음으로, 편향된 임베딩이 생성된 이미지와 프롬프트 간의 정렬 평가에 미치는 영향을 조사했어. 이 과정은 확산 모델을 평가할 때 일반적으로 사용돼. 우리는 CLIP 같은 편향된 다중 모달 임베딩이 대표적으로 균형 잡힌 TTI 모델에서 낮은 정렬 점수를 초래할 수 있다는 것을 발견했어. 이렇게 되면 불공정한 행동이 보상이 되는 거지.

마지막으로, 정렬 평가에서의 편견을 연구할 수 있는 이론적 프레임워크를 개발하고, 편견 완화 방법도 제안했어. 임베딩 공간의 관점을 특별히 적용함으로써, 우리는 확산 모델 개발 및 평가를 위한 새로운 공정성 조건을 설정했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09680.pdf

Title: Reliable Multi-View Learning with Conformal Prediction for Aortic Stenosis Classification in Echocardiography

Original Abstract:
The fundamental problem with ultrasound-guided diagnosis is that the acquired images are often 2-D cross-sections of a 3-D anatomy, potentially missing important anatomical details. This limitation leads to challenges in ultrasound echocardiography, such as poor visualization of heart valves or foreshortening of ventricles. Clinicians must interpret these images with inherent uncertainty, a nuance absent in machine learning's one-hot labels. We propose Re-Training for Uncertainty (RT4U), a data-centric method to introduce uncertainty to weakly informative inputs in the training set. This simple approach can be incorporated to existing state-of-the-art aortic stenosis classification methods to further improve their accuracy. When combined with conformal prediction techniques, RT4U can yield adaptively sized prediction sets which are guaranteed to contain the ground truth class to a high accuracy. We validate the effectiveness of RT4U on three diverse datasets: a public (TMED-2) and a private AS dataset, along with a CIFAR-10-derived toy dataset. Results show improvement on all the datasets.

Translated Abstract:
초음파 진단의 기본 문제는 얻어진 이미지가 3D 해부학의 2D 단면일 때가 많아서 중요한 해부학적 세부정보를 놓칠 수 있다는 거야. 이 제한 때문에 초음파 심장 초음파 검사에서 심장 판막 시각화가 불명확하거나 심실이 짧게 보이는 문제가 생겨. 의사들은 이런 이미지를 해석할 때 본래의 불확실성을 감안해야 하는데, 이건 기계 학습의 원-핫 레이블에는 없는 부분이야.

우리는 불확실성을 도입하는 데이터 중심 방법인 '불확실성을 위한 재훈련(Re-Training for Uncertainty, RT4U)'을 제안해. 이 간단한 접근 방식은 기존의 최첨단 대동맥 협착증 분류 방법에 추가해서 정확도를 더 높일 수 있어. RT4U와 적합한 예측 기법을 결합하면, 실제 클래스가 높은 정확도로 포함된 예측 세트를 만들 수 있어.

우리는 세 가지 다양한 데이터셋에서 RT4U의 효과를 검증했어: 공용 데이터셋(TMED-2), 사적 AS 데이터셋, 그리고 CIFAR-10에서 파생된 장난감 데이터셋. 결과는 모든 데이터셋에서 개선된 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09721.pdf

Title: Finetuning CLIP to Reason about Pairwise Differences

Original Abstract:
Vision-language models (VLMs) such as CLIP are trained via contrastive learning between text and image pairs, resulting in aligned image and text embeddings that are useful for many downstream tasks. A notable drawback of CLIP, however, is that the resulting embedding space seems to lack some of the structure of their purely text-based alternatives. For instance, while text embeddings have been long noted to satisfy \emph{analogies} in embedding space using vector arithmetic, CLIP has no such property. In this paper, we propose an approach to natively train CLIP in a contrastive manner to reason about differences in embedding space. We finetune CLIP so that the differences in image embedding space correspond to \emph{text descriptions of the image differences}, which we synthetically generate with large language models on image-caption paired datasets. We first demonstrate that our approach yields significantly improved capabilities in ranking images by a certain attribute (e.g., elephants are larger than cats), which is useful in retrieval or constructing attribute-based classifiers, and improved zeroshot classification performance on many downstream image classification tasks. In addition, our approach enables a new mechanism for inference that we refer to as comparative prompting, where we leverage prior knowledge of text descriptions of differences between classes of interest, achieving even larger performance gains in classification. Finally, we illustrate that the resulting embeddings obey a larger degree of geometric properties in embedding space, such as in text-to-image generation.

Translated Abstract:
비전-언어 모델(VLMs), 예를 들어 CLIP는 텍스트와 이미지 쌍 간의 대비 학습을 통해 훈련됩니다. 이 과정에서 이미지와 텍스트의 임베딩이 잘 정렬되는데, 이게 많은 후속 작업에 유용해요. 하지만 CLIP의 한 가지 큰 단점은 임베딩 공간이 순수 텍스트 기반 모델에 비해 구조가 부족하다는 점이에요. 예를 들어, 텍스트 임베딩은 벡터 산술을 사용해 \emph{유추}를 만족하는데, CLIP은 그런 성질이 없어요.

그래서 이 논문에서는 CLIP을 대비 방식으로 훈련하는 새로운 접근 방식을 제안해요. 이 방법은 임베딩 공간에서의 차이를 생각할 수 있게 해주는데, 이미지 임베딩 공간의 차이가 \emph{이미지 차이에 대한 텍스트 설명}에 대응하도록 CLIP을 미세 조정해요. 이 텍스트 설명은 이미지-캡션 쌍 데이터셋에서 대형 언어 모델을 통해 인위적으로 생성해요.

우리의 접근 방식이 어떤 속성으로 이미지를 순위 매기는 데 크게 개선된 능력을 보여준다는 것을 먼저 증명해요. 예를 들어, "코끼리는 고양이보다 크다" 같은 속성을 사용할 수 있어요. 이건 검색이나 속성 기반 분류기를 만드는 데 유용해요. 또한, 많은 후속 이미지 분류 작업에서 제로샷 분류 성능도 개선되었어요.

추가로, 우리의 접근 방식은 비교 프롬프트라고 불리는 새로운 추론 메커니즘을 가능하게 해요. 이건 관심 있는 클래스 간의 차이에 대한 텍스트 설명의 사전 지식을 활용해서 분류 성능을 더욱 높이는 방식이에요. 마지막으로, 결과적으로 생성된 임베딩이 텍스트-이미지 생성 같은 임베딩 공간 내에서 더 많은 기하학적 성질을 따르는 것을 보여줘요.

================================================================================

URL:
https://arxiv.org/pdf/2409.09725.pdf

Title: Precise Pick-and-Place using Score-Based Diffusion Networks

Original Abstract:
In this paper, we propose a novel coarse-to-fine continuous pose diffusion method to enhance the precision of pick-and-place operations within robotic manipulation tasks. Leveraging the capabilities of diffusion networks, we facilitate the accurate perception of object poses. This accurate perception enhances both pick-and-place success rates and overall manipulation precision. Our methodology utilizes a top-down RGB image projected from an RGB-D camera and adopts a coarse-to-fine architecture. This architecture enables efficient learning of coarse and fine models. A distinguishing feature of our approach is its focus on continuous pose estimation, which enables more precise object manipulation, particularly concerning rotational angles. In addition, we employ pose and color augmentation techniques to enable effective training with limited data. Through extensive experiments in simulated and real-world scenarios, as well as an ablation study, we comprehensively evaluate our proposed methodology. Taken together, the findings validate its effectiveness in achieving high-precision pick-and-place tasks.

Translated Abstract:
이 논문에서는 로봇 조작 작업에서 물체를 집고 놓는 작업의 정확성을 높이기 위해 새로운 연속 포즈 확산(coarse-to-fine continuous pose diffusion) 방법을 제안해. 

확산 네트워크의 능력을 활용해서 물체의 포즈를 정확하게 인식할 수 있게 돼. 이렇게 정확한 인식이 가능하면 물체를 집고 놓는 성공률과 전체 조작의 정확성이 높아져. 

우리 방법론은 RGB-D 카메라에서 찍은 위에서 아래로 본 RGB 이미지를 사용하고, 거칠고 세밀한 모델을 효율적으로 학습할 수 있는 아키텍처를 채택했어. 이 아키텍처의 특징은 연속적인 포즈 추정에 중점을 두어서, 특히 회전 각도와 관련된 물체 조작을 더 정확하게 할 수 있게 해. 

또한, 제한된 데이터로 효과적으로 학습할 수 있도록 포즈와 색상 증강 기법을 사용해. 여러 시뮬레이션과 실제 환경에서의 실험, 그리고 제거 연구(ablation study)를 통해 우리 방법론을 종합적으로 평가했어. 

이 모든 결과를 종합해보면, 고정밀 물체 집고 놓기 작업을 수행하는 데 이 방법의 효과가 입증됐어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09731.pdf

Title: Learning Two-factor Representation for Magnetic Resonance Image Super-resolution

Original Abstract:
Magnetic Resonance Imaging (MRI) requires a trade-off between resolution, signal-to-noise ratio, and scan time, making high-resolution (HR) acquisition challenging. Therefore, super-resolution for MR image is a feasible solution. However, most existing methods face challenges in accurately learning a continuous volumetric representation from low-resolution image or require HR image for supervision. To solve these challenges, we propose a novel method for MR image super-resolution based on two-factor representation. Specifically, we factorize intensity signals into a linear combination of learnable basis and coefficient factors, enabling efficient continuous volumetric representation from low-resolution MR image. Besides, we introduce a coordinate-based encoding to capture structural relationships between sparse voxels, facilitating smooth completion in unobserved regions. Experiments on BraTS 2019 and MSSEG 2016 datasets demonstrate that our method achieves state-of-the-art performance, providing superior visual fidelity and robustness, particularly in large up-sampling scale MR image super-resolution.

Translated Abstract:
MRI(자기 공명 영상)는 해상도, 신호 대 잡음 비율, 스캔 시간 간의 균형을 맞춰야 해서 고해상도(HR) 영상을 얻기가 어려워. 그래서 MR 이미지의 초해상도는 괜찮은 해결책이야. 하지만 기존 방법들은 저해상도 이미지에서 연속적인 볼륨 표현을 정확하게 배우는 데 어려움을 겪거나 HR 이미지가 필요해.

이런 문제를 해결하기 위해서 우리는 두 가지 요소 표현에 기반한 새로운 MR 이미지 초해상도 방법을 제안해. 구체적으로, 우리는 강도 신호를 학습 가능한 기초 요소와 계수 요소의 선형 조합으로 분해해서 저해상도 MR 이미지에서 효율적으로 연속적인 볼륨 표현을 가능하게 해. 게다가, 우리는 희소한 복셀 간의 구조적 관계를 포착하기 위해 좌표 기반 인코딩을 도입해서 관측되지 않은 영역에서도 부드럽게 완성할 수 있도록 해.

BraTS 2019와 MSSEG 2016 데이터셋에서 실험한 결과, 우리의 방법이 최첨단 성능을 달성했음을 보여줘. 특히 큰 업샘플링 비율의 MR 이미지 초해상도에서 뛰어난 시각적 충실성과 강인성을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09796.pdf

Title: Universal Topology Refinement for Medical Image Segmentation with Polynomial Feature Synthesis

Original Abstract:
Although existing medical image segmentation methods provide impressive pixel-wise accuracy, they often neglect topological correctness, making their segmentations unusable for many downstream tasks. One option is to retrain such models whilst including a topology-driven loss component. However, this is computationally expensive and often impractical. A better solution would be to have a versatile plug-and-play topology refinement method that is compatible with any domain-specific segmentation pipeline. Directly training a post-processing model to mitigate topological errors often fails as such models tend to be biased towards the topological errors of a target segmentation network. The diversity of these errors is confined to the information provided by a labelled training set, which is especially problematic for small datasets. Our method solves this problem by training a model-agnostic topology refinement network with synthetic segmentations that cover a wide variety of topological errors. Inspired by the Stone-Weierstrass theorem, we synthesize topology-perturbation masks with randomly sampled coefficients of orthogonal polynomial bases, which ensures a complete and unbiased representation. Practically, we verified the efficiency and effectiveness of our methods as being compatible with multiple families of polynomial bases, and show evidence that our universal plug-and-play topology refinement network outperforms both existing topology-driven learning-based and post-processing methods. We also show that combining our method with learning-based models provides an effortless add-on, which can further improve the performance of existing approaches.

Translated Abstract:
기존의 의료 이미지 분할 방법들은 픽셀 단위의 정확도는 뛰어나지만, 종종 위상적 정확성을 무시해서 많은 후속 작업에 사용할 수 없게 돼. 한 가지 방법은 위상 중심의 손실 요소를 포함해서 모델을 재훈련하는 건데, 이건 계산 비용이 많이 들고 실용적이지 않아. 그래서 더 나은 해결책은 어떤 특정 도메인 분할 파이프라인과도 호환되는 다재다능한 플러그 앤 플레이 위상 개선 방법을 갖는 거야.

위상 오류를 줄이기 위해 직접 후처리 모델을 훈련하는 건 자주 실패해. 왜냐하면 이런 모델들은 목표 분할 네트워크의 위상 오류에 편향되기 마련이거든. 이런 오류의 다양성은 레이블이 있는 훈련 세트가 제공하는 정보로 제한되는데, 작은 데이터셋에서는 특히 문제가 돼. 우리 방법은 다양한 위상 오류를 포함하는 합성 분할을 사용해서 모델-불가지론적 위상 개선 네트워크를 훈련함으로써 이 문제를 해결해.

Stone-Weierstrass 정리에 영감을 받아서, 우리는 무작위로 샘플링한 직교 다항식 기저의 계수를 사용해 위상 변화 마스크를 합성해. 이 방법은 완전하고 편향 없는 표현을 보장해. 실제로, 우리는 여러 가지 다항식 기저와 호환되는 방법의 효율성과 효과를 검증했어. 그리고 우리의 범용 플러그 앤 플레이 위상 개선 네트워크가 기존의 위상 중심 학습 기반 및 후처리 방법들보다 성능이 뛰어난다는 증거도 보여줬어. 마지막으로, 우리의 방법을 학습 기반 모델과 결합하면 기존 접근 방식의 성능을 더욱 개선할 수 있는 쉬운 추가 기능을 제공한다는 것도 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09797.pdf

Title: Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation

Original Abstract:
Recent advances in computer-aided diagnosis for histopathology have been largely driven by the use of deep learning models for automated image analysis. While these networks can perform on par with medical experts, their performance can be impeded by out-of-distribution data. The Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation (COSAS) challenge aimed to address the task of cross-domain adenocarcinoma segmentation in the presence of morphological and scanner-induced domain shifts. In this paper, we present a U-Net-based segmentation framework designed to tackle this challenge. Our approach achieved segmentation scores of 0.8020 for the cross-organ track and 0.8527 for the cross-scanner track on the final challenge test sets, ranking it the best-performing submission.

Translated Abstract:
최근 조직 병리학에서 컴퓨터 보조 진단의 발전은 자동화된 이미지 분석을 위한 딥러닝 모델 사용 덕분에 크게 향상되었어. 이런 네트워크는 의료 전문가와 비슷한 성능을 낼 수 있지만, 분포가 다른 데이터에서는 성능이 떨어질 수 있어. 

Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation (COSAS) 챌린지는 형태학적 변화와 스캐너로 인한 도메인 변화가 있는 상황에서 교차 도메인 선종 세분화 작업을 다루는 것이 목표였어. 

이 논문에서는 이 도전을 해결하기 위해 U-Net 기반의 세분화 프레임워크를 제안해. 우리의 접근법은 최종 챌린지 테스트 세트에서 교차 장기 트랙에서 0.8020, 교차 스캐너 트랙에서 0.8527의 세분화 점수를 기록하며 최고의 성과를 거두었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09829.pdf

Title: NARF24: Estimating Articulated Object Structure for Implicit Rendering

Original Abstract:
Articulated objects and their representations pose a difficult problem for robots. These objects require not only representations of geometry and texture, but also of the various connections and joint parameters that make up each articulation. We propose a method that learns a common Neural Radiance Field (NeRF) representation across a small number of collected scenes. This representation is combined with a parts-based image segmentation to produce an implicit space part localization, from which the connectivity and joint parameters of the articulated object can be estimated, thus enabling configuration-conditioned rendering.

Translated Abstract:
관절이 있는 물체와 그 표현은 로봇에게 어려운 문제야. 이런 물체들은 단순히 기하학과 질감뿐만 아니라 각 관절을 구성하는 다양한 연결과 관절 파라미터에 대한 표현도 필요해.

우리는 몇 개의 수집된 장면을 통해 공통의 신경 방사 필드(NeRF) 표현을 학습하는 방법을 제안해. 이 표현은 부품 기반 이미지 분할과 결합되어, 암묵적인 공간 부분 로컬라이제이션을 만들어. 이렇게 하면 관절이 있는 물체의 연결성과 관절 파라미터를 추정할 수 있어서, 설정에 따라 렌더링을 할 수 있게 돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.09860.pdf

Title: Revisiting Physical-World Adversarial Attack on Traffic Sign Recognition: A Commercial Systems Perspective

Original Abstract:
Traffic Sign Recognition (TSR) is crucial for safe and correct driving automation. Recent works revealed a general vulnerability of TSR models to physical-world adversarial attacks, which can be low-cost, highly deployable, and capable of causing severe attack effects such as hiding a critical traffic sign or spoofing a fake one. However, so far existing works generally only considered evaluating the attack effects on academic TSR models, leaving the impacts of such attacks on real-world commercial TSR systems largely unclear. In this paper, we conduct the first large-scale measurement of physical-world adversarial attacks against commercial TSR systems. Our testing results reveal that it is possible for existing attack works from academia to have highly reliable (100\%) attack success against certain commercial TSR system functionality, but such attack capabilities are not generalizable, leading to much lower-than-expected attack success rates overall. We find that one potential major factor is a spatial memorization design that commonly exists in today's commercial TSR systems. We design new attack success metrics that can mathematically model the impacts of such design on the TSR system-level attack success, and use them to revisit existing attacks. Through these efforts, we uncover 7 novel observations, some of which directly challenge the observations or claims in prior works due to the introduction of the new metrics.

Translated Abstract:
교통 표지 인식(TSR)은 안전하고 올바른 자율주행에 정말 중요해. 최근 연구에서는 TSR 모델이 실제 환경에서의 적대적 공격에 취약하다는 게 드러났어. 이런 공격은 비용이 적게 들고 쉽게 배포할 수 있으며, 중요한 교통 표지를 숨기거나 가짜 표지를 만들어내는 심각한 결과를 초래할 수 있어.

하지만 지금까지의 연구는 주로 학술적인 TSR 모델에 대한 공격 효과만 평가했지, 실제 상업용 TSR 시스템에 대한 공격의 영향은 잘 알려져 있지 않아. 이번 논문에서는 상업용 TSR 시스템에 대한 실제 공격을 대규모로 측정한 첫 번째 연구를 진행했어. 

테스트 결과에 따르면, 기존의 학술적 공격 방법이 특정 상업용 TSR 기능에 대해 100%의 성공률을 보일 수 있지만, 이런 공격 능력은 일반화되지 않아서 전체적으로는 기대보다 훨씬 낮은 성공률을 보였어. 우리가 찾아낸 한 가지 주요 원인은 오늘날 상업용 TSR 시스템에 흔히 있는 공간 기억 디자인이야.

우리는 이런 디자인이 TSR 시스템의 공격 성공률에 미치는 영향을 수학적으로 모델링할 수 있는 새로운 공격 성공 메트릭스를 설계했고, 이를 통해 기존 공격을 다시 조사했어. 이런 노력 덕분에 7가지 새로운 관측 결과를 발견했는데, 그 중 일부는 새로운 메트릭스 도입으로 인해 이전 연구의 관측이나 주장에 직접적으로 도전하는 내용이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09904.pdf

Title: Enhancing Visual Inertial SLAM with Magnetic Measurements

Original Abstract:
This paper presents an extension to visual inertial odometry (VIO) by introducing tightly-coupled fusion of magnetometer measurements. A sliding window of keyframes is optimized by minimizing re-projection errors, relative inertial errors, and relative magnetometer orientation errors. The results of IMU orientation propagation are used to efficiently transform magnetometer measurements between frames producing relative orientation constraints between consecutive frames. The soft and hard iron effects are calibrated using an ellipsoid fitting algorithm. The introduction of magnetometer data results in significant reductions in the orientation error and also in recovery of the true yaw orientation with respect to the magnetic north. The proposed framework operates in all environments with slow-varying magnetic fields, mainly outdoors and underwater. We have focused our work on the underwater domain, especially in underwater caves, as the narrow passage and turbulent flow make it difficult to perform loop closures and reset the localization drift. The underwater caves present challenges to VIO due to the absence of ambient light and the confined nature of the environment, while also being a crucial source of fresh water and providing valuable historical records. Experimental results from underwater caves demonstrate the improvements in accuracy and robustness introduced by the proposed VIO extension.

Translated Abstract:
이 논문은 비쥬얼 관성 항법(VIO)에 자력계 측정을 결합한 새로운 방법을 제안해. 핵심 프레임의 슬라이딩 윈도우를 최적화하면서 재투영 오차, 상대 관성 오차, 그리고 자력계 방향 오차를 최소화하는 방식이야. IMU 방향 전파 결과를 이용해 자력계 측정을 프레임 간에 효율적으로 변환해서 연속 프레임 간의 상대 방향 제약을 만들어내.

소프트와 하드 아이언 효과는 타원체 맞춤 알고리즘을 사용해 보정해. 자력계 데이터를 도입하면 방향 오차가 크게 줄어들고, 자기 북쪽에 대한 진짜 요우 방향도 회복할 수 있어. 이 방법은 주로 야외와 수중 같은 느리게 변하는 자기장을 가진 모든 환경에서 작동해.

우리는 특히 수중 동굴에 초점을 맞췄어. 좁은 통로와 난류 때문에 루프 클로저를 수행하고 로컬라이제이션 드리프트를 리셋하는 게 어려워. 수중 동굴은 주변 조명이 없고 환경이 제한적이라 VIO에 도전 과제가 되지만, 신선한 물의 중요한 공급원이고 귀중한 역사적 기록도 제공해.

수중 동굴에서의 실험 결과는 제안한 VIO 확장이 정확성과 강인성을 얼마나 개선했는지를 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09921.pdf

Title: Towards Real-Time Generation of Delay-Compensated Video Feeds for Outdoor Mobile Robot Teleoperation

Original Abstract:
Teleoperation is an important technology to enable supervisors to control agricultural robots remotely. However, environmental factors in dense crop rows and limitations in network infrastructure hinder the reliability of data streamed to teleoperators. These issues result in delayed and variable frame rate video feeds that often deviate significantly from the robot's actual viewpoint. We propose a modular learning-based vision pipeline to generate delay-compensated images in real-time for supervisors. Our extensive offline evaluations demonstrate that our method generates more accurate images compared to state-of-the-art approaches in our setting. Additionally, we are one of the few works to evaluate a delay-compensation method in outdoor field environments with complex terrain on data from a real robot in real-time. Additional videos are provided at this https URL.

Translated Abstract:
원거리 조작은 농업 로봇을 원격으로 제어할 수 있게 해주는 중요한 기술이야. 하지만, 밀집된 농작물 사이의 환경 요인과 네트워크 인프라의 한계 때문에 데이터가 조작자에게 신뢰성 있게 전달되기 힘들어. 이런 문제로 인해 영상 전송이 지연되거나 프레임 속도가 변동이 심해서 로봇의 실제 시점과 많이 다르게 보여.

우리는 감독자에게 실시간으로 지연 보정된 이미지를 생성할 수 있는 모듈형 학습 기반 비전 파이프라인을 제안해. 우리의 광범위한 오프라인 평가 결과, 우리의 방법이 현재의 최첨단 방법들보다 더 정확한 이미지를 생성한다는 걸 보여주었어. 게다가, 우리는 복잡한 지형의 야외 환경에서 실제 로봇의 데이터를 이용해 지연 보정 방법을 평가한 몇 안 되는 연구 중 하나야. 추가 영상은 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10089.pdf

Title: Cross-modality image synthesis from TOF-MRA to CTA using diffusion-based models

Original Abstract:
Cerebrovascular disease often requires multiple imaging modalities for accurate diagnosis, treatment, and monitoring. Computed Tomography Angiography (CTA) and Time-of-Flight Magnetic Resonance Angiography (TOF-MRA) are two common non-invasive angiography techniques, each with distinct strengths in accessibility, safety, and diagnostic accuracy. While CTA is more widely used in acute stroke due to its faster acquisition times and higher diagnostic accuracy, TOF-MRA is preferred for its safety, as it avoids radiation exposure and contrast agent-related health risks. Despite the predominant role of CTA in clinical workflows, there is a scarcity of open-source CTA data, limiting the research and development of AI models for tasks such as large vessel occlusion detection and aneurysm segmentation. This study explores diffusion-based image-to-image translation models to generate synthetic CTA images from TOF-MRA input. We demonstrate the modality conversion from TOF-MRA to CTA and show that diffusion models outperform a traditional U-Net-based approach. Our work compares different state-of-the-art diffusion architectures and samplers, offering recommendations for optimal model performance in this cross-modality translation task.

Translated Abstract:
뇌혈관 질환은 정확한 진단, 치료, 모니터링을 위해 여러 가지 이미징 기법이 필요해. 컴퓨터 단층촬영 혈관조영술(CTA)과 시간 비행 자기공명 혈관조영술(TOF-MRA)은 두 가지 일반적인 비침습적 혈관조영술 기법인데, 각각 접근성, 안전성, 진단 정확도에서 장점이 있어.

CTA는 급성 뇌졸중에서 더 많이 사용되는데, 그 이유는 빠른 촬영 시간과 높은 진단 정확도 때문이야. 반면 TOF-MRA는 방사선 노출이나 조영제 관련 건강 위험을 피할 수 있어서 더 안전하게 여겨져. CTA가 임상에서 주로 사용되지만, 오픈소스 CTA 데이터가 부족해서 대형 혈관 폐쇄 탐지나 동맥류 분할 같은 AI 모델 연구와 개발에 제약이 있어.

이번 연구에서는 TOF-MRA 이미지를 바탕으로 합성 CTA 이미지를 생성하기 위해 확산 기반 이미지-투-이미지 변환 모델을 살펴봤어. TOF-MRA에서 CTA로의 변환을 보여주고, 확산 모델이 전통적인 U-Net 기반 접근 방식보다 더 나은 성능을 보인다는 걸 입증했어. 우리는 다양한 최신 확산 아키텍처와 샘플러를 비교하고, 이 교차 모드 변환 작업에서 최적의 모델 성능을 위한 추천도 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.10120.pdf

Title: Data-Centric Strategies for Overcoming PET/CT Heterogeneity: Insights from the AutoPET III Lesion Segmentation Challenge

Original Abstract:
The third autoPET challenge introduced a new data-centric task this year, shifting the focus from model development to improving metastatic lesion segmentation on PET/CT images through data quality and handling strategies. In response, we developed targeted methods to enhance segmentation performance tailored to the characteristics of PET/CT imaging. Our approach encompasses two key elements. First, to address potential alignment errors between CT and PET modalities as well as the prevalence of punctate lesions, we modified the baseline data augmentation scheme and extended it with misalignment augmentation. This adaptation aims to improve segmentation accuracy, particularly for tiny metastatic lesions. Second, to tackle the variability in image dimensions significantly affecting the prediction time, we implemented a dynamic ensembling and test-time augmentation (TTA) strategy. This method optimizes the use of ensembling and TTA within a 5-minute prediction time limit, effectively leveraging the generalization potential for both small and large images. Both of our solutions are designed to be robust across different tracers and institutional settings, offering a general, yet imaging-specific approach to the multi-tracer and multi-institutional challenges of the competition. We made the challenge repository with our modifications publicly available at \url{this https URL}.

Translated Abstract:
올해 제3회 autoPET 챌린지는 새로운 데이터 중심의 과제를 소개했어. 이제는 모델 개발보다는 PET/CT 이미지에서 전이성 병변 세분화를 개선하는 데 데이터 품질과 처리 전략에 초점을 맞추고 있어.

우리 팀은 PET/CT 이미징의 특성에 맞춘 세분화 성능 향상을 위한 방법들을 개발했어. 우리의 접근 방식은 두 가지 주요 요소로 구성되어 있어.

첫째, CT와 PET 모달리티 간의 정렬 오류와 점 모양 병변의 빈발성을 해결하기 위해, 기본 데이터 증강 방법을 수정하고 비정렬 증강을 추가했어. 이 수정은 특히 작은 전이성 병변의 세분화 정확도를 높이는 데 도움을 줄 거야.

둘째, 예측 시간에 큰 영향을 미치는 이미지 크기의 변동성을 해결하기 위해, 동적 앙상블과 테스트 시간 증강(TTA) 전략을 적용했어. 이 방법은 5분의 예측 시간 제한 내에서 앙상블과 TTA를 최적화하여, 작은 이미지와 큰 이미지 모두에서 일반화 가능성을 효과적으로 활용할 수 있게 해.

우리의 두 가지 솔루션은 다양한 트레이서와 기관 환경에서 견고하게 설계되었어. 경쟁의 다중 트레이서 및 다중 기관 문제에 대해 일반적이면서도 이미징 특화된 접근 방식을 제공해. 수정된 챌린지 저장소는 공개적으로 이용할 수 있도록 했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10143.pdf

Title: P2U-SLAM: A Monocular Wide-FoV SLAM System Based on Point Uncertainty and Pose Uncertainty

Original Abstract:
This paper presents P2U-SLAM, a visual Simultaneous Localization And Mapping (SLAM) system with a wide Field of View (FoV) camera, which utilizes pose uncertainty and point uncertainty. While the wide FoV enables considerable repetitive observations of historical map points for matching cross-view features, the data properties of the historical map points and the poses of historical keyframes have changed during the optimization process. The neglect of data property changes triggers the absence of a partial information matrix in optimization and leads to the risk of long-term positioning performance degradation. The purpose of our research is to reduce the risk of the wide field of view visual input to the SLAM system. Based on the conditional probability model, this work reveals the definite impact of the above data properties changes on the optimization process, concretizes it as point uncertainty and pose uncertainty, and gives a specific mathematical form. P2U-SLAM respectively embeds point uncertainty and pose uncertainty into the tracking module and local mapping, and updates these uncertainties after each optimization operation including local mapping, map merging, and loop closing. We present an exhaustive evaluation in 27 sequences from two popular public datasets with wide-FoV visual input. P2U-SLAM shows excellent performance compared with other state-of-the-art methods. The source code will be made publicly available at this https URL.

Translated Abstract:
이 논문에서는 P2U-SLAM이라는 시각적 SLAM 시스템을 소개해. 이 시스템은 넓은 시야각(FoV) 카메라를 사용하고, 자세 불확실성과 점 불확실성을 활용해. 넓은 FoV 덕분에 과거 맵 포인트를 반복적으로 관찰할 수 있어, 그래서 서로 다른 관점에서 특징을 맞출 수 있지. 하지만 최적화 과정 중에 과거 맵 포인트의 데이터 특성과 역사적 키프레임의 자세가 변해. 이런 데이터 특성 변화를 무시하면 최적화에서 부분 정보 행렬이 사라지고, 장기적인 위치 성능이 저하될 위험이 생겨.

우리 연구의 목적은 SLAM 시스템에 대한 넓은 시야각 시각 입력의 위험을 줄이는 거야. 조건부 확률 모델을 바탕으로, 우리는 위에서 언급한 데이터 특성 변화가 최적화 과정에 미치는 영향을 밝혀냈고, 이를 점 불확실성과 자세 불확실성으로 구체화했어. 그리고 이를 구체적인 수학적 형태로 제시했지. P2U-SLAM은 점 불확실성과 자세 불확실성을 각각 추적 모듈과 로컬 맵핑에 포함시키고, 로컬 맵핑, 맵 병합, 루프 클로징 같은 최적화 작업 후에 이러한 불확실성을 업데이트해.

우리는 넓은 FoV 시각 입력을 가진 두 개의 유명한 공개 데이터셋에서 27개의 시퀀스를 통해 철저한 평가를 진행했어. P2U-SLAM은 다른 최신 방법들과 비교했을 때 뛰어난 성능을 보여줬어. 소스 코드는 이 URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10161.pdf

Title: SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using Gaussian Splatting

Original Abstract:
Sim2Real transfer, particularly for manipulation policies relying on RGB images, remains a critical challenge in robotics due to the significant domain shift between synthetic and real-world visual data. In this paper, we propose SplatSim, a novel framework that leverages Gaussian Splatting as the primary rendering primitive to reduce the Sim2Real gap for RGB-based manipulation policies. By replacing traditional mesh representations with Gaussian Splats in simulators, SplatSim produces highly photorealistic synthetic data while maintaining the scalability and cost-efficiency of simulation. We demonstrate the effectiveness of our framework by training manipulation policies within SplatSim}and deploying them in the real world in a zero-shot manner, achieving an average success rate of 86.25%, compared to 97.5% for policies trained on real-world data.

Translated Abstract:
Sim2Real 전이, 특히 RGB 이미지를 기반으로 한 조작 정책에서는 합성 데이터와 실제 데이터 간의 큰 차이 때문에 여전히 큰 도전 과제가 있어. 이 논문에서는 SplatSim이라는 새로운 프레임워크를 제안해. SplatSim은 Gaussian Splatting을 주요 렌더링 방식으로 사용해서 RGB 기반 조작 정책의 Sim2Real 격차를 줄이는 거야.

전통적인 메쉬 표현을 시뮬레이터에서 Gaussian Splats로 바꿈으로써, SplatSim은 매우 사실적인 합성 데이터를 생성하면서도 시뮬레이션의 확장성과 비용 효율성을 유지해. 우리는 SplatSim 내에서 조작 정책을 훈련시키고, 이를 실제 세계에 제로샷 방식으로 배포해 86.25%의 평균 성공률을 달성했어. 실제 데이터로 훈련된 정책의 성공률이 97.5%인 것과 비교하면 꽤 괜찮은 결과야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10196.pdf

Title: NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous Perception, Reasoning, and Planning in Complex UAV Search Missions

Original Abstract:
This paper addresses the problem of autonomous UAV search missions, where a UAV must locate specific Entities of Interest (EOIs) within a time limit, based on brief descriptions in large, hazard-prone environments with keep-out zones. The UAV must perceive, reason, and make decisions with limited and uncertain information. We propose NEUSIS, a compositional neuro-symbolic system designed for interpretable UAV search and navigation in realistic scenarios. NEUSIS integrates neuro-symbolic visual perception, reasoning, and grounding (GRiD) to process raw sensory inputs, maintains a probabilistic world model for environment representation, and uses a hierarchical planning component (SNaC) for efficient path planning. Experimental results from simulated urban search missions using AirSim and Unreal Engine show that NEUSIS outperforms a state-of-the-art (SOTA) vision-language model and a SOTA search planning model in success rate, search efficiency, and 3D localization. These results demonstrate the effectiveness of our compositional neuro-symbolic approach in handling complex, real-world scenarios, making it a promising solution for autonomous UAV systems in search missions.

Translated Abstract:
이 논문은 자율 UAV(무인항공기) 탐색 임무의 문제를 다루고 있어. UAV가 제한된 시간 안에 특정 관심 대상을 찾아야 하는데, 이 과정에서 위험이 많은 환경과 출입 금지 구역이 있어. UAV는 제한적이고 불확실한 정보로 인식하고, 추론하며, 결정을 내려야 해.

우리는 NEUSIS라는 시스템을 제안하는데, 이건 해석 가능한 UAV 탐색과 내비게이션을 위해 설계된 신경-상징적 시스템이야. NEUSIS는 신경-상징적 시각 인식, 추론, 그리고 그라운딩(GRiD)을 통합해서 원시 감각 입력을 처리하고, 환경을 나타내기 위해 확률적 세계 모델을 유지해. 그리고 효율적인 경로 계획을 위해 계층적 계획 구성 요소(SNaC)를 사용해.

AirSim과 Unreal Engine을 이용한 도시 탐색 임무의 실험 결과, NEUSIS는 최신 비전-언어 모델과 최신 탐색 계획 모델보다 성공률, 탐색 효율, 3D 위치 추적에서 더 나은 성과를 보여줬어. 이런 결과는 우리가 제안하는 조합적 신경-상징적 접근 방식이 복잡한 현실 세계의 상황을 처리하는 데 효과적이라는 걸 보여줘. 그래서 자율 UAV 시스템의 탐색 임무에 유망한 해결책이 될 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10202.pdf

Title: SteeredMarigold: Steering Diffusion Towards Depth Completion of Largely Incomplete Depth Maps

Original Abstract:
Even if the depth maps captured by RGB-D sensors deployed in real environments are often characterized by large areas missing valid depth measurements, the vast majority of depth completion methods still assumes depth values covering all areas of the scene. To address this limitation, we introduce SteeredMarigold, a training-free, zero-shot depth completion method capable of producing metric dense depth, even for largely incomplete depth maps. SteeredMarigold achieves this by using the available sparse depth points as conditions to steer a denoising diffusion probabilistic model. Our method outperforms relevant top-performing methods on the NYUv2 dataset, in tests where no depth was provided for a large area, achieving state-of-art performance and exhibiting remarkable robustness against depth map incompleteness. Our code will be publicly available.

Translated Abstract:
RGB-D 센서로 찍은 깊이 맵은 실제 환경에서 유효한 깊이 측정값이 많이 없지만, 대부분의 깊이 보완 방법은 여전히 모든 영역에 깊이 값이 있다고 가정해. 이 문제를 해결하기 위해 우리는 SteeredMarigold라는 방법을 소개해. 이건 훈련 없이, 제로샷으로 깊이 보완을 할 수 있는 방법이야. 이 방법은 깊이 맵이 많이 불완전해도 밀리미터 단위의 밀도 있는 깊이를 만들어낼 수 있어.

SteeredMarigold는 사용 가능한 희소한 깊이 포인트를 조건으로 해서 잡음 제거 확산 확률 모델을 조정해. 우리 방법은 NYUv2 데이터셋에서 관련된 최상위 성능의 방법들을 이겼어. 특히, 넓은 영역에 깊이가 제공되지 않은 테스트에서 최첨단 성능을 보여줬고, 깊이 맵이 불완전해도 놀라울 정도로 강인함을 보여줬어. 우리의 코드는 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10246.pdf

Title: FGR-Net:Interpretable fundus imagegradeability classification based on deepreconstruction learning

Original Abstract:
The performance of diagnostic Computer-Aided Design (CAD) systems for retinal diseases depends on the quality of the retinal images being screened. Thus, many studies have been developed to evaluate and assess the quality of such retinal images. However, most of them did not investigate the relationship between the accuracy of the developed models and the quality of the visualization of interpretability methods for distinguishing between gradable and non-gradable retinal images. Consequently, this paper presents a novel framework called FGR-Net to automatically assess and interpret underlying fundus image quality by merging an autoencoder network with a classifier network. The FGR-Net model also provides an interpretable quality assessment through visualizations. In particular, FGR-Net uses a deep autoencoder to reconstruct the input image in order to extract the visual characteristics of the input fundus images based on self-supervised learning. The extracted features by the autoencoder are then fed into a deep classifier network to distinguish between gradable and ungradable fundus images. FGR-Net is evaluated with different interpretability methods, which indicates that the autoencoder is a key factor in forcing the classifier to focus on the relevant structures of the fundus images, such as the fovea, optic disk, and prominent blood vessels. Additionally, the interpretability methods can provide visual feedback for ophthalmologists to understand how our model evaluates the quality of fundus images. The experimental results showed the superiority of FGR-Net over the state-of-the-art quality assessment methods, with an accuracy of 89% and an F1-score of 87%.

Translated Abstract:
진단용 컴퓨터 지원 설계(CAD) 시스템의 성능은 망막 질환을 검사할 때 사용하는 망막 이미지의 품질에 따라 달라져. 그래서 망막 이미지의 품질을 평가하고 측정하는 연구가 많이 진행되었어. 하지만 대부분의 연구는 개발된 모델의 정확성과 gradable(등급을 매길 수 있는) 이미지와 non-gradable(등급을 매길 수 없는) 이미지를 구분하는 해석 가능성 방법의 품질 사이의 관계를 살펴보지 않았어.

그래서 이 논문에서는 FGR-Net이라는 새로운 프레임워크를 제안해. 이 프레임워크는 오토인코더 네트워크와 분류기 네트워크를 결합해서 망막 이미지의 품질을 자동으로 평가하고 해석할 수 있도록 해. FGR-Net 모델은 시각화를 통해 해석 가능한 품질 평가도 제공해. 특히, FGR-Net은 깊은 오토인코더를 사용해서 입력 이미지를 재구성함으로써 입력 망막 이미지의 시각적 특성을 추출해. 오토인코더로 추출된 특징들은 깊은 분류기 네트워크로 전달되어 gradable 이미지와 ungradable 이미지를 구분하는 데 사용돼.

FGR-Net은 다양한 해석 가능성 방법으로 평가되었는데, 그 결과 오토인코더가 분류기가 망막 이미지의 중요한 구조, 즉 황반, 시신경 유두, 그리고 두드러진 혈관에 집중하도록 하는 데 중요한 역할을 한다는 걸 보여줬어. 또한, 해석 가능성 방법은 안과 의사들이 우리 모델이 망막 이미지의 품질을 어떻게 평가하는지를 이해할 수 있도록 시각적인 피드백도 제공해. 실험 결과, FGR-Net은 최첨단 품질 평가 방법에 비해 우수한 성능을 보였고, 정확도는 89%, F1 점수는 87%였어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10259.pdf

Title: Self-Updating Vehicle Monitoring Framework Employing Distributed Acoustic Sensing towards Real-World Settings

Original Abstract:
The recent emergence of Distributed Acoustic Sensing (DAS) technology has facilitated the effective capture of traffic-induced seismic data. The traffic-induced seismic wave is a prominent contributor to urban vibrations and contain crucial information to advance urban exploration and governance. However, identifying vehicular movements within massive noisy data poses a significant challenge. In this study, we introduce a real-time semi-supervised vehicle monitoring framework tailored to urban settings. It requires only a small fraction of manual labels for initial training and exploits unlabeled data for model improvement. Additionally, the framework can autonomously adapt to newly collected unlabeled data. Before DAS data undergo object detection as two-dimensional images to preserve spatial information, we leveraged comprehensive one-dimensional signal preprocessing to mitigate noise. Furthermore, we propose a novel prior loss that incorporates the shapes of vehicular traces to track a single vehicle with varying speeds. To evaluate our model, we conducted experiments with seismic data from the Stanford 2 DAS Array. The results showed that our model outperformed the baseline model Efficient Teacher and its supervised counterpart, YOLO (You Only Look Once), in both accuracy and robustness. With only 35 labeled images, our model surpassed YOLO's mAP 0.5:0.95 criterion by 18% and showed a 7% increase over Efficient Teacher. We conducted comparative experiments with multiple update strategies for self-updating and identified an optimal approach. This approach surpasses the performance of non-overfitting training conducted with all data in a single pass.

Translated Abstract:
최근에 나온 분산 음향 센싱(DAS) 기술 덕분에 교통으로 인한 지진 데이터를 효과적으로 수집할 수 있게 되었어. 교통으로 생긴 지진파는 도심의 진동에 큰 영향을 미치고, 도시 탐색과 관리에 중요한 정보를 담고 있어. 하지만, 이렇게 많은 잡음 속에서 차량의 움직임을 파악하는 건 정말 어려운 일이야.

이 연구에서는 도시 환경에 맞춘 실시간 반지도 학습 차량 모니터링 시스템을 소개해. 이 시스템은 초기 훈련을 위해서 아주 적은 양의 수동 레이블만 필요하고, 레이블이 없는 데이터를 모델 개선에 활용해. 게다가, 이 프레임워크는 새로 수집한 레이블 없는 데이터에 스스로 적응할 수 있어.

DAS 데이터를 2차원 이미지로 변환하기 전에 공간 정보를 유지하기 위해, 우리는 종합적인 1차원 신호 전처리를 통해 잡음을 줄였어. 그리고 차량의 흔적 형태를 포함한 새로운 이전 손실을 제안해서, 속도가 다른 단일 차량을 추적할 수 있게 했어.

모델 평가를 위해 스탠포드 2 DAS 배열의 지진 데이터를 이용해 실험을 진행했어. 결과적으로, 우리의 모델이 기본 모델인 Efficient Teacher와 그 감독 모델인 YOLO(You Only Look Once)보다 정확성과 견고성 모두에서 더 뛰어난 성능을 보였어. 레이블이 있는 이미지 35장만으로도 우리의 모델이 YOLO의 mAP 0.5:0.95 기준을 18% 초과했고, Efficient Teacher보다 7% 더 나은 성능을 나타냈어. 여러 업데이트 전략을 비교 실험해서 최적의 방법도 찾아냈어. 이 방법은 모든 데이터를 한 번에 처리하는 비과적합 훈련보다 성능이 더 뛰어난 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10293.pdf

Title: SPAC: Sampling-based Progressive Attribute Compression for Dense Point Clouds

Original Abstract:
We propose an end-to-end attribute compression method for dense point clouds. The proposed method combines a frequency sampling module, an adaptive scale feature extraction module with geometry assistance, and a global hyperprior entropy model. The frequency sampling module uses a Hamming window and the Fast Fourier Transform to extract high-frequency components of the point cloud. The difference between the original point cloud and the sampled point cloud is divided into multiple sub-point clouds. These sub-point clouds are then partitioned using an octree, providing a structured input for feature extraction. The feature extraction module integrates adaptive convolutional layers and uses offset-attention to capture both local and global features. Then, a geometry-assisted attribute feature refinement module is used to refine the extracted attribute features. Finally, a global hyperprior model is introduced for entropy encoding. This model propagates hyperprior parameters from the deepest (base) layer to the other layers, further enhancing the encoding efficiency. At the decoder, a mirrored network is used to progressively restore features and reconstruct the color attribute through transposed convolutional layers. The proposed method encodes base layer information at a low bitrate and progressively adds enhancement layer information to improve reconstruction accuracy. Compared to the latest G-PCC test model (TMC13v23) under the MPEG common test conditions (CTCs), the proposed method achieved an average Bjontegaard delta bitrate reduction of 24.58% for the Y component (21.23% for YUV combined) on the MPEG Category Solid dataset and 22.48% for the Y component (17.19% for YUV combined) on the MPEG Category Dense dataset. This is the first instance of a learning-based codec outperforming the G-PCC standard on these datasets under the MPEG CTCs.

Translated Abstract:
우리는 밀집 포인트 클라우드를 위한 엔드 투 엔드 속성 압축 방법을 제안해. 이 방법은 주파수 샘플링 모듈, 적응형 스케일 특징 추출 모듈(지오메트리 보조 포함), 그리고 글로벌 하이퍼프라이어 엔트로피 모델을 결합해.

주파수 샘플링 모듈은 해밍 창과 빠른 푸리에 변환을 사용해서 포인트 클라우드의 고주파 성분을 추출해. 원래 포인트 클라우드와 샘플된 포인트 클라우드의 차이는 여러 개의 서브 포인트 클라우드로 나눠져. 이 서브 포인트 클라우드는 옥트리로 파티션되어 특징 추출을 위한 구조화된 입력을 제공해.

특징 추출 모듈은 적응형 컨볼루션 레이어를 통합하고 오프셋 어텐션을 사용해서 지역적이고 전역적인 특징을 모두 잡아내. 그런 다음, 지오메트리 보조 속성 특징 정제 모듈이 사용되어 추출된 속성 특징을 정제해. 마지막으로, 엔트로피 인코딩을 위한 글로벌 하이퍼프라이어 모델이 도입돼. 이 모델은 가장 깊은(base) 레이어에서 다른 레이어로 하이퍼프라이어 파라미터를 전파해 인코딩 효율성을 높여.

디코더에서는 미러 네트워크를 사용해 점진적으로 특징을 복원하고 전치 컨볼루션 레이어를 통해 색상 속성을 재구성해. 이 방법은 기본 레이어 정보를 낮은 비트레이트로 인코딩하고, 점진적으로 향상 레이어 정보를 추가해서 재구성 정확도를 높여.

최신 G-PCC 테스트 모델(TMC13v23)과 비교했을 때, MPEG 공통 테스트 조건(CTCs) 하에서 제안된 방법은 MPEG 카테고리 솔리드 데이터셋에서 Y 성분에 대해 평균 24.58%의 비욘타가르 델타 비트레이트 감소를 달성했어(결합된 YUV는 21.23%). 그리고 MPEG 카테고리 덴스 데이터셋에서는 Y 성분에 대해 22.48% 감소(결합된 YUV는 17.19%)를 기록했어. 이건 학습 기반 코덱이 MPEG CTCs 하에서 이 데이터셋에서 G-PCC 표준을 초과한 첫 사례야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10330.pdf

Title: DRIVE: Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving

Original Abstract:
Recent advancements in autonomous driving have seen a paradigm shift towards end-to-end learning paradigms, which map sensory inputs directly to driving actions, thereby enhancing the robustness and adaptability of autonomous vehicles. However, these models often sacrifice interpretability, posing significant challenges to trust, safety, and regulatory compliance. To address these issues, we introduce DRIVE -- Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving, a comprehensive framework designed to improve the dependability and stability of explanations in end-to-end unsupervised autonomous driving models. Our work specifically targets the inherent instability problems observed in the Driving through the Concept Gridlock (DCG) model, which undermine the trustworthiness of its explanations and decision-making processes. We define four key attributes of DRIVE: consistent interpretability, stable interpretability, consistent output, and stable output. These attributes collectively ensure that explanations remain reliable and robust across different scenarios and perturbations. Through extensive empirical evaluations, we demonstrate the effectiveness of our framework in enhancing the stability and dependability of explanations, thereby addressing the limitations of current models. Our contributions include an in-depth analysis of the dependability issues within the DCG model, a rigorous definition of DRIVE with its fundamental properties, a framework to implement DRIVE, and novel metrics for evaluating the dependability of concept-based explainable autonomous driving models. These advancements lay the groundwork for the development of more reliable and trusted autonomous driving systems, paving the way for their broader acceptance and deployment in real-world applications.

Translated Abstract:
최근 자율주행 기술이 발전하면서, 감각 입력을 직접 주행 행동으로 변환하는 끝-to-end 학습 방식이 주목받고 있어. 이 방식은 자율차의 견고함과 적응력을 높이는데 도움을 주고 있어. 하지만 이런 모델들은 해석 가능성을 희생하기 때문에, 신뢰성, 안전성, 규제 준수에 큰 도전 과제가 되고 있어.

이 문제를 해결하기 위해 우리는 DRIVE라는 새로운 프레임워크를 소개해. DRIVE는 Dependable Robust Interpretable Visionary Ensemble Framework의 약자로, 끝-to-end 비지도 자율주행 모델에서 설명의 신뢰성과 안정성을 높이기 위해 설계된 포괄적인 프레임워크야. 우리의 연구는 특히 Driving through the Concept Gridlock (DCG) 모델에서 나타나는 불안정성 문제를 해결하는 데 초점을 맞추고 있어. 이 문제는 모델의 설명과 의사결정 과정의 신뢰성을 떨어뜨려.

DRIVE의 네 가지 주요 특성을 정의했어: 일관된 해석 가능성, 안정적인 해석 가능성, 일관된 출력, 안정적인 출력. 이 특성들은 서로 다른 상황과 변동에서도 설명이 신뢰할 수 있고 견고하게 유지되도록 보장해. 우리는 광범위한 실험을 통해 우리의 프레임워크가 설명의 안정성과 신뢰성을 높이는 데 효과적이라는 것을 증명했어.

우리가 기여한 부분은 DCG 모델 내의 신뢰성 문제에 대한 심층 분석, DRIVE의 기본 속성을 정의한 것, DRIVE를 구현하기 위한 프레임워크, 그리고 개념 기반 설명 가능한 자율주행 모델의 신뢰성을 평가할 새로운 메트릭스야. 이러한 발전은 더 신뢰할 수 있는 자율주행 시스템 개발의 기초가 되어, 실제 세계에서의 더 넓은 수용과 배치를 위한 길을 열어줄 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10335.pdf

Title: Phys3DGS: Physically-based 3D Gaussian Splatting for Inverse Rendering

Original Abstract:
We propose two novel ideas (adoption of deferred rendering and mesh-based representation) to improve the quality of 3D Gaussian splatting (3DGS) based inverse rendering. We first report a problem incurred by hidden Gaussians, where Gaussians beneath the surface adversely affect the pixel color in the volume rendering adopted by the existing methods. In order to resolve the problem, we propose applying deferred rendering and report new problems incurred in a naive application of deferred rendering to the existing 3DGS-based inverse rendering. In an effort to improve the quality of 3DGS-based inverse rendering under deferred rendering, we propose a novel two-step training approach which (1) exploits mesh extraction and utilizes a hybrid mesh-3DGS representation and (2) applies novel regularization methods to better exploit the mesh. Our experiments show that, under relighting, the proposed method offers significantly better rendering quality than the existing 3DGS-based inverse rendering methods. Compared with the SOTA voxel grid-based inverse rendering method, it gives better rendering quality while offering real-time rendering.

Translated Abstract:
우리는 3D Gaussian splatting (3DGS) 기반의 역 렌더링 품질을 개선하기 위해 두 가지 새로운 아이디어를 제안해. 첫 번째는 지연 렌더링을 사용하는 것이고, 두 번째는 메쉬 기반 표현이야. 

먼저, 숨겨진 가우시안 때문에 생기는 문제를 보고할게. 표면 아래에 있는 가우시안들이 기존 방법에서 사용하는 볼륨 렌더링의 픽셀 색상에 나쁜 영향을 미치거든. 이 문제를 해결하기 위해 지연 렌더링을 적용하는 방법을 제안하고, 이걸 기존의 3DGS 기반 역 렌더링에 단순히 적용했을 때 생기는 새로운 문제들도 다룰 거야. 

지연 렌더링 하에서 3DGS 기반 역 렌더링의 품질을 높이기 위해, 우리는 새로운 두 단계 훈련 방법을 제안해. 첫 번째 단계는 메쉬 추출을 활용하고 하이브리드 메쉬-3DGS 표현을 사용하는 거고, 두 번째 단계는 메쉬를 더 잘 활용하기 위한 새로운 정규화 방법을 적용하는 거야. 

우리 실험 결과, 제안한 방법이 조명 변화(relighting) 하에서도 기존의 3DGS 기반 역 렌더링 방법보다 훨씬 더 좋은 렌더링 품질을 제공한다는 걸 보여줬어. 최신의 복셀 그리드 기반 역 렌더링 방법과 비교했을 때도, 더 나은 렌더링 품질을 제공하면서 실시간 렌더링이 가능해.

================================================================================

URL:
https://arxiv.org/pdf/2409.10339.pdf

Title: VAE-QWGAN: Improving Quantum GANs for High Resolution Image Generation

Original Abstract:
This paper presents a novel hybrid quantum generative model, the VAE-QWGAN, which combines the strengths of a classical Variational AutoEncoder (VAE) with a hybrid Quantum Wasserstein Generative Adversarial Network (QWGAN). The VAE-QWGAN integrates the VAE decoder and QGAN generator into a single quantum model with shared parameters, utilizing the VAE's encoder for latent vector sampling during training. To generate new data from the trained model at inference, input latent vectors are sampled from a Gaussian Mixture Model (GMM), learnt on the training latent vectors. This, in turn, enhances the diversity and quality of generated images. We evaluate the model's performance on MNIST/Fashion-MNIST datasets, and demonstrate improved quality and diversity of generated images compared to existing approaches.

Translated Abstract:
이 논문은 새로운 하이브리드 양자 생성 모델인 VAE-QWGAN을 소개해. 이 모델은 고전적인 변분 오토인코더(VAE)와 하이브리드 양자 워서타인 생성적 적대 신경망(QWGAN)의 장점을 결합하고 있어.

VAE-QWGAN은 VAE 디코더와 QGAN 생성기를 하나의 양자 모델로 통합했어. 이 모델은 공유 파라미터를 사용하고, 훈련할 때 VAE의 인코더를 이용해 잠재 벡터를 샘플링해. 훈련된 모델에서 새로운 데이터를 생성할 때는, 훈련된 잠재 벡터를 기반으로 한 가우시안 혼합 모델(GMM)에서 입력 잠재 벡터를 샘플링해. 이 과정이 생성된 이미지의 다양성과 품질을 높여줘.

우리는 MNIST와 패션-MNIST 데이터셋에서 모델의 성능을 평가했어. 기존 방법들과 비교했을 때, 생성된 이미지의 품질과 다양성이 개선된 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10350.pdf

Title: Point2Graph: An End-to-end Point Cloud-based 3D Open-Vocabulary Scene Graph for Robot Navigation

Original Abstract:
Current open-vocabulary scene graph generation algorithms highly rely on both 3D scene point cloud data and posed RGB-D images and thus have limited applications in scenarios where RGB-D images or camera poses are not readily available. To solve this problem, we propose Point2Graph, a novel end-to-end point cloud-based 3D open-vocabulary scene graph generation framework in which the requirement of posed RGB-D image series is eliminated. This hierarchical framework contains room and object detection/segmentation and open-vocabulary classification. For the room layer, we leverage the advantage of merging the geometry-based border detection algorithm with the learning-based region detection to segment rooms and create a "Snap-Lookup" framework for open-vocabulary room classification. In addition, we create an end-to-end pipeline for the object layer to detect and classify 3D objects based solely on 3D point cloud data. Our evaluation results show that our framework can outperform the current state-of-the-art (SOTA) open-vocabulary object and room segmentation and classification algorithm on widely used real-scene datasets.

Translated Abstract:
현재의 오픈 보캐블러리 씬 그래프 생성 알고리즘은 3D 씬 포인트 클라우드 데이터와 포즈가 있는 RGB-D 이미지에 크게 의존해. 그래서 RGB-D 이미지나 카메라 포즈가 쉽게 구할 수 없는 상황에서는 사용이 제한적이야.

이 문제를 해결하기 위해 우리는 Point2Graph라는 새로운 엔드 투 엔드 포인트 클라우드 기반 3D 오픈 보캐블러리 씬 그래프 생성 프레임워크를 제안해. 이 프레임워크는 포즈가 있는 RGB-D 이미지 시리즈의 필요성을 없애버려.

이 계층적 프레임워크는 방과 물체 탐지/세분화 및 오픈 보캐블러리 분류를 포함해. 방 층에서는 기하학 기반의 경계 탐지 알고리즘과 학습 기반의 영역 탐지를 결합하여 방을 세분화하고, 오픈 보캐블러리 방 분류를 위한 "Snap-Lookup" 프레임워크를 만들어.

또한, 우리는 3D 포인트 클라우드 데이터만으로 3D 물체를 탐지하고 분류하는 엔드 투 엔드 파이프라인을 만들었어. 평가 결과에 따르면, 우리의 프레임워크는 널리 사용되는 실장면 데이터셋에서 현재의 최고 성능(SOTA) 오픈 보캐블러리 물체 및 방 세분화와 분류 알고리즘보다 더 나은 성능을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10441.pdf

Title: CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera

Original Abstract:
Camera-to-robot calibration is crucial for vision-based robot control and requires effort to make it accurate. Recent advancements in markerless pose estimation methods have eliminated the need for time-consuming physical setups for camera-to-robot calibration. While the existing markerless pose estimation methods have demonstrated impressive accuracy without the need for cumbersome setups, they rely on the assumption that all the robot joints are visible within the camera's field of view. However, in practice, robots usually move in and out of view, and some portion of the robot may stay out-of-frame during the whole manipulation task due to real-world constraints, leading to a lack of sufficient visual features and subsequent failure of these approaches. To address this challenge and enhance the applicability to vision-based robot control, we propose a novel framework capable of estimating the robot pose with partially visible robot manipulators. Our approach leverages the Vision-Language Models for fine-grained robot components detection, and integrates it into a keypoint-based pose estimation network, which enables more robust performance in varied operational conditions. The framework is evaluated on both public robot datasets and self-collected partial-view datasets to demonstrate our robustness and generalizability. As a result, this method is effective for robot pose estimation in a wider range of real-world manipulation scenarios.

Translated Abstract:
카메라와 로봇의 보정은 비전 기반 로봇 제어에 정말 중요하고, 정확하게 하려면 많은 노력이 필요해. 최근에는 마커 없이 자세를 추정하는 방법들이 발전하면서, 카메라와 로봇 보정을 위한 귀찮은 물리적 세팅이 필요 없어졌어. 

기존의 마커 없는 자세 추정 방법들은 복잡한 세팅 없이도 놀라운 정확성을 보여주지만, 모든 로봇 관절이 카메라의 시야에 있어야 한다는 가정에 의존해. 하지만 실제로는 로봇이 시야 안팎으로 움직이기 때문에, 조작하는 동안 로봇의 일부가 화면 밖에 남아있을 수 있어. 이러면 충분한 시각적 특징이 부족해져서 이런 접근법들이 실패할 수 있어.

이런 문제를 해결하고 비전 기반 로봇 제어에 더 잘 적용할 수 있도록, 우리는 부분적으로 보이는 로봇 조작기를 가지고 로봇 자세를 추정할 수 있는 새로운 프레임워크를 제안해. 우리의 방법은 비전-언어 모델을 활용해서 로봇의 세밀한 구성 요소를 감지하고, 이를 키포인트 기반 자세 추정 네트워크에 통합해서 다양한 작업 조건에서도 더 강력한 성능을 낼 수 있게 해.

이 프레임워크는 공개된 로봇 데이터셋과 우리가 직접 수집한 부분적 시야 데이터셋에서 평가되어, 우리의 강건성과 일반화 가능성을 보여줬어. 결과적으로, 이 방법은 더 넓은 범위의 실제 조작 시나리오에서 로봇 자세 추정에 효과적이야.

================================================================================

URL:
https://arxiv.org/pdf/2103.05423.pdf

Title: Deep Learning Based 3D Segmentation: A Survey

Original Abstract:
3D segmentation is a fundamental and challenging problem in computer vision with applications in autonomous driving and robotics. It has received significant attention from the computer vision, graphics and machine learning communities. Conventional methods for 3D segmentation, based on hand-crafted features and machine learning classifiers, lack generalization ability. Driven by their success in 2D computer vision, deep learning techniques have recently become the tool of choice for 3D segmentation tasks. This has led to an influx of many methods in the literature that have been evaluated on different benchmark datasets. Whereas survey papers on RGB-D and point cloud segmentation exist, there is a lack of a recent in-depth survey that covers all 3D data modalities and application domains. This paper fills the gap and comprehensively surveys the recent progress in deep learning-based 3D segmentation techniques. We cover over 220 works from the last six years, analyze their strengths and limitations, and discuss their competitive results on benchmark datasets. The survey provides a summary of the most commonly used pipelines and finally highlights promising research directions for the future.

Translated Abstract:
3D 세분화는 자율주행과 로봇공학 등에서 중요한 문제인데, 컴퓨터 비전에서 꽤 도전적인 과제야. 이 분야는 컴퓨터 비전, 그래픽스, 머신러닝 관련 연구자들로부터 많은 관심을 받고 있어. 기존의 3D 세분화 방법들은 수작업으로 만든 특징과 머신러닝 분류기를 기반으로 하는데, 일반화 능력이 부족해.

2D 컴퓨터 비전에서의 성공 덕분에, 최근에는 딥러닝 기술이 3D 세분화 작업의 주요 도구로 자리잡았어. 그래서 다양한 방법들이 문헌에 쏟아져 나왔고, 여러 벤치마크 데이터셋에서 평가되고 있어. RGB-D와 포인트 클라우드 세분화에 대한 서베이 논문은 있지만, 모든 3D 데이터 모달리티와 응용 분야를 아우르는 최근의 심층 서베이는 부족해.

이 논문은 그런 공백을 메우고, 딥러닝 기반 3D 세분화 기술의 최근 발전을 종합적으로 조사해. 지난 6년 동안의 220개 이상의 연구를 다루고, 각각의 강점과 한계를 분석하며, 벤치마크 데이터셋에서의 경쟁 결과를 논의해. 서베이는 가장 일반적으로 사용되는 파이프라인을 정리하고, 마지막으로 미래의 유망한 연구 방향을 강조해.

================================================================================

URL:
https://arxiv.org/pdf/2206.06420.pdf

Title: GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation

Original Abstract:
Modern multi-layer perceptron (MLP) models have shown competitive results in learning visual representations without self-attention. However, existing MLP models are not good at capturing local details and lack prior knowledge of human body configurations, which limits their modeling power for skeletal representation learning. To address these issues, we propose a simple yet effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines MLPs and graph convolutional networks (GCNs) in a global-local-graphical unified architecture for 3D human pose estimation. GraphMLP incorporates the graph structure of human bodies into an MLP model to meet the domain-specific demand of the 3D human pose, while allowing for both local and global spatial interactions. Furthermore, we propose to flexibly and efficiently extend the GraphMLP to the video domain and show that complex temporal dynamics can be effectively modeled in a simple way with negligible computational cost gains in the sequence length. To the best of our knowledge, this is the first MLP-Like architecture for 3D human pose estimation in a single frame and a video sequence. Extensive experiments show that the proposed GraphMLP achieves state-of-the-art performance on two datasets, i.e., Human3.6M and MPI-INF-3DHP. Code and models are available at this https URL.

Translated Abstract:
현대의 다층 퍼셉트론(MLP) 모델은 자기 주의(attention) 없이도 시각적 표현을 잘 학습하는 성능을 보여줬어. 하지만 기존 MLP 모델은 지역적인 세부사항을 잘 포착하지 못하고, 인간 신체 구조에 대한 사전 지식이 부족해서, 뼈대 표현 학습에서 한계를 가졌어.

이런 문제를 해결하기 위해, 우리는 GraphMLP라는 간단하지만 효과적인 그래프 강화 MLP 유사 아키텍처를 제안해. 이건 MLP와 그래프 컨볼루션 네트워크(GCN)를 결합해서 3D 인간 자세 추정을 위한 전역-지역-그래픽 통합 아키텍처를 만들어낸 거야. GraphMLP는 인간 신체의 그래프 구조를 MLP 모델에 통합해서 3D 인간 자세라는 특정 도메인의 요구를 충족시키고, 지역적 및 전역적 공간 상호작용도 가능하게 해.

게다가, 우리는 GraphMLP를 비디오 도메인으로 유연하고 효율적으로 확장하는 방법도 제안했어. 복잡한 시간적 동역학을 간단한 방식으로 모델링할 수 있다는 걸 보여줬고, 시퀀스 길이에 대한 계산 비용이 거의 증가하지 않아. 우리가 아는 한, 이건 단일 프레임과 비디오 시퀀스에서 3D 인간 자세 추정을 위한 첫 번째 MLP 유사 아키텍처야.

많은 실험을 통해 제안한 GraphMLP가 Human3.6M과 MPI-INF-3DHP 두 개의 데이터셋에서 최첨단 성능을 달성했다는 걸 보여줬어. 코드와 모델은 이 특정 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2207.06817.pdf

Title: Pseudo-Labeling Based Practical Semi-Supervised Meta-Training for Few-Shot Learning

Original Abstract:
Most existing few-shot learning (FSL) methods require a large amount of labeled data in meta-training, which is a major limit. To reduce the requirement of labels, a semi-supervised meta-training (SSMT) setting has been proposed for FSL, which includes only a few labeled samples and numbers of unlabeled samples in base classes. However, existing methods under this setting require class-aware sample selection from the unlabeled set, which violates the assumption of unlabeled set. In this paper, we propose a practical semi-supervised meta-training setting with truly unlabeled data to facilitate the applications of FSL in realistic scenarios. To better utilize both the labeled and truly unlabeled data, we propose a simple and effective meta-training framework, called pseudo-labeling based meta-learning (PLML). Firstly, we train a classifier via common semi-supervised learning (SSL) and use it to obtain the pseudo-labels of unlabeled data. Then we build few-shot tasks from labeled and pseudo-labeled data and design a novel finetuning method with feature smoothing and noise suppression to better learn the FSL model from noise labels. Surprisingly, through extensive experiments across two FSL datasets, we find that this simple meta-training framework effectively prevents the performance degradation of various FSL models under limited labeled data, and also significantly outperforms the state-of-the-art SSMT models. Besides, benefiting from meta-training, our method also improves two representative SSL algorithms as well.

Translated Abstract:
대부분의 기존 몇 샷 학습(FSL) 방법은 메타 훈련에 많은 레이블이 있는 데이터가 필요해. 이게 큰 제약이야. 그래서 레이블 필요성을 줄이기 위해, 몇 개의 레이블이 있는 샘플과 많은 레이블이 없는 샘플을 포함하는 반지도 메타 훈련(SSMT) 설정이 제안됐어. 하지만 기존 방법들은 레이블이 없는 세트에서 클래스 인지 샘플 선택이 필요해서, 이게 레이블이 없는 세트의 가정과 맞지 않아.

이 논문에서는 현실적인 상황에서 FSL을 적용할 수 있도록 진짜 레이블이 없는 데이터를 사용하는 실용적인 반지도 메타 훈련 설정을 제안해. 레이블이 있는 데이터와 진짜 레이블이 없는 데이터를 잘 활용하기 위해, 우리는 간단하고 효과적인 메타 훈련 프레임워크인 가짜 레이블 기반 메타 학습(PLML)을 제안해. 

먼저, 일반적인 반지도 학습(SSL)을 통해 분류기를 훈련시키고, 이를 사용해 레이블이 없는 데이터의 가짜 레이블을 얻어. 그 다음, 레이블이 있는 데이터와 가짜 레이블이 있는 데이터로부터 몇 샷 과제를 만들고, 노이즈 레이블로부터 FSL 모델을 더 잘 학습할 수 있도록 특성 부드럽게 하기와 노이즈 억제 기법을 적용한 새로운 미세 조정 방법을 설계해.

놀랍게도, 두 개의 FSL 데이터셋에서 광범위한 실험을 통해, 이 간단한 메타 훈련 프레임워크가 제한된 레이블 데이터 하에서도 다양한 FSL 모델의 성능 저하를 효과적으로 방지하고, 최신 SSMT 모델보다도 훨씬 우수하다는 걸 발견했어. 게다가, 메타 훈련의 이점 덕분에 우리 방법은 두 개의 대표적인 SSL 알고리즘도 개선해.

================================================================================

URL:
https://arxiv.org/pdf/2210.03437.pdf

Title: PCKRF: Point Cloud Completion and Keypoint Refinement With Fusion Data for 6D Pose Estimation

Original Abstract:
Some robust point cloud registration approaches with controllable pose refinement magnitude, such as ICP and its variants, are commonly used to improve 6D pose estimation accuracy. However, the effectiveness of these methods gradually diminishes with the advancement of deep learning techniques and the enhancement of initial pose accuracy, primarily due to their lack of specific design for pose refinement. In this paper, we propose Point Cloud Completion and Keypoint Refinement with Fusion Data (PCKRF), a new pose refinement pipeline for 6D pose estimation. The pipeline consists of two steps. First, it completes the input point clouds via a novel pose-sensitive point completion network. The network uses both local and global features with pose information during point completion. Then, it registers the completed object point cloud with the corresponding target point cloud by our proposed Color supported Iterative KeyPoint (CIKP) method. The CIKP method introduces color information into registration and registers a point cloud around each keypoint to increase stability. The PCKRF pipeline can be integrated with existing popular 6D pose estimation methods, such as the full flow bidirectional fusion network, to further improve their pose estimation accuracy. Experiments demonstrate that our method exhibits superior stability compared to existing approaches when optimizing initial poses with relatively high precision. Notably, the results indicate that our method effectively complements most existing pose estimation techniques, leading to improved performance in most cases. Furthermore, our method achieves promising results even in challenging scenarios involving textureless and symmetrical objects. Our source code is available at this https URL.

Translated Abstract:
강력한 포인트 클라우드 정합 방법 중 ICP와 그 변형들이 6D 포즈 추정 정확도를 높이기 위해 자주 사용돼. 하지만 딥러닝 기술이 발전하고 초기 포즈 정확도가 향상됨에 따라 이런 방법들의 효과는 점점 줄어들고 있어. 주로 포즈 보정을 위한 특정 설계가 부족해서 그래. 

이 논문에서는 6D 포즈 추정을 위한 새로운 포즈 보정 파이프라인인 포인트 클라우드 완성과 키포인트 보정 융합 데이터(PCKRF)를 제안해. 이 파이프라인은 두 단계로 이루어져 있어. 첫 번째로, 새로운 포즈 민감 포인트 완성 네트워크를 통해 입력 포인트 클라우드를 완성해. 이 네트워크는 포인트 완성 과정에서 포즈 정보를 활용해 지역적 및 전역적 특징을 모두 사용해. 

두 번째로, 완성된 객체 포인트 클라우드를 해당 타겟 포인트 클라우드와 등록하는데, 이때 우리가 제안한 색상 지원 반복 키포인트(CIKP) 방법을 사용해. CIKP 방법은 등록 과정에 색상 정보를 도입하고 각 키포인트 주변의 포인트 클라우드를 등록해서 안정성을 높여. 

PCKRF 파이프라인은 전체 흐름 양방향 융합 네트워크 같은 기존의 인기 있는 6D 포즈 추정 방법과 통합할 수 있어, 이들의 포즈 추정 정확도를 더 높일 수 있어. 실험 결과, 우리의 방법이 초기 포즈를 상대적으로 높은 정확도로 최적화할 때 기존 방법들보다 더 뛰어난 안정성을 보이는 것으로 나타났어. 특히, 우리의 방법이 대부분의 기존 포즈 추정 기술을 효과적으로 보완해서 대부분의 경우 성능을 향상시킨다는 결과가 있어. 게다가, 우리의 방법은 텍스처가 없거나 대칭인 객체와 같은 어려운 상황에서도 괜찮은 결과를 보여줘. 우리의 소스 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2211.01783.pdf

Title: Quantifying and Learning Static vs. Dynamic Information in Deep Spatiotemporal Networks

Original Abstract:
There is limited understanding of the information captured by deep spatiotemporal models in their intermediate representations. For example, while evidence suggests that action recognition algorithms are heavily influenced by visual appearance in single frames, no quantitative methodology exists for evaluating such static bias in the latent representation compared to bias toward dynamics. We tackle this challenge by proposing an approach for quantifying the static and dynamic biases of any spatiotemporal model, and apply our approach to three tasks, action recognition, automatic video object segmentation (AVOS) and video instance segmentation (VIS). Our key findings are: (i) Most examined models are biased toward static information. (ii) Some datasets that are assumed to be biased toward dynamics are actually biased toward static information. (iii) Individual channels in an architecture can be biased toward static, dynamic or a combination of the two. (iv) Most models converge to their culminating biases in the first half of training. We then explore how these biases affect performance on dynamically biased datasets. For action recognition, we propose StaticDropout, a semantically guided dropout that debiases a model from static information toward dynamics. For AVOS, we design a better combination of fusion and cross connection layers compared with previous architectures.

Translated Abstract:
딥 스페이시오템포럴 모델의 중간 표현에서 캡처되는 정보에 대한 이해가 부족해. 예를 들어, 액션 인식 알고리즘이 단일 프레임의 시각적 외모에 많이 영향을 받는다는 증거가 있지만, 이러한 정적 편향을 동적 편향과 비교해 평가할 수 있는 방법론이 없어. 우리는 이 문제를 해결하기 위해 어떤 스페이시오템포럴 모델의 정적 및 동적 편향을 정량화하는 방법을 제안하고, 이를 세 가지 작업인 액션 인식, 자동 비디오 객체 분할(AVOS), 비디오 인스턴스 분할(VIS)에 적용했어.

주요 발견은 다음과 같아: 
(i) 조사한 대부분의 모델은 정적 정보에 편향되어 있어. 
(ii) 동적 편향이 있다고 가정된 일부 데이터셋은 실제로 정적 정보에 편향되어 있어. 
(iii) 아키텍처의 개별 채널은 정적, 동적 또는 둘의 조합에 편향될 수 있어. 
(iv) 대부분의 모델은 훈련의 전반부에 최종 편향으로 수렴해.

그 다음에는 이런 편향이 동적 편향 데이터셋에서 성능에 어떤 영향을 미치는지 살펴봤어. 액션 인식을 위해서는 StaticDropout이라는 개념을 제안했어. 이건 정적 정보에서 동적으로 모델을 디바이싱하는 의미론적으로 안내된 드롭아웃이야. AVOS를 위해서는 이전 아키텍처들과 비교해서 더 나은 융합 및 교차 연결 레이어 조합을 설계했어.

================================================================================

URL:
https://arxiv.org/pdf/2304.02848.pdf

Title: Patch-aware Batch Normalization for Improving Cross-domain Robustness

Original Abstract:
Despite the significant success of deep learning in computer vision tasks, cross-domain tasks still present a challenge in which the model's performance will degrade when the training set and the test set follow different distributions. Most existing methods employ adversarial learning or instance normalization for achieving data augmentation to solve this task. In contrast, considering that the batch normalization (BN) layer may not be robust for unseen domains and there exist the differences between local patches of an image, we propose a novel method called patch-aware batch normalization (PBN). To be specific, we first split feature maps of a batch into non-overlapping patches along the spatial dimension, and then independently normalize each patch to jointly optimize the shared BN parameter at each iteration. By exploiting the differences between local patches of an image, our proposed PBN can effectively enhance the robustness of the model's parameters. Besides, considering the statistics from each patch may be inaccurate due to their smaller size compared to the global feature maps, we incorporate the globally accumulated statistics with the statistics from each batch to obtain the final statistics for normalizing each patch. Since the proposed PBN can replace the typical BN, it can be integrated into most existing state-of-the-art methods. Extensive experiments and analysis demonstrate the effectiveness of our PBN in multiple computer vision tasks, including classification, object detection, instance retrieval, and semantic segmentation.

Translated Abstract:
딥러닝이 컴퓨터 비전 작업에서 큰 성공을 거두긴 했지만, 서로 다른 분포를 가진 훈련 세트와 테스트 세트에서 모델 성능이 떨어지는 크로스 도메인 작업은 여전히 어려운 문제야. 기존의 방법들은 주로 적대적 학습이나 인스턴스 정규화를 사용해서 데이터 증강을 시도해왔어. 

우리는 배치 정규화(BN) 층이 보지 못한 도메인에서는 강건하지 않을 수 있고, 이미지의 로컬 패치 간에 차이가 있다는 점을 고려해서 패치 인식 배치 정규화(PBN)라는 새로운 방법을 제안해. 구체적으로 말하면, 먼저 배치의 특징 맵을 공간 차원에서 겹치지 않는 패치로 나눈 다음, 각 패치를 독립적으로 정규화해서 매 반복마다 공유되는 BN 파라미터를 함께 최적화해. 이렇게 이미지의 로컬 패치 간의 차이를 활용함으로써, 우리가 제안한 PBN은 모델 파라미터의 강건성을 효과적으로 향상시킬 수 있어.

또한, 각 패치의 통계가 전역 특징 맵에 비해 크기가 작아서 부정확할 수 있다는 점을 고려해서, 각 배치의 통계와 전역적으로 누적된 통계를 결합해 각 패치를 정규화할 최종 통계를 얻어. 제안한 PBN은 일반적인 BN을 대체할 수 있어서, 기존의 최첨단 방법들에 쉽게 통합될 수 있어. 

다양한 컴퓨터 비전 작업인 분류, 객체 감지, 인스턴스 검색, 의미 분할 등에서 우리의 PBN의 효과를 입증하기 위한 광범위한 실험과 분석이 이루어졌어.

================================================================================

URL:
https://arxiv.org/pdf/2304.05653.pdf

Title: A Closer Look at the Explainability of Contrastive Language-Image Pre-training

Original Abstract:
Contrastive language-image pre-training (CLIP) is a powerful vision-language model that has shown great benefits for various tasks. However, we have identified some issues with its explainability, which undermine its credibility and limit the capacity for related tasks. Specifically, we find that CLIP tends to focus on background regions rather than foregrounds, with noisy activations at irrelevant positions on the visualization results. These phenomena conflict with conventional explainability methods based on the class attention map (CAM), where the raw model can highlight the local foreground regions using global supervision without alignment. To address these problems, we take a closer look at its architecture and features. Based on thorough analyses, we find the raw self-attentions link to inconsistent semantic regions, resulting in the opposite visualization. Besides, the noisy activations are owing to redundant features among categories. Building on these insights, we propose the CLIP Surgery for reliable CAM, a method that allows surgery-like modifications to the inference architecture and features, without further fine-tuning as classical CAM methods. This approach significantly improves the explainability of CLIP, surpassing existing methods by large margins. Besides, it enables multimodal visualization and extends the capacity of raw CLIP on open-vocabulary tasks without extra alignment. The code is available at this https URL.

Translated Abstract:
대조적 언어-이미지 사전 학습(Contrastive Language-Image Pre-training, CLIP)은 다양한 작업에 큰 도움을 주는 강력한 비전-언어 모델이야. 하지만, 우리는 이 모델의 설명 가능성에 몇 가지 문제를 발견했어. 이 문제들은 신뢰성을 떨어뜨리고 관련 작업의 능력을 제한해. 

특히, CLIP은 배경 영역에 더 집중하는 경향이 있고, 시각화 결과에서 무관한 위치에 노이즈가 있는 활성화가 나타나. 이런 현상은 클래스 주의 맵(class attention map, CAM)에 기반한 전통적인 설명 가능성 방법과 상충해. 원래 모델은 정렬 없이도 글로벌 감독을 통해 국소적인 전경 영역을 강조할 수 있어야 하는데 말이야. 

이 문제를 해결하기 위해 우리는 CLIP의 구조와 특징을 좀 더 깊이 살펴봤어. 분석을 통해, 원래의 자기 주의(attention)가 일관되지 않은 의미 영역과 연결되어 있어서 반대의 시각화를 초래한다는 걸 발견했어. 게다가, 노이즈가 있는 활성화는 카테고리 간의 중복된 특징 때문이야. 

이러한 통찰을 바탕으로, 우리는 신뢰할 수 있는 CAM을 위한 CLIP 수술(CLIP Surgery) 방법을 제안해. 이 방법은 기존의 CAM 방식처럼 추가적인 미세 조정 없이 추론 아키텍처와 특징에 수술 같은 수정을 가능하게 해. 이 접근법은 CLIP의 설명 가능성을 크게 향상시키고, 기존 방법들을 큰 폭으로 초월해. 게다가, 다중 모달 시각화를 가능하게 하고, 추가적인 정렬 없이 원래 CLIP의 개방 어휘(open-vocabulary) 작업의 능력을 확장해. 코드에 대한 링크는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2304.10727.pdf

Title: RoCOCO: Robustness Benchmark of MS-COCO to Stress-test Image-Text Matching Models

Original Abstract:
With the extensive use of vision-language models in various downstream tasks, evaluating their robustness is crucial. In this paper, we propose a benchmark for assessing the robustness of vision-language models. We believe that a robust model should properly understand both linguistic and visual semantics and be resilient to explicit variations. In pursuit of this goal, we create new variants of texts and images in the MS-COCO test set and re-evaluate the state-of-the-art (SOTA) models with the new data. Specifically, we alter the meaning of text by replacing a word, and generate visually altered images that maintain some visual context while introducing noticeable pixel changes through image mixing techniques.Our evaluations on the proposed benchmark reveal substantial performance degradation in many SOTA models (e.g., Image-to-Text Recall@1: 81.9\% $\rightarrow$ 48.4\% in BLIP, 66.1\% $\rightarrow$ 37.6\% in VSE$\infty$), with the models often favoring the altered texts/images over the original ones. This indicates the current vision-language models struggle with subtle changes and often fail to understand the overall context of texts and images. Based on these findings, we propose semantic contrastive loss and visual contrastive loss to learn more robust embedding. Datasets and code are available at {\url{this https URL}}.

Translated Abstract:
비전-언어 모델이 다양한 작업에서 많이 사용되면서, 이 모델들의 강건성을 평가하는 게 정말 중요해. 이 논문에서는 비전-언어 모델의 강건성을 평가할 수 있는 기준을 제안해. 우리가 생각하기에 강건한 모델은 언어와 시각적 의미를 잘 이해하고 명확한 변형에도 잘 견뎌야 해.

이 목표를 위해, 우리는 MS-COCO 테스트 세트에서 텍스트와 이미지의 새로운 변형을 만들어냈고, 이 새로운 데이터로 최첨단(SOTA) 모델들을 다시 평가했어. 구체적으로, 단어를 바꿔서 텍스트의 의미를 바꾸고, 이미지 믹싱 기법을 사용해 시각적으로 변형된 이미지를 생성했어. 이때 일부 시각적 맥락은 유지하면서 눈에 띄는 픽셀 변화도 도입했어.

우리의 평가 결과, 많은 SOTA 모델에서 성능이 크게 저하된 걸 확인했어 (예: 이미지-텍스트 회수@1: BLIP에서 81.9%에서 48.4%로, VSE∞에서 66.1%에서 37.6%로 감소). 이 모델들이 종종 원본 텍스트나 이미지보다 변형된 것들을 더 선호하더라고. 이건 현재의 비전-언어 모델들이 미세한 변화에 어려움을 겪고 있고, 텍스트와 이미지의 전체 맥락을 이해하는 데 실패하는 경우가 많다는 걸 보여줘.

이런 발견을 바탕으로, 우리는 더 강건한 임베딩을 배우기 위해 의미 대조 손실과 시각 대조 손실을 제안해. 데이터셋과 코드는 {\url{this https URL}}에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2304.11906.pdf

Title: Transformer-based stereo-aware 3D object detection from binocular images

Original Abstract:
Transformers have shown promising progress in various visual object detection tasks, including monocular 2D/3D detection and surround-view 3D detection. More importantly, the attention mechanism in the Transformer model and the 3D information extraction in binocular stereo are both similarity-based. However, directly applying existing Transformer-based detectors to binocular stereo 3D object detection leads to slow convergence and significant precision drops. We argue that a key cause of that defect is that existing Transformers ignore the binocular-stereo-specific image correspondence information. In this paper, we explore the model design of Transformers in binocular 3D object detection, focusing particularly on extracting and encoding task-specific image correspondence information. To achieve this goal, we present TS3D, a Transformer-based Stereo-aware 3D object detector. In the TS3D, a Disparity-Aware Positional Encoding (DAPE) module is proposed to embed the image correspondence information into stereo features. The correspondence is encoded as normalized sub-pixel-level disparity and is used in conjunction with sinusoidal 2D positional encoding to provide the 3D location information of the scene. To enrich multi-scale stereo features, we propose a Stereo Preserving Feature Pyramid Network (SPFPN). The SPFPN is designed to preserve the correspondence information while fusing intra-scale and aggregating cross-scale stereo features. Our proposed TS3D achieves a 41.29% Moderate Car detection average precision on the KITTI test set and takes 88 ms to detect objects from each binocular image pair. It is competitive with advanced counterparts in terms of both precision and inference speed.

Translated Abstract:
트랜스포머는 단안 2D/3D 탐지와 주변 시야 3D 탐지를 포함한 여러 시각적 물체 탐지 작업에서 좋은 성과를 보여줬어. 더 중요한 건, 트랜스포머 모델의 주의(attention) 메커니즘과 이원 스테레오에서의 3D 정보 추출이 모두 유사성 기반이라는 거야. 하지만 기존의 트랜스포머 기반 탐지기를 이원 스테레오 3D 물체 탐지에 직접 적용하면 수렴 속도가 느려지고 정확도가 크게 떨어져. 우리는 이 문제의 주요 원인이 기존 트랜스포머가 이원 스테레오에 특화된 이미지 대응 정보를 무시하기 때문이라고 주장해.

이 논문에서는 이원 3D 물체 탐지에서 트랜스포머 모델 디자인을 탐구하고, 특히 작업에 맞는 이미지 대응 정보를 추출하고 인코딩하는 데 집중했어. 이를 위해 우리는 TS3D라는 트랜스포머 기반 스테레오 인식 3D 물체 탐지기를 제안해. TS3D에서는 Disparity-Aware Positional Encoding(DAPE) 모듈을 통해 이미지 대응 정보를 스테레오 특징에 포함시켜. 이 대응 정보는 정규화된 서브픽셀 수준의 차이(disparity)로 인코딩되고, 사인 함수 기반의 2D 위치 인코딩과 함께 사용돼서 장면의 3D 위치 정보를 제공해.

다양한 스케일의 스테레오 특징을 풍부하게 하기 위해 Stereo Preserving Feature Pyramid Network(SPFPN)를 제안해. SPFPN은 내부 스케일을 융합하고 외부 스케일의 스테레오 특징을 집계하면서 대응 정보를 유지하도록 설계됐어. 우리가 제안한 TS3D는 KITTI 테스트 세트에서 41.29%의 중간 자동차 탐지 평균 정확도를 달성했고, 각 이원 이미지 쌍에서 물체를 탐지하는 데 88ms가 걸려. 정확도와 추론 속도 면에서 모두 경쟁력이 있어.

================================================================================

URL:
https://arxiv.org/pdf/2305.04743.pdf

Title: MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation

Original Abstract:
Evaluating car damages from misfortune is critical to the car insurance industry. However, the accuracy is still insufficient for real-world applications since the deep learning network is not designed for car damage images as inputs, and its segmented masks are still very coarse. This paper presents MARS (Mask Attention Refinement with Sequential quadtree nodes) for car damage instance segmentation. Our MARS represents self-attention mechanisms to draw global dependencies between the sequential quadtree nodes layer and quadtree transformer to recalibrate channel weights and predict highly accurate instance masks. Our extensive experiments demonstrate that MARS outperforms state-of-the-art (SOTA) instance segmentation methods on three popular benchmarks such as Mask R-CNN [9], PointRend [13], and Mask Transfiner [12], by a large margin of +1.3 maskAP-based R50-FPN backbone and +2.3 maskAP-based R101-FPN backbone on Thai car-damage dataset. Our demos are available at this https URL.

Translated Abstract:
자동차 손상 평가를 하는 건 보험 산업에 정말 중요해. 하지만 딥러닝 네트워크가 자동차 손상 이미지에 맞춰 설계되지 않아서 실제 응용에서는 정확도가 아직 부족해. 그리고 분할된 마스크도 너무 거칠어. 

이 논문에서는 자동차 손상 인스턴스 분할을 위해 MARS(마스크 주의 정제와 연속 쿼드트리 노드)를 소개해. MARS는 자기 주의 메커니즘을 활용해서 연속 쿼드트리 노드 레이어와 쿼드트리 변환기 간의 전역 의존성을 연결해 채널 가중치를 조정하고 매우 정확한 인스턴스 마스크를 예측해. 

우리가 진행한 실험에서는 MARS가 Mask R-CNN, PointRend, Mask Transfiner 같은 최신 인스턴스 분할 방법들을 크게 능가하는 성능을 보여줬어. 태국 자동차 손상 데이터셋에서는 R50-FPN 백본 기준으로 +1.3 maskAP, R101-FPN 백본 기준으로는 +2.3 maskAP를 기록했어. 우리 데모는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2305.06024.pdf

Title: A Survey on the Robustness of Computer Vision Models against Common Corruptions

Original Abstract:
The performance of computer vision models are susceptible to unexpected changes in input images caused by sensor errors or extreme imaging environments, known as common corruptions (e.g. noise, blur, illumination changes). These corruptions can significantly hinder the reliability of these models when deployed in real-world scenarios, yet they are often overlooked when testing model generalization and robustness. In this survey, we present a comprehensive overview of methods that improve the robustness of computer vision models against common corruptions. We categorize methods into three groups based on the model components and training methods they target: data augmentation, learning strategies, and network components. We release a unified benchmark framework (available at \url{this https URL}) to compare robustness performance across several datasets, and we address the inconsistencies of evaluation practices in the literature. Our experimental analysis highlights the base corruption robustness of popular vision backbones, revealing that corruption robustness does not necessarily scale with model size and data size. Large models gain negligible robustness improvements, considering the increased computational requirements. To achieve generalizable and robust computer vision models, we foresee the need of developing new learning strategies that efficiently exploit limited data and mitigate unreliable learning behaviors.

Translated Abstract:
컴퓨터 비전 모델의 성능은 센서 오류나 극단적인 촬영 환경 때문에 입력 이미지에 예상치 못한 변화가 생기면 영향을 받기 쉬워. 이런 현상을 흔히 '일반적인 손상(common corruptions)'이라고 부르는데, 예를 들면 노이즈, 흐림, 조명 변화 같은 것들이야. 이러한 손상은 실제 환경에서 모델의 신뢰성을 크게 떨어뜨릴 수 있지만, 모델의 일반화 능력이나 강건성을 테스트할 때는 종종 간과되곤 해.

이 조사에서는 컴퓨터 비전 모델이 일반적인 손상에 대해 더 강건해지도록 하는 방법들을 포괄적으로 정리했어. 우리는 방법들을 모델 구성 요소와 훈련 방법에 따라 세 가지 그룹으로 나눴어: 데이터 증가(data augmentation), 학습 전략(learning strategies), 네트워크 구성 요소(network components). 여러 데이터셋에서 강건성 성능을 비교할 수 있는 통합 벤치마크 프레임워크도 공개했어(여기서 확인할 수 있어: \url{this https URL}). 그리고 문헌에서 평가 관행의 일관성 문제도 다뤘어.

실험 분석 결과, 인기 있는 비전 백본(backbone) 모델의 기본적인 손상 강건성이 드러났는데, 손상 강건성이 반드시 모델 크기나 데이터 크기와 비례하지는 않는다는 것을 보여줬어. 큰 모델들은 계산 요구량이 증가하는 것에 비해 강건성 향상이 미미하더라고. 일반화 가능하고 강건한 컴퓨터 비전 모델을 만들기 위해서는 제한된 데이터를 효율적으로 활용하고 신뢰할 수 없는 학습 행동을 완화하는 새로운 학습 전략이 필요할 거라고 생각해.

================================================================================

URL:
https://arxiv.org/pdf/2307.10097.pdf

Title: Boundary-Refined Prototype Generation: A General End-to-End Paradigm for Semi-Supervised Semantic Segmentation

Original Abstract:
Semi-supervised semantic segmentation has attracted increasing attention in computer vision, aiming to leverage unlabeled data through latent supervision. To achieve this goal, prototype-based classification has been introduced and achieved lots of success. However, the current approaches isolate prototype generation from the main training framework, presenting a non-end-to-end workflow. Furthermore, most methods directly perform the K-Means clustering on features to generate prototypes, resulting in their proximity to category semantic centers, while overlooking the clear delineation of class boundaries. To address the above problems, we propose a novel end-to-end boundary-refined prototype generation (BRPG) method. Specifically, we perform online clustering on sampled features to incorporate the prototype generation into the whole training framework. In addition, to enhance the classification boundaries, we sample and cluster high- and low-confidence features separately based on confidence estimation, facilitating the generation of prototypes closer to the class boundaries. Moreover, an adaptive prototype optimization strategy is proposed to increase the number of prototypes for categories with scattered feature distributions, which further refines the class boundaries. Extensive experiments demonstrate the remarkable robustness and scalability of our method across diverse datasets, segmentation networks, and semi-supervised frameworks, outperforming the state-of-the-art approaches on three benchmark datasets: PASCAL VOC 2012, Cityscapes and MS COCO. The code is available at this https URL.

Translated Abstract:
세미-슈퍼바이즈드 의미 분할은 컴퓨터 비전에서 점점 더 주목받고 있어. 이 방식은 라벨이 없는 데이터를 활용하려고 해. 이를 위해 프로토타입 기반 분류 방법이 도입됐고, 많은 성공을 거두었어. 

하지만 현재 방법들은 프로토타입 생성을 주요 훈련 프레임워크와 분리해서 진행하고 있어. 그래서 완전한(end-to-end) 작업 흐름이 아닌 거지. 게다가 대부분의 방법은 피처(feature)에 K-평균 클러스터링을 직접 수행해서 프로토타입을 생성하는데, 이 과정에서 카테고리의 의미 중심 근처에만 위치하게 돼. 그래서 클래스 경계를 명확하게 구분하는 걸 간과하고 있어.

이런 문제들을 해결하기 위해 우리는 새로운 end-to-end 경계 정제 프로토타입 생성(BRPG) 방법을 제안해. 구체적으로는, 샘플링된 피처에 대해 온라인 클러스터링을 수행해서 프로토타입 생성을 전체 훈련 프레임워크에 통합해. 그리고 분류 경계를 강화하기 위해, 신뢰도 추정을 기반으로 신뢰도가 높은 피처와 낮은 피처를 따로 샘플링하고 클러스터링해. 이 방법으로 클래스 경계에 더 가까운 프로토타입을 생성할 수 있게 돼.

추가로, 분산된 피처 분포를 가진 카테고리의 프로토타입 수를 늘리기 위한 적응형 프로토타입 최적화 전략도 제안해. 이렇게 하면 클래스 경계를 더 정제할 수 있어. 다양한 데이터셋, 분할 네트워크, 세미-슈퍼바이즈드 프레임워크에서 우리의 방법이 뛰어난 강건성과 확장성을 보여줘. 세 가지 벤치마크 데이터셋인 PASCAL VOC 2012, Cityscapes, MS COCO에서 최신 기술보다 더 나은 성과를 거뒀어. 코드도 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2308.05659.pdf

Title: AD-CLIP: Adapting Domains in Prompt Space Using CLIP

Original Abstract:
Although deep learning models have shown impressive performance on supervised learning tasks, they often struggle to generalize well when the training (source) and test (target) domains differ. Unsupervised domain adaptation (DA) has emerged as a popular solution to this problem. However, current DA techniques rely on visual backbones, which may lack semantic richness. Despite the potential of large-scale vision-language foundation models like CLIP, their effectiveness for DA has yet to be fully explored. To address this gap, we introduce \textsc{AD-CLIP}, a domain-agnostic prompt learning strategy for CLIP that aims to solve the DA problem in the prompt space. We leverage the frozen vision backbone of CLIP to extract both image style (domain) and content information, which we apply to learn prompt tokens. Our prompts are designed to be domain-invariant and class-generalizable, by conditioning prompt learning on image style and content features simultaneously. We use standard supervised contrastive learning in the source domain, while proposing an entropy minimization strategy to align domains in the embedding space given the target domain data. We also consider a scenario where only target domain samples are available during testing, without any source domain data, and propose a cross-domain style mapping network to hallucinate domain-agnostic tokens. Our extensive experiments on three benchmark DA datasets demonstrate the effectiveness of \textsc{AD-CLIP} compared to existing literature. Code is available at \url{this https URL}

Translated Abstract:
딥러닝 모델은 감독 학습 작업에서 뛰어난 성능을 보였지만, 훈련(소스) 도메인과 테스트(타겟) 도메인이 다를 때 일반화하는 데 어려움을 겪는 경우가 많아. 이 문제를 해결하기 위해 비지도 도메인 적응(DA)이 인기를 끌고 있어. 하지만 현재의 DA 기법들은 시각적 백본에 의존하는데, 이건 의미적인 풍부함이 부족할 수 있어. CLIP 같은 대규모 비전-언어 기초 모델의 잠재력은 있지만, DA에 대한 효과는 아직 충분히 연구되지 않았어.

그래서 우리는 \textsc{AD-CLIP}이라는 CLIP을 위한 도메인 비특화 프롬프트 학습 전략을 소개해. 이건 프롬프트 공간에서 DA 문제를 해결하는 걸 목표로 해. CLIP의 고정된 비전 백본을 활용해서 이미지 스타일(도메인)과 콘텐츠 정보를 추출하고, 이걸로 프롬프트 토큰을 학습해. 우리의 프롬프트는 도메인에 구애받지 않고 클래스 일반화가 가능하도록 설계되었어. 이미지 스타일과 콘텐츠 특성에 동시에 조건을 걸어 프롬프트 학습을 진행해.

우리는 소스 도메인에서 표준 감독 대조 학습을 사용하고, 타겟 도메인 데이터를 바탕으로 임베딩 공간에서 도메인을 정렬하기 위해 엔트로피 최소화 전략을 제안해. 테스트 중에는 소스 도메인 데이터 없이 타겟 도메인 샘플만 사용 가능한 시나리오도 고려하고, 도메인 비특화 토큰을 생성하기 위해 크로스 도메인 스타일 매핑 네트워크를 제안해.

세 가지 벤치마크 DA 데이터셋에서의 광범위한 실험 결과, \textsc{AD-CLIP}이 기존 문헌들에 비해 효과적임을 보여줬어. 코드도 제공돼 있어.

================================================================================

URL:
https://arxiv.org/pdf/2309.07760.pdf

Title: PRE: Vision-Language Prompt Learning with Reparameterization Encoder

Original Abstract:
Large pre-trained vision-language models such as CLIP have demonstrated great potential in zero-shot transferability to downstream tasks. However, to attain optimal performance, the manual selection of prompts is necessary to improve alignment between the downstream image distribution and the textual class descriptions. This manual prompt engineering is the major challenge for deploying such models in practice since it requires domain expertise and is extremely time-consuming. To avoid non-trivial prompt engineering, recent work Context Optimization (CoOp) introduced the concept of prompt learning to the vision domain using learnable textual tokens. While CoOp can achieve substantial improvements over manual prompts, its learned context is worse generalizable to wider unseen classes within the same dataset. In this work, we present Prompt Learning with Reparameterization Encoder (PRE) - a simple and efficient method that enhances the generalization ability of the learnable prompt to unseen classes while maintaining the capacity to learn Base classes. Instead of directly optimizing the prompts, PRE employs a prompt encoder to reparameterize the input prompt embeddings, enhancing the exploration of task-specific knowledge from few-shot samples. Experiments and extensive ablation studies on 8 benchmarks demonstrate that our approach is an efficient method for prompt learning. Specifically, PRE achieves a notable enhancement of 5.60% in average accuracy on New classes and 3% in Harmonic mean compared to CoOp in the 16-shot setting, all achieved within a good training time.

Translated Abstract:
대규모로 사전 학습된 비전-언어 모델인 CLIP은 제로샷 전이 가능성에서 큰 잠재력을 보여줬어. 하지만 최적의 성능을 내기 위해서는 이미지 분포와 텍스트 클래스 설명 간의 정렬을 개선하기 위해 수동으로 프롬프트를 선택해야 해. 이 수동 프롬프트 엔지니어링은 실질적으로 모델을 배포하는 데 큰 도전 과제인데, 도메인 전문 지식이 필요하고 시간이 정말 많이 걸리거든.

그래서 최근에 나온 Context Optimization (CoOp)이라는 연구에서는 학습 가능한 텍스트 토큰을 사용해 비전 영역에서 프롬프트 학습 개념을 도입했어. CoOp는 수동 프롬프트보다 상당한 성능 향상을 이뤘지만, 학습된 컨텍스트는 같은 데이터셋 내의 더 넓은 보지 못한 클래스에 대해서는 일반화가 잘 안 돼.

이번 연구에서는 Prompt Learning with Reparameterization Encoder (PRE)라는 간단하고 효율적인 방법을 제시해. 이 방법은 보지 못한 클래스에 대한 학습 가능한 프롬프트의 일반화 능력을 높이면서도 기본 클래스 학습 능력은 유지해. PRE는 프롬프트를 직접 최적화하는 대신, 프롬프트 인코더를 사용해 입력 프롬프트 임베딩을 재매개변수화해. 이걸 통해 몇 가지 샘플에서 작업-specific 지식을 더 잘 탐색할 수 있어.

8개의 벤치마크에서 실험과 광범위한 제거 연구를 통해, 우리의 접근 방식이 프롬프트 학습에 효율적인 방법이라는 것을 보여줬어. 특히, PRE는 16-shot 설정에서 CoOp에 비해 새로운 클래스에서 평균 정확도를 5.60% 향상시키고, 조화 평균을 3% 향상시키는 성과를 거뒀어. 이 모든 게 좋은 훈련 시간 내에 이루어졌어.

================================================================================

URL:
https://arxiv.org/pdf/2310.02815.pdf

Title: CoBEV: Elevating Roadside 3D Object Detection with Depth and Height Complementarity

Original Abstract:
Roadside camera-driven 3D object detection is a crucial task in intelligent transportation systems, which extends the perception range beyond the limitations of vision-centric vehicles and enhances road safety. While previous studies have limitations in using only depth or height information, we find both depth and height matter and they are in fact complementary. The depth feature encompasses precise geometric cues, whereas the height feature is primarily focused on distinguishing between various categories of height intervals, essentially providing semantic context. This insight motivates the development of Complementary-BEV (CoBEV), a novel end-to-end monocular 3D object detection framework that integrates depth and height to construct robust BEV representations. In essence, CoBEV estimates each pixel's depth and height distribution and lifts the camera features into 3D space for lateral fusion using the newly proposed two-stage complementary feature selection (CFS) module. A BEV feature distillation framework is also seamlessly integrated to further enhance the detection accuracy from the prior knowledge of the fusion-modal CoBEV teacher. We conduct extensive experiments on the public 3D detection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well as the private Supremind-Road dataset, demonstrating that CoBEV not only achieves the accuracy of the new state-of-the-art, but also significantly advances the robustness of previous methods in challenging long-distance scenarios and noisy camera disturbance, and enhances generalization by a large margin in heterologous settings with drastic changes in scene and camera parameters. For the first time, the vehicle AP score of a camera model reaches 80% on DAIR-V2X-I in terms of easy mode. The source code will be made publicly available at this https URL.

Translated Abstract:
도로 옆 카메라로 하는 3D 객체 탐지는 지능형 교통 시스템에서 중요한 작업이야. 이 기술은 차량의 시각적 한계를 넘어서서 인식 범위를 넓히고 도로 안전성을 높여. 이전 연구들은 깊이나 높이 정보만 사용하는 한계가 있었는데, 우리는 깊이와 높이 둘 다 중요하다는 걸 발견했어. 사실 이 두 정보는 서로 보완적이야. 깊이 정보는 정밀한 기하학적 단서를 포함하고, 높이 정보는 다양한 높이 구간을 구별하는 데 주로 집중해서 의미 있는 맥락을 제공해.

이런 인사이트는 깊이와 높이를 통합해 강력한 BEV(Top-Down View) 표현을 만드는 새로운 끝에서 끝으로 연결되는 단안 3D 객체 탐지 프레임워크인 CoBEV 개발로 이어졌어. CoBEV는 각 픽셀의 깊이와 높이 분포를 추정하고 카메라 특성을 3D 공간으로 올려서 새로운 두 단계 보완적 특성 선택(CFS) 모듈을 통해 측면 융합을 해. BEV 특성 증류 프레임워크도 통합돼서 융합 모달 CoBEV 교사의 이전 지식으로부터 탐지 정확도를 더 높여.

우리는 도로 옆 카메라 기반의 공공 3D 탐지 벤치마크인 DAIR-V2X-I와 Rope3D, 그리고 개인 데이터셋인 Supremind-Road에서 광범위한 실험을 했어. 그 결과 CoBEV는 새로운 최첨단 정확도를 달성했을 뿐만 아니라, 도전적인 장거리 상황과 노이즈가 있는 카메라 간섭에서도 이전 방법들의 강인성을 크게 향상시켰어. 그리고 장면과 카메라 매개변수에서 큰 변화가 있는 이질적인 환경에서도 일반화 능력을 크게 향상시켰어. 처음으로, 카메라 모델의 차량 AP 점수가 DAIR-V2X-I의 쉬운 모드에서 80%에 도달했어. 소스 코드는 이 URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2310.10647.pdf

Title: A Survey on Video Diffusion Models

Original Abstract:
The recent wave of AI-generated content (AIGC) has witnessed substantial success in computer vision, with the diffusion model playing a crucial role in this achievement. Due to their impressive generative capabilities, diffusion models are gradually superseding methods based on GANs and auto-regressive Transformers, demonstrating exceptional performance not only in image generation and editing, but also in the realm of video-related research. However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain. To address this gap, this paper presents a comprehensive review of video diffusion models in the AIGC era. Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at this https URL.

Translated Abstract:
최근 AI 생성 콘텐츠(AIGC) 분야에서 컴퓨터 비전의 성과가 크게 늘어나고 있는데, 그중 확산 모델이 중요한 역할을 하고 있어. 이 모델들은 뛰어난 생성 능력 덕분에 GANs나 오토 리그레시브 트랜스포머 기반의 방법들을 점차 대체하고 있어. 이미지 생성과 편집뿐만 아니라 비디오 관련 연구에서도 아주 좋은 성과를 내고 있지.

하지만 기존의 조사들은 주로 이미지 생성에 초점을 맞추고 있어서, 비디오 분야에서의 최신 리뷰는 거의 없어. 그래서 이 논문에서는 AIGC 시대의 비디오 확산 모델에 대한 포괄적인 리뷰를 제공해. 먼저 확산 모델의 기본 개념과 발전 과정을 간단히 소개하고, 그 다음에는 비디오 분야에서의 연구를 세 가지 주요 영역으로 나눠서 살펴볼 거야: 비디오 생성, 비디오 편집, 그리고 기타 비디오 이해 작업들.

이 세 가지 주요 영역에서의 문헌을 철저히 검토하고, 구체적인 분류와 실질적인 기여도 다룰 거야. 마지막으로, 이 분야에서 연구가 직면한 도전 과제와 앞으로의 발전 방향도 논의할 거야. 이 조사에서 다룬 비디오 확산 모델의 포괄적인 목록은 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2310.15161.pdf

Title: SAM-Med3D: Towards General-purpose Segmentation Models for Volumetric Medical Images

Original Abstract:
Existing volumetric medical image segmentation models are typically task-specific, excelling at specific target but struggling to generalize across anatomical structures or modalities. This limitation restricts their broader clinical use. In this paper, we introduce SAM-Med3D for general-purpose segmentation on volumetric medical images. Given only a few 3D prompt points, SAM-Med3D can accurately segment diverse anatomical structures and lesions across various modalities. To achieve this, we gather and process a large-scale 3D medical image dataset, SA-Med3D-140K, from a blend of public sources and licensed private datasets. This dataset includes 22K 3D images and 143K corresponding 3D masks. Then SAM-Med3D, a promptable segmentation model characterized by the fully learnable 3D structure, is trained on this dataset using a two-stage procedure and exhibits impressive performance on both seen and unseen segmentation targets. We comprehensively evaluate SAM-Med3D on 16 datasets covering diverse medical scenarios, including different anatomical structures, modalities, targets, and zero-shot transferability to new/unseen tasks. The evaluation shows the efficiency and efficacy of SAM-Med3D, as well as its promising application to diverse downstream tasks as a pre-trained model. Our approach demonstrates that substantial medical resources can be utilized to develop a general-purpose medical AI for various potential applications. Our dataset, code, and models are available at this https URL.

Translated Abstract:
기존의 볼륨 의료 이미지 분할 모델은 보통 특정 작업에 맞춰져 있어서 특정 목표에서는 잘 작동하지만, 해부학적 구조나 다른 방식에 대해서는 일반화가 잘 안 되는 문제가 있어. 이런 한계 때문에 넓은 임상 활용이 제한돼. 

이 논문에서는 볼륨 의료 이미지에 대한 일반 목적 분할을 위해 SAM-Med3D라는 모델을 소개해. 몇 개의 3D 프롬프트 포인트만 주면 SAM-Med3D는 다양한 해부학적 구조와 병변을 정확하게 분할할 수 있어. 이를 위해 우리는 공공 소스와 라이센스가 있는 개인 데이터셋을 혼합해서 대규모 3D 의료 이미지 데이터셋인 SA-Med3D-140K를 수집하고 처리했어. 이 데이터셋에는 22,000개의 3D 이미지와 143,000개의 대응하는 3D 마스크가 포함되어 있어. 

그 다음, 완전히 학습 가능한 3D 구조를 가진 프롬프트 가능한 분할 모델인 SAM-Med3D를 이 데이터셋으로 두 단계 절차를 통해 훈련했어. 이 모델은 보았던 분할 목표와 보지 못했던 분할 목표 모두에서 인상적인 성능을 보여줘. 우리는 다양한 해부학적 구조, 방식, 목표, 그리고 새로운/보지 못한 작업에 대한 제로샷 전이 가능성을 포함한 16개 데이터셋에서 SAM-Med3D를 종합적으로 평가했어. 평가 결과 SAM-Med3D의 효율성과 효과성을 확인할 수 있었고, 다양한 하위 작업에 대한 사전 훈련된 모델로서의 유망한 활용 가능성도 나타났어. 

우리의 접근법은 상당한 의료 자원을 활용해서 다양한 잠재적 응용을 위한 일반 목적의 의료 AI를 개발할 수 있음을 보여줘. 우리의 데이터셋, 코드, 모델은 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2311.00436.pdf

Title: Enhancing Traffic Object Detection in Variable Illumination with RGB-Event Fusion

Original Abstract:
Traffic object detection under variable illumination is challenging due to the information loss caused by the limited dynamic range of conventional frame-based cameras. To address this issue, we introduce bio-inspired event cameras and propose a novel Structure-aware Fusion Network (SFNet) that extracts sharp and complete object structures from the event stream to compensate for the lost information in images through cross-modality fusion, enabling the network to obtain illumination-robust representations for traffic object detection. Specifically, to mitigate the sparsity or blurriness issues arising from diverse motion states of traffic objects in fixed-interval event sampling methods, we propose the Reliable Structure Generation Network (RSGNet) to generate Speed Invariant Frames (SIF), ensuring the integrity and sharpness of object structures. Next, we design a novel Adaptive Feature Complement Module (AFCM) which guides the adaptive fusion of two modality features to compensate for the information loss in the images by perceiving the global lightness distribution of the images, thereby generating illumination-robust representations. Finally, considering the lack of large-scale and high-quality annotations in the existing event-based object detection datasets, we build a DSEC-Det dataset, which consists of 53 sequences with 63,931 images and more than 208,000 labels for 8 classes. Extensive experimental results demonstrate that our proposed SFNet can overcome the perceptual boundaries of conventional cameras and outperform the frame-based method by 8.0% in mAP50 and 5.9% in mAP50:95. Our code and dataset will be available at this https URL.

Translated Abstract:
변화하는 조명 조건에서 교통 물체를 감지하는 건 어려워. 기존의 프레임 기반 카메라가 가진 제한된 다이내믹 레인지 때문에 정보가 손실되거든. 이 문제를 해결하기 위해 우리는 생물에서 영감을 받은 이벤트 카메라를 도입하고, 이벤트 스트림에서 날카롭고 완전한 물체 구조를 추출하는 새로운 구조 인식 융합 네트워크(SFNet)를 제안해. 이렇게 해서 이미지에서 손실된 정보를 보완하고, 교통 물체 감지를 위한 조명 강인한 표현을 얻을 수 있어.

특히, 고정 간격 이벤트 샘플링 방법에서 교통 물체의 다양한 움직임 상태로 인해 발생하는 희소성이나 흐릿함 문제를 줄이기 위해, 신뢰할 수 있는 구조 생성 네트워크(RSGNet)를 제안해. 이 네트워크는 속도 불변 프레임(SIF)을 생성해서 물체 구조의 완전성과 선명함을 보장해.

다음으로, 두 가지 모달리티 특징의 적응형 융합을 유도하는 새로운 적응형 특징 보완 모듈(AFCM)을 설계했어. 이 모듈은 이미지의 전반적인 밝기 분포를 인식해서 이미지에서 손실된 정보를 보완하고, 조명 강인한 표현을 생성해.

마지막으로, 기존 이벤트 기반 물체 감지 데이터셋에서 대규모이고 고품질의 주석이 부족한 문제를 고려해, 우리는 DSEC-Det 데이터셋을 구축했어. 이 데이터셋은 53개의 시퀀스와 63,931개의 이미지, 8개 클래스에 대한 208,000개 이상의 레이블로 구성되어 있어. 실험 결과, 우리가 제안한 SFNet은 기존 카메라의 인지 한계를 넘어서는 성능을 보여주고, 프레임 기반 방법보다 mAP50에서 8.0% 더 높고, mAP50:95에서 5.9% 더 나은 결과를 냈어. 우리의 코드와 데이터셋은 이 https URL에서 이용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2312.02366.pdf

Title: Evaluating General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks

Original Abstract:
The integration of deep learning systems into healthcare has been hindered by the resource-intensive process of data annotation and the inability of these systems to generalize to different data distributions. Foundation models, which are models pre-trained on large datasets, have emerged as a solution to reduce reliance on annotated data and enhance model generalizability and robustness. DINOv2 is an open-source foundation model pre-trained with self-supervised learning on 142 million curated natural images that exhibits promising capabilities across various vision tasks. Nevertheless, a critical question remains unanswered regarding DINOv2's adaptability to radiological imaging, and whether its features are sufficiently general to benefit radiology image analysis. Therefore, this study comprehensively evaluates the performance DINOv2 for radiology, conducting over 200 evaluations across diverse modalities (X-ray, CT, and MRI). To measure the effectiveness and generalizability of DINOv2's feature representations, we analyze the model across medical image analysis tasks including disease classification and organ segmentation on both 2D and 3D images, and under different settings like kNN, few-shot learning, linear-probing, end-to-end fine-tuning, and parameter-efficient fine-tuning. Comparative analyses with established supervised, self-supervised, and weakly-supervised models reveal DINOv2's superior performance and cross-task generalizability. The findings contribute insights to potential avenues for optimizing pre-training strategies for medical imaging and enhancing the broader understanding of DINOv2's role in bridging the gap between natural and radiological image analysis. Our code is available at this https URL

Translated Abstract:
딥 러닝 시스템이 헬스케어에 통합되는 데는 데이터 주석 작업이 자원 소모가 크고, 이런 시스템들이 다양한 데이터 분포에 일반화되는 데 한계가 있어서 어려움이 있었어. 그래서 대규모 데이터셋에서 미리 학습된 모델인 기초 모델들이 주목받고 있어. 이 모델들은 주석이 달린 데이터에 대한 의존도를 줄이고, 모델의 일반화 능력과 강인성을 높이는 데 도움을 줘. 

DINOv2는 1억 4200만 개의 정제된 자연 이미지로 자기 지도 학습을 통해 미리 학습된 오픈소스 기초 모델이야. 이 모델은 다양한 비전 작업에서 유망한 능력을 보여주고 있어. 하지만 DINOv2가 방사선 영상에 얼마나 잘 적응할 수 있는지, 그리고 그 특성이 방사선 이미지 분석에 충분히 일반화될 수 있는지에 대한 중요한 질문이 남아 있어. 

그래서 이 연구는 DINOv2의 방사선 성능을 전반적으로 평가했어. X-ray, CT, MRI 같은 다양한 영상 모달리티에서 200개 이상의 평가를 진행했지. DINOv2의 특성 표현의 효과성과 일반화 능력을 측정하기 위해, 우리는 질병 분류와 장기 분할 같은 의료 이미지 분석 작업을 2D와 3D 이미지에서 다양한 설정(kNN, 몇 장의 학습, 선형 탐색, 엔드-투-엔드 미세 조정, 파라미터 효율적인 미세 조정)으로 분석했어. 

기존의 감독 학습, 자기 지도 학습, 약한 감독 학습 모델들과 비교 분석을 통해 DINOv2의 뛰어난 성능과 다양한 작업 간 일반화 능력을 확인했어. 이 결과는 의료 이미징을 위한 미리 학습 전략 최적화에 대한 통찰을 제공하고, DINOv2가 자연 이미지 분석과 방사선 이미지 분석 사이의 격차를 메우는 데 어떤 역할을 하는지에 대한 이해를 높이는 데 기여해. 우리의 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2312.03018.pdf

Title: DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance

Original Abstract:
Image-to-video generation, which aims to generate a video starting from a given reference image, has drawn great attention. Existing methods try to extend pre-trained text-guided image diffusion models to image-guided video generation models. Nevertheless, these methods often result in either low fidelity or flickering over time due to their limitation to shallow image guidance and poor temporal consistency. To tackle these problems, we propose a high-fidelity image-to-video generation method by devising a frame retention branch based on a pre-trained video diffusion model, named DreamVideo. Instead of integrating the reference image into the diffusion process at a semantic level, our DreamVideo perceives the reference image via convolution layers and concatenates the features with the noisy latents as model input. By this means, the details of the reference image can be preserved to the greatest extent. In addition, by incorporating double-condition classifier-free guidance, a single image can be directed to videos of different actions by providing varying prompt texts. This has significant implications for controllable video generation and holds broad application prospects. We conduct comprehensive experiments on the public dataset, and both quantitative and qualitative results indicate that our method outperforms the state-of-the-art method. Especially for fidelity, our model has a powerful image retention ability and delivers the best results in UCF101 compared to other image-to-video models to our best knowledge. Also, precise control can be achieved by giving different text prompts. Further details and comprehensive results of our model will be presented in this https URL.

Translated Abstract:
이미지에서 비디오 생성은 주어진 참조 이미지를 바탕으로 비디오를 만들어내는 작업으로, 많은 관심을 받고 있어. 기존의 방법들은 미리 훈련된 텍스트 기반 이미지 확산 모델을 비디오 생성 모델로 확장하려고 했지만, 이 과정에서 자주 낮은 품질이나 시간에 따라 깜빡이는 문제가 발생하더라. 그 이유는 얕은 이미지 가이드와 시간적인 일관성이 부족하기 때문이야.

이 문제를 해결하기 위해, 우리는 DreamVideo라는 이름의 프레임 유지 분기를 가진 고품질 이미지에서 비디오로 생성하는 방법을 제안해. 이 방법은 미리 훈련된 비디오 확산 모델을 기반으로 하고 있어. 참조 이미지를 의미적으로 확산 과정에 통합하는 대신, DreamVideo는 합성곱 레이어를 통해 참조 이미지를 인식하고, 노이즈가 있는 잠재 변수와 특징을 결합해 모델 입력으로 사용해. 이렇게 하면 참조 이미지의 세부 사항을 최대한 보존할 수 있어.

또한, 이중 조건 분류기 없는 가이드를 포함시켜서, 하나의 이미지를 다양한 프롬프트 텍스트를 제공함으로써 다른 동작의 비디오로 유도할 수 있어. 이건 제어 가능한 비디오 생성에 큰 의미가 있고, 폭넓은 응용 가능성을 가지고 있어. 우리는 공개 데이터셋을 사용해 포괄적인 실험을 진행했으며, 정량적 및 정성적 결과 모두 우리 방법이 최신 기법보다 뛰어나다는 것을 보여줬어. 특히 충실성 면에서, 우리 모델은 강력한 이미지 유지 능력을 가지고 있고, 현재까지의 지식으로는 다른 이미지에서 비디오로 생성하는 모델들 중 UCF101에서 최고의 결과를 보여주고 있어. 또한, 다양한 텍스트 프롬프트를 주면 정밀한 제어도 가능해. 더 자세한 내용과 포괄적인 결과는 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2312.15268.pdf

Title: Manydepth2: Motion-Aware Self-Supervised Monocular Depth Estimation in Dynamic Scenes

Original Abstract:
Despite advancements in self-supervised monocular depth estimation, challenges persist in dynamic scenarios due to the dependence on assumptions about a static world. In this paper, we present Manydepth2, a Motion-Guided Cost Volume Depth Net, to achieve precise depth estimation for both dynamic objects and static backgrounds, all while maintaining computational efficiency. To tackle the challenges posed by dynamic content, we incorporate optical flow and coarse monocular depth to create a novel static reference frame. This frame is then utilized to build a motion-guided cost volume in collaboration with the target frame. Additionally, to enhance the accuracy and resilience of the network structure, we introduce an attention-based depth net architecture to effectively integrate information from feature maps with varying resolutions. Compared to methods with similar computational costs, Manydepth2 achieves a significant reduction of approximately five percent in root-mean-square error for self-supervised monocular depth estimation on the KITTI-2015 dataset. The code could be found: this https URL

Translated Abstract:
자기 감독 방식의 단안 깊이 추정이 발전했지만, 동적인 상황에서는 여전히 문제가 있어요. 이건 정적인 세계에 대한 가정에 의존하기 때문이죠. 

이 논문에서는 동적 물체와 정적 배경 모두에 대해 정확한 깊이 추정을 할 수 있는 Manydepth2라는 Motion-Guided Cost Volume Depth Net을 소개해요. 이걸로 계산 효율성을 유지하면서도 깊이를 잘 측정할 수 있어요.

동적 콘텐츠가 주는 문제를 해결하기 위해, 우리는 광학 흐름과 대략적인 단안 깊이를 활용해 새로운 정적 참조 프레임을 만들었어요. 이 프레임은 목표 프레임과 함께 움직임에 맞춘 비용 볼륨을 구축하는 데 사용돼요. 그리고 네트워크 구조의 정확성과 강인성을 높이기 위해 주의 기반 깊이 네트워크 아키텍처를 도입해서 다양한 해상도의 특징 맵에서 정보를 효과적으로 통합해요.

비슷한 계산 비용을 가진 방법들과 비교했을 때, Manydepth2는 KITTI-2015 데이터셋에서 자기 감독 방식의 단안 깊이 추정에서 약 5% 정도 루트 평균 제곱 오차를 줄였어요. 코드는 이 URL에서 찾을 수 있어요: this https URL

================================================================================

URL:
https://arxiv.org/pdf/2401.00935.pdf

Title: Boundary Attention: Learning curves, corners, junctions and grouping

Original Abstract:
We present a lightweight network that infers grouping and boundaries, including curves, corners and junctions. It operates in a bottom-up fashion, analogous to classical methods for sub-pixel edge localization and edge-linking, but with a higher-dimensional representation of local boundary structure, and notions of local scale and spatial consistency that are learned instead of designed. Our network uses a mechanism that we call boundary attention: a geometry-aware local attention operation that, when applied densely and repeatedly, progressively refines a pixel-resolution field of variables that specify the boundary structure in every overlapping patch within an image. Unlike many edge detectors that produce rasterized binary edge maps, our model provides a rich, unrasterized representation of the geometric structure in every local region. We find that its intentional geometric bias allows it to be trained on simple synthetic shapes and then generalize to extracting boundaries from noisy low-light photographs.

Translated Abstract:
우리는 곡선, 모서리, 교차점을 포함한 그룹화와 경계를 추론하는 경량 네트워크를 소개해. 이 네트워크는 고전적인 서브픽셀 엣지 로컬라이제이션과 엣지 링크 방법처럼 아래에서 위로 작동해. 하지만 로컬 경계 구조의 고차원 표현을 사용하고, 설계된 게 아니라 학습된 지역 스케일과 공간 일관성 개념을 포함해.

우리 네트워크는 경계 주의(boundary attention)라는 메커니즘을 사용해. 이건 기하학에 기반한 로컬 주의 작업으로, 밀집하게 반복 적용할 때 이미지 내의 겹치는 패치에서 경계 구조를 지정하는 픽셀 해상도 변수를 점진적으로 정제해. 많은 에지 검출기들이 이진 엣지 맵을 생성하는 것과 달리, 우리의 모델은 모든 지역에서 기하학적 구조의 풍부하고 비래스터화된 표현을 제공해.

우리는 이 의도적인 기하학적 편향 덕분에 모델이 간단한 합성 형태로 훈련된 뒤, 소음이 많은 저조도 사진에서 경계를 추출하는 데 일반화할 수 있다는 것을 발견했어.

================================================================================

URL:
https://arxiv.org/pdf/2401.01256.pdf

Title: VideoStudio: Generating Consistent-Content and Multi-Scene Videos

Original Abstract:
The recent innovations and breakthroughs in diffusion models have significantly expanded the possibilities of generating high-quality videos for the given prompts. Most existing works tackle the single-scene scenario with only one video event occurring in a single background. Extending to generate multi-scene videos nevertheless is not trivial and necessitates to nicely manage the logic in between while preserving the consistent visual appearance of key content across video scenes. In this paper, we propose a novel framework, namely VideoStudio, for consistent-content and multi-scene video generation. Technically, VideoStudio leverages Large Language Models (LLM) to convert the input prompt into comprehensive multi-scene script that benefits from the logical knowledge learnt by LLM. The script for each scene includes a prompt describing the event, the foreground/background entities, as well as camera movement. VideoStudio identifies the common entities throughout the script and asks LLM to detail each entity. The resultant entity description is then fed into a text-to-image model to generate a reference image for each entity. Finally, VideoStudio outputs a multi-scene video by generating each scene video via a diffusion process that takes the reference images, the descriptive prompt of the event and camera movement into account. The diffusion model incorporates the reference images as the condition and alignment to strengthen the content consistency of multi-scene videos. Extensive experiments demonstrate that VideoStudio outperforms the SOTA video generation models in terms of visual quality, content consistency, and user preference. Source code is available at \url{this https URL}.

Translated Abstract:
최근 확산 모델에서의 혁신과 breakthroughs 덕분에 주어진 프롬프트에 대해 고품질 비디오를 생성할 수 있는 가능성이 크게 확장되었어. 대부분의 기존 연구는 하나의 배경에서 하나의 비디오 이벤트만 발생하는 단일 장면 시나리오를 다루고 있어. 하지만 여러 장면 비디오를 생성하는 건 쉽지 않고, 각 장면 사이의 논리를 잘 관리하면서도 주요 콘텐츠의 시각적 일관성을 유지해야 해.

이 논문에서는 VideoStudio라는 새로운 프레임워크를 제안해. 이건 일관된 콘텐츠와 다중 장면 비디오 생성을 위한 거야. 기술적으로, VideoStudio는 대형 언어 모델(LLM)을 활용해서 입력된 프롬프트를 종합적인 다중 장면 스크립트로 변환해. 이 스크립트는 각 장면에 대해 이벤트를 설명하는 프롬프트, 전경/배경의 엔티티, 그리고 카메라 움직임을 포함해. VideoStudio는 스크립트 전체에서 공통되는 엔티티를 찾아내고, LLM에게 각 엔티티를 자세히 설명해달라고 요청해.

결과적으로 나온 엔티티 설명은 텍스트-이미지 모델에 입력되어 각 엔티티의 참조 이미지를 생성해. 마지막으로, VideoStudio는 참조 이미지와 이벤트의 설명 프롬프트, 카메라 움직임을 고려해서 각 장면 비디오를 생성하는 확산 과정을 통해 다중 장면 비디오를 출력해. 확산 모델은 참조 이미지를 조건과 정렬로 사용해서 다중 장면 비디오의 콘텐츠 일관성을 강화해. 

광범위한 실험 결과, VideoStudio는 시각적 품질, 콘텐츠 일관성, 사용자 선호도 측면에서 최신 비디오 생성 모델보다 우수한 성능을 보여줬어. 소스 코드는 \url{this https URL}에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2401.02955.pdf

Title: Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively

Original Abstract:
The CLIP and Segment Anything Model (SAM) are remarkable vision foundation models (VFMs). SAM excels in segmentation tasks across diverse domains, whereas CLIP is renowned for its zero-shot recognition capabilities. This paper presents an in-depth exploration of integrating these two models into a unified framework. Specifically, we introduce the Open-Vocabulary SAM, a SAM-inspired model designed for simultaneous interactive segmentation and recognition, leveraging two unique knowledge transfer modules: SAM2CLIP and CLIP2SAM. The former adapts SAM's knowledge into the CLIP via distillation and learnable transformer adapters, while the latter transfers CLIP knowledge into SAM, enhancing its recognition capabilities. Extensive experiments on various datasets and detectors show the effectiveness of Open-Vocabulary SAM in both segmentation and recognition tasks, significantly outperforming the naïve baselines of simply combining SAM and CLIP. Furthermore, aided with image classification data training, our method can segment and recognize approximately 22,000 classes.

Translated Abstract:
CLIP와 Segment Anything Model (SAM)은 정말 뛰어난 비전 기초 모델(VFM)들이야. SAM은 다양한 분야에서 분할 작업을 잘하고, CLIP은 제로샷 인식 능력으로 유명해. 이 논문에서는 이 두 모델을 하나의 프레임워크로 통합하는 방법을 깊이 있게 탐구해.

특히, 우리는 Open-Vocabulary SAM을 소개해. 이건 SAM에서 영감을 받은 모델로, 동시에 상호작용하는 분할과 인식을 수행할 수 있도록 설계됐어. 여기에는 두 가지 독특한 지식 전이 모듈인 SAM2CLIP과 CLIP2SAM이 포함돼. SAM2CLIP은 SAM의 지식을 CLIP으로 옮기고, CLIP2SAM은 CLIP의 지식을 SAM으로 전이해 인식 능력을 향상시켜.

여러 데이터셋과 탐지기에서 진행한 실험 결과, Open-Vocabulary SAM이 분할과 인식 작업에서 정말 효과적이라는 걸 보여줬어. 단순히 SAM과 CLIP을 결합한 기본 모델보다 훨씬 더 좋은 성능을 냈고. 게다가 이미지 분류 데이터를 활용한 훈련 덕분에, 우리 방법은 약 22,000개의 클래스를 분할하고 인식할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2401.03771.pdf

Title: NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation

Original Abstract:
The capabilities of monocular depth estimation (MDE) models are limited by the availability of sufficient and diverse datasets. In the case of MDE models for autonomous driving, this issue is exacerbated by the linearity of the captured data trajectories. We propose a NeRF-based data augmentation pipeline to introduce synthetic data with more diverse viewing directions into training datasets and demonstrate the benefits of our approach to model performance and robustness. Our data augmentation pipeline, which we call \textit{NeRFmentation}, trains NeRFs on each scene in a dataset, filters out subpar NeRFs based on relevant metrics, and uses them to generate synthetic RGB-D images captured from new viewing directions. In this work, we apply our technique in conjunction with three state-of-the-art MDE architectures on the popular autonomous driving dataset, KITTI, augmenting its training set of the Eigen split. We evaluate the resulting performance gain on the original test set, a separate popular driving dataset, and our own synthetic test set.

Translated Abstract:
모노클 깊이 추정(MDE) 모델의 성능은 충분하고 다양한 데이터셋의 존재에 의해 제한돼. 자율주행을 위한 MDE 모델의 경우, 캡처된 데이터 궤적의 선형성 때문에 이 문제가 더 심각해져. 우리는 신경 방사장(NeRF)을 기반으로 한 데이터 증강 파이프라인을 제안해서, 훈련 데이터셋에 더 다양한 시점의 합성 데이터를 추가하려고 해. 이 방법이 모델의 성능과 강건성에 미치는 이점을 보여줄 거야.

우리의 데이터 증강 파이프라인을 \textit{NeRFmentation}이라고 부르는데, 이건 데이터셋의 각 장면에 대해 NeRF를 훈련시키고, 관련 지표를 기반으로 저품질 NeRF를 걸러내. 그런 다음, 새로운 시점에서 캡처된 합성 RGB-D 이미지를 생성해. 이 연구에서는 이 기술을 세 가지 최첨단 MDE 아키텍처와 함께 인기 있는 자율주행 데이터셋인 KITTI에 적용해서, Eigen 분할의 훈련 세트를 증강했어.

우리는 원래 테스트 세트와 별도의 인기 있는 자율주행 데이터셋, 그리고 우리 자신의 합성 테스트 세트에서 성능 향상을 평가했어.

================================================================================

URL:
https://arxiv.org/pdf/2401.11617.pdf

Title: The State of Computer Vision Research in Africa

Original Abstract:
Despite significant efforts to democratize artificial intelligence (AI), computer vision which is a sub-field of AI, still lags in Africa. A significant factor to this, is the limited access to computing resources, datasets, and collaborations. As a result, Africa's contribution to top-tier publications in this field has only been 0.06% over the past decade. Towards improving the computer vision field and making it more accessible and inclusive, this study analyzes 63,000 Scopus-indexed computer vision publications from Africa. We utilize large language models to automatically parse their abstracts, to identify and categorize topics and datasets. This resulted in listing more than 100 African datasets. Our objective is to provide a comprehensive taxonomy of dataset categories to facilitate better understanding and utilization of these resources. We also analyze collaboration trends of researchers within and outside the continent. Additionally, we conduct a large-scale questionnaire among African computer vision researchers to identify the structural barriers they believe require urgent attention. In conclusion, our study offers a comprehensive overview of the current state of computer vision research in Africa, to empower marginalized communities to participate in the design and development of computer vision systems.

Translated Abstract:
인공지능(AI)을 민주화하려는 많은 노력에도 불구하고, AI의 한 분야인 컴퓨터 비전은 아프리카에서 여전히 뒤처져 있어. 그 이유 중 하나는 컴퓨팅 자원, 데이터셋, 협업에 대한 접근이 제한적이기 때문이야. 그래서 지난 10년 동안 아프리카가 이 분야의 최상위 논문에 기여한 비율은 겨우 0.06%야.

이 연구는 아프리카의 컴퓨터 비전 분야를 개선하고 더 접근 가능하고 포용적으로 만들기 위해 63,000개의 Scopus에 색인된 아프리카의 컴퓨터 비전 논문을 분석했어. 우리는 대규모 언어 모델을 사용해서 이 논문들의 초록을 자동으로 분석하고, 주제와 데이터셋을 식별하고 분류했어. 그 결과 100개 이상의 아프리카 데이터셋을 나열할 수 있었어.

우리의 목표는 데이터셋 카테고리에 대한 포괄적인 분류 체계를 제공해서 이 자원들을 더 잘 이해하고 활용할 수 있도록 하는 거야. 또 아프리카 내외의 연구자들 간 협업 추세도 분석했어. 추가로 아프리카 컴퓨터 비전 연구자들을 대상으로 대규모 설문조사를 실시해 그들이 긴급히 해결해야 한다고 생각하는 구조적 장벽도 파악했어.

결론적으로, 이 연구는 아프리카의 컴퓨터 비전 연구 현황을 종합적으로 보여주고, 소외된 커뮤니티가 컴퓨터 비전 시스템의 설계와 개발에 참여할 수 있도록 힘을 주는 데 초점을 맞추고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2401.13560.pdf

Title: SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation

Original Abstract:
The Transformer architecture has shown a remarkable ability in modeling global relationships. However, it poses a significant computational challenge when processing high-dimensional medical images. This hinders its development and widespread adoption in this task. Mamba, as a State Space Model (SSM), recently emerged as a notable manner for long-range dependencies in sequential modeling, excelling in natural language processing filed with its remarkable memory efficiency and computational speed. Inspired by its success, we introduce SegMamba, a novel 3D medical image \textbf{Seg}mentation \textbf{Mamba} model, designed to effectively capture long-range dependencies within whole volume features at every scale. Our SegMamba, in contrast to Transformer-based methods, excels in whole volume feature modeling from a state space model standpoint, maintaining superior processing speed, even with volume features at a resolution of {$64\times 64\times 64$}. Comprehensive experiments on the BraTS2023 dataset demonstrate the effectiveness and efficiency of our SegMamba. The code for SegMamba is available at: this https URL

Translated Abstract:
Transformer 구조는 전 세계적인 관계를 모델링하는 데 뛰어난 능력을 보여줬어. 하지만 고차원 의료 이미지를 처리할 때는 계산적으로 큰 도전이 돼. 이 때문에 이 작업에서 발전하고 널리 사용되는 데 어려움이 있어. 

최근 Mamba라는 상태 공간 모델(SSM)이 긴 거리 의존성을 다루는 좋은 방법으로 떠올랐고, 자연어 처리 분야에서 메모리 효율성과 계산 속도에서 뛰어난 성과를 냈어. 이 성공에 영감을 받아서, 우리는 SegMamba라는 새로운 3D 의료 이미지 분할 모델을 소개해. SegMamba는 모든 스케일에서 전체 볼륨 특징의 긴 거리 의존성을 효과적으로 포착하도록 설계됐어. 

우리의 SegMamba는 Transformer 기반 방법과는 달리, 상태 공간 모델 관점에서 전체 볼륨 특징 모델링에서 뛰어난 성능을 보여주고, {$64\times 64\times 64$} 해상도의 볼륨 특징을 가지고도 처리 속도가 우수해. BraTS2023 데이터셋에 대한 종합적인 실험 결과, SegMamba의 효과성과 효율성을 입증했어. SegMamba의 코드는 이 링크에서 확인할 수 있어: this https URL

================================================================================

URL:
https://arxiv.org/pdf/2402.00407.pdf

Title: InfMAE: A Foundation Model in the Infrared Modality

Original Abstract:
In recent years, the foundation models have swept the computer vision field and facilitated the development of various tasks within different modalities. However, it remains an open question on how to design an infrared foundation model. In this paper, we propose InfMAE, a foundation model in infrared modality. We release an infrared dataset, called Inf30 to address the problem of lacking large-scale data for self-supervised learning in the infrared vision community. Besides, we design an information-aware masking strategy, which is suitable for infrared images. This masking strategy allows for a greater emphasis on the regions with richer information in infrared images during the self-supervised learning process, which is conducive to learning the generalized representation. In addition, we adopt a multi-scale encoder to enhance the performance of the pre-trained encoders in downstream tasks. Finally, based on the fact that infrared images do not have a lot of details and texture information, we design an infrared decoder module, which further improves the performance of downstream tasks. Extensive experiments show that our proposed method InfMAE outperforms other supervised methods and self-supervised learning methods in three downstream tasks.

Translated Abstract:
최근 몇 년 동안, 기본 모델들이 컴퓨터 비전 분야를 강타했어. 다양한 작업들을 여러 방식으로 발전시키는 데 도움을 줬지. 하지만 적외선 기반 모델을 어떻게 설계할지는 여전히 질문이야. 

이 논문에서는 InfMAE라는 적외선 모드의 기본 모델을 제안해. 우리는 적외선 비전 커뮤니티에서 자가 감독 학습을 위한 대규모 데이터가 부족한 문제를 해결하기 위해 Inf30이라는 적외선 데이터셋을 공개해. 

게다가, 적외선 이미지에 적합한 정보 인식 마스킹 전략을 설계했어. 이 마스킹 전략은 자가 감독 학습 과정에서 적외선 이미지의 정보가 풍부한 영역에 더 많은 비중을 두게 해줘. 그래서 일반화된 표현을 학습하는 데 도움이 돼. 

또한, 다운스트림 작업에서 사전 훈련된 인코더의 성능을 높이기 위해 다중 스케일 인코더를 사용해. 마지막으로, 적외선 이미지가 세부사항과 텍스처 정보가 별로 없다는 점을 고려해서, 성능을 더 향상시키기 위한 적외선 디코더 모듈을 설계했어. 

많은 실험 결과, 우리가 제안한 방법인 InfMAE가 다른 감독 방법과 자가 감독 학습 방법들보다 세 가지 다운스트림 작업에서 더 뛰어난 성능을 보였어.

================================================================================

URL:
https://arxiv.org/pdf/2402.02003.pdf

Title: GenFace: A Large-Scale Fine-Grained Face Forgery Benchmark and Cross Appearance-Edge Learning

Original Abstract:
The rapid advancement of photorealistic generators has reached a critical juncture where the discrepancy between authentic and manipulated images is increasingly indistinguishable. Thus, benchmarking and advancing techniques detecting digital manipulation become an urgent issue. Although there have been a number of publicly available face forgery datasets, the forgery faces are mostly generated using GAN-based synthesis technology, which does not involve the most recent technologies like diffusion. The diversity and quality of images generated by diffusion models have been significantly improved and thus a much more challenging face forgery dataset shall be used to evaluate SOTA forgery detection literature. In this paper, we propose a large-scale, diverse, and fine-grained high-fidelity dataset, namely GenFace, to facilitate the advancement of deepfake detection, which contains a large number of forgery faces generated by advanced generators such as the diffusion-based model and more detailed labels about the manipulation approaches and adopted generators. In addition to evaluating SOTA approaches on our benchmark, we design an innovative cross appearance-edge learning (CAEL) detector to capture multi-grained appearance and edge global representations, and detect discriminative and general forgery traces. Moreover, we devise an appearance-edge cross-attention (AECA) module to explore the various integrations across two domains. Extensive experiment results and visualizations show that our detection model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations. Code and datasets will be available at \url{this https URL

Translated Abstract:
포토리얼리즘 생성 기술이 빠르게 발전하면서 진짜 이미지와 조작된 이미지를 구분하기가 점점 어려워지고 있어. 그래서 디지털 조작을 감지하는 기술을 평가하고 발전시키는 것이 시급한 문제로 떠올랐어. 여러 공개된 얼굴 위조 데이터셋이 있지만, 대부분 GAN 기반 기술로 생성된 얼굴들이라 최신 기술인 확산 모델은 포함되어 있지 않아.

확산 모델로 생성된 이미지의 다양성과 품질이 크게 향상되면서, 더 도전적인 얼굴 위조 데이터셋을 사용해 최신 위조 감지 기술을 평가해야 해. 이 논문에서는 GenFace라는 대규모, 다양하고 세밀한 고품질 데이터셋을 제안해. 이 데이터셋은 확산 기반 모델 같은 최신 생성기들로 만들어진 많은 위조 얼굴과 조작 방식 및 생성기에 대한 더 자세한 라벨을 포함하고 있어.

우리의 벤치마크에서 최신 기술을 평가하는 것 외에도, 다중 세밀한 외관과 엣지 전역 표현을 포착하고 구별 가능한 위조 흔적을 감지하는 혁신적인 크로스 외관-엣지 학습(CAEL) 탐지기를 설계했어. 또한 두 도메인 간 다양한 통합을 탐구하기 위해 외관-엣지 크로스 어텐션(AECA) 모듈도 개발했어. 광범위한 실험 결과와 시각화는 우리의 탐지 모델이 크로스 생성기, 크로스 위조, 크로스 데이터셋 평가 같은 다양한 설정에서 최신 기술보다 더 뛰어난 성능을 보여준다는 것을 보여줘. 코드와 데이터셋은 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2403.09307.pdf

Title: Annotation Free Semantic Segmentation with Vision Foundation Models

Original Abstract:
Semantic Segmentation is one of the most challenging vision tasks, usually requiring large amounts of training data with expensive pixel level annotations. With the success of foundation models and especially vision-language models, recent works attempt to achieve zeroshot semantic segmentation while requiring either large-scale training or additional image/pixel level annotations. In this work, we generate free annotations for any semantic segmentation dataset using existing foundation models. We use CLIP to detect objects and SAM to generate high quality object masks. Next, we build a lightweight module on top of a self-supervised vision encoder, DinoV2, to align the patch features with a pretrained text encoder for zeroshot semantic segmentation. Our approach can bring language-based semantics to any pretrained vision encoder with minimal training, uses foundation models as the sole source of supervision and generalizes from little training data with no annotation.

Translated Abstract:
시맨틱 세그멘테이션은 가장 어려운 비전 작업 중 하나야. 보통은 많은 양의 훈련 데이터와 비싼 픽셀 수준의 주석이 필요해. 최근에는 파운데이션 모델, 특히 비전-언어 모델의 성공 덕분에 제로샷 시맨틱 세그멘테이션을 시도하는 연구들이 늘어나고 있어. 하지만 대부분 큰 규모의 훈련이나 추가적인 이미지/픽셀 수준의 주석이 필요해.

이번 연구에서는 기존의 파운데이션 모델을 이용해 어떤 시맨틱 세그멘테이션 데이터셋에 대해서도 무료 주석을 생성할 수 있어. 우리는 CLIP을 사용해 객체를 탐지하고, SAM을 이용해 고품질의 객체 마스크를 생성해. 그 다음, 자가 감독 비전 인코더인 DinoV2 위에 가벼운 모듈을 만들어서 패치 특징을 사전 훈련된 텍스트 인코더와 정렬해 제로샷 시맨틱 세그멘테이션을 수행해.

우리의 접근 방식은 최소한의 훈련으로 어떤 사전 훈련된 비전 인코더에도 언어 기반의 의미를 가져올 수 있어. 또한, 파운데이션 모델을 유일한 감독 소스로 사용하고, 주석 없이 적은 훈련 데이터로 일반화할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2403.10164.pdf

Title: CoReEcho: Continuous Representation Learning for 2D+time Echocardiography Analysis

Original Abstract:
Deep learning (DL) models have been advancing automatic medical image analysis on various modalities, including echocardiography, by offering a comprehensive end-to-end training pipeline. This approach enables DL models to regress ejection fraction (EF) directly from 2D+time echocardiograms, resulting in superior performance. However, the end-to-end training pipeline makes the learned representations less explainable. The representations may also fail to capture the continuous relation among echocardiogram clips, indicating the existence of spurious correlations, which can negatively affect the generalization. To mitigate this issue, we propose CoReEcho, a novel training framework emphasizing continuous representations tailored for direct EF regression. Our extensive experiments demonstrate that CoReEcho: 1) outperforms the current state-of-the-art (SOTA) on the largest echocardiography dataset (EchoNet-Dynamic) with MAE of 3.90 & R2 of 82.44, and 2) provides robust and generalizable features that transfer more effectively in related downstream tasks. The code is publicly available at this https URL.

Translated Abstract:
딥러닝(DL) 모델은 심초음파 같은 다양한 의료 영상 분석에서 자동화가 발전하고 있어. 이 모델들은 전체적인 end-to-end 훈련 파이프라인 덕분에 2D+시간 심초음파에서 직접 박출 분율(EF)을 예측할 수 있게 해줘, 그래서 성능이 훨씬 좋아. 

하지만 이런 end-to-end 훈련 파이프라인은 학습된 표현이 덜 설명 가능하게 만들고, 심초음파 클립 간의 지속적인 관계를 잘 잡아내지 못할 수 있어. 이로 인해 가짜 상관관계가 생길 수 있는데, 이게 일반화에 안 좋은 영향을 줄 수 있어. 

이 문제를 해결하기 위해 우리는 CoReEcho라는 새로운 훈련 프레임워크를 제안해. 이건 직접 EF 회귀를 위해 지속적인 표현에 중점을 두고 있어. 우리의 광범위한 실험 결과는 CoReEcho가 1) 가장 큰 심초음파 데이터셋인 EchoNet-Dynamic에서 MAE 3.90과 R2 82.44로 현재 최고 성능을 자랑하고, 2) 관련 하위 작업에서도 더 효과적으로 전이되는 강력하고 일반화 가능한 특징을 제공한다는 걸 보여줘. 코드도 이 URL에서 공개되어 있어.

================================================================================

URL:
https://arxiv.org/pdf/2403.14376.pdf

Title: InfNeRF: Towards Infinite Scale NeRF Rendering with O(log n) Space Complexity

Original Abstract:
The conventional mesh-based Level of Detail (LoD) technique, exemplified by applications such as Google Earth and many game engines, exhibits the capability to holistically represent a large scene even the Earth, and achieves rendering with a space complexity of O(log n). This constrained data requirement not only enhances rendering efficiency but also facilitates dynamic data fetching, thereby enabling a seamless 3D navigation experience for users. In this work, we extend this proven LoD technique to Neural Radiance Fields (NeRF) by introducing an octree structure to represent the scenes in different scales. This innovative approach provides a mathematically simple and elegant representation with a rendering space complexity of O(log n), aligned with the efficiency of mesh-based LoD techniques. We also present a novel training strategy that maintains a complexity of O(n). This strategy allows for parallel training with minimal overhead, ensuring the scalability and efficiency of our proposed method. Our contribution is not only in extending the capabilities of existing techniques but also in establishing a foundation for scalable and efficient large-scale scene representation using NeRF and octree structures.

Translated Abstract:
전통적인 메쉬 기반의 세부 수준(Level of Detail, LoD) 기술은 Google Earth나 여러 게임 엔진 같은 응용 프로그램에서 볼 수 있어. 이 기술은 큰 장면, 심지어 지구 전체를 효과적으로 표현할 수 있고, O(log n)의 공간 복잡도로 렌더링을 할 수 있어. 이렇게 제한된 데이터 요구 사항 덕분에 렌더링 효율이 높아지고, 동적인 데이터 가져오기도 쉽게 되면서 사용자에게 매끄러운 3D 탐색 경험을 제공할 수 있어.

이번 연구에서는 이 검증된 LoD 기술을 Neural Radiance Fields(NeRF)에 확장해 보려고 해. 장면을 다양한 규모로 표현하기 위해 옥트리 구조를 도입했어. 이 새로운 접근 방식은 수학적으로 간단하고 우아한 표현을 제공하며, 메쉬 기반 LoD 기술과 동일하게 O(log n)의 렌더링 공간 복잡도를 가져. 또한 O(n)의 복잡도를 유지하는 새로운 훈련 전략도 제시해. 이 전략은 최소한의 오버헤드로 병렬 훈련이 가능하게 해서, 우리가 제안하는 방법의 확장성과 효율성을 보장해.

우리의 기여는 기존 기술의 능력을 확장하는 것뿐만 아니라 NeRF와 옥트리 구조를 사용한 대규모 장면 표현의 확장 가능하고 효율적인 기초를 마련하는 데 있어.

================================================================================

URL:
https://arxiv.org/pdf/2403.15709.pdf

Title: Contact-aware Human Motion Generation from Textual Descriptions

Original Abstract:
This paper addresses the problem of generating 3D interactive human motion from text. Given a textual description depicting the actions of different body parts in contact with static objects, we synthesize sequences of 3D body poses that are visually natural and physically plausible. Yet, this task poses a significant challenge due to the inadequate consideration of interactions by physical contacts in both motion and textual descriptions, leading to unnatural and implausible sequences. To tackle this challenge, we create a novel dataset named RICH-CAT, representing "Contact-Aware Texts" constructed from the RICH dataset. RICH-CAT comprises high-quality motion, accurate human-object contact labels, and detailed textual descriptions, encompassing over 8,500 motion-text pairs across 26 indoor/outdoor actions. Leveraging RICH-CAT, we propose a novel approach named CATMO for text-driven interactive human motion synthesis that explicitly integrates human body contacts as evidence. We employ two VQ-VAE models to encode motion and body contact sequences into distinct yet complementary latent spaces and an intertwined GPT for generating human motions and contacts in a mutually conditioned manner. Additionally, we introduce a pre-trained text encoder to learn textual embeddings that better discriminate among various contact types, allowing for more precise control over synthesized motions and contacts. Our experiments demonstrate the superior performance of our approach compared to existing text-to-motion methods, producing stable, contact-aware motion sequences. Code and data will be available for research purposes at this https URL

Translated Abstract:
이 논문은 텍스트로부터 3D 인터랙티브한 인간 동작을 생성하는 문제를 다루고 있어. 정적 물체와 접촉하는 여러 신체 부분의 행동을 설명하는 텍스트가 주어지면, 우리는 시각적으로 자연스럽고 물리적으로 그럴듯한 3D 몸 자세 시퀀스를 만들어. 하지만, 이 작업은 동작과 텍스트 설명에서 물리적 접촉에 대한 상호작용을 충분히 고려하지 않아서 자연스럽지 않고 그럴듯하지 않은 시퀀스가 나오는 큰 도전이 있어.

이 문제를 해결하기 위해, 우리는 RICH 데이터셋에서 만든 "Contact-Aware Texts"라는 새로운 데이터셋인 RICH-CAT을 만들었어. RICH-CAT은 고품질의 동작, 정확한 인간-물체 접촉 레이블, 그리고 자세한 텍스트 설명을 포함하고 있는데, 26개의 실내/실외 동작에 걸쳐 8,500개 이상의 동작-텍스트 쌍이 들어 있어.

RICH-CAT을 활용해서, 인간 동작 합성을 위해 텍스트 기반의 새로운 접근법인 CATMO를 제안해. 이 방법은 인간 신체 접촉을 증거로 명확하게 통합해. 우리는 두 개의 VQ-VAE 모델을 사용해서 동작과 신체 접촉 시퀀스를 서로 다른 그러나 보완적인 잠재 공간으로 인코딩하고, 얽힌 GPT를 이용해 인간의 동작과 접촉을 상호 조건적으로 생성해.

게다가, 다양한 접촉 유형을 더 잘 구분할 수 있도록 텍스트 임베딩을 학습하는 사전 훈련된 텍스트 인코더도 도입했어. 이 덕분에 합성된 동작과 접촉을 더 정밀하게 조절할 수 있어. 실험 결과, 우리의 접근법이 기존의 텍스트-모션 방법들보다 뛰어난 성과를 보였고, 안정적이고 접촉을 인지하는 동작 시퀀스를 생성할 수 있었어. 연구 목적으로 사용할 수 있는 코드와 데이터는 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2403.15918.pdf

Title: Towards Adversarial Robustness And Backdoor Mitigation in SSL

Original Abstract:
Self-Supervised Learning (SSL) has shown great promise in learning representations from unlabeled data. The power of learning representations without the need for human annotations has made SSL a widely used technique in real-world problems. However, SSL methods have recently been shown to be vulnerable to backdoor attacks, where the learned model can be exploited by adversaries to manipulate the learned representations, either through tampering the training data distribution, or via modifying the model itself. This work aims to address defending against backdoor attacks in SSL, where the adversary has access to a realistic fraction of the SSL training data, and no access to the model. We use novel methods that are computationally efficient as well as generalizable across different problem settings. We also investigate the adversarial robustness of SSL models when trained with our method, and show insights into increased robustness in SSL via frequency domain augmentations. We demonstrate the effectiveness of our method on a variety of SSL benchmarks, and show that our method is able to mitigate backdoor attacks while maintaining high performance on downstream tasks. Code for our work is available at this http URL

Translated Abstract:
자기 지도 학습(SSL)은 라벨이 없는 데이터에서 표현을 학습하는 데 큰 가능성을 보여주고 있어. 사람의 주석 없이 표현을 학습할 수 있는 능력 덕분에 SSL은 실제 문제에 널리 사용되는 기술이 되었어. 

하지만 최근에 SSL 방법이 백도어 공격에 취약하다는 것이 밝혀졌어. 이 공격에서는 적이 학습된 모델을 이용해 학습된 표현을 조작할 수 있어. 예를 들어, 훈련 데이터 분포를 조작하거나 모델 자체를 수정하는 방법으로 말이지. 

이 연구는 SSL에서 백도어 공격에 대한 방어 방법을 다루고 있어. 여기서 적은 SSL 훈련 데이터의 일부에만 접근할 수 있고, 모델에는 접근할 수 없다는 점이 중요해. 우리는 계산적으로 효율적이면서 다양한 문제 설정에서 일반화 가능한 새로운 방법을 사용해. 

또한, 우리 방법으로 훈련된 SSL 모델의 적대적 강인성도 조사하고, 주파수 도메인 증강을 통해 SSL의 강인성을 높일 수 있다는 통찰을 보여줘. 

우리는 다양한 SSL 벤치마크에서 우리 방법의 효과를 입증했고, 이 방법이 높은 성능을 유지하면서 백도어 공격을 완화할 수 있음을 보여줬어. 우리 연구의 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2404.03876.pdf

Title: Accurately Classifying Out-Of-Distribution Data in Facial Recognition

Original Abstract:
Standard classification theory assumes that the distribution of images in the test and training sets are identical. Unfortunately, real-life scenarios typically feature unseen data (''out-of-distribution data") which is different from data in the training distribution(''in-distribution"). This issue is most prevalent in social justice problems where data from under-represented groups may appear in the test data without representing an equal proportion of the training data. This may result in a model returning confidently wrong decisions and predictions. We are interested in the following question: Can the performance of a neural network improve on facial images of out-of-distribution data when it is trained simultaneously on multiple datasets of in-distribution data? We approach this problem by incorporating the Outlier Exposure model and investigate how the model's performance changes when other datasets of facial images were implemented. We observe that the accuracy and other metrics of the model can be increased by applying Outlier Exposure, incorporating a trainable weight parameter to increase the machine's emphasis on outlier images, and by re-weighting the importance of different class labels. We also experimented with whether sorting the images and determining outliers via image features would have more of an effect on the metrics than sorting by average pixel value, and found no conclusive results. Our goal was to make models not only more accurate but also more fair by scanning a more expanded range of images. We also tested the datasets in reverse order to see whether a more fair dataset with balanced features has an effect on the model's accuracy. Utilizing Python and the Pytorch package, we found that models that utilizing outlier exposure could make models more fair.

Translated Abstract:
표준 분류 이론은 테스트 세트와 훈련 세트의 이미지 분포가 동일하다고 가정해. 근데 현실에서는 보지 못했던 데이터, 즉 "분포 밖 데이터"가 존재해. 이 데이터는 훈련 데이터와는 다르게 나타나. 이 문제는 사회 정의 문제에서 특히 심각한데, 대표성이 부족한 그룹의 데이터가 테스트 데이터에 나타나면서 훈련 데이터와 비율이 맞지 않을 수 있어. 이럴 경우 모델이 자신 있게 잘못된 결정이나 예측을 할 수 있어.

우리는 다음 질문에 관심이 있어: 여러 개의 훈련 데이터셋을 동시에 사용해서 훈련할 때, 분포 밖 데이터의 얼굴 이미지에서 신경망의 성능이 향상될 수 있을까? 이 문제에 접근하기 위해 Outlier Exposure 모델을 도입하고, 다른 얼굴 이미지 데이터셋을 적용할 때 모델 성능이 어떻게 변하는지 조사했어. 

그 결과, Outlier Exposure를 적용하고, 기계가 아웃라이어 이미지에 더 중점을 두도록 훈련 가능한 가중치 매개변수를 추가하며, 서로 다른 클래스 레이블의 중요성을 재조정함으로써 모델의 정확도와 다른 지표를 높일 수 있었어. 

또한, 이미지 특징을 통해 이미지를 정렬하고 아웃라이어를 결정하는 방법이 평균 픽셀 값으로 정렬하는 것보다 지표에 더 큰 영향을 미치는지 실험했지만, 뚜렷한 결과는 없었어. 우리의 목표는 모델을 더 정확하게 만드는 것뿐만 아니라, 더 많은 이미지를 스캔함으로써 공정하게 만드는 것이었어. 

마지막으로, 우리는 균형 잡힌 특징을 가진 더 공정한 데이터셋이 모델의 정확도에 영향을 미치는지 확인하기 위해 데이터셋을 역순으로 테스트했어. Python과 Pytorch 패키지를 활용해서, 아웃라이어 노출을 사용하는 모델이 더 공정해질 수 있다는 것을 발견했어.

================================================================================

URL:
https://arxiv.org/pdf/2404.04256.pdf

Title: Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation

Original Abstract:
Multi-modal semantic segmentation significantly enhances AI agents' perception and scene understanding, especially under adverse conditions like low-light or overexposed environments. Leveraging additional modalities (X-modality) like thermal and depth alongside traditional RGB provides complementary information, enabling more robust and reliable prediction. In this work, we introduce Sigma, a Siamese Mamba network for multi-modal semantic segmentation utilizing the advanced Mamba. Unlike conventional methods that rely on CNNs, with their limited local receptive fields, or Vision Transformers (ViTs), which offer global receptive fields at the cost of quadratic complexity, our model achieves global receptive fields with linear complexity. By employing a Siamese encoder and innovating a Mamba-based fusion mechanism, we effectively select essential information from different modalities. A decoder is then developed to enhance the channel-wise modeling ability of the model. Our proposed method is rigorously evaluated on both RGB-Thermal and RGB-Depth semantic segmentation tasks, demonstrating its superiority and marking the first successful application of State Space Models (SSMs) in multi-modal perception tasks. Code is available at this https URL.

Translated Abstract:
다중 모달 의미 분할은 AI 에이전트의 인식과 장면 이해를 크게 향상시켜줘. 특히 저조도나 노출 과다 환경 같은 힘든 조건에서 효과적이야. 전통적인 RGB 외에 열 감지 및 깊이 같은 추가 모달리티(X-모달리티)를 활용하면 보완적인 정보를 제공해서 더 견고하고 신뢰할 수 있는 예측이 가능해. 

이번 연구에서 우리는 Sigma라는 이름의 시암식 마밤바 네트워크를 소개해. 이 네트워크는 고급 마밤바를 활용한 다중 모달 의미 분할을 위한 거야. 기존 방법들은 CNN에 의존해서 지역 수용 필드가 제한적이거나, 비전 변환기(ViT)는 전역 수용 필드를 제공하지만 복잡도가 커지는 단점이 있어. 반면, 우리 모델은 선형 복잡도로 전역 수용 필드를 달성해. 

시암식 인코더를 사용하고 마밤바 기반의 융합 메커니즘을 혁신적으로 도입해서 다양한 모달리티에서 중요한 정보를 효과적으로 선택해. 그리고 디코더를 개발해서 모델의 채널별 모델링 능력을 향상시켜. 제안한 방법은 RGB-열 감지와 RGB-깊이 의미 분할 작업에서 철저하게 평가되었고, 그 우수성을 입증했어. 또한 다중 모달 인식 작업에서 상태 공간 모델(SSM)의 첫 번째 성공적인 적용을 보여줬어. 코드도 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2404.04556.pdf

Title: Rethinking Self-training for Semi-supervised Landmark Detection: A Selection-free Approach

Original Abstract:
Self-training is a simple yet effective method for semi-supervised learning, during which pseudo-label selection plays an important role for handling confirmation bias. Despite its popularity, applying self-training to landmark detection faces three problems: 1) The selected confident pseudo-labels often contain data bias, which may hurt model performance; 2) It is not easy to decide a proper threshold for sample selection as the localization task can be sensitive to noisy pseudo-labels; 3) coordinate regression does not output confidence, making selection-based self-training infeasible. To address the above issues, we propose Self-Training for Landmark Detection (STLD), a method that does not require explicit pseudo-label selection. Instead, STLD constructs a task curriculum to deal with confirmation bias, which progressively transitions from more confident to less confident tasks over the rounds of self-training. Pseudo pretraining and shrink regression are two essential components for such a curriculum, where the former is the first task of the curriculum for providing a better model initialization and the latter is further added in the later rounds to directly leverage the pseudo-labels in a coarse-to-fine manner. Experiments on three facial and one medical landmark detection benchmark show that STLD outperforms the existing methods consistently in both semi- and omni-supervised settings. The code is available at this https URL.

Translated Abstract:
자기 학습은 반지도 학습에서 간단하면서도 효과적인 방법인데, 이 과정에서 가짜 레이블 선택이 확인 편향을 처리하는 데 중요한 역할을 해. 그런데 랜드마크 감지에 자기 학습을 적용할 때는 세 가지 문제가 있어: 

1) 선택된 확신 있는 가짜 레이블이 데이터 편향을 포함할 수 있어서 모델 성능에 나쁜 영향을 줄 수 있어. 
2) 샘플 선택을 위한 적절한 임계치를 정하기가 어려워, 왜냐하면 위치 지정 작업이 노이즈가 많은 가짜 레이블에 민감할 수 있기 때문이야. 
3) 좌표 회귀는 신뢰도를 출력하지 않아서 선택 기반 자기 학습이 불가능해.

이런 문제들을 해결하기 위해, 우리는 랜드마크 감지를 위한 자기 학습(STLD) 방법을 제안해. 이 방법은 명시적인 가짜 레이블 선택이 필요하지 않아. 대신 STLD는 확인 편향을 다루기 위해 작업 커리큘럼을 구성해, 자기 학습의 여러 라운드에서 더 확신 있는 작업에서 덜 확신 있는 작업으로 점진적으로 전환해.

가짜 사전 훈련과 축소 회귀는 이런 커리큘럼의 두 가지 중요한 요소야. 전자는 더 나은 모델 초기화를 제공하기 위해 커리큘럼의 첫 번째 작업이고, 후자는 이후 라운드에서 가짜 레이블을 조잡하게부터 세밀하게 활용할 수 있도록 추가돼.

세 가지 얼굴 랜드마크와 하나의 의료 랜드마크 감지 벤치마크에서 실험한 결과, STLD가 기존 방법들보다 반지도 및 전면 지도 설정 모두에서 일관되게 성능이 뛰어난 걸 보여줬어. 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2404.06666.pdf

Title: SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image Models

Original Abstract:
Text-to-image (T2I) models, such as Stable Diffusion, have exhibited remarkable performance in generating high-quality images from text descriptions in recent years. However, text-to-image models may be tricked into generating not-safe-for-work (NSFW) content, particularly in sexually explicit scenarios. Existing countermeasures mostly focus on filtering inappropriate inputs and outputs, or suppressing improper text embeddings, which can block sexually explicit content (e.g., naked) but may still be vulnerable to adversarial prompts -- inputs that appear innocent but are ill-intended. In this paper, we present SafeGen, a framework to mitigate sexual content generation by text-to-image models in a text-agnostic manner. The key idea is to eliminate explicit visual representations from the model regardless of the text input. In this way, the text-to-image model is resistant to adversarial prompts since such unsafe visual representations are obstructed from within. Extensive experiments conducted on four datasets and large-scale user studies demonstrate SafeGen's effectiveness in mitigating sexually explicit content generation while preserving the high-fidelity of benign images. SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.4% sexual content removal performance. Furthermore, our constructed benchmark of adversarial prompts provides a basis for future development and evaluation of anti-NSFW-generation methods.

Translated Abstract:
텍스트-이미지(T2I) 모델, 예를 들어 Stable Diffusion 같은 것들이 최근 몇 년 사이에 텍스트 설명을 바탕으로 고품질 이미지를 생성하는 데 놀라운 성능을 보여줬어. 그런데 이 모델들이 특히 성적인 상황에서 NSFW(직장 내 부적합) 콘텐츠를 생성하도록 속일 수 있는 경우가 있어.

기존의 대응책들은 주로 부적절한 입력과 출력을 필터링하거나, 잘못된 텍스트 임베딩을 억제하는 데 초점을 맞추고 있어. 이 방법들은 성적인 콘텐츠(예: 나체)를 차단할 수 있지만, 무해해 보이는 입력이 악의적인 경우에는 여전히 취약할 수 있어.

이 논문에서는 SafeGen이라는 프레임워크를 제안해. 이건 텍스트 입력과 상관없이 텍스트-이미지 모델에서 성적인 콘텐츠 생성을 줄이는 방법이야. 핵심 아이디어는 모델에서 노골적인 시각적 표현을 제거하는 거야. 이렇게 하면 텍스트-이미지 모델이 악의적인 입력에 저항력을 갖게 돼, 왜냐하면 그런 위험한 시각적 표현이 내부에서 차단되니까.

네 개의 데이터셋과 대규모 사용자 연구를 통해 SafeGen의 효과를 입증했어. 성적인 콘텐츠 생성을 줄이면서도 무해한 이미지의 고충실도를 유지하는 데 성공했지. SafeGen은 여덟 개의 최신 기법보다 뛰어난 성능을 보였고, 99.4%의 성적인 콘텐츠 제거 성능을 달성했어. 게다가 우리가 만든 악의적인 입력 기준은 앞으로 NSFW 생성 방지 방법의 개발과 평가를 위한 기초가 될 거야.

================================================================================

URL:
https://arxiv.org/pdf/2404.09359.pdf

Title: Evaluation Framework for Feedback Generation Methods in Skeletal Movement Assessment

Original Abstract:
The application of machine-learning solutions to movement assessment from skeleton videos has attracted significant research attention in recent years. This advancement has made rehabilitation at home more accessible, utilizing movement assessment algorithms that can operate on affordable equipment for human pose detection and analysis from 2D or 3D videos. While the primary objective of automatic assessment tasks is to score movements, the automatic generation of feedback highlighting key movement issues has the potential to significantly enhance and accelerate the rehabilitation process. While numerous research works exist in the field of automatic movement assessment, only a handful address feedback generation. In this study, we propose terminology and criteria for the classification, evaluation, and comparison of feedback generation solutions. We discuss the challenges associated with each feedback generation approach and use our proposed criteria to classify existing solutions. To our knowledge, this is the first work that formulates feedback generation in skeletal movement assessment.

Translated Abstract:
최근 몇 년 동안, 스켈레톤 비디오를 활용한 움직임 평가에 머신러닝 솔루션을 적용하는 연구가 많은 관심을 받고 있어. 이 발전 덕분에 집에서 재활을 더 쉽게 할 수 있게 되었어. 저렴한 장비로 2D나 3D 비디오에서 사람의 자세를 감지하고 분석하는 알고리즘을 사용할 수 있으니까.

자동 평가 작업의 주된 목표는 움직임을 점수 내는 거지만, 주요 움직임 문제를 강조하는 피드백을 자동으로 생성하는 것은 재활 과정을 크게 향상시키고 빠르게 할 수 있는 가능성이 있어. 자동 움직임 평가 분야에서 많은 연구가 있지만, 피드백 생성을 다룬 연구는 정말 몇 개 없어.

이번 연구에서는 피드백 생성 솔루션을 분류, 평가, 비교하기 위한 용어와 기준을 제안해. 각 피드백 생성 접근 방식과 관련된 도전 과제를 논의하고, 제안한 기준을 사용해 기존 솔루션을 분류할 거야. 우리가 아는 한, 이 연구는 스켈레탈 움직임 평가에서 피드백 생성을 공식화한 첫 번째 작업이야.

================================================================================

URL:
https://arxiv.org/pdf/2404.09942.pdf

Title: Knowledge-enhanced Visual-Language Pretraining for Computational Pathology

Original Abstract:
In this paper, we consider the problem of visual representation learning for computational pathology, by exploiting large-scale image-text pairs gathered from public resources, along with the domain-specific knowledge in pathology. Specifically, we make the following contributions: (i) We curate a pathology knowledge tree that consists of 50,470 informative attributes for 4,718 diseases requiring pathology diagnosis from 32 human tissues. To our knowledge, this is the first comprehensive structured pathology knowledge base; (ii) We develop a knowledge-enhanced visual-language pretraining approach, where we first project pathology-specific knowledge into latent embedding space via a language model, and use it to guide the visual representation learning; (iii) We conduct thorough experiments to validate the effectiveness of our proposed components, demonstrating significant performance improvement on various downstream tasks, including cross-modal retrieval, zero-shot classification on pathology patches, and zero-shot tumor subtyping on whole slide images (WSIs).

Translated Abstract:
이 논문에서는 컴퓨터 병리학을 위한 시각적 표현 학습 문제를 다루고, 공공 자원에서 수집한 대규모 이미지-텍스트 쌍과 병리학의 도메인 특화 지식을 활용해요.

구체적으로, 다음과 같은 기여를 해요: 

(i) 32개의 인체 조직에서 병리 진단이 필요한 4,718개의 질병에 대한 50,470개의 유용한 속성으로 구성된 병리학 지식 트리를 만들었어요. 우리가 아는 한, 이건 첫 번째로 포괄적인 구조화된 병리학 지식 기반이에요.

(ii) 우리는 지식 기반의 시각-언어 사전 훈련 접근법을 개발했어요. 여기서 병리학 특화 지식을 언어 모델을 통해 잠재적 임베딩 공간에 투영하고, 이를 사용해 시각적 표현 학습을 안내해요.

(iii) 제안한 구성 요소들의 효과를 검증하기 위해 철저한 실험을 수행했어요. 그 결과, 다양한 하위 작업, 즉 교차 모달 검색, 병리 패치에 대한 제로샷 분류, 전체 슬라이드 이미지(WSI)에 대한 제로샷 종양 하위 유형 분류에서 성능이 크게 향상되었음을 보여주었어요.

================================================================================

URL:
https://arxiv.org/pdf/2404.15721.pdf

Title: SPARO: Selective Attention for Robust and Compositional Transformer Encodings for Vision

Original Abstract:
Selective attention helps us focus on task-relevant aspects in the constant flood of our sensory input. This constraint in our perception allows us to robustly generalize under distractions and to new compositions of perceivable concepts. Transformers employ a similar notion of attention in their architecture, but representation learning models with transformer backbones like CLIP and DINO often fail to demonstrate robustness and compositionality. We highlight a missing architectural prior: unlike human perception, transformer encodings do not separately attend over individual concepts. In response, we propose SPARO, a read-out mechanism that partitions encodings into separately-attended slots, each produced by a single attention head. Using SPARO with CLIP imparts an inductive bias that the vision and text modalities are different views of a shared compositional world with the same corresponding concepts. Using SPARO, we demonstrate improvements on downstream recognition, robustness, retrieval, and compositionality benchmarks with CLIP (up to +14% for ImageNet, +4% for SugarCrepe), and on nearest neighbors and linear probe for ImageNet with DINO (+3% each). We also showcase a powerful ability to intervene and select individual SPARO concepts to further improve downstream task performance (up from +4% to +9% for SugarCrepe) and use this ability to study the robustness of SPARO's representation structure. Finally, we provide insights through ablation experiments and visualization of learned concepts.

Translated Abstract:
선택적 주의는 우리가 감각 입력의 끊임없는 홍수 속에서 중요한 부분에 집중할 수 있게 도와줘. 이런 인식의 제약 덕분에 우리는 방해가 있는 상황에서도 잘 일반화할 수 있고, 새로운 개념 조합을 이해할 수 있어. 트랜스포머도 비슷한 주의 개념을 사용하는데, CLIP이나 DINO 같은 트랜스포머 기반의 표현 학습 모델은 종종 강건성과 조합성을 잘 보여주지 않아.

우리는 여기서 중요한 구조적 요소가 빠져 있다는 점을 강조해. 인간의 인식과 달리 트랜스포머 인코딩은 개별 개념에 대해 따로 주의를 기울이지 않아. 그래서 우리는 SPARO라는 읽기 메커니즘을 제안했어. 이 메커니즘은 인코딩을 각각의 주의 헤드가 생성한 따로 주목받는 슬롯으로 나눠. CLIP과 함께 SPARO를 사용하면 비전 모드와 텍스트 모드가 같은 개념을 가진 공유된 조합 세계의 서로 다른 시각이라는 편향이 생겨.

SPARO를 사용해서 CLIP의 하위 인식, 강건성, 검색, 조합성 벤치마크에서 개선을 보여줬어 (ImageNet에서 최대 +14%, SugarCrepe에서 +4% 향상). DINO의 ImageNet에 대한 최근접 이웃과 선형 프로브에서도 각각 +3% 향상됐어. 또, SPARO 개념을 개별적으로 선택하고 개입할 수 있는 강력한 능력을 보여주면서 하위 작업 성능을 더 향상시킬 수 있었어 (SugarCrepe에서 +4%에서 +9%로 증가). 이 능력을 사용해 SPARO의 표현 구조의 강건성을 연구했어.

마지막으로, 우리는 제거 실험과 학습된 개념을 시각화하여 통찰력을 제공했어.

================================================================================

URL:
https://arxiv.org/pdf/2404.16471.pdf

Title: COBRA -- COnfidence score Based on shape Regression Analysis for method-independent quality assessment of object pose estimation from single images

Original Abstract:
We present a generic algorithm for scoring pose estimation methods that rely on single image semantic analysis. The algorithm employs a lightweight putative shape representation using a combination of multiple Gaussian Processes. Each Gaussian Process (GP) yields distance normal distributions from multiple reference points in the object's coordinate system to its surface, thus providing a geometric evaluation framework for scoring predicted poses. Our confidence measure comprises the average mixture probability of pixel back-projections onto the shape template. In the reported experiments, we compare the accuracy of our GP based representation of objects versus the actual geometric models and demonstrate the ability of our method to capture the influence of outliers as opposed to the corresponding intrinsic measures that ship with the segmentation and pose estimation methods.

Translated Abstract:
우리는 단일 이미지 의미 분석에 기반한 자세 추정 방법을 점수 매기는 일반적인 알고리즘을 소개해. 이 알고리즘은 여러 개의 가우시안 프로세스를 조합해서 가벼운 형태 표현을 사용해. 각 가우시안 프로세스(GP)는 물체의 좌표계에서 여러 기준점까지의 거리 정규 분포를 만들어서, 예측된 자세를 점수 매길 수 있는 기하학적 평가 프레임워크를 제공해.

우리의 신뢰도 측정은 픽셀 역투영의 평균 혼합 확률로 구성돼. 실험에서는 우리의 GP 기반 물체 표현과 실제 기하학적 모델의 정확성을 비교해 보고, 아웃라이어의 영향을 잘 포착할 수 있는 방법을 보여줘. 이는 세분화(segmentation)와 자세 추정 방법과 함께 제공되는 고유 측정치와는 다르게 작동해.

================================================================================

URL:
https://arxiv.org/pdf/2404.17961.pdf

Title: Random Walk on Pixel Manifolds for Anomaly Segmentation of Complex Driving Scenes

Original Abstract:
In anomaly segmentation for complex driving scenes, state-of-the-art approaches utilize anomaly scoring functions to calculate anomaly scores. For these functions, accurately predicting the logits of inlier classes for each pixel is crucial for precisely inferring the anomaly score. However, in real-world driving scenarios, the diversity of scenes often results in distorted manifolds of pixel embeddings in the space. This effect is not conducive to directly using the pixel embeddings for the logit prediction during inference, a concern overlooked by existing methods. To address this problem, we propose a novel method called Random Walk on Pixel Manifolds (RWPM). RWPM utilizes random walks to reveal the intrinsic relationships among pixels to refine the pixel embeddings. The refined pixel embeddings alleviate the distortion of manifolds, improving the accuracy of anomaly scores. Our extensive experiments show that RWPM consistently improve the performance of the existing anomaly segmentation methods and achieve the best results. Code is available at: \url{this https URL}.

Translated Abstract:
복잡한 주행 장면에서 이상 분할을 할 때, 최신 방법들은 이상 점수 계산을 위해 이상 점수 함수(anomaly scoring function)를 사용해. 이 함수에서는 각 픽셀의 정상 클래스 로그잇(logit)을 정확하게 예측하는 게 이상 점수를 정확하게 추론하는 데 중요해. 하지만 실제 주행 상황에서는 장면의 다양성 때문에 픽셀 임베딩의 왜곡된 다각형(manifold)이 생겨. 이런 점은 기존 방법들이 간과하고 있는 문제야. 

이 문제를 해결하기 위해 우리는 RWPM(Random Walk on Pixel Manifolds)이라는 새로운 방법을 제안해. RWPM은 랜덤 워크를 활용해서 픽셀 간의 본질적인 관계를 밝혀내고 픽셀 임베딩을 개선해. 이렇게 개선된 픽셀 임베딩은 왜곡된 다각형을 완화시켜서 이상 점수의 정확도를 높여줘. 

우리가 진행한 여러 실험 결과, RWPM이 기존의 이상 분할 방법들의 성능을 꾸준히 향상시키고 최상의 결과를 도출하는 걸 보여줬어. 코드는 여기에서 확인할 수 있어: \url{this https URL}.

================================================================================

URL:
https://arxiv.org/pdf/2405.13722.pdf

Title: LightningDrag: Lightning Fast and Accurate Drag-based Image Editing Emerging from Videos

Original Abstract:
Accuracy and speed are critical in image editing tasks. Pan et al. introduced a drag-based image editing framework that achieves pixel-level control using Generative Adversarial Networks (GANs). A flurry of subsequent studies enhanced this framework's generality by leveraging large-scale diffusion models. However, these methods often suffer from inordinately long processing times (exceeding 1 minute per edit) and low success rates. Addressing these issues head on, we present LightningDrag, a rapid approach enabling high quality drag-based image editing in ~1 second. Unlike most previous methods, we redefine drag-based editing as a conditional generation task, eliminating the need for time-consuming latent optimization or gradient-based guidance during inference. In addition, the design of our pipeline allows us to train our model on large-scale paired video frames, which contain rich motion information such as object translations, changing poses and orientations, zooming in and out, etc. By learning from videos, our approach can significantly outperform previous methods in terms of accuracy and consistency. Despite being trained solely on videos, our model generalizes well to perform local shape deformations not presented in the training data (e.g., lengthening of hair, twisting rainbows, etc.). Extensive qualitative and quantitative evaluations on benchmark datasets corroborate the superiority of our approach. The code and model will be released at this https URL.

Translated Abstract:
이미지 편집 작업에서는 정확성과 속도가 정말 중요해. Pan et al.은 생성적 적대 신경망(GANs)을 사용해서 픽셀 수준의 제어가 가능한 드래그 기반 이미지 편집 프레임워크를 소개했어. 이후 여러 연구들이 대규모 확산 모델을 활용해서 이 프레임워크의 범위를 넓혔지만, 이 방법들은 보통 처리 시간이 너무 길고(편집당 1분 이상) 성공률이 낮다는 문제가 있었어.

이런 문제를 해결하기 위해 우리는 LightningDrag이라는 빠른 접근 방식을 제안해. 이 방법은 고품질 드래그 기반 이미지 편집을 약 1초 만에 가능하게 해. 대부분의 기존 방법들과 달리, 우리는 드래그 기반 편집을 조건부 생성 작업으로 재정의해서, 추론 중에 시간이 많이 걸리는 잠재 최적화나 기울기 기반 가이드를 없앴어.

또한, 우리의 파이프라인 디자인 덕분에 우리는 객체 이동, 자세와 방향 변화, 확대 및 축소 같은 풍부한 모션 정보를 포함한 대규모 쌍 영상 프레임에서 모델을 학습할 수 있어. 비디오에서 배우면서, 우리의 접근 방식은 정확성과 일관성 면에서 이전 방법들을 크게 능가할 수 있어. 비디오에서만 학습했지만, 우리의 모델은 훈련 데이터에 없던 지역 형태 변형(예: 머리 길어지기, 무지개 비틀기 등)을 잘 수행할 수 있어.

벤치마크 데이터셋에 대한 광범위한 질적 및 정량적 평가가 우리의 접근 방식이 우수하다는 것을 증명해. 코드와 모델은 이 URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2405.14869.pdf

Title: PuzzleAvatar: Assembling 3D Avatars from Personal Albums

Original Abstract:
Generating personalized 3D avatars is crucial for AR/VR. However, recent text-to-3D methods that generate avatars for celebrities or fictional characters, struggle with everyday people. Methods for faithful reconstruction typically require full-body images in controlled settings. What if a user could just upload their personal "OOTD" (Outfit Of The Day) photo collection and get a faithful avatar in return? The challenge is that such casual photo collections contain diverse poses, challenging viewpoints, cropped views, and occlusion (albeit with a consistent outfit, accessories and hairstyle). We address this novel "Album2Human" task by developing PuzzleAvatar, a novel model that generates a faithful 3D avatar (in a canonical pose) from a personal OOTD album, while bypassing the challenging estimation of body and camera pose. To this end, we fine-tune a foundational vision-language model (VLM) on such photos, encoding the appearance, identity, garments, hairstyles, and accessories of a person into (separate) learned tokens and instilling these cues into the VLM. In effect, we exploit the learned tokens as "puzzle pieces" from which we assemble a faithful, personalized 3D avatar. Importantly, we can customize avatars by simply inter-changing tokens. As a benchmark for this new task, we collect a new dataset, called PuzzleIOI, with 41 subjects in a total of nearly 1K OOTD configurations, in challenging partial photos with paired ground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high reconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique scalability to album photos, and strong robustness. Our code and data are publicly available for research purpose at this https URL

Translated Abstract:
개인화된 3D 아바타 생성은 AR/VR에서 정말 중요해. 하지만 최근의 텍스트-투-3D 방법들은 유명인이나 가상의 캐릭터 아바타는 잘 만들지만, 일반 사람들에겐 어려워. 보통 정확한 재구성을 하려면 통제된 환경에서 전체 몸 사진이 필요해. 그런데 사용자가 자신의 "OOTD" (오늘의 의상) 사진 모음을 업로드하면 신뢰할 수 있는 아바타를 받을 수 있다면 어떨까? 

문제는 이런 캐주얼한 사진 모음이 다양한 포즈, 어려운 시점, 잘려진 뷰, 그리고 가림 현상을 포함하고 있다는 거야 (물론 의상, 액세서리, 헤어스타일은 일관되게 유지되지만). 우리는 이 새로운 "Album2Human" 작업을 PuzzleAvatar라는 새로운 모델을 개발해 해결했어. 이 모델은 개인 OOTD 앨범에서 신뢰할 수 있는 3D 아바타를 생성하는데, 몸과 카메라 포즈를 복잡하게 추정할 필요가 없어.

이를 위해, 우리는 이런 사진들로 기본 비전-언어 모델(VLM)을 미세 조정했어. 이 과정에서 사람의 외모, 정체성, 의상, 헤어스타일, 액세서리를 각각의 학습된 토큰으로 인코딩하고, 이 단서들을 VLM에 주입했어. 이렇게 해서 우리는 학습된 토큰을 "퍼즐 조각"처럼 활용해 신뢰할 수 있고 개인화된 3D 아바타를 조립할 수 있어. 중요한 점은, 토큰을 간단히 교환함으로써 아바타를 커스터마이즈할 수 있다는 거야.

이 새로운 작업의 기준으로 우리는 PuzzleIOI라는 새로운 데이터셋을 수집했어. 이 데이터셋은 총 41명의 주제와 1천 개에 가까운 OOTD 구성을 포함하고 있으며, 어려운 부분 사진과 쌍을 이룬 실제 3D 몸체가 있어. 평가 결과, PuzzleAvatar는 높은 재구성 정확도를 보여주고 TeCH와 MVDreamBooth보다 뛰어난 성능을 보였어. 또한 앨범 사진에 대한 독특한 확장성과 강한 견고성도 가지고 있어. 우리의 코드와 데이터는 연구 목적으로 공개되어 있으니 이 링크에서 확인해봐.

================================================================================

URL:
https://arxiv.org/pdf/2405.20606.pdf

Title: Vision-Language Meets the Skeleton: Progressively Distillation with Cross-Modal Knowledge for 3D Action Representation Learning

Original Abstract:
Skeleton-based action representation learning aims to interpret and understand human behaviors by encoding the skeleton sequences, which can be categorized into two primary training paradigms: supervised learning and self-supervised learning. However, the former one-hot classification requires labor-intensive predefined action categories annotations, while the latter involves skeleton transformations (e.g., cropping) in the pretext tasks that may impair the skeleton structure. To address these challenges, we introduce a novel skeleton-based training framework (C$^2$VL) based on Cross-modal Contrastive learning that uses the progressive distillation to learn task-agnostic human skeleton action representation from the Vision-Language knowledge prompts. Specifically, we establish the vision-language action concept space through vision-language knowledge prompts generated by pre-trained large multimodal models (LMMs), which enrich the fine-grained details that the skeleton action space lacks. Moreover, we propose the intra-modal self-similarity and inter-modal cross-consistency softened targets in the cross-modal representation learning process to progressively control and guide the degree of pulling vision-language knowledge prompts and corresponding skeletons closer. These soft instance discrimination and self-knowledge distillation strategies contribute to the learning of better skeleton-based action representations from the noisy skeleton-vision-language pairs. During the inference phase, our method requires only the skeleton data as the input for action recognition and no longer for vision-language prompts. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate that our method outperforms the previous methods and achieves state-of-the-art results. Code is available at: this https URL.

Translated Abstract:
스켈레톤 기반 행동 표현 학습은 사람의 행동을 해석하고 이해하는 데 도움을 주기 위해 스켈레톤 시퀀스를 인코딩하는 걸 목표로 해. 이 과정은 주로 두 가지 훈련 방식으로 나눌 수 있어: 감독 학습과 자기 감독 학습. 

감독 학습은 미리 정의된 행동 카테고리에 대한 주석이 필요해서 작업이 번거롭고, 자기 감독 학습은 스켈레톤 구조를 손상시킬 수 있는 변형(예: 크롭)을 포함하는 전제 작업이 있어. 이런 문제를 해결하기 위해 우리는 새로운 스켈레톤 기반 훈련 프레임워크인 C$^2$VL을 소개해. 이건 크로스 모달 대비 학습에 기반하고, 비전-언어 지식 프롬프트에서 작업에 구애받지 않는 인간 스켈레톤 행동 표현을 학습하기 위해 점진적인 증류 과정을 사용해.

구체적으로, 우리는 미리 훈련된 대형 다중 모달 모델(LMM)로 생성한 비전-언어 지식 프롬프트를 통해 비전-언어 행동 개념 공간을 구축해. 이게 스켈레톤 행동 공간이 부족한 세부 사항을 보완해줘. 게다가, 우리는 크로스 모달 표현 학습 과정에서 내부 모달 자기 유사성과 외부 모달 교차 일관성의 부드러운 목표를 제안해. 이걸 통해 비전-언어 지식 프롬프트와 해당 스켈레톤을 더 가깝게 끌어오는 정도를 점진적으로 조정하고 안내할 수 있어.

이런 부드러운 인스턴스 구별 및 자기 지식 증류 전략은 시끄러운 스켈레톤-비전-언어 쌍으로부터 더 나은 스켈레톤 기반 행동 표현을 학습하는 데 기여해. 추론 단계에서는 우리의 방법이 행동 인식을 위해 스켈레톤 데이터만 입력으로 필요하고, 더 이상 비전-언어 프롬프트가 필요하지 않아. NTU RGB+D 60, NTU RGB+D 120, PKU-MMD 데이터셋에서의 다양한 실험 결과, 우리의 방법이 이전 방법들을 능가하고 최신 기술 수준의 결과를 달성했음을 보여줘. 코드도 이 링크에서 확인할 수 있어: this https URL.

================================================================================

URL:
https://arxiv.org/pdf/2406.00625.pdf

Title: SAM-LAD: Segment Anything Model Meets Zero-Shot Logic Anomaly Detection

Original Abstract:
Visual anomaly detection is vital in real-world applications, such as industrial defect detection and medical diagnosis. However, most existing methods focus on local structural anomalies and fail to detect higher-level functional anomalies under logical conditions. Although recent studies have explored logical anomaly detection, they can only address simple anomalies like missing or addition and show poor generalizability due to being heavily data-driven. To fill this gap, we propose SAM-LAD, a zero-shot, plug-and-play framework for logical anomaly detection in any scene. First, we obtain a query image's feature map using a pre-trained backbone. Simultaneously, we retrieve the reference images and their corresponding feature maps via the nearest neighbor search of the query image. Then, we introduce the Segment Anything Model (SAM) to obtain object masks of the query and reference images. Each object mask is multiplied with the entire image's feature map to obtain object feature maps. Next, an Object Matching Model (OMM) is proposed to match objects in the query and reference images. To facilitate object matching, we further propose a Dynamic Channel Graph Attention (DCGA) module, treating each object as a keypoint and converting its feature maps into feature vectors. Finally, based on the object matching relations, an Anomaly Measurement Model (AMM) is proposed to detect objects with logical anomalies. Structural anomalies in the objects can also be detected. We validate our proposed SAM-LAD using various benchmarks, including industrial datasets (MVTec Loco AD, MVTec AD), and the logical dataset (DigitAnatomy). Extensive experimental results demonstrate that SAM-LAD outperforms existing SoTA methods, particularly in detecting logical anomalies.

Translated Abstract:
시각적 이상 탐지는 산업 결함 탐지나 의료 진단 같은 실제 응용에서 매우 중요해. 하지만 기존의 많은 방법들은 지역적인 구조적 이상에만 집중하고, 논리적 조건 아래에서 더 높은 수준의 기능적 이상을 탐지하는 데 실패해. 최근 연구들은 논리적 이상 탐지를 다루긴 했지만, 단순한 이상(예: 결측이나 추가)만 해결할 수 있고, 데이터 기반이라 일반화가 잘 안 되는 문제가 있어.

이런 문제를 해결하기 위해 우리는 SAM-LAD라는 제로샷, 플러그 앤 플레이 프레임워크를 제안해. 이건 어떤 장면에서도 논리적 이상을 탐지할 수 있어. 먼저, 쿼리 이미지의 특징 맵을 미리 학습된 백본을 사용해 얻어. 동시에 쿼리 이미지와 가장 가까운 이웃 검색을 통해 참조 이미지와 그에 해당하는 특징 맵을 가져와. 그런 다음, Segment Anything Model(SAM)을 도입해서 쿼리와 참조 이미지의 객체 마스크를 얻어. 각 객체 마스크는 전체 이미지의 특징 맵과 곱해져 객체 특징 맵을 만들어.

다음으로, 쿼리와 참조 이미지의 객체를 매칭하기 위한 객체 매칭 모델(Object Matching Model, OMM)을 제안해. 객체 매칭을 쉽게 하기 위해, 각 객체를 키포인트로 보고 그 특징 맵을 특징 벡터로 변환하는 동적 채널 그래프 주의 모듈(Dynamic Channel Graph Attention, DCGA)도 제안해. 마지막으로, 객체 매칭 관계를 기반으로 논리적 이상이 있는 객체를 탐지하기 위한 이상 측정 모델(Anomaly Measurement Model, AMM)을 제안해. 객체에서 구조적 이상도 탐지할 수 있어.

우리가 제안한 SAM-LAD는 MVTec Loco AD, MVTec AD 같은 산업 데이터셋과 DigitAnatomy 같은 논리적 데이터셋을 포함한 다양한 벤치마크를 통해 검증했어. 실험 결과, SAM-LAD는 기존의 최첨단 방법들보다 성능이 뛰어나고, 특히 논리적 이상 탐지에서 강력한 결과를 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2406.01062.pdf

Title: Layout Agnostic Scene Text Image Synthesis with Diffusion Models

Original Abstract:
While diffusion models have significantly advanced the quality of image generation their capability to accurately and coherently render text within these images remains a substantial challenge. Conventional diffusion-based methods for scene text generation are typically limited by their reliance on an intermediate layout output. This dependency often results in a constrained diversity of text styles and fonts an inherent limitation stemming from the deterministic nature of the layout generation phase. To address these challenges this paper introduces SceneTextGen a novel diffusion-based model specifically designed to circumvent the need for a predefined layout stage. By doing so SceneTextGen facilitates a more natural and varied representation of text. The novelty of SceneTextGen lies in its integration of three key components: a character-level encoder for capturing detailed typographic properties coupled with a character-level instance segmentation model and a word-level spotting model to address the issues of unwanted text generation and minor character inaccuracies. We validate the performance of our method by demonstrating improved character recognition rates on generated images across different public visual text datasets in comparison to both standard diffusion based methods and text specific methods.

Translated Abstract:
확산 모델들은 이미지 생성 품질을 크게 향상시켰지만, 이미지 안에서 텍스트를 정확하고 일관되게 표현하는 건 여전히 큰 도전 과제야. 기존의 장면 텍스트 생성을 위한 확산 기반 방법들은 중간 레이아웃 출력에 의존하는 경우가 많아. 이 때문에 텍스트 스타일과 폰트의 다양성이 제한되는데, 이는 레이아웃 생성 단계가 결정론적이기 때문이야.

이런 문제를 해결하기 위해, 이 논문에서는 SceneTextGen이라는 새로운 확산 기반 모델을 소개해. 이 모델은 미리 정의된 레이아웃 단계가 필요 없도록 설계됐어. 그래서 SceneTextGen은 텍스트를 더 자연스럽고 다양하게 표현할 수 있어.

SceneTextGen의 혁신적인 점은 세 가지 주요 요소를 통합한 거야. 첫 번째는 세부적인 타이포그래픽 특성을 포착하는 문자 수준의 인코더고, 두 번째는 원하지 않는 텍스트 생성을 해결하고 작은 문자 오류를 다루기 위한 문자 수준의 인스턴스 분할 모델, 세 번째는 단어 수준의 탐지 모델이야.

우리는 여러 공공 시각 텍스트 데이터셋에서 생성된 이미지의 문자 인식률이 개선됐음을 보여주면서 우리의 방법 성능을 검증했어. 이건 표준 확산 기반 방법들과 텍스트 전용 방법들과 비교했을 때 더욱 두드러져.

================================================================================

URL:
https://arxiv.org/pdf/2406.01380.pdf

Title: Convolutional Unscented Kalman Filter for Multi-Object Tracking with Outliers

Original Abstract:
Multi-object tracking (MOT) is an essential technique for navigation in autonomous driving. In tracking-by-detection systems, biases, false positives, and misses, which are referred to as outliers, are inevitable due to complex traffic scenarios. Recent tracking methods are based on filtering algorithms that overlook these outliers, leading to reduced tracking accuracy or even loss of the objects trajectory. To handle this challenge, we adopt a probabilistic perspective, regarding the generation of outliers as misspecification between the actual distribution of measurement data and the nominal measurement model used for filtering. We further demonstrate that, by designing a convolutional operation, we can mitigate this misspecification. Incorporating this operation into the widely used unscented Kalman filter (UKF) in commonly adopted tracking algorithms, we derive a variant of the UKF that is robust to outliers, called the convolutional UKF (ConvUKF). We show that ConvUKF maintains the Gaussian conjugate property, thus allowing for real-time tracking. We also prove that ConvUKF has a bounded tracking error in the presence of outliers, which implies robust stability. The experimental results on the KITTI and nuScenes datasets show improved accuracy compared to representative baseline algorithms for MOT tasks.

Translated Abstract:
다중 객체 추적(MOT)은 자율주행에서 내비게이션을 위한 중요한 기술이야. 추적-탐지 시스템에서는 복잡한 교통 상황 때문에 편향, 허위 긍정(잘못된 탐지), 놓치는 경우처럼 아웃라이어가 발생하는 건 피할 수 없어. 최근의 추적 방법들은 이런 아웃라이어를 간과하는 필터링 알고리즘에 기반하고 있어서, 추적 정확도가 떨어지거나 객체의 경로를 잃어버리는 문제가 생겨.

이 문제를 해결하기 위해, 우리는 아웃라이어 생성을 실제 측정 데이터의 분포와 필터링에 사용되는 기준 측정 모델 간의 잘못된 정의(misspecification)로 보고 확률적 관점을 채택했어. 그리고 컨볼루션 연산을 설계함으로써 이 잘못된 정의를 완화할 수 있다는 걸 보여줬어. 이 연산을 널리 사용되는 무향 칼만 필터(UKF)에 포함시켜서, 아웃라이어에 강한 UKF의 변형인 컨볼루션 UKF(ConvUKF)를 만들었어.

ConvUKF는 가우시안 쌍대 속성을 유지하기 때문에 실시간 추적이 가능해. 또한, ConvUKF는 아웃라이어가 있을 때도 제한된 추적 오류를 가진다는 걸 증명했어. 이는 강력한 안정성을 의미해. KITTI와 nuScenes 데이터셋에서의 실험 결과로 MOT 작업을 위한 대표적인 기준 알고리즘에 비해 정확도가 향상됐다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2406.03556.pdf

Title: Npix2Cpix: A GAN-Based Image-to-Image Translation Network With Retrieval- Classification Integration for Watermark Retrieval From Historical Document Images

Original Abstract:
The identification and restoration of ancient watermarks have long been a major topic in codicology and history. Classifying historical documents based on watermarks is challenging due to their diversity, noisy samples, multiple representation modes, and minor distinctions between classes and intra-class variations. This paper proposes a modified U-net-based conditional generative adversarial network (GAN) named Npix2Cpix to translate noisy raw historical watermarked images into clean, handwriting-free watermarked images by performing image translation from degraded (noisy) pixels to clean pixels. Using image-to-image translation and adversarial learning, the network creates clutter-free images for watermark restoration and categorization. The generator and discriminator of the proposed GAN are trained using two separate loss functions, each based on the distance between images, to learn the mapping from the input noisy image to the output clean image. After using the proposed GAN to pre-process noisy watermarked images, Siamese-based one-shot learning is employed for watermark classification. Experimental results on a large-scale historical watermark dataset demonstrate that cleaning the noisy watermarked images can help to achieve high one-shot classification accuracy. The qualitative and quantitative evaluation of the retrieved watermarked image highlights the effectiveness of the proposed approach.

Translated Abstract:
고대 워터마크를 식별하고 복원하는 것은 오랫동안 고서학과 역사에서 중요한 주제였어. 워터마크를 기반으로 역사적 문서를 분류하는 건 다양한 종류와 노이즈가 많은 샘플, 여러 표현 방식, 클래스 간의 작은 차이로 인해 어려워. 

이 논문에서는 Npix2Cpix라는 수정된 U-net 기반의 조건부 생성적 적대 신경망(GAN)을 제안해. 이 모델은 시끄러운 원본 역사적 워터마크 이미지를 깨끗하고 손글씨가 없는 워터마크 이미지로 변환해. 이 과정은 노이즈가 많은 픽셀에서 깨끗한 픽셀로의 이미지 변환을 통해 이루어져. 이미지 간 변환과 적대적 학습을 이용해, 이 네트워크는 워터마크 복원과 분류를 위해 혼란 없는 이미지를 만든다.

제안된 GAN의 생성자와 판별자는 각기 다른 두 개의 손실 함수를 사용해 훈련돼. 이 손실 함수는 이미지 간의 거리를 기반으로 해, 입력된 노이즈가 많은 이미지에서 출력된 깨끗한 이미지로의 매핑을 배우게 돼. 

노이즈가 많은 워터마크 이미지를 사전 처리한 후에는 시암ese 기반의 원샷 학습을 사용해 워터마크를 분류해. 대규모 역사적 워터마크 데이터셋에서의 실험 결과, 노이즈가 있는 이미지를 깨끗하게 만드는 게 높은 원샷 분류 정확도를 달성하는 데 도움이 된다는 걸 보여줬어. 복원된 워터마크 이미지의 질적 및 양적 평가는 제안된 접근 방식의 효과성을 강조해.

================================================================================

URL:
https://arxiv.org/pdf/2406.07113.pdf

Title: Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene Graph

Original Abstract:
Locating objects described in natural language presents a significant challenge for autonomous agents. Existing CLIP-based open-vocabulary methods successfully perform 3D object grounding with simple (bare) queries, but cannot cope with ambiguous descriptions that demand an understanding of object relations. To tackle this problem, we propose a modular approach called BBQ (Beyond Bare Queries), which constructs 3D scene graph representation with metric and semantic edges and utilizes a large language model as a human-to-agent interface through our deductive scene reasoning algorithm. BBQ employs robust DINO-powered associations to construct 3D object-centric map and an advanced raycasting algorithm with a 2D vision-language model to describe them as graph nodes. On the Replica and ScanNet datasets, we have demonstrated that BBQ takes a leading place in open-vocabulary 3D semantic segmentation compared to other zero-shot methods. Also, we show that leveraging spatial relations is especially effective for scenes containing multiple entities of the same semantic class. On challenging Sr3D+, Nr3D and ScanRefer benchmarks, our deductive approach demonstrates a significant improvement, enabling objects grounding by complex queries compared to other state-of-the-art methods. The combination of our design choices and software implementation has resulted in significant data processing speed in experiments on the robot on-board computer. This promising performance enables the application of our approach in intelligent robotics projects. We made the code publicly available at this https URL.

Translated Abstract:
자연어로 설명된 객체를 찾는 건 자율 에이전트에게 큰 도전 과제야. 기존의 CLIP 기반의 오픈 어휘 방법들은 간단한 쿼리로 3D 객체를 잘 찾아내지만, 객체 간의 관계를 이해해야 하는 모호한 설명은 처리하지 못해. 이 문제를 해결하기 위해 우리는 BBQ(Beyond Bare Queries)라는 모듈형 접근 방식을 제안해. 이 방법은 메트릭과 의미론적 엣지를 갖춘 3D 장면 그래프 표현을 만들고, 대형 언어 모델을 인간과 에이전트 간의 인터페이스로 활용해.

BBQ는 강력한 DINO 기반의 연관성을 이용해 3D 객체 중심의 맵을 만들고, 2D 비전-언어 모델을 사용해 이를 그래프 노드로 설명하는 고급 레이캐스팅 알고리즘을 사용해. Replica와 ScanNet 데이터셋에서 BBQ는 다른 제로샷 방법들과 비교해 오픈 어휘 3D 의미론적 분할에서 선두 위치를 차지했어. 또한, 공간 관계를 활용하는 것이 같은 의미 클래스의 여러 개체가 있는 장면에서 특히 효과적임을 보여줬어.

어려운 Sr3D+, Nr3D, ScanRefer 벤치마크에서 우리의 유도 접근 방식은 다른 최신 방법들과 비교했을 때 복잡한 쿼리로 객체를 찾는 데 큰 개선을 보여줬어. 우리의 설계 선택과 소프트웨어 구현의 조합 덕분에 로봇 온보드 컴퓨터에서 실험할 때 데이터 처리 속도가 많이 빨라졌어. 이런 좋은 성능 덕분에 우리의 접근 방식을 지능형 로봇 프로젝트에 적용할 수 있게 되었어. 우리는 코드도 공개했어, 여기서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2406.09407.pdf

Title: Towards Evaluating the Robustness of Visual State Space Models

Original Abstract:
Vision State Space Models (VSSMs), a novel architecture that combines the strengths of recurrent neural networks and latent variable models, have demonstrated remarkable performance in visual perception tasks by efficiently capturing long-range dependencies and modeling complex visual dynamics. However, their robustness under natural and adversarial perturbations remains a critical concern. In this work, we present a comprehensive evaluation of VSSMs' robustness under various perturbation scenarios, including occlusions, image structure, common corruptions, and adversarial attacks, and compare their performance to well-established architectures such as transformers and Convolutional Neural Networks. Furthermore, we investigate the resilience of VSSMs to object-background compositional changes on sophisticated benchmarks designed to test model performance in complex visual scenes. We also assess their robustness on object detection and segmentation tasks using corrupted datasets that mimic real-world scenarios. To gain a deeper understanding of VSSMs' adversarial robustness, we conduct a frequency-based analysis of adversarial attacks, evaluating their performance against low-frequency and high-frequency perturbations. Our findings highlight the strengths and limitations of VSSMs in handling complex visual corruptions, offering valuable insights for future research. Our code and models will be available at this https URL.

Translated Abstract:
비전 상태 공간 모델(VSSMs)은 순환 신경망과 잠재 변수 모델의 장점을 결합한 새로운 아키텍처야. 이 모델은 긴 거리의 의존성을 잘 잡아내고 복잡한 시각적 동작을 모델링해서 시각 인식 작업에서 뛰어난 성능을 보여줬어. 하지만 자연적인 방해나 적대적 방해에 대한 강인성은 여전히 큰 문제야.

이번 연구에서는 VSSMs의 강인성을 다양한 방해 상황에서 종합적으로 평가했어. 여기에는 가림, 이미지 구조, 일반적인 손상, 적대적 공격 등이 포함돼. 그리고 VSSMs의 성능을 잘 알려진 아키텍처인 트랜스포머와 합성곱 신경망(CNN)과 비교했어.

또한, VSSMs가 복잡한 시각 장면에서 모델 성능을 시험하기 위해 설계된 정교한 벤치마크에서 물체와 배경의 조합 변화에 얼마나 잘 견디는지도 조사했어. 우리는 실제 상황을 모방한 손상된 데이터셋을 사용해 물체 탐지와 분할 작업에서의 강인성도 평가했어.

VSSMs의 적대적 강인성을 더 깊이 이해하기 위해, 우리는 적대적 공격에 대한 주파수 기반 분석을 실시했어. 저주파와 고주파 방해에 대한 성능을 평가했지. 우리의 연구 결과는 VSSMs가 복잡한 시각적 손상을 처리하는 데 강점과 한계를 보여주며, 향후 연구에 유용한 통찰력을 제공해. 코드와 모델은 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2406.10115.pdf

Title: Shelf-Supervised Cross-Modal Pre-Training for 3D Object Detection

Original Abstract:
State-of-the-art 3D object detectors are often trained on massive labeled datasets. However, annotating 3D bounding boxes remains prohibitively expensive and time-consuming, particularly for LiDAR. Instead, recent works demonstrate that self-supervised pre-training with unlabeled data can improve detection accuracy with limited labels. Contemporary methods adapt best-practices for self-supervised learning from the image domain to point clouds (such as contrastive learning). However, publicly available 3D datasets are considerably smaller and less diverse than those used for image-based self-supervised learning, limiting their effectiveness. We do note, however, that such data is naturally collected in a multimodal fashion, often paired with images. Rather than pre-training with only self-supervised objectives, we argue that it is better to bootstrap point cloud representations using image-based foundation models trained on internet-scale image data. Specifically, we propose a shelf-supervised approach (e.g. supervised with off-the-shelf image foundation models) for generating zero-shot 3D bounding boxes from paired RGB and LiDAR data. Pre-training 3D detectors with such pseudo-labels yields significantly better semi-supervised detection accuracy than prior self-supervised pretext tasks. Importantly, we show that image-based shelf-supervision is helpful for training LiDAR-only and multi-modal (RGB + LiDAR) detectors. We demonstrate the effectiveness of our approach on nuScenes and WOD, significantly improving over prior work in limited data settings. Our code is available at this https URL

Translated Abstract:
최신 3D 물체 탐지기는 보통 엄청나게 큰 라벨이 있는 데이터셋으로 훈련돼. 하지만 3D 바운딩 박스를 주석 다는 건 정말 비싸고 시간이 많이 걸려, 특히 LiDAR의 경우에는 더 그렇지. 그래서 최근 연구들은 라벨이 없는 데이터로 자기 지도 학습을 통해 탐지 정확도를 높일 수 있다는 걸 보여줬어.

요즘 방법들은 이미지 도메인에서 자기 지도 학습의 좋은 방법들을 포인트 클라우드로 옮기고 있어(예: 대조 학습). 하지만 공개된 3D 데이터셋은 이미지 기반 자기 지도 학습에 비해 훨씬 작고 다양성이 떨어져서 효과가 제한적이야. 하지만 우리는 이런 데이터가 자연스럽게 다중 모드로 수집된다는 걸 알아. 보통 이미지와 함께 짝을 이루고 있거든.

그래서 우리는 자기 지도 목표만으로 사전 훈련하는 것보다, 인터넷 규모의 이미지 데이터로 훈련된 이미지 기반의 기본 모델을 사용해서 포인트 클라우드 표현을 부트스트랩하는 게 더 좋다고 주장해. 구체적으로는 RGB와 LiDAR 데이터가 짝을 이루는 걸 이용해 제로샷 3D 바운딩 박스를 생성하는 선반-지도 접근 방식을 제안해. 이렇게 생성된 준 라벨로 3D 탐지기를 사전 훈련시키면 이전의 자기 지도 프리텍스트 작업보다 반지도 탐지 정확도가 훨씬 좋아져.

중요한 건, 이미지 기반 선반-지도 학습이 LiDAR 전용 탐지기와 다중 모드(RGB + LiDAR) 탐지기 훈련에 도움이 된다는 거야. 우리는 nuScenes와 WOD에서 우리의 접근 방식이 효과적임을 보여줬고, 제한된 데이터 환경에서도 이전 연구보다 크게 개선된 결과를 얻었어. 우리의 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2406.10723.pdf

Title: Eye in the Sky: Detection and Compliance Monitoring of Brick Kilns using Satellite Imagery

Original Abstract:
Air pollution kills 7 million people annually. The brick manufacturing industry accounts for 8%-14% of air pollution in the densely populated Indo-Gangetic plain. Due to the unorganized nature of brick kilns, policy violation detection, such as proximity to human habitats, remains challenging. While previous studies have utilized computer vision-based machine learning methods for brick kiln detection from satellite imagery, they utilize proprietary satellite data and rarely focus on compliance with government policies. In this research, we introduce a scalable framework for brick kiln detection and automatic compliance monitoring. We use Google Maps Static API to download the satellite imagery followed by the YOLOv8x model for detection. We identified and hand-verified 19579 new brick kilns across 9 states within the Indo-Gangetic plain. Furthermore, we automate and test the compliance to the policies affecting human habitats, rivers and hospitals. Our results show that a substantial number of brick kilns do not meet the compliance requirements. Our framework offers a valuable tool for governments worldwide to automate and enforce policy regulations for brick kilns, addressing critical environmental and public health concerns.

Translated Abstract:
대기 오염은 매년 700만 명을 죽입니다. 벽돌 제조 산업은 인구가 밀집한 인도-갠지평원에서 대기 오염의 8%-14%를 차지해요. 벽돌 가마가 정리되지 않은 방식으로 운영되기 때문에, 사람 거주지와의 거리 같은 정책 위반을 감지하는 게 쉽지 않아요. 이전 연구들은 위성 이미지에서 벽돌 가마를 찾기 위해 컴퓨터 비전 기반의 머신러닝 방법을 사용했지만, 주로 독점적인 위성 데이터를 사용하고 정부 정책 준수에는 잘 신경 쓰지 않았어요.

이 연구에서는 벽돌 가마 감지와 자동 준수 모니터링을 위한 확장 가능한 프레임워크를 소개해요. 우리는 Google Maps Static API를 사용해 위성 이미지를 다운로드하고, YOLOv8x 모델로 감지해요. 인도-갠지평원 내 9개 주에서 19,579개의 새로운 벽돌 가마를 확인하고 수동으로 검증했어요. 또, 사람 거주지, 강, 병원에 영향을 미치는 정책 준수를 자동화하고 테스트했어요. 

결과적으로 많은 벽돌 가마가 준수 기준을 충족하지 않는 것으로 나타났어요. 우리의 프레임워크는 전 세계 정부가 벽돌 가마에 대한 정책 규제를 자동화하고 시행할 수 있는 유용한 도구를 제공해요. 이는 환경과 공공 보건 문제를 해결하는 데 중요한 역할을 할 수 있어요.

================================================================================

URL:
https://arxiv.org/pdf/2406.13987.pdf

Title: Image anomaly detection and prediction scheme based on SSA optimized ResNet50-BiGRU model

Original Abstract:
Image anomaly detection is a popular research direction, with many methods emerging in recent years due to rapid advancements in computing. The use of artificial intelligence for image anomaly detection has been widely studied. By analyzing images of athlete posture and movement, it is possible to predict injury status and suggest necessary adjustments. Most existing methods rely on convolutional networks to extract information from irrelevant pixel data, limiting model accuracy. This paper introduces a network combining Residual Network (ResNet) and Bidirectional Gated Recurrent Unit (BiGRU), which can predict potential injury types and provide early warnings by analyzing changes in muscle and bone poses from video images. To address the high complexity of this network, the Sparrow search algorithm was used for optimization. Experiments conducted on four datasets demonstrated that our model has the smallest error in image anomaly detection compared to other models, showing strong adaptability. This provides a new approach for anomaly detection and predictive analysis in images, contributing to the sustainable development of human health and performance.

Translated Abstract:
이미지 이상 탐지는 요즘 인기 있는 연구 분야야. 최근 컴퓨터 기술이 빠르게 발전하면서 많은 방법들이 나오고 있어. 인공지능을 이용한 이미지 이상 탐지도 많이 연구되고 있어. 

운동선수의 자세와 움직임을 분석하면 부상 상태를 예측하고 필요한 조정을 제안할 수 있어. 하지만 기존 방법들은 대부분 컨볼루션 네트워크를 사용해서 관련 없는 픽셀 데이터에서 정보를 추출하는데, 이로 인해 모델의 정확도가 제한돼. 

이 논문에서는 Residual Network(ResNet)와 Bidirectional Gated Recurrent Unit(BiGRU)를 결합한 네트워크를 소개해. 이 네트워크는 비디오 이미지에서 근육과 뼈의 자세 변화를 분석해서 잠재적인 부상 유형을 예측하고 조기 경고를 제공할 수 있어. 

이 네트워크의 복잡성을 해결하기 위해 Sparrow 탐색 알고리즘을 최적화에 사용했어. 네 개의 데이터셋에서 진행된 실험 결과, 우리 모델이 다른 모델들에 비해 이미지 이상 탐지에서 가장 작은 오류를 보이고 강한 적응력을 보여줬어. 

이 연구는 이미지에서의 이상 탐지와 예측 분석을 위한 새로운 접근 방식을 제공하고, 인체 건강과 성능의 지속 가능한 발전에 기여하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2406.14367.pdf

Title: PoseBench: Benchmarking the Robustness of Pose Estimation Models under Corruptions

Original Abstract:
Pose estimation aims to accurately identify anatomical keypoints in humans and animals using monocular images, which is crucial for various applications such as human-machine interaction, embodied AI, and autonomous driving. While current models show promising results, they are typically trained and tested on clean data, potentially overlooking the corruption during real-world deployment and thus posing safety risks in practical scenarios. To address this issue, we introduce PoseBench, a comprehensive benchmark designed to evaluate the robustness of pose estimation models against real-world corruption. We evaluated 60 representative models, including top-down, bottom-up, heatmap-based, regression-based, and classification-based methods, across three datasets for human and animal pose estimation. Our evaluation involves 10 types of corruption in four categories: 1) blur and noise, 2) compression and color loss, 3) severe lighting, and 4) masks. Our findings reveal that state-of-the-art models are vulnerable to common real-world corruptions and exhibit distinct behaviors when tackling human and animal pose estimation tasks. To improve model robustness, we delve into various design considerations, including input resolution, pre-training datasets, backbone capacity, post-processing, and data augmentations. We hope that our benchmark will serve as a foundation for advancing research in robust pose estimation. The benchmark and source code will be released at this https URL

Translated Abstract:
포즈 추정은 단일 이미지를 이용해 사람과 동물의 해부학적 주요 점들을 정확하게 식별하는 거야. 이건 인간-기계 상호작용, 구현된 AI, 자율주행 등 다양한 응용 프로그램에 매우 중요해. 현재의 모델들은 좋은 결과를 보여주고 있지만, 일반적으로 깨끗한 데이터로 훈련되고 테스트돼서, 실제 환경에서 발생할 수 있는 오류를 간과할 수 있어. 이게 실제 상황에서 안전 위험을 초래할 수 있지.

이 문제를 해결하기 위해 우리는 PoseBench라는 포괄적인 벤치마크를 소개해. 이 벤치마크는 포즈 추정 모델이 실제 환경의 오류에 대해 얼마나 강건한지를 평가하기 위해 만들어졌어. 우리는 사람과 동물 포즈 추정을 위한 세 개의 데이터셋을 통해, 60개의 대표적인 모델(탑다운, 바텀업, 히트맵 기반, 회귀 기반, 분류 기반 방법 포함)을 평가했어.

우리가 평가한 건 네 가지 카테고리에서 10가지 종류의 오류야: 1) 흐림과 잡음, 2) 압축과 색상 손실, 3) 심한 조명, 4) 마스크. 결과적으로, 최신 모델들이 일반적인 실제 오류에 취약하다는 것을 알게 됐고, 인간과 동물 포즈 추정 작업을 수행할 때 각기 다른 행동을 보인다는 사실도 발견했어.

모델의 강건성을 개선하기 위해 우리는 입력 해상도, 사전 훈련 데이터셋, 백본 용량, 후처리, 데이터 증강 등 다양한 디자인 고려사항을 살펴봤어. 우리의 벤치마크가 강건한 포즈 추정 연구를 발전시키는 기초가 되기를 바라. 벤치마크와 소스 코드는 이 URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2406.14815.pdf

Title: Latent diffusion models for parameterization and data assimilation of facies-based geomodels

Original Abstract:
Geological parameterization entails the representation of a geomodel using a small set of latent variables and a mapping from these variables to grid-block properties such as porosity and permeability. Parameterization is useful for data assimilation (history matching), as it maintains geological realism while reducing the number of variables to be determined. Diffusion models are a new class of generative deep-learning procedures that have been shown to outperform previous methods, such as generative adversarial networks, for image generation tasks. Diffusion models are trained to "denoise", which enables them to generate new geological realizations from input fields characterized by random noise. Latent diffusion models, which are the specific variant considered in this study, provide dimension reduction through use of a low-dimensional latent variable. The model developed in this work includes a variational autoencoder for dimension reduction and a U-net for the denoising process. Our application involves conditional 2D three-facies (channel-levee-mud) systems. The latent diffusion model is shown to provide realizations that are visually consistent with samples from geomodeling software. Quantitative metrics involving spatial and flow-response statistics are evaluated, and general agreement between the diffusion-generated models and reference realizations is observed. Stability tests are performed to assess the smoothness of the parameterization method. The latent diffusion model is then used for ensemble-based data assimilation. Two synthetic "true" models are considered. Significant uncertainty reduction, posterior P$_{10}$-P$_{90}$ forecasts that generally bracket observed data, and consistent posterior geomodels, are achieved in both cases.

Translated Abstract:
지질 매개변수화는 지오모델을 작은 잠재 변수 집합으로 표현하고, 이 변수들을 사용해 다공성과 투과성 같은 그리드 블록 속성으로 매핑하는 과정을 말해. 매개변수화는 데이터 동화(역사 매칭)에 유용한데, 변수의 수를 줄이면서도 지질학적 현실성을 유지할 수 있어.

확산 모델은 새로운 종류의 생성적 딥러닝 기법으로, 이미지 생성 작업에서 이전의 생성적 적대 신경망보다 더 뛰어난 성능을 보여줘. 이 모델은 "노이즈 제거"를 학습하는데, 이를 통해 랜덤 노이즈가 있는 입력 필드에서 새로운 지질적 실현을 생성할 수 있어. 이 연구에서 다루는 잠재 확산 모델은 저차원 잠재 변수를 사용해서 차원 축소를 돕는 특정 변형이야. 우리가 개발한 모델은 차원 축소를 위한 변분 오토인코더와 노이즈 제거 과정을 위한 U-net을 포함하고 있어.

이번 연구에서는 조건부 2D 3상 (채널-제방-진흙) 시스템을 적용했어. 잠재 확산 모델이 지오모델링 소프트웨어에서 샘플과 시각적으로 일치하는 실현을 제공하는 것으로 나타났어. 공간적 및 흐름 응답 통계와 관련된 정량적 지표를 평가했는데, 확산으로 생성된 모델과 기준 실현 간의 일반적인 일치가 관찰되었어. 매개변수화 방법의 부드러움을 평가하기 위해 안정성 테스트도 수행했어. 이후 잠재 확산 모델이 집합 기반 데이터 동화에 사용되었어. 두 개의 합성 "진짜" 모델을 고려했는데, 두 경우 모두 상당한 불확실성 감소와 관측 데이터에 일반적으로 맞는 후속 P$_{10}$-P$_{90}$ 예측, 일관된 후속 지오모델이 달성되었어.

================================================================================

URL:
https://arxiv.org/pdf/2406.17639.pdf

Title: Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP

Original Abstract:
Contrastive Language--Image Pre-training (CLIP) has manifested remarkable improvements in zero-shot classification and cross-modal vision-language tasks. Yet, from a geometrical point of view, the CLIP embedding space has been found to have a pronounced modality gap. This gap renders the embedding space overly sparse and disconnected, with different modalities being densely distributed in distinct subregions of the hypersphere. In this work, we aim at answering three main questions: 1. Does sharing the parameter space between the multi-modal encoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart the uni-modal embeddings via intra-modality separation? 3. How do these gap reduction approaches affect the downstream performance? We design AlignCLIP, in order to answer these questions and through extensive experiments, we show that AlignCLIP achieves noticeable enhancements in the cross-modal alignment of the embeddings, and thereby, reduces the modality gap, while improving the performance across several zero-shot and fine-tuning downstream evaluations.

Translated Abstract:
대조적 언어-이미지 사전 훈련(Contrastive Language--Image Pre-training, CLIP)은 제로샷 분류와 크로스 모달 비전-언어 작업에서 눈에 띄는 향상을 보여줬어. 그런데, 기하학적인 관점에서 보면 CLIP 임베딩 공간에 모달리티 간의 차이가 크게 나타나는 문제가 있어. 이 차이 때문에 임베딩 공간이 너무 희박하고 연결이 끊긴 느낌이야. 서로 다른 모달리티들이 하이퍼구의 다른 하위 영역에 밀집해 분포돼 있거든.

이번 연구에서는 세 가지 주요 질문에 답하려고 해:  
1. 멀티 모달 인코더 간에 파라미터 공간을 공유하면 모달리티 간의 차이가 줄어들까?  
2. 유니 모달 임베딩을 인트라 모달리티 분리를 통해 멀리 밀어내면 차이를 줄일 수 있을까?  
3. 이런 차이 줄이기 방법들이 다운스트림 성능에 어떻게 영향을 줄까?

우리는 AlignCLIP를 설계해서 이 질문들에 답하려고 했고, 다양한 실험을 통해 AlignCLIP이 임베딩의 크로스 모달 정렬을 눈에 띄게 향상시킨다는 걸 보여줬어. 그래서 모달리티 간의 차이를 줄이면서 여러 제로샷 및 파인 튜닝 다운스트림 평가에서 성능을 향상시켰어.

================================================================================

URL:
https://arxiv.org/pdf/2406.19280.pdf

Title: HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale

Original Abstract:
The rapid development of multimodal large language models (MLLMs), such as GPT-4V, has led to significant advancements. However, these models still face challenges in medical multimodal capabilities due to limitations in the quantity and quality of medical vision-text data, stemming from data privacy concerns and high annotation costs. While pioneering approaches utilize PubMed's large-scale, de-identified medical image-text pairs to address these limitations, they still fall short due to inherent data noise. To tackle this, we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in an 'unblinded' capacity to denoise and reformat the data, resulting in the creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our validation demonstrates that: (1) PubMedVision can significantly enhance the medical multimodal capabilities of current MLLMs, showing significant improvement in benchmarks including the MMMU Health & Medicine track; (2) manual checks by medical experts and empirical results validate the superior data quality of our dataset compared to other data construction methods. Using PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows superior performance in medical multimodal scenarios among open-source MLLMs.

Translated Abstract:
다양한 모드의 대형 언어 모델(MLLMs), 예를 들어 GPT-4V의 빠른 발전 덕분에 많은 성과가 있긴 하지만, 의료 분야에서의 멀티모달 기능에는 여전히 문제들이 있어. 그 이유는 데이터 프라이버시 문제와 높은 주석 비용 때문에 의료 관련 비전-텍스트 데이터의 양과 질이 부족하기 때문이야.

선구적인 접근법들은 PubMed의 대규모 비식별 의료 이미지-텍스트 쌍을 활용해서 이 문제를 해결하려고 했지만, 데이터에 원래부터 있는 노이즈 때문에 여전히 부족한 점이 있어. 그래서 우리는 PubMed에서 의료 이미지-텍스트 쌍을 정제하고, MLLMs(GPT-4V)를 '블라인드 해제' 방식으로 사용해서 데이터를 노이즈 제거하고 포맷을 다시 바꾸었어. 그 결과 130만 개의 의료 VQA 샘플이 포함된 PubMedVision 데이터셋이 만들어졌지.

우리의 검증 결과는 다음과 같아: (1) PubMedVision은 현재 MLLMs의 의료 멀티모달 기능을 크게 향상시킬 수 있고, MMMU 건강 및 의학 트랙을 포함한 벤치마크에서 눈에 띄는 개선을 보여줘; (2) 의료 전문가들의 수동 검토와 실험 결과를 통해 우리 데이터셋의 품질이 다른 데이터 생성 방법보다 우수하다는 것이 확인됐어. 

PubMedVision을 사용해서 우리는 34B 규모의 의료 MLLM인 HuatuoGPT-Vision을 훈련시켰고, 이 모델은 오픈 소스 MLLMs 중에서 의료 멀티모달 시나리오에서 뛰어난 성능을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2407.00921.pdf

Title: PointViG: A Lightweight GNN-based Model for Efficient Point Cloud Analysis

Original Abstract:
In the domain of point cloud analysis, despite the significant capabilities of Graph Neural Networks (GNNs) in managing complex 3D datasets, existing approaches encounter challenges like high computational costs and scalability issues with extensive scenarios. These limitations restrict the practical deployment of GNNs, notably in resource-constrained environments. To address these issues, this study introduce <b>Point<\b> <b>Vi<\b>sion <b>G<\b>NN (PointViG), an efficient framework for point cloud analysis. PointViG incorporates a lightweight graph convolutional module to efficiently aggregate local features and mitigate over-smoothing. For large-scale point cloud scenes, we propose an adaptive dilated graph convolution technique that searches for sparse neighboring nodes within a dilated neighborhood based on semantic correlation, thereby expanding the receptive field and ensuring computational efficiency. Experiments demonstrate that PointViG achieves performance comparable to state-of-the-art models while balancing performance and complexity. On the ModelNet40 classification task, PointViG achieved 94.3% accuracy with 1.5M parameters. For the S3DIS segmentation task, it achieved an mIoU of 71.7% with 5.3M parameters. These results underscore the potential and efficiency of PointViG in point cloud analysis.

Translated Abstract:
포인트 클라우드 분석 분야에서, 그래프 신경망(GNN)이 복잡한 3D 데이터셋을 다루는 데 뛰어난 능력을 가지고 있지만, 기존 방법들은 높은 계산 비용과 대규모 시나리오에서의 확장성 문제 같은 어려움에 직면하고 있어. 이런 한계로 인해 자원이 제한된 환경에서는 GNN의 실제 배치가 어려워. 

이 문제를 해결하기 위해, 이 연구에서는 포인트 클라우드 분석을 위한 효율적인 프레임워크인 <b>Point</b> <b>Vi</b>sion <b>G</b>NN (PointViG)를 소개해. PointViG는 가벼운 그래프 컨볼루션 모듈을 포함해서 지역 특징을 효율적으로 집계하고 과도한 부드러움을 완화해. 대규모 포인트 클라우드 장면을 위해, 우리는 의미적 연관성에 기반한 희소 이웃 노드를 찾는 적응형 팽창 그래프 컨볼루션 기법을 제안해서 수용 영역을 확장하고 계산 효율성을 유지해.

실험 결과, PointViG는 최신 모델들과 비슷한 성능을 보이면서 성능과 복잡성의 균형을 잘 맞췄어. ModelNet40 분류 작업에서는 1.5M 파라미터로 94.3%의 정확도를 달성했고, S3DIS 세분화 작업에서는 5.3M 파라미터로 mIoU 71.7%를 기록했어. 이 결과들은 포인트 클라우드 분석에서 PointViG의 가능성과 효율성을 잘 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2407.03386.pdf

Title: Visual Robustness Benchmark for Visual Question Answering (VQA)

Original Abstract:
Can Visual Question Answering (VQA) systems perform just as well when deployed in the real world? Or are they susceptible to realistic corruption effects e.g. image blur, which can be detrimental in sensitive applications, such as medical VQA? While linguistic or textual robustness has been thoroughly explored in the VQA literature, there has yet to be any significant work on the visual robustness of VQA models. We propose the first large-scale benchmark comprising 213,000 augmented images, challenging the visual robustness of multiple VQA models and assessing the strength of realistic visual corruptions. Additionally, we have designed several robustness evaluation metrics that can be aggregated into a unified metric and tailored to fit a variety of use cases. Our experiments reveal several insights into the relationships between model size, performance, and robustness with the visual corruptions. Our benchmark highlights the need for a balanced approach in model development that considers model performance without compromising the robustness.

Translated Abstract:
시각 질문 답변(VQA) 시스템이 실제 환경에서도 잘 작동할 수 있을까? 아니면 이미지 흐림 같은 현실적인 왜곡에 영향을 받을까? 이런 문제는 의료 VQA 같은 민감한 분야에서 문제가 될 수 있어. VQA 관련 연구에서는 언어적 또는 텍스트의 강건성에 대해 많이 다뤘지만, VQA 모델의 시각적 강건성에 대한 연구는 아직 부족해. 

우리는 213,000개의 증강 이미지를 포함한 첫 번째 대규모 벤치마크를 제안해. 이 벤치마크는 여러 VQA 모델의 시각적 강건성을 시험하고 현실적인 시각적 왜곡의 영향을 평가해. 또한, 다양한 사용 사례에 맞게 조정할 수 있는 여러 강건성 평가 지표도 설계했어. 

실험 결과를 통해 모델 크기, 성능, 시각적 왜곡 간의 관계에 대한 몇 가지 통찰을 얻었어. 우리의 벤치마크는 모델 개발 시 성능을 고려하면서도 강건성을 저해하지 않는 균형 잡힌 접근이 필요하다는 점을 강조해.

================================================================================

URL:
https://arxiv.org/pdf/2407.10159.pdf

Title: RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation

Original Abstract:
3D point clouds play a pivotal role in outdoor scene perception, especially in the context of autonomous driving. Recent advancements in 3D LiDAR segmentation often focus intensely on the spatial positioning and distribution of points for accurate segmentation. However, these methods, while robust in variable conditions, encounter challenges due to sole reliance on coordinates and point intensity, leading to poor isometric invariance and suboptimal segmentation. To tackle this challenge, our work introduces Range-Aware Pointwise Distance Distribution (RAPiD) features and the associated RAPiD-Seg architecture. Our RAPiD features exhibit rigid transformation invariance and effectively adapt to variations in point density, with a design focus on capturing the localized geometry of neighboring structures. They utilize inherent LiDAR isotropic radiation and semantic categorization for enhanced local representation and computational efficiency, while incorporating a 4D distance metric that integrates geometric and surface material reflectivity for improved semantic segmentation. To effectively embed high-dimensional RAPiD features, we propose a double-nested autoencoder structure with a novel class-aware embedding objective to encode high-dimensional features into manageable voxel-wise embeddings. Additionally, we propose RAPiD-Seg which incorporates a channel-wise attention fusion and two effective RAPiD-Seg variants, further optimizing the embedding for enhanced performance and generalization. Our method outperforms contemporary LiDAR segmentation work in terms of mIoU on SemanticKITTI (76.1) and nuScenes (83.6) datasets.

Translated Abstract:
3D 포인트 클라우드는 야외 장면 인식에서 중요한 역할을 해. 특히 자율주행에서 그렇지. 최근 3D LiDAR 분할 기술은 정확한 분할을 위해 포인트의 위치와 분포에 많이 집중하고 있어. 하지만 이런 방법들은 변동성이 있는 조건에서는 잘 작동하지만, 좌표와 포인트 강도에만 의존하다 보니 이소메트릭 불변성에 문제가 생기고, 결과적으로 분할이 최적이 아닌 경우가 많아.

그래서 우리가 제안하는 건 Range-Aware Pointwise Distance Distribution (RAPiD) 기능과 RAPiD-Seg 아키텍처야. 우리 RAPiD 기능은 강체 변환 불변성을 가지고 있고, 포인트 밀도가 변해도 효과적으로 적응해. 이건 주변 구조의 국소 기하학을 잘 포착하도록 설계됐어. 라이다의 고유한 등방성 방사와 의미론적 분류를 활용해 지역 표현을 강화하고 계산 효율성을 높이고, 기하학적 정보와 표면 재질 반사율을 통합한 4D 거리 메트릭을 사용해서 의미론적 분할을 개선했어.

고차원 RAPiD 기능을 효과적으로 임베딩하기 위해서, 우리는 이중 중첩 오토인코더 구조를 제안해. 이 구조는 고차원 기능을 다루기 쉬운 복셀 단위 임베딩으로 변환하기 위한 새로운 클래스 인식 임베딩 목표를 가지고 있어. 그리고 RAPiD-Seg도 제안하는데, 채널 기반의 주의 융합을 포함하고 두 가지 효과적인 RAPiD-Seg 변형을 추가해서 임베딩을 최적화해 성능과 일반화를 더 높였어.

우리 방법은 SemanticKITTI(76.1)와 nuScenes(83.6) 데이터셋에서 최신 LiDAR 분할 작업보다 성능이 더 뛰어나.

================================================================================

URL:
https://arxiv.org/pdf/2407.14279.pdf

Title: OpenSU3D: Open World 3D Scene Understanding using Foundation Models

Original Abstract:
In this paper, we present a novel, scalable approach for constructing open set, instance-level 3D scene representations, advancing open world understanding of 3D environments. Existing methods require pre-constructed 3D scenes and face scalability issues due to per-point feature vector learning, limiting their efficacy with complex queries. Our method overcomes these limitations by incrementally building instance-level 3D scene representations using 2D foundation models, efficiently aggregating instance-level details such as masks, feature vectors, names, and captions. We introduce fusion schemes for feature vectors to enhance their contextual knowledge and performance on complex queries. Additionally, we explore large language models for robust automatic annotation and spatial reasoning tasks. We evaluate our proposed approach on multiple scenes from ScanNet and Replica datasets demonstrating zero-shot generalization capabilities, exceeding current state-of-the-art methods in open world 3D scene understanding.

Translated Abstract:
이 논문에서는 오픈 세트(instance-level) 3D 장면 표현을 만드는 새로운 접근 방식을 소개해. 이 방식은 3D 환경을 더 잘 이해할 수 있게 해줘. 기존 방법들은 미리 만들어진 3D 장면이 필요하고, 각 점마다 특징 벡터를 배우는 과정에서 확장성 문제가 있어서 복잡한 쿼리에 대한 효율성이 떨어져.

우리 방법은 2D 기초 모델을 사용해서 인스턴스 레벨 3D 장면 표현을 점진적으로 구축하는 방식을 통해 이런 한계를 극복해. 마스크, 특징 벡터, 이름, 캡션 같은 인스턴스 레벨 세부 정보를 효율적으로 모아. 또한, 특징 벡터의 맥락 지식과 복잡한 쿼리에 대한 성능을 향상시키기 위해 융합 기법도 도입했어.

게다가, 우리는 강력한 자동 주석 달기와 공간 추론 작업을 위해 대형 언어 모델도 탐구했어. 우리의 방법을 ScanNet과 Replica 데이터셋의 여러 장면에서 평가해봤는데, 제로샷 일반화 능력을 보여주며 현재 최신 기술들을 초과하는 성능을 보였어.

================================================================================

URL:
https://arxiv.org/pdf/2407.15051.pdf

Title: Prior Knowledge Integration via LLM Encoding and Pseudo Event Regulation for Video Moment Retrieval

Original Abstract:
In this paper, we investigate the feasibility of leveraging large language models (LLMs) for integrating general knowledge and incorporating pseudo-events as priors for temporal content distribution in video moment retrieval (VMR) models. The motivation behind this study arises from the limitations of using LLMs as decoders for generating discrete textual descriptions, which hinders their direct application to continuous outputs like salience scores and inter-frame embeddings that capture inter-frame relations. To overcome these limitations, we propose utilizing LLM encoders instead of decoders. Through a feasibility study, we demonstrate that LLM encoders effectively refine inter-concept relations in multimodal embeddings, even without being trained on textual embeddings. We also show that the refinement capability of LLM encoders can be transferred to other embeddings, such as BLIP and T5, as long as these embeddings exhibit similar inter-concept similarity patterns to CLIP embeddings. We present a general framework for integrating LLM encoders into existing VMR architectures, specifically within the fusion module. Through experimental validation, we demonstrate the effectiveness of our proposed methods by achieving state-of-the-art performance in VMR. The source code can be accessed at this https URL.

Translated Abstract:
이 논문에서는 대형 언어 모델(LLM)을 활용해 일반 지식을 통합하고, 비디오 순간 검색(VMR) 모델의 시간적 콘텐츠 분포를 위해 가상 이벤트를 사전 정보로 사용하는 가능성을 조사했어. 

이 연구의 동기는 LLM을 디코더로 사용해 텍스트 설명을 생성하는 데 한계가 있다는 점에서 나왔어. 이런 방식은 연속적인 출력, 즉 중요도 점수나 프레임 간 관계를 포착하는 임베딩 같은 것들에 직접 적용하기 어려워. 그래서 우리는 디코더 대신 LLM 인코더를 활용하는 방안을 제안했어. 

타당성 연구를 통해 LLM 인코더가 멀티모달 임베딩에서 개념 간 관계를 효과적으로 개선할 수 있다는 걸 보여줬어. 심지어 텍스트 임베딩으로 훈련되지 않았더라도 말이야. 또 LLM 인코더의 개선 능력이 다른 임베딩, 예를 들어 BLIP나 T5로도 이전될 수 있다는 걸 보였어. 이때 이 임베딩들이 CLIP 임베딩과 비슷한 개념 간 유사성 패턴을 보여야 해. 

우리는 LLM 인코더를 기존의 VMR 구조에 통합하는 일반적인 프레임워크를 제시했어. 특히 융합 모듈 내에서 말이지. 실험 검증을 통해 우리의 방법이 VMR에서 최첨단 성능을 달성하는 데 효과적임을 입증했어. 소스 코드는 해당 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2407.15841.pdf

Title: SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models

Original Abstract:
We propose SlowFast-LLaVA (or SF-LLaVA for short), a training-free video large language model (LLM) that can jointly capture detailed spatial semantics and long-range temporal context without exceeding the token budget of commonly used LLMs. This is realized by using a two-stream SlowFast design of inputs for Video LLMs to aggregate features from sampled frames in an effective way. Specifically, the Slow pathway extracts features at a low frame rate while keeping as much spatial detail as possible (e.g., with 12x24 tokens), and the Fast pathway operates on a high frame rate but uses a larger spatial pooling stride (e.g., downsampling 6x) to focus on the motion cues. As a result, this design allows us to adequately capture both spatial and temporal features that are beneficial for detailed video understanding. Experimental results show that SF-LLaVA outperforms existing training-free methods on a wide range of video tasks. On some benchmarks, it achieves comparable or even better performance compared to state-of-the-art Video LLMs that are fine-tuned on video datasets. Code has been made available at: this https URL.

Translated Abstract:
우리는 SlowFast-LLaVA(줄여서 SF-LLaVA)를 제안해. 이건 훈련이 필요 없는 비디오 대형 언어 모델(LLM)로, 일반적으로 쓰이는 LLM의 토큰 예산을 넘지 않으면서도 디테일한 공간 의미와 긴 시간적 맥락을 동시에 잡을 수 있어.

이건 비디오 LLM을 위한 두 개의 스트림 설계인 SlowFast 방식을 사용해서 샘플링된 프레임에서 효과적으로 특징을 모으는 방식으로 구현돼. 구체적으로는, Slow 경로는 낮은 프레임 속도에서 최대한 많은 공간적 디테일을 유지하면서 특징을 추출해(예: 12x24 토큰), Fast 경로는 높은 프레임 속도에서 더 큰 공간 풀링 보폭(예: 6배 다운샘플링)을 사용해서 움직임 신호에 집중해. 이 덕분에 공간적이고 시간적인 특징을 잘 잡아서 비디오를 더 잘 이해할 수 있게 돼.

실험 결과로는, SF-LLaVA가 다양한 비디오 작업에서 기존의 훈련이 필요 없는 방법들보다 더 뛰어난 성능을 보였어. 어떤 벤치마크에서는 비디오 데이터셋으로 미세 조정된 최신 비디오 LLM과 비슷하거나 더 나은 성능을 내기도 했어. 코드도 공개돼 있으니, 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2407.18097.pdf

Title: SSTD: Stripe-Like Space Target Detection Using Single-Point Weak Supervision

Original Abstract:
Stripe-like space target detection (SSTD) plays a key role in enhancing space situational awareness and assessing spacecraft behaviour. This domain faces three challenges: the lack of publicly available datasets, interference from stray light and stars, and the variability of stripe-like targets, which makes manual labeling both inaccurate and labor-intensive. In response, we introduces `AstroStripeSet', a pioneering dataset designed for SSTD, aiming to bridge the gap in academic resources and advance research in SSTD. Furthermore, we propose a novel teacher-student label evolution framework with single-point weak supervision, providing a new solution to the challenges of manual labeling. This framework starts with generating initial pseudo-labels using the zero-shot capabilities of the Segment Anything Model (SAM) in a single-point setting. After that, the fine-tuned StripeSAM serves as the teacher and the newly developed StripeNet as the student, consistently improving segmentation performance through label evolution, which iteratively refines these labels. We also introduce `GeoDice', a new loss function customized for the linear characteristics of stripe-like targets. Extensive experiments show that our method matches fully supervised approaches, exhibits strong zero-shot generalization for diverse space-based and ground-based real-world images, and sets a new state-of-the-art (SOTA) benchmark. Our AstroStripeSet dataset and code will be made publicly available.

Translated Abstract:
스트라이프 같은 우주 목표 감지(SSTD)는 우주 상황 인식을 높이고 우주선 행동을 평가하는 데 중요한 역할을 해. 하지만 이 분야는 세 가지 문제를 겪고 있어: 공개된 데이터셋이 부족하고, 주위의 빛이나 별로부터 방해를 받으며, 스트라이프 같은 목표의 다양성 때문에 수작업 레이블링이 부정확하고 많은 시간과 노력이 들어. 

이런 문제를 해결하기 위해 우리는 'AstroStripeSet'이라는 새로운 데이터셋을 소개해. 이 데이터셋은 SSTD를 위해 설계된 최초의 데이터셋으로, 학술 자원의 부족을 해소하고 SSTD 연구를 발전시키는 게 목표야. 

또한, 우리는 단일 지점 약한 감독 하에 레이블 진화라는 새로운 교사-학생 프레임워크를 제안해. 이 프레임워크는 Segment Anything Model(SAM)의 제로샷 능력을 활용해 초기 가짜 레이블을 생성하는 것으로 시작해. 이후에, 미세 조정된 StripeSAM이 교사가 되고 새로 개발된 StripeNet이 학생이 되어, 레이블 진화 과정을 통해 세분화 성능을 지속적으로 개선해. 이 과정은 레이블을 점진적으로 다듬는 방식이야. 

마지막으로, 우리는 스트라이프 같은 목표의 선형 특성에 맞춘 새로운 손실 함수인 'GeoDice'를 소개해. 다양한 우주 기반과 지상 기반의 실제 이미지에 대해 우리의 방법이 완전 감독 접근 방식과 동등한 성능을 보이고, 강력한 제로샷 일반화를 보여주며, 새로운 최첨단(SOTA) 기준을 세운다는 걸 광범위한 실험을 통해 입증했어. 우리의 AstroStripeSet 데이터셋과 코드는 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2407.19323.pdf

Title: MSP-MVS: Multi-granularity Segmentation Prior Guided Multi-View Stereo

Original Abstract:
Reconstructing textureless areas in MVS poses challenges due to the absence of reliable pixel correspondences within fixed patch. Although certain methods employ patch deformation to expand the receptive field, their patches mistakenly skip depth edges to calculate areas with depth discontinuity, thereby causing ambiguity. Consequently, we introduce Multi-granularity Segmentation Prior Multi-View Stereo (MSP-MVS). Specifically, we first propose multi-granularity segmentation prior by integrating multi-granularity depth edges to restrict patch deformation within homogeneous areas. Moreover, we present anchor equidistribution that bring deformed patches with more uniformly distributed anchors to ensure an adequate coverage of their own homogeneous areas. Furthermore, we introduce iterative local search optimization to represent larger patch with sparse representative candidates, significantly boosting the expressive capacity for each patch. The state-of-the-art results on ETH3D and Tanks & Temples benchmarks demonstrate the effectiveness and robust generalization ability of our proposed method.

Translated Abstract:
MVS에서 텍스처가 없는 영역을 재구성하는 건 어려워. 고정된 패치 안에서 신뢰할 수 있는 픽셀 대응이 없어서 그렇지. 어떤 방법들은 패치 변형을 사용해서 수용 영역을 넓히긴 하지만, 그 패치들이 깊이 엣지를 놓치면서 깊이 불연속성이 있는 영역을 계산할 때 애매한 결과를 초래해.

그래서 우리는 Multi-granularity Segmentation Prior Multi-View Stereo (MSP-MVS)를 소개해. 먼저, 다중 깊이 엣지를 통합해서 동질적인 영역 안에서 패치 변형을 제한하는 다중 세분화 우선순위를 제안해. 그리고 변형된 패치가 고르게 분포된 앵커를 갖도록 하는 앵커 균등 분포를 제시해, 이로 인해 자신의 동질적인 영역을 충분히 커버할 수 있어.

또한, 더 큰 패치를 표현하기 위해 희소한 대표 후보를 사용하는 반복적인 로컬 탐색 최적화를 도입해. 이게 각 패치의 표현 능력을 크게 향상시켜. ETH3D와 Tanks & Temples 벤치마크에서 최첨단 결과를 보여주며, 우리가 제안한 방법의 효과성과 강력한 일반화 능력을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2407.19340.pdf

Title: Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification

Original Abstract:
Major Depressive Disorder (MDD) is a pervasive mental health condition that affects 300 million people worldwide. This work presents a novel, BiLSTM-based tri-modal model-level fusion architecture for the binary classification of depression from clinical interview recordings. The proposed architecture incorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses a two-shot learning based GPT-4 model to process text data. This is the first work to incorporate large language models into a multi-modal architecture for this task. It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge cross-validation split and Leave-One-Subject-Out cross-validation split, surpassing all baseline models and multiple state-of-the-art models. In Leave-One-Subject-Out testing, it achieves an accuracy of 91.01%, an F1-Score of 85.95%, a precision of 80%, and a recall of 92.86%.

Translated Abstract:
주요 우울 장애(MDD)는 전 세계적으로 3억 명에게 영향을 미치는 널리 퍼진 정신 건강 문제야. 이 연구는 임상 인터뷰 녹음을 통해 우울증을 이진 분류하는 새로운 BiLSTM 기반의 삼중 모달 모델 융합 아키텍처를 제시해. 이 아키텍처는 멜 주파수 켑스트럼 계수, 얼굴 행동 단위(Facial Action Units)를 포함하고, 텍스트 데이터를 처리하기 위해 두 번 학습하는 GPT-4 모델을 사용해.

이번 연구는 이 작업을 위해 대형 언어 모델을 다중 모달 아키텍처에 통합한 첫 번째 사례야. DAIC-WOZ AVEC 2016 챌린지의 교차 검증 분할과 한 명의 피험자를 제외한 교차 검증에서 놀라운 성과를 거두었고, 모든 기준 모델과 여러 최첨단 모델을 초월했어. 한 명의 피험자를 제외한 테스트에서 정확도는 91.01%, F1-스코어는 85.95%, 정밀도는 80%, 재현율은 92.86%를 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2407.21652.pdf

Title: Spatial Transformer Network YOLO Model for Agricultural Object Detection

Original Abstract:
Object detection plays a crucial role in the field of computer vision by autonomously locating and identifying objects of interest. The You Only Look Once (YOLO) model is an effective single-shot detector. However, YOLO faces challenges in cluttered or partially occluded scenes and can struggle with small, low-contrast objects. We propose a new method that integrates spatial transformer networks (STNs) into YOLO to improve performance. The proposed STN-YOLO aims to enhance the model's effectiveness by focusing on important areas of the image and improving the spatial invariance of the model before the detection process. Our proposed method improved object detection performance both qualitatively and quantitatively. We explore the impact of different localization networks within the STN module as well as the robustness of the model across different spatial transformations. We apply the STN-YOLO on benchmark datasets for Agricultural object detection as well as a new dataset from a state-of-the-art plant phenotyping greenhouse facility. Our code and dataset are publicly available.

Translated Abstract:
물체 탐지는 컴퓨터 비전 분야에서 매우 중요한 역할을 해. 이 기술은 관심 있는 물체를 자동으로 찾아내고 식별하는 거야. You Only Look Once (YOLO) 모델은 효과적인 단일 샷 탐지기인데, 복잡한 장면이나 부분적으로 가려진 물체를 탐지하는 데 어려움을 겪어. 또, 작고 저대비인 물체를 찾는 것도 힘들어.

그래서 우리는 YOLO에 공간 변환 네트워크(STN)를 통합하는 새로운 방법을 제안해. 이 STN-YOLO는 이미지의 중요한 부분에 집중하고, 탐지 과정 전에 모델의 공간 불변성을 향상시켜서 성능을 높이는 걸 목표로 해. 우리가 제안한 방법은 물체 탐지 성능을 질적으로나 양적으로 개선했어.

우리는 STN 모듈 내에서 다양한 위치 추적 네트워크의 영향과 서로 다른 공간 변환에 대한 모델의 강건성도 살펴봤어. 또한, STN-YOLO를 농업 물체 탐지 벤치마크 데이터셋과 최신 식물 표현형 분석 온실 시설에서 수집한 새로운 데이터셋에 적용했어. 우리의 코드와 데이터셋은 공개되어 있어.

================================================================================

URL:
https://arxiv.org/pdf/2408.02079.pdf

Title: Improving Neural Surface Reconstruction with Feature Priors from Multi-View Image

Original Abstract:
Recent advancements in Neural Surface Reconstruction (NSR) have significantly improved multi-view reconstruction when coupled with volume rendering. However, relying solely on photometric consistency in image space falls short of addressing complexities posed by real-world data, including occlusions and non-Lambertian surfaces. To tackle these challenges, we propose an investigation into feature-level consistent loss, aiming to harness valuable feature priors from diverse pretext visual tasks and overcome current limitations. It is crucial to note the existing gap in determining the most effective pretext visual task for enhancing NSR. In this study, we comprehensively explore multi-view feature priors from seven pretext visual tasks, comprising thirteen methods. Our main goal is to strengthen NSR training by considering a wide range of possibilities. Additionally, we examine the impact of varying feature resolutions and evaluate both pixel-wise and patch-wise consistent losses, providing insights into effective strategies for improving NSR performance. By incorporating pre-trained representations from MVSFormer and QuadTree, our approach can generate variations of MVS-NeuS and Match-NeuS, respectively. Our results, analyzed on DTU and EPFL datasets, reveal that feature priors from image matching and multi-view stereo outperform other pretext tasks. Moreover, we discover that extending patch-wise photometric consistency to the feature level surpasses the performance of pixel-wise approaches. These findings underscore the effectiveness of these techniques in enhancing NSR outcomes.

Translated Abstract:
최근 신경 표면 재구성(Neural Surface Reconstruction, NSR) 기술이 볼륨 렌더링과 함께 사용되면서 다중 뷰 재구성이 크게 개선되었어. 하지만 이미지 공간에서 사진 일관성만으로는 실제 데이터에서 생기는 복잡한 문제들, 예를 들어 가림 현상이나 비 램버트 표면 같은 것들을 해결하기에는 부족해. 

그래서 우리는 특징 수준 일관성 손실(feature-level consistent loss)을 조사해보려고 해. 이 방법은 다양한 사전 학습 시각 작업(pretext visual tasks)에서 유용한 특징 선행 정보를 활용해서 현재의 한계를 극복하는 걸 목표로 하고 있어. 특히, NSR을 향상시키기 위한 가장 효과적인 사전 학습 시각 작업을 찾는 게 중요하다는 걸 주목해야 해.

이번 연구에서는 13개의 방법으로 구성된 7개의 사전 학습 시각 작업에서 다중 뷰 특징 선행 정보를 종합적으로 탐구했어. 우리의 주된 목표는 다양한 가능성을 고려해서 NSR 훈련을 강화하는 거야. 또, 다양한 특징 해상도가 NSR에 미치는 영향도 살펴보고, 픽셀 단위와 패치 단위의 일관성 손실을 평가해봤어. 이를 통해 NSR 성능 향상에 효과적인 전략에 대한 통찰을 제공하고 있어.

MVSFormer와 QuadTree의 사전 훈련된 표현을 포함함으로써, 우리의 접근 방식은 각각 MVS-NeuS와 Match-NeuS의 변형을 생성할 수 있어. DTU와 EPFL 데이터셋에서 분석한 결과, 이미지 매칭과 다중 뷰 스테레오에서 얻은 특징 선행 정보가 다른 사전 학습 작업들보다 뛰어난 성능을 보여줬어. 또한, 패치 단위의 사진 일관성을 특징 수준으로 확장하는 것이 픽셀 단위 접근 방식보다 성능이 더 뛰어난 걸 발견했어. 이러한 발견들은 NSR 결과를 향상시키는 데 이 기술들이 효과적이라는 걸 강조하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2408.02245.pdf

Title: A Two-Stage Progressive Pre-training using Multi-Modal Contrastive Masked Autoencoders

Original Abstract:
In this paper, we propose a new progressive pre-training method for image understanding tasks which leverages RGB-D datasets. The method utilizes Multi-Modal Contrastive Masked Autoencoder and Denoising techniques. Our proposed approach consists of two stages. In the first stage, we pre-train the model using contrastive learning to learn cross-modal representations. In the second stage, we further pre-train the model using masked autoencoding and denoising/noise prediction used in diffusion models. Masked autoencoding focuses on reconstructing the missing patches in the input modality using local spatial correlations, while denoising learns high frequency components of the input data. Moreover, it incorporates global distillation in the second stage by leveraging the knowledge acquired in stage one. Our approach is scalable, robust and suitable for pre-training RGB-D datasets. Extensive experiments on multiple datasets such as ScanNet, NYUv2 and SUN RGB-D show the efficacy and superior performance of our approach. Specifically, we show an improvement of +1.3% mIoU against Mask3D on ScanNet semantic segmentation. We further demonstrate the effectiveness of our approach in low-data regime by evaluating it for semantic segmentation task against the state-of-the-art methods.

Translated Abstract:
이번 논문에서는 RGB-D 데이터셋을 활용한 이미지 이해 작업을 위한 새로운 점진적 사전 훈련 방법을 제안해. 이 방법은 다중 모달 대비 마스킹 오토인코더와 잡음 제거 기술을 사용해. 우리가 제안하는 접근 방식은 두 단계로 나뉘어져 있어.

첫 번째 단계에서는 대비 학습을 사용해서 모델을 사전 훈련하고, 서로 다른 모달 간의 표현을 배우게 해. 두 번째 단계에서는 마스킹 오토인코딩과 확산 모델에서 사용되는 잡음 제거/예측 기법을 사용해서 모델을 추가로 사전 훈련해. 마스킹 오토인코딩은 입력 모달리티에서 누락된 패치를 지역적인 공간 상관관계를 이용해 재구성하는 데 집중하고, 잡음 제거는 입력 데이터의 고주파 성분을 학습해. 게다가 두 번째 단계에서는 첫 번째 단계에서 얻은 지식을 활용해 글로벌 증류를 포함하고 있어.

우리의 방법은 확장 가능하고, 강건하며 RGB-D 데이터셋의 사전 훈련에 적합해. ScanNet, NYUv2, SUN RGB-D 같은 여러 데이터셋에서 광범위한 실험을 통해 우리 접근 방식의 효과성과 우수한 성능을 보여줬어. 특히, ScanNet의 의미 분할 작업에서 Mask3D 대비 +1.3% mIoU 개선을 보였고, 최신 방법들과 비교해서 데이터가 적은 상황에서도 의미 분할 작업에서 우리의 접근 방식의 효과성을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2408.03326.pdf

Title: LLaVA-OneVision: Easy Visual Task Transfer

Original Abstract:
We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.

Translated Abstract:
우리는 LLaVA-OneVision을 소개하는데, 이건 데이터, 모델, 그리고 시각적 표현에 대한 우리의 통찰을 바탕으로 개발된 오픈 대형 다중 모달 모델(LMM) 패밀리야. 

실험 결과에 따르면, LLaVA-OneVision은 단일 모델로서 오픈 LMM의 성능 한계를 동시에 끌어올릴 수 있는 첫 번째 모델이야. 이 모델은 세 가지 중요한 컴퓨터 비전 시나리오인 단일 이미지, 다중 이미지, 그리고 비디오 상황에서 성능을 높일 수 있어. 

특히 LLaVA-OneVision의 설계는 서로 다른 모달리티/시나리오 간에 강한 전이 학습이 가능하게 해줘서 새로운 능력이 나타나게 해. 특히, 이미지에서 비디오로의 작업 전이를 통해 강력한 비디오 이해와 크로스 시나리오 능력이 입증되었어.

================================================================================

URL:
https://arxiv.org/pdf/2408.04077.pdf

Title: PushPull-Net: Inhibition-driven ResNet robust to image corruptions

Original Abstract:
We introduce a novel computational unit, termed PushPull-Conv, in the first layer of a ResNet architecture, inspired by the anti-phase inhibition phenomenon observed in the primary visual cortex. This unit redefines the traditional convolutional layer by implementing a pair of complementary filters: a trainable push kernel and its counterpart, the pull kernel. The push kernel (analogous to traditional convolution) learns to respond to specific stimuli, while the pull kernel reacts to the same stimuli but of opposite contrast. This configuration enhances stimulus selectivity and effectively inhibits response in regions lacking preferred stimuli. This effect is attributed to the push and pull kernels, which produce responses of comparable magnitude in such regions, thereby neutralizing each other. The incorporation of the PushPull-Conv into ResNets significantly increases their robustness to image corruption. Our experiments with benchmark corruption datasets show that the PushPull-Conv can be combined with other data augmentation techniques to further improve model robustness. We set a new robustness benchmark on ResNet50 achieving an $mCE$ of 49.95$\%$ on ImageNet-C when combining PRIME augmentation with PushPull inhibition.

Translated Abstract:
우리는 ResNet 아키텍처의 첫 번째 레이어에 새로운 계산 단위인 PushPull-Conv를 도입했어. 이건 주 시각 피질에서 관찰된 반위상 억제 현상에서 영감을 받았어. 

이 단위는 전통적인 합성곱 레이어를 재정의하는데, 여기에는 서로 보완적인 두 개의 필터가 있어: 학습 가능한 푸시 커널과 그에 대응하는 풀 커널이야. 푸시 커널은 특정 자극에 반응하도록 학습되고, 풀 커널은 같은 자극에 반응하지만 반대의 대비를 가지지. 

이런 구성은 자극 선택성을 높이고, 선호하는 자극이 없는 영역에서는 반응을 효과적으로 억제해. 이 효과는 푸시와 풀 커널 덕분인데, 이 두 커널은 그러한 영역에서 유사한 크기의 반응을 만들어 서로 중화시켜. 

PushPull-Conv를 ResNet에 포함시키면 이미지 손상에 대한 강건성이 크게 증가해. 우리는 기준 손상 데이터셋으로 실험해봤고, PushPull-Conv를 다른 데이터 증강 기법과 결합해서 모델의 강건성을 더 향상시킬 수 있다는 걸 보여줬어. 

우리는 ResNet50에서 새로운 강건성 기준을 세웠고, PRIME 증강과 PushPull 억제를 결합했을 때 ImageNet-C에서 49.95%의 $mCE$를 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2408.04823.pdf

Title: One Shot is Enough for Sequential Infrared Small Target Segmentation

Original Abstract:
Infrared small target sequences exhibit strong similarities between frames and contain rich contextual information, which motivates us to achieve sequential infrared small target segmentation (IRSTS) with minimal data. Inspired by the success of Segment Anything Model (SAM) across various downstream tasks, we propose a one-shot and training-free method that perfectly adapts SAM's zero-shot generalization capability to sequential IRSTS. Specifically, we first obtain a confidence map through local feature matching (LFM). The highest point in the confidence map is used as the prompt to replace the manual prompt. Then, to address the over-segmentation issue caused by the domain gap, we design the point prompt-centric focusing (PPCF) module. Subsequently, to prevent miss and false detections, we introduce the triple-level ensemble (TLE) module to produce the final mask. Experiments demonstrate that our method requires only one shot to achieve comparable performance to state-of-the-art IRSTS methods and significantly outperforms other one-shot segmentation methods. Moreover, ablation studies confirm the robustness of our method in the type of annotations and the selection of reference images.

Translated Abstract:
적외선 소형 목표 시퀀스는 프레임 간에 강한 유사성을 보이고 풍부한 맥락 정보를 담고 있어, 최소한의 데이터로 연속적 적외선 소형 목표 분할(IRSTS)을 달성하려는 동기가 됩니다. 다양한 다운스트림 작업에서 Segment Anything Model (SAM)의 성공에 영감을 받아, 우리는 SAM의 제로샷 일반화 능력을 연속 IRSTS에 완벽하게 적용하는 원샷 및 훈련이 필요 없는 방법을 제안합니다.

구체적으로, 먼저 지역 특징 매칭(LFM)을 통해 신뢰도 맵을 얻습니다. 신뢰도 맵에서 가장 높은 점을 수동 프롬프트 대신 사용할 프롬프트로 설정합니다. 그 다음, 도메인 간의 격차로 인해 발생하는 과분할 문제를 해결하기 위해 포인트 프롬프트 중심 집중(PPCF) 모듈을 설계합니다. 이후, 놓치거나 잘못 탐지하는 것을 방지하기 위해 최종 마스크를 생성하는 삼중 수준 앙상블(TLE) 모듈을 도입합니다.

실험 결과, 우리의 방법은 단 한 번의 샷으로 최신 IRSTS 방법과 유사한 성능을 달성하며, 다른 원샷 분할 방법보다 훨씬 뛰어난 성과를 보였습니다. 또한, 아블레이션 연구를 통해 우리의 방법이 주석 유형과 참조 이미지 선택에 대해 견고하다는 것을 확인했습니다.

================================================================================

URL:
https://arxiv.org/pdf/2408.05508.pdf

Title: PointMT: Efficient Point Cloud Analysis with Hybrid MLP-Transformer Architecture

Original Abstract:
In recent years, point cloud analysis methods based on the Transformer architecture have made significant progress, particularly in the context of multimedia applications such as 3D modeling, virtual reality, and autonomous systems. However, the high computational resource demands of the Transformer architecture hinder its scalability, real-time processing capabilities, and deployment on mobile devices and other platforms with limited computational resources. This limitation remains a significant obstacle to its practical application in scenarios requiring on-device intelligence and multimedia processing. To address this challenge, we propose an efficient point cloud analysis architecture, \textbf{Point} \textbf{M}LP-\textbf{T}ransformer (PointMT). This study tackles the quadratic complexity of the self-attention mechanism by introducing a linear complexity local attention mechanism for effective feature aggregation. Additionally, to counter the Transformer's focus on token differences while neglecting channel differences, we introduce a parameter-free channel temperature adaptation mechanism that adaptively adjusts the attention weight distribution in each channel, enhancing the precision of feature aggregation. To improve the Transformer's slow convergence speed due to the limited scale of point cloud datasets, we propose an MLP-Transformer hybrid module, which significantly enhances the model's convergence speed. Furthermore, to boost the feature representation capability of point tokens, we refine the classification head, enabling point tokens to directly participate in prediction. Experimental results on multiple evaluation benchmarks demonstrate that PointMT achieves performance comparable to state-of-the-art methods while maintaining an optimal balance between performance and accuracy.

Translated Abstract:
최근 몇 년간, Transformer 아키텍처를 기반으로 한 포인트 클라우드 분석 방법들이 3D 모델링, 가상 현실, 자율 시스템 같은 멀티미디어 응용 분야에서 큰 발전을 이루었어. 하지만 Transformer 아키텍처는 높은 계산 자원을 요구해서, 확장성이나 실시간 처리, 그리고 모바일 기기 같은 계산 자원이 제한된 플랫폼에서의 사용이 어려워. 이 제한은 디바이스 내 인공지능이나 멀티미디어 처리가 필요한 상황에서 실질적인 적용에 큰 장애가 되고 있어.

이 문제를 해결하기 위해, 우리는 효율적인 포인트 클라우드 분석 아키텍처인 \textbf{Point} \textbf{M}LP-\textbf{T}ransformer (PointMT)를 제안해. 이 연구는 자기 주의 메커니즘의 복잡성을 줄이기 위해 선형 복잡도의 지역 주의 메커니즘을 도입해서 효과적인 특징 집합을 할 수 있도록 했어. 또한, Transformer가 토큰 차이에 집중하는 반면 채널 차이를 무시하는 문제를 해결하기 위해, 우리는 매개변수 없는 채널 온도 조정 메커니즘을 도입했어. 이 메커니즘은 각 채널의 주의 가중치 분포를 적절하게 조정해서 특징 집합의 정확성을 높여줘.

포인트 클라우드 데이터셋의 한정된 규모로 인해 Transformer의 느린 수렴 속도를 개선하기 위해, 우리는 MLP-Transformer 하이브리드 모듈을 제안했어. 이 모듈은 모델의 수렴 속도를 크게 향상시켜. 그리고 포인트 토큰의 특징 표현 능력을 높이기 위해 분류 헤드를 개선해서 포인트 토큰이 직접 예측에 참여할 수 있도록 했어.

여러 평가 기준에서 실험한 결과, PointMT가 최신 방법들과 비슷한 성능을 내면서도 성능과 정확성 간의 최적 균형을 유지한다는 것을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2408.06899.pdf

Title: EEPPR: Event-based Estimation of Periodic Phenomena Rate using Correlation in 3D

Original Abstract:
We present a novel method for measuring the rate of periodic phenomena (e.g., rotation, flicker, and vibration), by an event camera, a device asynchronously reporting brightness changes at independently operating pixels with high temporal resolution. The approach assumes that for a periodic phenomenon, a highly similar set of events is generated within a spatio-temporal window at a time difference corresponding to its period. The sets of similar events are detected by a correlation in the spatio-temporal event stream space. The proposed method, EEPPR, is evaluated on a dataset of 12 sequences of periodic phenomena, i.e. flashing light and vibration, and periodic motion, e.g., rotation, ranging from 3.2 Hz to 2 kHz (equivalent to 192 - 120 000 RPM). EEPPR significantly outperforms published methods on this dataset, achieving a mean relative error of 0.1%, setting new state-of-the-art. The dataset and codes are publicly available on GitHub.

Translated Abstract:
우리는 주기적인 현상(예: 회전, 깜박임, 진동)의 속도를 측정하는 새로운 방법을 제안해. 이 방법은 이벤트 카메라를 사용하는데, 이 카메라는 독립적으로 작동하는 픽셀에서 밝기 변화가 발생할 때 비동기적으로 보고하는 장치야. 이 방법은 주기적인 현상에 대해, 특정 시간 차이(주기와 관련된)로 발생하는 매우 유사한 이벤트 집합이 공간-시간 창 내에서 생성된다고 가정해.

유사한 이벤트 집합은 공간-시간 이벤트 스트림 공간에서 상관관계를 통해 탐지돼. 우리가 제안한 방법, EEPPR는 12개의 주기적인 현상 시퀀스(예: 깜박이는 빛, 진동, 회전 등)를 포함한 데이터셋에서 평가됐어. 이 현상들은 3.2 Hz에서 2 kHz까지(192에서 120,000 RPM에 해당) 다양해. EEPPR는 이 데이터셋에서 기존 방법들보다 훨씬 우수한 성능을 보여주며, 평균 상대 오차가 0.1%로 새로운 최신 기술 기준을 세웠어. 데이터셋과 코드는 GitHub에서 공개돼 있어.

================================================================================

URL:
https://arxiv.org/pdf/2408.08105.pdf

Title: Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language Models to Infer Causal Links Between Siamese Images

Original Abstract:
Large Language Models (LLMs) have showcased exceptional ability in causal reasoning from textual information. However, will these causalities remain straightforward for Vision Large Language Models (VLLMs) when only visual hints are provided? Motivated by this, we propose a novel Multimodal Causal Reasoning benchmark, namely MuCR, to challenge VLLMs to infer semantic cause-and-effect relationship when solely relying on visual cues such as action, appearance, clothing, and environment. Specifically, we introduce a prompt-driven image synthesis approach to create siamese images with embedded semantic causality and visual cues, which can effectively evaluate VLLMs' causal reasoning capabilities. Additionally, we develop tailored metrics from multiple perspectives, including image-level match, phrase-level understanding, and sentence-level explanation, to comprehensively assess VLLMs' comprehension abilities. Our extensive experiments reveal that the current state-of-the-art VLLMs are not as skilled at multimodal causal reasoning as we might have hoped. Furthermore, we perform a comprehensive analysis to understand these models' shortcomings from different views and suggest directions for future research. We hope MuCR can serve as a valuable resource and foundational benchmark in multimodal causal reasoning research. The project is available at: this https URL

Translated Abstract:
대형 언어 모델(LLM)은 텍스트 정보에서 인과 관계를 잘 추론할 수 있는 능력을 보여줬어. 그런데 시각적 단서만 제공될 때 비전 대형 언어 모델(VLLM)도 같은 방식으로 인과 관계를 잘 이해할 수 있을까? 이 질문에서 출발해서 우리는 VLLM이 행동, 외모, 의상, 환경 같은 시각적 단서만으로 의미 있는 인과 관계를 추론할 수 있도록 도전하는 새로운 다중 모달 인과 추론 벤치마크인 MuCR을 제안해.

특히, 우리는 프롬프트 기반 이미지 합성 방법을 도입해서 의미 있는 인과 관계와 시각적 단서를 포함한 쌍둥이 이미지를 만들어. 이 방법은 VLLM의 인과 추론 능력을 효과적으로 평가할 수 있어. 게다가, 이미지 수준의 일치, 구문 수준의 이해, 문장 수준의 설명 등 여러 측면에서 맞춤형 평가 기준을 개발해서 VLLM의 이해 능력을 종합적으로 평가할 거야.

우리의 광범위한 실험 결과, 현재의 최신 VLLM들이 다중 모달 인과 추론에서 우리가 생각했던 것만큼 잘하지 못한다는 걸 밝혀냈어. 추가로, 우리는 다양한 관점에서 이 모델들의 단점을 이해하기 위한 포괄적인 분석을 진행하고, 향후 연구 방향에 대한 제안도 할 거야. MuCR이 다중 모달 인과 추론 연구에 소중한 자원과 기본 벤치마크로 활용되기를 바래. 프로젝트에 대한 정보는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2408.10012.pdf

Title: CLIPCleaner: Cleaning Noisy Labels with CLIP

Original Abstract:
Learning with Noisy labels (LNL) poses a significant challenge for the Machine Learning community. Some of the most widely used approaches that select as clean samples for which the model itself (the in-training model) has high confidence, e.g., `small loss', can suffer from the so called `self-confirmation' bias. This bias arises because the in-training model, is at least partially trained on the noisy labels. Furthermore, in the classification case, an additional challenge arises because some of the label noise is between classes that are visually very similar (`hard noise'). This paper addresses these challenges by proposing a method (\textit{CLIPCleaner}) that leverages CLIP, a powerful Vision-Language (VL) model for constructing a zero-shot classifier for efficient, offline, clean sample selection. This has the advantage that the sample selection is decoupled from the in-training model and that the sample selection is aware of the semantic and visual similarities between the classes due to the way that CLIP is trained. We provide theoretical justifications and empirical evidence to demonstrate the advantages of CLIP for LNL compared to conventional pre-trained models. Compared to current methods that combine iterative sample selection with various techniques, \textit{CLIPCleaner} offers a simple, single-step approach that achieves competitive or superior performance on benchmark datasets. To the best of our knowledge, this is the first time a VL model has been used for sample selection to address the problem of Learning with Noisy Labels (LNL), highlighting their potential in the domain.

Translated Abstract:
노이즈가 있는 라벨로 학습하는 것(LNL)은 머신러닝 분야에서 큰 도전 과제가 되고 있어. 모델이 높은 신뢰도를 가진 깨끗한 샘플을 선택하는 일반적인 방법들, 예를 들면 '작은 손실' 같은 것들이 '자기확인' 편향으로 고생할 수 있어. 이 편향은 학습 중인 모델이 노이즈가 있는 라벨로 부분적으로 학습되기 때문에 생겨. 게다가 분류 문제에서는 시각적으로 매우 비슷한 클래스 간의 라벨 노이즈가 있어서 추가적인 도전이 돼 ('어려운 노이즈').

이 논문은 CLIP이라는 강력한 비전-언어 모델을 활용한 방법인 \textit{CLIPCleaner}를 제안하면서 이런 문제들을 다루고 있어. 이 방법은 효율적이고 오프라인에서 깨끗한 샘플을 선택하는 제로샷 분류기를 만드는 데 사용돼. \textit{CLIPCleaner}의 장점은 샘플 선택이 학습 중인 모델과 분리되어 있고, CLIP의 훈련 방식 덕분에 클래스 간의 의미적, 시각적 유사성을 인식할 수 있다는 거야.

우리는 CLIP이 LNL 문제를 해결하는 데 있어서 기존의 사전 훈련된 모델들과 비교해 어떤 이점이 있는지 이론적 근거와 실증적 증거를 통해 보여줄 거야. 현재의 방법들이 여러 기술과 함께 반복 샘플 선택을 결합하는 것과 달리, \textit{CLIPCleaner}는 간단하고 단일 단계로 벤치마크 데이터셋에서 경쟁력 있는 성능을 내는 방법이야. 우리가 아는 한, VL 모델이 노이즈가 있는 라벨 문제를 해결하기 위해 샘플 선택에 사용된 것은 이번이 처음이야. 이로 인해 VL 모델들이 이 분야에서의 잠재력을 강조하게 됐어.

================================================================================

URL:
https://arxiv.org/pdf/2408.13005.pdf

Title: EasyControl: Transfer ControlNet to Video Diffusion for Controllable Generation and Interpolation

Original Abstract:
Following the advancements in text-guided image generation technology exemplified by Stable Diffusion, video generation is gaining increased attention in the academic community. However, relying solely on text guidance for video generation has serious limitations, as videos contain much richer content than images, especially in terms of motion. This information can hardly be adequately described with plain text. Fortunately, in computer vision, various visual representations can serve as additional control signals to guide generation. With the help of these signals, video generation can be controlled in finer detail, allowing for greater flexibility for different applications. Integrating various controls, however, is nontrivial. In this paper, we propose a universal framework called EasyControl. By propagating and injecting condition features through condition adapters, our method enables users to control video generation with a single condition map. With our framework, various conditions including raw pixels, depth, HED, etc., can be integrated into different Unet-based pre-trained video diffusion models at a low practical cost. We conduct comprehensive experiments on public datasets, and both quantitative and qualitative results indicate that our method outperforms state-of-the-art methods. EasyControl significantly improves various evaluation metrics across multiple validation datasets compared to previous works. Specifically, for the sketch-to-video generation task, EasyControl achieves an improvement of 152.0 on FVD and 19.9 on IS, respectively, in UCF101 compared with VideoComposer. For fidelity, our model demonstrates powerful image retention ability, resulting in high FVD and IS in UCF101 and MSR-VTT compared to other image-to-video models.

Translated Abstract:
최근 Stable Diffusion 같은 텍스트 기반 이미지 생성 기술이 발전하면서, 비디오 생성이 학계에서 더욱 주목받고 있어. 하지만 비디오 생성에 텍스트만 의존하는 건 한계가 있어. 비디오는 이미지보다 훨씬 더 풍부한 내용을 담고 있는데, 특히 움직임에 대한 정보는 단순한 텍스트로는 잘 설명할 수 없어. 다행히 컴퓨터 비전에서는 다양한 시각적 표현이 추가적인 제어 신호로 사용될 수 있어. 이런 신호들을 활용하면 비디오 생성의 세부 조정을 할 수 있어서, 다양한 응용에 더 큰 유연성을 제공할 수 있어.

하지만 여러 가지 제어를 통합하는 건 쉽지 않아. 이 논문에서는 EasyControl이라는 범용 프레임워크를 제안해. 우리의 방법은 조건 어댑터를 통해 조건 특징을 전파하고 주입함으로써, 사용자가 단 하나의 조건 맵으로 비디오 생성을 제어할 수 있게 해. 이 프레임워크를 사용하면 원시 픽셀, 깊이, HED 등 다양한 조건을 Unet 기반의 사전 훈련된 비디오 확산 모델에 적은 비용으로 통합할 수 있어.

우리는 공공 데이터셋에서 포괄적인 실험을 진행했고, 정량적 및 정성적 결과 모두에서 우리의 방법이 최신 기술보다 우수하다는 걸 보여줬어. EasyControl은 이전 연구와 비교했을 때 여러 검증 데이터셋에서 다양한 평가 지표를 크게 개선해. 특히 스케치에서 비디오로 생성하는 작업에서는, UCF101에서 EasyControl이 VideoComposer와 비교해 FVD에서 152.0, IS에서 19.9의 개선을 이뤘어. 그리고 충실도 측면에서도 우리 모델은 강력한 이미지 보존 능력을 보여줘서, 다른 이미지-비디오 모델에 비해 UCF101과 MSR-VTT에서 높은 FVD와 IS를 기록했어.

================================================================================

URL:
https://arxiv.org/pdf/2408.15503.pdf

Title: RoboSense: Large-scale Dataset and Benchmark for Multi-sensor Low-speed Autonomous Driving

Original Abstract:
Robust object detection and tracking under arbitrary sight of view is challenging yet essential for the development of Autonomous Vehicle technology. With the growing demand of unmanned function vehicles, near-field scene understanding becomes an important research topic in the areas of low-speed autonomous driving. Due to the complexity of driving conditions and diversity of near obstacles such as blind spots and high occlusion, the perception capability of near-field environment is still inferior than its farther counterpart. To further enhance the intelligent ability of unmanned vehicles, in this paper, we construct a multimodal data collection platform based on 3 main types of sensors (Camera, LiDAR and Fisheye), which supports flexible sensor configurations to enable dynamic sight of view for ego vehicle, either global view or local view. Meanwhile, a large-scale multi-sensor dataset is built, named RoboSense, to facilitate near-field scene understanding. RoboSense contains more than 133K synchronized data with 1.4M 3D bounding box and IDs annotated in the full $360^{\circ}$ view, forming 216K trajectories across 7.6K temporal sequences. It has $270\times$ and $18\times$ as many annotations of near-field obstacles within 5$m$ as the previous single-vehicle datasets such as KITTI and nuScenes. Moreover, we define a novel matching criterion for near-field 3D perception and prediction metrics. Based on RoboSense, we formulate 6 popular tasks to facilitate the future development of related research, where the detailed data analysis as well as benchmarks are also provided accordingly.

Translated Abstract:
자율주행차 기술을 발전시키기 위해서는 다양한 시야에서의 강력한 물체 탐지와 추적이 중요하지만, 이게 쉽지 않아. 무인 차량의 수요가 늘어남에 따라, 저속 자율주행에서 근거리 장면 이해가 중요한 연구 주제가 되고 있어. 하지만 운전 조건이 복잡하고, 사각지대나 높은 가림막 같은 다양한 근처 장애물 때문에, 근거리 환경 인식 능력이 먼 거리보다 떨어져.

이 논문에서는 무인 차량의 지능적인 능력을 더 향상시키기 위해, 카메라, 라이다, 어안 렌즈 등 3가지 주요 센서를 기반으로 한 다중 모드 데이터 수집 플랫폼을 만들었어. 이 플랫폼은 자율주행차가 전방위 또는 국소적으로 동적인 시야를 가질 수 있도록 유연한 센서 구성을 지원해. 

그리고 RoboSense라는 대규모 다중 센서 데이터셋도 구축했어. 이 데이터셋은 133,000개 이상의 동기화된 데이터와 1.4백만 개의 3D 바운딩 박스 및 ID가 포함되어 있어. 전체 360도 시야에서 216,000개의 궤적이 7,600개의 시간적 시퀀스에 걸쳐 형성되었지. RoboSense는 KITTI나 nuScenes 같은 기존 단일 차량 데이터셋보다 270배, 18배 많은 근거리 장애물 주석을 가지고 있어.

또한, 근거리 3D 인식 및 예측 메트릭을 위한 새로운 매칭 기준도 정의했어. RoboSense를 기반으로 6가지 인기 있는 작업을 정리해서 관련 연구의 미래 발전을 도울 수 있도록 했고, 이에 대한 상세한 데이터 분석과 벤치마크도 제공하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2408.16445.pdf

Title: Mismatched: Evaluating the Limits of Image Matching Approaches and Benchmarks

Original Abstract:
Three-dimensional (3D) reconstruction from two-dimensional images is an active research field in computer vision, with applications ranging from navigation and object tracking to segmentation and three-dimensional modeling. Traditionally, parametric techniques have been employed for this task. However, recent advancements have seen a shift towards learning-based methods. Given the rapid pace of research and the frequent introduction of new image matching methods, it is essential to evaluate them. In this paper, we present a comprehensive evaluation of various image matching methods using a structure-from-motion pipeline. We assess the performance of these methods on both in-domain and out-of-domain datasets, identifying key limitations in both the methods and benchmarks. We also investigate the impact of edge detection as a pre-processing step. Our analysis reveals that image matching for 3D reconstruction remains an open challenge, necessitating careful selection and tuning of models for specific scenarios, while also highlighting mismatches in how metrics currently represent method performance.

Translated Abstract:
2D 이미지에서 3D 재구성을 하는 것은 컴퓨터 비전 분야에서 활발히 연구되고 있는 주제야. 이 기술은 내비게이션, 물체 추적, 분할, 3D 모델링 등 다양한 분야에 쓰여. 예전에는 주로 매개변수 기반 기법을 사용했지만, 최근에는 학습 기반 방법으로 많이 바뀌고 있어. 

이런 연구가 빠르게 진행되고 있고 새로운 이미지 매칭 방법도 자주 등장하니까, 이들을 평가하는 게 정말 중요해. 이번 논문에서는 구조에서 움직임을 추출하는 파이프라인을 이용해서 여러 이미지 매칭 방법을 종합적으로 평가했어. 우리는 이 방법들이 도메인 내와 도메인 외 데이터셋에서 어떻게 성능을 발휘하는지 살펴보면서, 방법과 벤치마크에서의 주요 제한점을 찾아냈어. 

또한, 엣지 감지가 전처리 단계로 어떤 영향을 미치는지도 조사했어. 우리의 분석 결과, 3D 재구성을 위한 이미지 매칭은 여전히 해결해야 할 도전 과제가 많고, 특정 상황에 맞게 모델을 신중하게 선택하고 조정해야 한다는 걸 보여줬어. 게다가 현재의 지표들이 방법 성능을 나타내는 방식에서 불일치가 있다는 점도 강조했어.

================================================================================

URL:
https://arxiv.org/pdf/2408.16662.pdf

Title: Space3D-Bench: Spatial 3D Question Answering Benchmark

Original Abstract:
Answering questions about the spatial properties of the environment poses challenges for existing language and vision foundation models due to a lack of understanding of the 3D world notably in terms of relationships between objects. To push the field forward, multiple 3D Q&A datasets were proposed which, overall, provide a variety of questions, but they individually focus on particular aspects of 3D reasoning or are limited in terms of data modalities. To address this, we present Space3D-Bench - a collection of 1000 general spatial questions and answers related to scenes of the Replica dataset which offers a variety of data modalities: point clouds, posed RGB-D images, navigation meshes and 3D object detections. To ensure that the questions cover a wide range of 3D objectives, we propose an indoor spatial questions taxonomy inspired by geographic information systems and use it to balance the dataset accordingly. Moreover, we provide an assessment system that grades natural language responses based on predefined ground-truth answers by leveraging a Vision Language Model's comprehension of both text and images to compare the responses with ground-truth textual information or relevant visual data. Finally, we introduce a baseline called RAG3D-Chat integrating the world understanding of foundation models with rich context retrieval, achieving an accuracy of 67% on the proposed dataset.

Translated Abstract:
환경의 공간적 속성에 대한 질문에 답하는 것은 기존의 언어 및 비전 모델에게 어려움을 주고 있어. 그 이유는 3D 세계, 특히 물체들 간의 관계를 이해하는 데 부족함이 있어서야. 이 문제를 해결하기 위해 여러 3D Q&A 데이터셋이 제안되었는데, 각각은 다양한 질문을 제공하지만 특정 3D 추론의 측면에만 초점을 맞추거나 데이터 유형이 제한적이야.

그래서 우리는 Space3D-Bench라는 것을 소개할 거야. 이건 Replica 데이터셋의 장면과 관련된 1000개의 일반적인 공간 질문과 답변을 모은 거야. 여기서는 포인트 클라우드, RGB-D 이미지, 내비게이션 메시, 3D 물체 탐지 같은 다양한 데이터 유형을 제공해. 질문이 다양한 3D 목표를 다룰 수 있도록, 우리는 지리 정보 시스템에서 영감을 받은 실내 공간 질문 분류 체계를 제안하고 이를 통해 데이터셋을 균형 있게 구성했어.

게다가, 우리는 자연어 응답을 미리 정해진 정답에 따라 평가할 수 있는 시스템도 제공해. 이건 비전 언어 모델이 텍스트와 이미지를 이해하는 능력을 활용해서 응답을 정답 텍스트나 관련 시각 데이터와 비교하는 방식이야. 마지막으로, 우리는 RAG3D-Chat이라는 기준 모델을 소개해. 이 모델은 기초 모델의 세계 이해와 풍부한 맥락 검색을 통합해서 제안된 데이터셋에서 67%의 정확도를 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.00346.pdf

Title: SMAFormer: Synergistic Multi-Attention Transformer for Medical Image Segmentation

Original Abstract:
In medical image segmentation, specialized computer vision techniques, notably transformers grounded in attention mechanisms and residual networks employing skip connections, have been instrumental in advancing performance. Nonetheless, previous models often falter when segmenting small, irregularly shaped tumors. To this end, we introduce SMAFormer, an efficient, Transformer-based architecture that fuses multiple attention mechanisms for enhanced segmentation of small tumors and organs. SMAFormer can capture both local and global features for medical image segmentation. The architecture comprises two pivotal components. First, a Synergistic Multi-Attention (SMA) Transformer block is proposed, which has the benefits of Pixel Attention, Channel Attention, and Spatial Attention for feature enrichment. Second, addressing the challenge of information loss incurred during attention mechanism transitions and feature fusion, we design a Feature Fusion Modulator. This module bolsters the integration between the channel and spatial attention by mitigating reshaping-induced information attrition. To evaluate our method, we conduct extensive experiments on various medical image segmentation tasks, including multi-organ, liver tumor, and bladder tumor segmentation, achieving state-of-the-art results. Code and models are available at: \url{this https URL}.

Translated Abstract:
의료 이미지 분할에서, 주목 메커니즘과 잔여 네트워크를 활용한 변환기 같은 전문 컴퓨터 비전 기술들이 성능 향상에 큰 역할을 해왔어. 하지만 이전 모델들은 작고 불규칙한 모양의 종양을 분할할 때 흔히 어려움을 겪어. 

그래서 우리는 SMAFormer라는 효율적인 변환기 기반 아키텍처를 소개해. 이 모델은 여러 가지 주목 메커니즘을 결합해서 작은 종양과 장기의 분할 성능을 높여줘. SMAFormer는 의료 이미지 분할을 위해 지역적이고 전반적인 특징을 모두 잘 포착할 수 있어.

이 아키텍처는 두 가지 중요한 구성 요소로 이루어져 있어. 첫째, 시너지 멀티-어텐션(SMA) 변환기 블록을 제안해. 이 블록은 픽셀 어텐션, 채널 어텐션, 공간 어텐션의 장점을 활용해 특징을 더욱 풍부하게 만들어. 둘째, 어텐션 메커니즘 전환과 특징 융합 중에 발생하는 정보 손실 문제를 해결하기 위해 특징 융합 조절기를 설계했어. 이 모듈은 채널 어텐션과 공간 어텐션 간의 통합을 강화하면서 재형성으로 인한 정보 손실을 줄여줘.

우리 방법을 평가하기 위해, 여러 의료 이미지 분할 작업, 즉 다기관, 간 종양, 방광 종양 분할에 대한 광범위한 실험을 수행했어. 그 결과, 최첨단 성능을 달성했어. 코드와 모델은 이 링크에서 확인할 수 있어: \url{this https URL}.

================================================================================

URL:
https://arxiv.org/pdf/2409.00606.pdf

Title: Style Transfer: From Stitching to Neural Networks

Original Abstract:
This article compares two style transfer methods in image processing: the traditional method, which synthesizes new images by stitching together small patches from existing images, and a modern machine learning-based approach that uses a segmentation network to isolate foreground objects and apply style transfer solely to the background. The traditional method excels in creating artistic abstractions but can struggle with seamlessness, whereas the machine learning method preserves the integrity of foreground elements while enhancing the background, offering improved aesthetic quality and computational efficiency. Our study indicates that machine learning-based methods are more suited for real-world applications where detail preservation in foreground elements is essential.

Translated Abstract:
이 논문은 이미지 처리에서 두 가지 스타일 전이 방법을 비교해. 하나는 전통적인 방법으로, 기존 이미지에서 작은 패치를 이어붙여서 새로운 이미지를 만드는 방식이야. 다른 하나는 현대적인 머신러닝 기반 접근법으로, 세분화 네트워크를 사용해 전경 객체를 분리하고 스타일 전이를 배경에만 적용해.

전통적인 방법은 예술적인 추상화를 만드는 데는 뛰어나지만, 이미지가 매끄럽게 연결되는 데는 어려움이 있어. 반면, 머신러닝 방법은 전경 요소의 형태를 잘 유지하면서 배경을 개선해. 그래서 미적 품질과 계산 효율성에서 더 나아.

우리 연구에서는 머신러닝 기반 방법이 전경 요소의 세부 정보를 유지하는 게 중요한 실제 응용에 더 적합하다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.00973.pdf

Title: IVGF: The Fusion-Guided Infrared and Visible General Framework

Original Abstract:
Infrared and visible dual-modality tasks such as semantic segmentation and object detection can achieve robust performance even in extreme scenes by fusing complementary information. Most current methods design task-specific frameworks, which are limited in generalization across multiple tasks. In this paper, we propose a fusion-guided infrared and visible general framework, IVGF, which can be easily extended to many high-level vision tasks. Firstly, we adopt the SOTA infrared and visible foundation models to extract the general representations. Then, to enrich the semantics information of these general representations for high-level vision tasks, we design the feature enhancement module and token enhancement module for feature maps and tokens, respectively. Besides, the attention-guided fusion module is proposed for effectively fusing by exploring the complementary information of two modalities. Moreover, we also adopt the cutout&mix augmentation strategy to conduct the data augmentation, which further improves the ability of the model to mine the regional complementary between the two modalities. Extensive experiments show that the IVGF outperforms state-of-the-art dual-modality methods in the semantic segmentation and object detection tasks. The detailed ablation studies demonstrate the effectiveness of each module, and another experiment explores the anti-missing modality ability of the proposed method in the dual-modality semantic segmentation task.

Translated Abstract:
적외선과 가시광선의 이중 모드 작업인 의미 분할과 객체 탐지가 극한 상황에서도 강력한 성능을 발휘할 수 있도록 보완적인 정보를 융합합니다. 현재 대부분의 방법들은 특정 작업에 맞춘 프레임워크를 설계하는데, 이는 여러 작업에 대한 일반화에는 한계가 있습니다. 

이 논문에서는 여러 고수준 비전 작업에 쉽게 확장할 수 있는 융합 안내 적외선 및 가시광선 일반 프레임워크인 IVGF를 제안합니다. 먼저, 최신 적외선과 가시광선 기초 모델을 사용해 일반적인 표현을 추출합니다. 그런 다음, 고수준 비전 작업을 위한 이 일반 표현의 의미 정보를 풍부하게 하기 위해, 피처 맵과 토큰을 각각 위한 피처 강화 모듈과 토큰 강화 모듈을 설계합니다. 

또한, 두 모드의 보완 정보를 탐색하여 효과적으로 융합할 수 있는 주의 기반 융합 모듈을 제안합니다. 더불어, 데이터 증강을 위해 컷아웃&믹스 증강 전략을 적용하여 두 모드 간의 지역 보완성을 탐색하는 모델의 능력을 더욱 향상시킵니다. 

광범위한 실험 결과, IVGF가 의미 분할 및 객체 탐지 작업에서 최신 이중 모드 방법보다 우수한 성능을 보인다는 것을 보여줍니다. 각 모듈의 효과를 입증하는 상세한 제거 연구도 포함되어 있으며, 또 다른 실험에서는 이중 모드 의미 분할 작업에서 제안된 방법의 결측 모드에 대한 저항 능력을 탐구합니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.01086.pdf

Title: DPDEdit: Detail-Preserved Diffusion Models for Multimodal Fashion Image Editing

Original Abstract:
Fashion image editing is a crucial tool for designers to convey their creative ideas by visualizing design concepts interactively. Current fashion image editing techniques, though advanced with multimodal prompts and powerful diffusion models, often struggle to accurately identify editing regions and preserve the desired garment texture detail. To address these challenges, we introduce a new multimodal fashion image editing architecture based on latent diffusion models, called Detail-Preserved Diffusion Models (DPDEdit). DPDEdit guides the fashion image generation of diffusion models by integrating text prompts, region masks, human pose images, and garment texture images. To precisely locate the editing region, we first introduce Grounded-SAM to predict the editing region based on the user's textual description, and then combine it with other conditions to perform local editing. To transfer the detail of the given garment texture into the target fashion image, we propose a texture injection and refinement mechanism. Specifically, this mechanism employs a decoupled cross-attention layer to integrate textual descriptions and texture images, and incorporates an auxiliary U-Net to preserve the high-frequency details of generated garment texture. Additionally, we extend the VITON-HD dataset using a multimodal large language model to generate paired samples with texture images and textual descriptions. Extensive experiments show that our DPDEdit outperforms state-of-the-art methods in terms of image fidelity and coherence with the given multimodal inputs.

Translated Abstract:
패션 이미지 편집은 디자이너들이 디자인 아이디어를 시각적으로 표현하는 데 중요한 도구야. 현재의 패션 이미지 편집 기술은 멀티모달 프롬프트와 강력한 확산 모델 덕분에 발전했지만, 편집할 영역을 정확히 찾거나 원하는 옷감의 질감을 잘 유지하는 데는 어려움이 있어.

이런 문제를 해결하기 위해, 우리는 잠재적 확산 모델을 기반으로 한 새로운 멀티모달 패션 이미지 편집 아키텍처인 Detail-Preserved Diffusion Models (DPDEdit)를 소개해. DPDEdit는 텍스트 프롬프트, 영역 마스크, 사람 자세 이미지, 옷감 질감 이미지를 통합해서 확산 모델의 패션 이미지 생성을 이끌어.

편집할 영역을 정확히 찾기 위해, 먼저 Grounded-SAM을 도입해서 사용자의 텍스트 설명에 기반해 편집 영역을 예측해. 그 후, 이 정보를 다른 조건들과 결합해 지역 편집을 수행해. 주어진 옷감의 질감을 목표 패션 이미지로 옮기기 위해, 우리는 질감 주입 및 정제 메커니즘을 제안해. 이 메커니즘은 텍스트 설명과 질감 이미지를 통합하기 위해 분리된 크로스-어텐션 레이어를 사용하고, 생성된 옷감 질감의 고주파 세부사항을 유지하기 위해 보조 U-Net을 사용해.

또한, 우리는 멀티모달 대형 언어 모델을 이용해 VITON-HD 데이터셋을 확장해 질감 이미지와 텍스트 설명이 있는 쌍 샘플을 생성해. 다양한 실험 결과, 우리의 DPDEdit가 이미지 충실도와 주어진 멀티모달 입력에 대한 일관성 면에서 최첨단 방법들보다 뛰어나다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.01781.pdf

Title: Dual Advancement of Representation Learning and Clustering for Sparse and Noisy Images

Original Abstract:
Sparse and noisy images (SNIs), like those in spatial gene expression data, pose significant challenges for effective representation learning and clustering, which are essential for thorough data analysis and interpretation. In response to these challenges, we propose Dual Advancement of Representation Learning and Clustering (DARLC), an innovative framework that leverages contrastive learning to enhance the representations derived from masked image modeling. Simultaneously, DARLC integrates cluster assignments in a cohesive, end-to-end approach. This integrated clustering strategy addresses the "class collision problem" inherent in contrastive learning, thus improving the quality of the resulting representations. To generate more plausible positive views for contrastive learning, we employ a graph attention network-based technique that produces denoised images as augmented data. As such, our framework offers a comprehensive approach that improves the learning of representations by enhancing their local perceptibility, distinctiveness, and the understanding of relational semantics. Furthermore, we utilize a Student's t mixture model to achieve more robust and adaptable clustering of SNIs. Extensive experiments, conducted across 12 different types of datasets consisting of SNIs, demonstrate that DARLC surpasses the state-of-the-art methods in both image clustering and generating image representations that accurately capture gene interactions. Code is available at this https URL.

Translated Abstract:
희소하고 노이즈가 많은 이미지(SNIs), 예를 들어 공간 유전자 발현 데이터 같은 것들은 효과적인 표현 학습과 클러스터링에 큰 도전을 줘. 이 두 가지는 데이터 분석과 해석에 꼭 필요해. 이런 문제를 해결하기 위해 우리는 DARLC(Dual Advancement of Representation Learning and Clustering)라는 새로운 프레임워크를 제안해. 이건 대조 학습을 활용해서 마스크 처리된 이미지 모델링에서 얻은 표현을 더 좋게 만들어줘. 동시에, DARLC는 클러스터 할당을 통합해서 전체적으로 끝까지 연결된 접근 방식을 사용해.

이 통합 클러스터링 전략은 대조 학습에서 생기는 "클래스 충돌 문제"를 해결해서, 결과적으로 더 나은 표현 품질을 만들어. 대조 학습을 위해 더 믿을 만한 긍정적인 뷰를 생성하기 위해, 우리는 그래프 어텐션 네트워크 기반의 기술을 사용해서 노이즈가 제거된 이미지를 보강 데이터로 만들어. 그래서 우리의 프레임워크는 표현 학습을 개선하는 포괄적인 접근 방식을 제공해, 그 과정에서 지역적 인식, 독창성, 그리고 관계적 의미 이해를 향상시켜.

또한, 우리는 학생의 t 혼합 모델을 활용해서 SNIs의 클러스터링을 더 튼튼하고 적응 가능하게 만들어. 12종의 SNIs 데이터셋을 통해 진행한 광범위한 실험 결과, DARLC가 이미지 클러스터링과 유전자 상호작용을 정확하게 포착하는 이미지 표현 생성 모두에서 최신 기술들을 뛰어넘는다는 걸 보여줬어. 코드도 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.02007.pdf

Title: PMT-MAE: Dual-Branch Self-Supervised Learning with Distillation for Efficient Point Cloud Classification

Original Abstract:
Advances in self-supervised learning are essential for enhancing feature extraction and understanding in point cloud processing. This paper introduces PMT-MAE (Point MLP-Transformer Masked Autoencoder), a novel self-supervised learning framework for point cloud classification. PMT-MAE features a dual-branch architecture that integrates Transformer and MLP components to capture rich features. The Transformer branch leverages global self-attention for intricate feature interactions, while the parallel MLP branch processes tokens through shared fully connected layers, offering a complementary feature transformation pathway. A fusion mechanism then combines these features, enhancing the model's capacity to learn comprehensive 3D representations. Guided by the sophisticated teacher model Point-M2AE, PMT-MAE employs a distillation strategy that includes feature distillation during pre-training and logit distillation during fine-tuning, ensuring effective knowledge transfer. On the ModelNet40 classification task, achieving an accuracy of 93.6\% without employing voting strategy, PMT-MAE surpasses the baseline Point-MAE (93.2\%) and the teacher Point-M2AE (93.4\%), underscoring its ability to learn discriminative 3D point cloud representations. Additionally, this framework demonstrates high efficiency, requiring only 40 epochs for both pre-training and fine-tuning. PMT-MAE's effectiveness and efficiency render it well-suited for scenarios with limited computational resources, positioning it as a promising solution for practical point cloud analysis.

Translated Abstract:
자기 지도 학습의 발전은 포인트 클라우드 처리에서 특징 추출과 이해를 향상시키는 데 필수적이야. 이 논문에서는 포인트 클라우드 분류를 위한 새로운 자기 지도 학습 프레임워크인 PMT-MAE(포인트 MLP-트랜스포머 마스크드 오토인코더)를 소개해.

PMT-MAE는 트랜스포머와 MLP(다층 퍼셉트론) 구성 요소를 통합한 이중 가지 구조를 가지고 있어. 트랜스포머 가지는 복잡한 특징 상호작용을 위해 전역 자기 주의(global self-attention)를 활용하고, 병렬 MLP 가지는 공유된 완전 연결 층을 통해 토큰을 처리해 보완적인 특징 변환 경로를 제공해. 그 다음, 융합 메커니즘이 이 특징들을 결합해서 모델이 포괄적인 3D 표현을 배우는 능력을 높여줘.

PMT-MAE는 정교한 교사 모델인 Point-M2AE의 가이드를 받아서, 사전 훈련 중 특징 증류와 세부 조정 중 로짓 증류를 포함한 증류 전략을 사용해 효과적인 지식 전이를 보장해. ModelNet40 분류 과제에서 투표 전략 없이 93.6%의 정확도를 달성하면서, PMT-MAE는 기준이 되는 Point-MAE(93.2%)와 교사 모델인 Point-M2AE(93.4%)를 초월해. 이는 3D 포인트 클라우드 표현을 잘 학습할 수 있는 능력을 강조해.

게다가 이 프레임워크는 효율성도 높아서, 사전 훈련과 세부 조정 모두 40 에포크만 필요해. PMT-MAE의 효과성과 효율성 덕분에 컴퓨터 자원이 제한된 상황에서도 잘 맞고, 실제 포인트 클라우드 분석에 유망한 해결책으로 자리잡을 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.02020.pdf

Title: Efficient Point Cloud Classification via Offline Distillation Framework and Negative-Weight Self-Distillation Technique

Original Abstract:
The rapid advancement in point cloud processing technologies has significantly increased the demand for efficient and compact models that achieve high-accuracy classification. Knowledge distillation has emerged as a potent model compression technique. However, traditional KD often requires extensive computational resources for forward inference of large teacher models, thereby reducing training efficiency for student models and increasing resource demands. To address these challenges, we introduce an innovative offline recording strategy that avoids the simultaneous loading of both teacher and student models, thereby reducing hardware demands. This approach feeds a multitude of augmented samples into the teacher model, recording both the data augmentation parameters and the corresponding logit outputs. By applying shape-level augmentation operations such as random scaling and translation, while excluding point-level operations like random jittering, the size of the records is significantly reduced. Additionally, to mitigate the issue of small student model over-imitating the teacher model's outputs and converging to suboptimal solutions, we incorporate a negative-weight self-distillation strategy. Experimental results demonstrate that the proposed distillation strategy enables the student model to achieve performance comparable to state-of-the-art models while maintaining lower parameter count. This approach strikes an optimal balance between performance and complexity. This study highlights the potential of our method to optimize knowledge distillation for point cloud classification tasks, particularly in resource-constrained environments, providing a novel solution for efficient point cloud analysis.

Translated Abstract:
점 구름 처리 기술의 빠른 발전 덕분에 고정확도 분류를 달성하는 효율적이고 간결한 모델에 대한 수요가 크게 늘어나고 있어. 지식 증류(Knowledge Distillation, KD)는 강력한 모델 압축 기법으로 떠올랐지만, 전통적인 KD는 큰 교사 모델의 순전파 추론에 많은 컴퓨팅 자원을 요구해. 이로 인해 학생 모델의 훈련 효율성이 떨어지고 자원 요구량이 늘어나.

이 문제를 해결하기 위해, 우리는 교사 모델과 학생 모델을 동시에 로드하지 않는 혁신적인 오프라인 녹화 전략을 제안해. 이 방법은 많은 증강 샘플을 교사 모델에 입력하고, 데이터 증강 매개변수와 해당 로짓 출력을 기록해. 랜덤 스케일링이나 번역 같은 형태 수준의 증강 작업을 적용하고, 랜덤 지터링 같은 점 수준의 작업은 제외해서 기록 크기를 크게 줄였어.

또한, 작은 학생 모델이 교사 모델의 출력을 과도하게 모방하고 최적이 아닌 솔루션으로 수렴하는 문제를 줄이기 위해 부정 가중치 자기 증류 전략을 도입했어. 실험 결과, 제안한 증류 전략 덕분에 학생 모델이 최첨단 모델과 비슷한 성능을 유지하면서도 파라미터 수는 적은 것을 보여줬어. 이 접근법은 성능과 복잡성 사이의 최적의 균형을 이루고 있어.

이 연구는 자원이 제한된 환경에서 점 구름 분류 작업을 위한 지식 증류 최적화의 가능성을 강조하며, 효율적인 점 구름 분석을 위한 새로운 솔루션을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.04218.pdf

Title: MpoxMamba: A Grouped Mamba-based Lightweight Hybrid Network for Mpox Detection

Original Abstract:
Due to the lack of effective mpox detection tools, the mpox virus continues to spread worldwide and has once again been declared a public health emergency of international concern by the World Health Organization. Lightweight deep learning model-based detection systems are crucial to alleviate mpox outbreaks since they are suitable for widespread deployment, especially in resource-limited scenarios. However, the key to its successful application depends on ensuring that the model can effectively model local features and long-range dependencies in mpox lesions while maintaining lightweight. Inspired by the success of Mamba in modeling long-range dependencies and its linear complexity, we proposed a lightweight hybrid architecture called MpoxMamba for efficient mpox detection. MpoxMamba utilizes depth-wise separable convolutions to extract local feature representations in mpox skin lesions and greatly enhances the model's ability to model the global contextual information by grouped Mamba modules. Notably, MpoxMamba's parameter size and FLOPs are 0.77M and 0.53G, respectively. Experimental results on two widely recognized benchmark datasets demonstrate that MpoxMamba outperforms state-of-the-art lightweight models and existing mpox detection methods. Importantly, we developed a web-based online application to provide free mpox detection (this http URL). The source codes of MpoxMamba are available at this https URL.

Translated Abstract:
mpox 검출 도구가 부족해서 mpox 바이러스가 전 세계적으로 계속 퍼지고 있어. 세계보건기구(WHO)는 이걸 다시 국제적 공중보건 비상사태로 선언했어. 경량 딥러닝 모델 기반의 검출 시스템이 필요한 이유는, 자원이 제한된 상황에서도 널리 배포할 수 있기 때문이야. 하지만 이 모델이 성공적으로 작동하려면, mpox 병변의 지역적 특징과 장거리 의존성을 잘 모델링하면서도 가벼워야 해.

Mamba의 장거리 의존성 모델링 성공에 영감을 받아서, 우리는 효율적인 mpox 검출을 위한 경량 혼합 아키텍처인 MpoxMamba를 제안했어. MpoxMamba는 깊이별 분리 가능한 합성을 사용해서 mpox 피부 병변의 지역적 특징을 추출하고, 그룹화된 Mamba 모듈 덕분에 전반적인 맥락 정보를 잘 모델링할 수 있어. 특히, MpoxMamba의 파라미터 크기와 FLOPs는 각각 0.77M과 0.53G야.

두 개의 잘 알려진 벤치마크 데이터셋에서 실험 결과, MpoxMamba가 최신 경량 모델과 기존 mpox 검출 방법보다 더 뛰어난 성능을 보였어. 중요한 건, 우리가 무료 mpox 검출을 제공하는 웹 기반 온라인 애플리케이션을 개발했다는 거야 (이 링크). MpoxMamba의 소스 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.04398.pdf

Title: HiSC4D: Human-centered interaction and 4D Scene Capture in Large-scale Space Using Wearable IMUs and LiDAR

Original Abstract:
We introduce HiSC4D, a novel Human-centered interaction and 4D Scene Capture method, aimed at accurately and efficiently creating a dynamic digital world, containing large-scale indoor-outdoor scenes, diverse human motions, rich human-human interactions, and human-environment interactions. By utilizing body-mounted IMUs and a head-mounted LiDAR, HiSC4D can capture egocentric human motions in unconstrained space without the need for external devices and pre-built maps. This affords great flexibility and accessibility for human-centered interaction and 4D scene capturing in various environments. Taking into account that IMUs can capture human spatially unrestricted poses but are prone to drifting for long-period using, and while LiDAR is stable for global localization but rough for local positions and orientations, HiSC4D employs a joint optimization method, harmonizing all sensors and utilizing environment cues, yielding promising results for long-term capture in large scenes. To promote research of egocentric human interaction in large scenes and facilitate downstream tasks, we also present a dataset, containing 8 sequences in 4 large scenes (200 to 5,000 $m^2$), providing 36k frames of accurate 4D human motions with SMPL annotations and dynamic scenes, 31k frames of cropped human point clouds, and scene mesh of the environment. A variety of scenarios, such as the basketball gym and commercial street, alongside challenging human motions, such as daily greeting, one-on-one basketball playing, and tour guiding, demonstrate the effectiveness and the generalization ability of HiSC4D. The dataset and code will be publicated on this http URL available for research purposes.

Translated Abstract:
우리는 HiSC4D라는 새로운 방법을 소개해. 이건 사람 중심의 상호작용과 4D 장면 캡처를 위한 방법인데, 대규모 실내외 장면, 다양한 인간 동작, 풍부한 인간 간의 상호작용, 그리고 인간과 환경 간의 상호작용을 정확하고 효율적으로 만들어내는 걸 목표로 해. 

HiSC4D는 몸에 장착하는 IMU와 머리에 장착하는 LiDAR를 이용해서, 외부 장치나 미리 만들어진 지도가 없어도 자유로운 공간에서 사람의 동작을 캡처할 수 있어. 이 덕분에 다양한 환경에서 사람 중심의 상호작용과 4D 장면 캡처를 할 수 있는 큰 유연성과 접근성을 제공해. 

IMU는 사람이 제한 없이 다양한 자세를 캡처할 수 있지만, 오랫동안 사용할 경우 드리프트 현상에 취약해. 반면에 LiDAR는 전역 위치 파악에는 안정적이지만, 지역적인 위치나 방향은 다소 부정확해. 그래서 HiSC4D는 모든 센서를 조화롭게 최적화하고 환경 단서를 활용하는 방법을 사용해, 큰 장면에서 오랜 시간 동안 캡처를 성공적으로 할 수 있도록 해. 

우리는 또한 큰 장면에서의 사람 중심 상호작용 연구를 촉진하고, 후속 작업을 돕기 위해 4개의 대형 장면(200에서 5,000 $m^2$)에 8개의 시퀀스를 포함한 데이터셋도 제공해. 이 데이터셋에는 SMPL 주석이 있는 36,000 프레임의 정확한 4D 인간 동작과 동적 장면, 잘라낸 인간 포인트 클라우드 31,000 프레임, 그리고 환경의 장면 메쉬가 포함되어 있어. 농구 체육관, 상업 거리 같은 다양한 시나리오와, 일상적인 인사, 일대일 농구, 투어 가이드 같은 도전적인 인간 동작이 HiSC4D의 효과성과 일반화 능력을 보여줘. 

이 데이터셋과 코드는 연구 목적으로 사용할 수 있도록 공개될 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.04828.pdf

Title: POINTS: Improving Your Vision-language Model with Affordable Strategies

Original Abstract:
In recent years, vision-language models have made significant strides, excelling in tasks like optical character recognition and geometric problem-solving. However, several critical issues remain: 1) Proprietary models often lack transparency about their architectures, while open-source models need more detailed ablations of their training strategies. 2) Pre-training data in open-source works is under-explored, with datasets added empirically, making the process cumbersome. 3) Fine-tuning often focuses on adding datasets, leading to diminishing returns. To address these issues, we propose the following contributions: 1) We trained a robust baseline model using the latest advancements in vision-language models, introducing effective improvements and conducting comprehensive ablation and validation for each technique. 2) Inspired by recent work on large language models, we filtered pre-training data using perplexity, selecting the lowest perplexity data for training. This approach allowed us to train on a curated 1M dataset, achieving competitive performance. 3) During visual instruction tuning, we used model soup on different datasets when adding more datasets yielded marginal improvements. These innovations resulted in a 9B parameter model that performs competitively with state-of-the-art models. Our strategies are efficient and lightweight, making them easily adoptable by the community.

Translated Abstract:
최근 몇 년 동안 비전-언어 모델이 큰 발전을 이루었고, 광학 문자 인식이나 기하학 문제 해결 같은 작업에서 뛰어난 성과를 보였어. 하지만 몇 가지 중요한 문제들이 여전히 남아 있어. 

첫째, 상용 모델들은 구조에 대한 투명성이 부족하고, 오픈 소스 모델들은 훈련 전략에 대한 자세한 설명이 부족해. 둘째, 오픈 소스에서의 사전 훈련 데이터가 충분히 탐색되지 않았고, 데이터셋이 경험적으로 추가되면서 과정이 복잡해졌어. 셋째, 미세 조정은 주로 데이터셋을 추가하는 데 집중되는데, 이로 인해 효과가 점점 줄어들어. 

이런 문제들을 해결하기 위해 우리는 다음과 같은 기여를 제안해: 첫째, 최신 비전-언어 모델의 발전을 활용해 강력한 기준 모델을 훈련하고, 각 기술에 대해 효과적인 개선과 종합적인 분석 및 검증을 수행했어. 둘째, 최근의 대형 언어 모델 연구에 영감을 받아, 당황도(perplexity)를 사용해 사전 훈련 데이터를 필터링했어. 가장 낮은 당황도를 가진 데이터를 선택해 100만 개의 데이터셋으로 훈련했더니 경쟁력 있는 성과를 얻었어. 셋째, 시각적 지시 조정 중에 데이터셋을 더 추가할 때 개선 효과가 미미했을 때는 다양한 데이터셋에서 모델 수프를 활용했어. 

이런 혁신 덕분에 90억 개의 매개변수를 가진 모델을 만들었고, 최신 모델들과 경쟁할 수 있는 성능을 보여줬어. 우리의 전략은 효율적이고 가벼워서 커뮤니티에서 쉽게 채택할 수 있을 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.05122.pdf

Title: PMT: Progressive Mean Teacher via Exploring Temporal Consistency for Semi-Supervised Medical Image Segmentation

Original Abstract:
Semi-supervised learning has emerged as a widely adopted technique in the field of medical image segmentation. The existing works either focuses on the construction of consistency constraints or the generation of pseudo labels to provide high-quality supervisory signals, whose main challenge mainly comes from how to keep the continuous improvement of model capabilities. In this paper, we propose a simple yet effective semi-supervised learning framework, termed Progressive Mean Teachers (PMT), for medical image segmentation, whose goal is to generate high-fidelity pseudo labels by learning robust and diverse features in the training process. Specifically, our PMT employs a standard mean teacher to penalize the consistency of the current state and utilizes two sets of MT architectures for co-training. The two sets of MT architectures are individually updated for prolonged periods to maintain stable model diversity established through performance gaps generated by iteration differences. Additionally, a difference-driven alignment regularizer is employed to expedite the alignment of lagging models with the representation capabilities of leading models. Furthermore, a simple yet effective pseudo-label filtering algorithm is employed for facile evaluation of models and selection of high-fidelity pseudo-labels outputted when models are operating at high performance for co-training purposes. Experimental results on two datasets with different modalities, i.e., CT and MRI, demonstrate that our method outperforms the state-of-the-art medical image segmentation approaches across various dimensions. The code is available at this https URL.

Translated Abstract:
반지도 학습은 의료 이미지 분할 분야에서 널리 사용되는 기술로 자리 잡았어. 기존 연구들은 주로 일관성 제약 조건을 만드는 것이나 고품질의 감독 신호를 제공하기 위해 가짜 라벨을 생성하는 데 집중했어. 여기서 가장 큰 문제는 모델의 능력을 지속적으로 향상시키는 방법이야.

우리는 의료 이미지 분할을 위한 간단하지만 효과적인 반지도 학습 프레임워크인 'Progressive Mean Teachers (PMT)'를 제안해. 이 방법의 목표는 훈련 과정에서 강력하고 다양한 특징을 학습해 고품질의 가짜 라벨을 만드는 거야. 구체적으로, PMT는 표준 평균 교사 모델을 사용해서 현재 상태의 일관성을 저해하고, 두 세트의 평균 교사 아키텍처를 사용해 공동 훈련을 진행해.

이 두 세트의 평균 교사 아키텍처는 각각 오랜 시간 동안 업데이트되면서 성능 차이로 만들어진 모델 다양성을 유지해. 또, 느린 모델을 앞선 모델의 표현 능력에 맞추기 위해 차이 기반 정렬 규제를 사용해. 더 나아가, 모델을 쉽게 평가하고 고성능일 때 출력되는 고품질의 가짜 라벨을 선택하는 간단하고 효과적인 가짜 라벨 필터링 알고리즘도 도입했어.

CT와 MRI라는 서로 다른 두 데이터셋에서 실험한 결과, 우리 방법이 다양한 측면에서 최신 의료 이미지 분할 접근법보다 더 뛰어나다는 걸 보여줬어. 코드도 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.05531.pdf

Title: HMAFlow: Learning More Accurate Optical Flow via Hierarchical Motion Field Alignment

Original Abstract:
Optical flow estimation is a fundamental and long-standing visual task. In this work, we present a novel method, dubbed HMAFlow, to improve optical flow estimation in challenging scenes, particularly those involving small objects. The proposed model mainly consists of two core components: a Hierarchical Motion Field Alignment (HMA) module and a Correlation Self-Attention (CSA) module. In addition, we rebuild 4D cost volumes by employing a Multi-Scale Correlation Search (MCS) layer and replacing average pooling in common cost volumes with a search strategy utilizing multiple search ranges. Experimental results demonstrate that our model achieves the best generalization performance compared to other state-of-the-art methods. Specifically, compared with RAFT, our method achieves relative error reductions of 14.2% and 3.4% on the clean pass and final pass of the Sintel online benchmark, respectively. On the KITTI test benchmark, HMAFlow surpasses RAFT and GMA in the Fl-all metric by relative margins of 6.8% and 7.7%, respectively. To facilitate future research, our code will be made available at this https URL.

Translated Abstract:
광학 흐름 추정은 기본적이고 오랫동안 연구된 시각적 작업이다. 이 연구에서는 작은 물체가 있는 어려운 장면에서 광학 흐름 추정을 개선하기 위해 HMAFlow라는 새로운 방법을 제안한다. 제안하는 모델은 주로 두 가지 핵심 구성 요소로 이루어져 있다: 계층적 움직임 필드 정렬(HMA) 모듈과 상관 자기 주의(CSA) 모듈이다. 

또한, 우리는 다중 스케일 상관 검색(MCS) 레이어를 사용해 4D 비용 볼륨을 재구성하고, 일반적인 비용 볼륨에서 평균 풀링 대신 여러 검색 범위를 사용하는 검색 전략으로 대체했다. 실험 결과, 우리의 모델은 최신 기술들과 비교했을 때 가장 우수한 일반화 성능을 나타냈다. 

특히, RAFT와 비교했을 때, 우리의 방법은 Sintel 온라인 벤치마크의 클린 패스와 최종 패스에서 각각 14.2%와 3.4%의 상대적 오류 감소를 달성했다. KITTI 테스트 벤치마크에서도 HMAFlow는 RAFT와 GMA를 각각 6.8%와 7.7%의 상대적인 차이로 초월했다. 미래 연구를 위해 우리의 코드는 이 URL에서 제공될 예정이다.

================================================================================

URL:
https://arxiv.org/pdf/2409.06509.pdf

Title: Aligning Machine and Human Visual Representations across Abstraction Levels

Original Abstract:
Deep neural networks have achieved success across a wide range of applications, including as models of human behavior in vision tasks. However, neural network training and human learning differ in fundamental ways, and neural networks often fail to generalize as robustly as humans do, raising questions regarding the similarity of their underlying representations. What is missing for modern learning systems to exhibit more human-like behavior? We highlight a key misalignment between vision models and humans: whereas human conceptual knowledge is hierarchically organized from fine- to coarse-scale distinctions, model representations do not accurately capture all these levels of abstraction. To address this misalignment, we first train a teacher model to imitate human judgments, then transfer human-like structure from its representations into pretrained state-of-the-art vision foundation models. These human-aligned models more accurately approximate human behavior and uncertainty across a wide range of similarity tasks, including a new dataset of human judgments spanning multiple levels of semantic abstractions. They also perform better on a diverse set of machine learning tasks, increasing generalization and out-of-distribution robustness. Thus, infusing neural networks with additional human knowledge yields a best-of-both-worlds representation that is both more consistent with human cognition and more practically useful, thus paving the way toward more robust, interpretable, and human-like artificial intelligence systems.

Translated Abstract:
딥 뉴럴 네트워크는 다양한 분야에서 성공을 거두었고, 특히 비전 작업에서 인간 행동 모델로도 사용되고 있어. 하지만 뉴럴 네트워크의 훈련 방식과 인간의 학습 방식은 기본적으로 다르기 때문에, 뉴럴 네트워크는 인간처럼 잘 일반화하지 못하는 경우가 많아. 그래서 이들의 기본적인 표현 방식이 얼마나 비슷한지에 대한 의문이 생겨. 

현대 학습 시스템이 더 인간 같은 행동을 보이기 위해선 뭘 더 해야 할까? 우리는 비전 모델과 인간 간의 중요한 불일치를 강조해. 인간의 개념적 지식은 세밀한 구분에서부터 거친 구분까지 계층적으로 조직되어 있는데, 모델의 표현은 이런 모든 추상화 수준을 제대로 반영하지 못하고 있어. 

이 불일치를 해결하기 위해 먼저 교사 모델을 훈련시켜서 인간의 판단을 모방하게 해. 그런 다음, 이 모델의 표현에서 인간 같은 구조를 최신 비전 기본 모델에 전이해. 이렇게 인간과 맞춘 모델은 다양한 유사성 작업에서 인간의 행동과 불확실성을 더 정확하게 근사해. 여기에는 여러 수준의 의미 추상화를 포함한 새로운 인간 판단 데이터셋도 포함돼. 

또한, 이 모델은 다양한 머신 러닝 작업에서도 더 잘 작동해서 일반화 능력과 배포 외의 강인함을 높여. 따라서 뉴럴 네트워크에 추가적인 인간 지식을 주는 것이 인간의 인지와 더 일치하면서도 실용적으로 유용한 최상의 표현을 만들어내는 길이 돼. 이렇게 하면 더 강력하고 해석 가능한, 인간 같은 인공지능 시스템을 향한 길이 열리게 돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.07186.pdf

Title: Enhancing Angular Resolution via Directionality Encoding and Geometric Constraints in Brain Diffusion Tensor Imaging

Original Abstract:
Diffusion-weighted imaging (DWI) is a type of Magnetic Resonance Imaging (MRI) technique sensitised to the diffusivity of water molecules, offering the capability to inspect tissue microstructures and is the only in-vivo method to reconstruct white matter fiber tracts non-invasively. The DWI signal can be analysed with the diffusion tensor imaging (DTI) model to estimate the directionality of water diffusion within voxels. Several scalar metrics, including axial diffusivity (AD), mean diffusivity (MD), radial diffusivity (RD), and fractional anisotropy (FA), can be further derived from DTI to quantitatively summarise the microstructural integrity of brain tissue. These scalar metrics have played an important role in understanding the organisation and health of brain tissue at a microscopic level in clinical studies. However, reliable DTI metrics rely on DWI acquisitions with high gradient directions, which often go beyond the commonly used clinical protocols. To enhance the utility of clinically acquired DWI and save scanning time for robust DTI analysis, this work proposes DirGeo-DTI, a deep learning-based method to estimate reliable DTI metrics even from a set of DWIs acquired with the minimum theoretical number (6) of gradient directions. DirGeo-DTI leverages directional encoding and geometric constraints to facilitate the training process. Two public DWI datasets were used for evaluation, demonstrating the effectiveness of the proposed method. Extensive experimental results show that the proposed method achieves the best performance compared to existing DTI enhancement methods and potentially reveals further clinical insights with routine clinical DWI scans.

Translated Abstract:
확산강조영상(DWI)은 물 분자의 확산 능력에 민감한 자기공명영상(MRI) 기법이야. 이 방법은 조직의 미세 구조를 검사할 수 있는 능력을 제공하고, 백질 섬유를 비침습적으로 재구성할 수 있는 유일한 방법이기도 해. DWI 신호는 확산 텐서 영상(DTI) 모델로 분석돼서, 복셀 내에서 물의 확산 방향을 추정할 수 있어.

DTI에서 유도할 수 있는 여러 스칼라 메트릭스가 있어. 예를 들면 축 방향 확산성(AD), 평균 확산성(MD), 방사 방향 확산성(RD), 그리고 분수 이방성(FA) 같은 것들이지. 이런 스칼라 메트릭스는 임상 연구에서 뇌 조직의 미세 구조적 무결성을 정량적으로 요약하는 데 중요한 역할을 해. 하지만 신뢰할 수 있는 DTI 메트릭스는 고급 방향으로 DWI를 획득해야 하는데, 이건 보통 사용되는 임상 프로토콜을 넘어가는 경우가 많아.

이 연구에서는 임상에서 얻은 DWI의 유용성을 높이고, 강력한 DTI 분석을 위한 스캔 시간을 절약하기 위해 DirGeo-DTI라는 방법을 제안해. 이건 딥러닝 기반의 방법으로, 최소한의 이론적 방향 수(6개)로 획득한 DWI 세트에서도 신뢰할 수 있는 DTI 메트릭스를 추정할 수 있어. DirGeo-DTI는 방향 인코딩과 기하학적 제약을 활용해서 훈련 과정을 쉽게 만들어.

두 개의 공개 DWI 데이터셋을 사용해서 평가했는데, 제안한 방법의 효과를 보여줬어. 다양한 실험 결과로 봤을 때, 이 방법이 기존의 DTI 향상 방법들과 비교했을 때 가장 좋은 성능을 보여주고, 일상적인 임상 DWI 스캔을 통해 더 많은 임상 통찰을 제공할 가능성이 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07267.pdf

Title: MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving

Original Abstract:
Vision-language models (VLMs) serve as general-purpose end-to-end models in autonomous driving, performing subtasks such as prediction, planning, and perception through question-and-answer interactions. However, most existing methods rely on computationally expensive visual encoders and large language models (LLMs), making them difficult to deploy in real-world scenarios and real-time applications. Meanwhile, most existing VLMs lack the ability to process multiple images, making it difficult to adapt to multi-camera perception in autonomous driving. To address these issues, we propose a novel framework called MiniDrive, which incorporates our proposed Feature Engineering Mixture of Experts (FE-MoE) module and Dynamic Instruction Adapter (DI-Adapter). The FE-MoE effectively maps 2D features into visual token embeddings before being input into the language model. The DI-Adapter enables the visual token embeddings to dynamically change with the instruction text embeddings, resolving the issue of static visual token embeddings for the same image in previous approaches. Compared to previous works, MiniDrive achieves state-of-the-art performance in terms of parameter size, floating point operations, and response efficiency, with the smallest version containing only 83M parameters.

Translated Abstract:
비전-언어 모델(VLMs)은 자율주행에서 일반적인 엔드 투 엔드 모델로 작동해. 예를 들어, 질문과 답변 상호작용을 통해 예측, 계획, 인식 같은 서브 작업을 수행해. 하지만 기존 방법들은 대부분 계산 비용이 많이 드는 비주얼 인코더와 큰 언어 모델(LLMs)에 의존하고 있어서, 현실 세계의 상황이나 실시간 애플리케이션에서 사용하기 어려워.

또한, 기존 VLM들은 여러 이미지를 처리하는 능력이 부족해서 자율주행에서 다중 카메라 인식에 적응하기 힘들어. 이 문제를 해결하기 위해 우리는 MiniDrive라는 새로운 프레임워크를 제안해. 여기에는 우리가 제안한 특징 공학 전문가 혼합 모듈(FE-MoE)과 동적 지시 어댑터(DI-Adapter)가 포함돼.

FE-MoE는 2D 특징을 언어 모델에 입력하기 전에 비주얼 토큰 임베딩으로 효과적으로 변환해. DI-Adapter는 비주얼 토큰 임베딩이 지시 텍스트 임베딩에 따라 동적으로 변하도록 해줘. 이로 인해 이전 방법에서 같은 이미지에 대한 정적인 비주얼 토큰 임베딩 문제를 해결할 수 있어.

MiniDrive는 이전 작업들과 비교했을 때, 파라미터 크기, 부동 소수점 연산, 반응 효율성 면에서 최첨단 성능을 달성해. 가장 작은 버전은 단 83M 파라미터만 가지고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07762.pdf

Title: Exploring Kolmogorov-Arnold networks for realistic image sharpness assessment

Original Abstract:
Score prediction is crucial in realistic image sharpness assessment after informative features are collected. Recently, Kolmogorov-Arnold networks (KANs) have been developed and witnessed remarkable success in data fitting. This study presents Taylor series based KAN (TaylorKAN). Then, different KANs are explored on four realistic image databases (BID2011, CID2013, CLIVE, and KonIQ-10k) for score prediction by using 15 mid-level features and 2048 high-level features. When setting support vector regression as the baseline, experimental results indicate KANs are generally better or competitive, TaylorKAN is the best on three databases using mid-level feature input, while KANs are inferior on CLIVE when high-level features are used. This is the first study that explores KANs for image quality assessment. It sheds lights on how to select and improve KANs on related tasks.

Translated Abstract:
점수 예측은 유용한 특성을 수집한 후 현실적인 이미지 선명도 평가에서 매우 중요해. 최근에 Kolmogorov-Arnold 네트워크(KANs)가 개발되었고 데이터 피팅에서 놀라운 성공을 거두었어. 이 연구는 Taylor 급수 기반의 KAN(TaylorKAN)을 소개해.

그 다음, 네 가지 현실적인 이미지 데이터베이스(BID2011, CID2013, CLIVE, KonIQ-10k)에서 15개의 중간 수준 특성과 2048개의 고급 특성을 사용해서 다양한 KAN을 점수 예측을 위해 탐색했어. 서포트 벡터 회귀를 기준선으로 설정했을 때, 실험 결과 KAN이 일반적으로 더 좋거나 경쟁력이 있다는 걸 보여줬어. 특히 TaylorKAN은 중간 수준 특성 입력을 사용할 때 세 개의 데이터베이스에서 가장 좋은 성과를 냈지만, 고급 특성을 사용할 때는 CLIVE에서 KAN이 성능이 떨어졌어.

이 연구는 이미지 품질 평가를 위해 KAN을 탐색한 첫 번째 연구야. KAN을 관련 작업에서 선택하고 개선하는 방법에 대한 통찰을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.08513.pdf

Title: Mamba-YOLO-World: Marrying YOLO-World with Mamba for Open-Vocabulary Detection

Original Abstract:
Open-vocabulary detection (OVD) aims to detect objects beyond a predefined set of categories. As a pioneering model incorporating the YOLO series into OVD, YOLO-World is well-suited for scenarios prioritizing speed and efficiency. However, its performance is hindered by its neck feature fusion mechanism, which causes the quadratic complexity and the limited guided receptive fields. To address these limitations, we present Mamba-YOLO-World, a novel YOLO-based OVD model employing the proposed MambaFusion Path Aggregation Network (MambaFusion-PAN) as its neck architecture. Specifically, we introduce an innovative State Space Model-based feature fusion mechanism consisting of a Parallel-Guided Selective Scan algorithm and a Serial-Guided Selective Scan algorithm with linear complexity and globally guided receptive fields. It leverages multi-modal input sequences and mamba hidden states to guide the selective scanning process. Experiments demonstrate that our model outperforms the original YOLO-World on the COCO and LVIS benchmarks in both zero-shot and fine-tuning settings while maintaining comparable parameters and FLOPs. Additionally, it surpasses existing state-of-the-art OVD methods with fewer parameters and FLOPs.

Translated Abstract:
오픈-어휘 탐지(OVD)는 미리 정의된 카테고리 집합을 넘어서 객체를 탐지하는 것을 목표로 해. YOLO 시리즈를 OVD에 접목한 선구적인 모델인 YOLO-World는 속도와 효율성을 중요시하는 상황에 잘 맞아. 하지만, neck feature fusion 메커니즘 때문에 성능이 제한돼. 이 메커니즘은 복잡도가 제곱으로 증가하고, 제한된 guided receptive fields를 초래해.

이 문제를 해결하기 위해 우리는 Mamba-YOLO-World라는 새로운 YOLO 기반 OVD 모델을 제안해. 이 모델은 MambaFusion Path Aggregation Network(MambaFusion-PAN)를 neck 아키텍처로 사용해. 특히, 우리는 혁신적인 State Space Model 기반의 feature fusion 메커니즘을 도입했어. 이 메커니즘에는 Parallel-Guided Selective Scan 알고리즘과 Serial-Guided Selective Scan 알고리즘이 포함되어 있는데, 둘 다 선형 복잡도와 전역적으로 안내되는 receptive fields를 가지고 있어.

이 모델은 멀티모달 입력 시퀀스와 mamba hidden states를 활용해 선택적 스캐닝 과정을 안내해. 실험 결과, 우리의 모델이 COCO와 LVIS 벤치마크에서 원래의 YOLO-World보다 더 나은 성능을 보여줬어. 제로샷과 파인튜닝 설정 모두에서 말이야. 게다가, 기존의 최첨단 OVD 방법들보다 적은 매개변수와 FLOPs로 성능을 초월했어.

================================================================================

URL:
https://arxiv.org/pdf/2310.10461.pdf

Title: Model Selection of Anomaly Detectors in the Absence of Labeled Validation Data

Original Abstract:
Anomaly detection is the task of identifying abnormal samples in large unlabeled datasets. While the advent of foundation models has produced powerful zero-shot anomaly detection methods, their deployment in practice is often hindered by the absence of labeled validation data -- without it, their detection performance cannot be evaluated reliably. In this work, we propose SWSA (Selection With Synthetic Anomalies): a general-purpose framework to select image-based anomaly detectors without labeled validation data. Instead of collecting labeled validation data, we generate synthetic anomalies without any training or fine-tuning, using only a small support set of normal images. Our synthetic anomalies are used to create detection tasks that compose a validation framework for model selection. In an empirical study, we evaluate SWSA with three types of synthetic anomalies and on two selection tasks: model selection of image-based anomaly detectors and prompt selection for CLIP-based anomaly detection. SWSA often selects models and prompts that match selections made with a ground-truth validation set, outperforming baseline selection strategies.

Translated Abstract:
이상 탐지는 큰 레이블이 없는 데이터셋에서 비정상 샘플을 찾아내는 작업이야. 최근에 나온 기초 모델 덕분에 강력한 제로샷 이상 탐지 방법들이 생겼지만, 실제로 사용하는 데는 레이블이 있는 검증 데이터가 없어서 어려움이 많아. 레이블이 없으면 탐지 성능을 신뢰성 있게 평가할 수 없거든.

이번 연구에서는 SWSA(합성 이상으로 선택하기)를 제안해. 이건 레이블이 없는 검증 데이터 없이 이미지 기반의 이상 탐지기를 선택할 수 있는 일반적인 프레임워크야. 레이블이 있는 검증 데이터를 모으는 대신에, 우리는 소량의 정상 이미지만 가지고 훈련이나 미세 조정 없이 합성 이상을 만들어. 이 합성 이상들을 이용해서 탐지 작업을 만들어 모델 선택을 위한 검증 프레임워크를 구성해.

경험적 연구에서 우리는 세 가지 종류의 합성 이상과 두 가지 선택 작업에서 SWSA를 평가했어. 여기서 하나는 이미지 기반의 이상 탐지기 모델 선택이고, 다른 하나는 CLIP 기반 이상 탐지를 위한 프롬프트 선택이야. SWSA는 종종 실제 검증 세트로 선택한 모델과 프롬프트와 잘 맞는 것들을 고르며, 기본 선택 전략보다 성능이 더 뛰어나.

================================================================================

URL:
https://arxiv.org/pdf/2311.02558.pdf

Title: Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity with Free-Flying Robots

Original Abstract:
Assistive free-flyer robots autonomously caring for future crewed outposts -- such as NASA's Astrobee robots on the International Space Station (ISS) -- must be able to detect day-to-day interior changes to track inventory, detect and diagnose faults, and monitor the outpost status. This work presents a framework for multi-agent cooperative mapping and change detection to enable robotic maintenance of space outposts. One agent is used to reconstruct a 3D model of the environment from sequences of images and corresponding depth information. Another agent is used to periodically scan the environment for inconsistencies against the 3D model. Change detection is validated after completing the surveys using real image and pose data collected by Astrobee robots in a ground testing environment and from microgravity aboard the ISS. This work outlines the objectives, requirements, and algorithmic modules for the multi-agent reconstruction system, including recommendations for its use by assistive free-flyers aboard future microgravity outposts.

Translated Abstract:
우주에서 인간이 거주할 기지를 돕는 자율 비행 로봇, 예를 들어 NASA의 아스트로비 로봇처럼, 내부의 일상적인 변화를 감지해야 해. 이 로봇들은 재고를 추적하고, 문제를 찾아내고, 기지의 상태를 모니터링하는 역할을 해.

이 연구는 여러 로봇이 협력해서 환경을 맵핑하고 변화를 감지하는 방법을 제시해. 하나의 로봇은 이미지와 깊이 정보를 이용해 환경의 3D 모델을 재구성해. 다른 로봇은 주기적으로 환경을 스캔해서 3D 모델과의 불일치 여부를 확인해.

변화 감지는 아스트로비 로봇이 지상 테스트 환경에서 수집한 실제 이미지와 자세 데이터를 사용해 설문 조사를 완료한 후 검증돼. 이 연구는 다중 에이전트 재구성 시스템의 목표, 요구사항, 알고리즘 모듈을 설명하고, 향후 미세 중력 기지에서 보조 비행 로봇이 사용할 수 있는 방법에 대한 추천도 포함하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2311.12539.pdf

Title: GMISeg: General Medical Image Segmentation without Re-Training

Original Abstract:
Deep learning models have become the dominant method for medical image segmentation. However, they often struggle to be generalisable to unknown tasks involving new anatomical structures, labels, or shapes. In these cases, the model needs to be re-trained for the new tasks, posing a significant challenge for non-machine learning experts and requiring a considerable time investment. Here I developed a general model that can solve unknown medical image segmentation tasks without requiring additional training. Given an example set of images and visual prompts for defining new segmentation tasks, GMISeg (General Medical Image Segmentation) leverages a pre-trained image encoder based on ViT and applies a low-rank fine-tuning strategy to the prompt encoder and mask decoder to fine-tune the model without in an efficient manner. I evaluated the performance of the proposed method on medical image datasets with different imaging modalities and anatomical structures. The proposed method facilitated the deployment of pre-trained AI models to new segmentation works in a user-friendly way.

Translated Abstract:
딥러닝 모델은 의료 영상 분할에 주로 사용되고 있어. 하지만 이 모델들은 새로운 해부학적 구조, 레이블, 또는 형태가 포함된 새로운 작업에 일반화되기 힘들어. 이런 경우, 모델을 새 작업에 맞게 다시 훈련시켜야 하는데, 이게 비전문가들에게는 큰 도전이 되고 시간도 많이 들어.

그래서 나는 추가 훈련 없이도 알 수 없는 의료 영상 분할 작업을 해결할 수 있는 일반 모델을 개발했어. GMISeg(General Medical Image Segmentation)는 이미지 샘플 세트와 새로운 분할 작업을 정의하는 시각적 프롬프트를 이용해, ViT 기반의 사전 훈련된 이미지 인코더를 활용해. 그리고 프롬프트 인코더와 마스크 디코더에 저랭크 미세 조정 전략을 적용해 효율적으로 모델을 조정할 수 있어.

나는 제안한 방법의 성능을 다양한 영상 모달리티와 해부학적 구조를 가진 의료 영상 데이터셋에서 평가했어. 이 방법은 사전 훈련된 AI 모델을 새로운 분할 작업에 사용자 친화적으로 배포하는 데 도움을 줬어.

================================================================================

URL:
https://arxiv.org/pdf/2311.14875.pdf

Title: Bayesian Neural Networks for 2D MRI Segmentation

Original Abstract:
Uncertainty quantification is vital for safety-critical Deep Learning applications like medical image segmentation. We introduce BA U-Net, an uncertainty-aware model for MRI segmentation that integrates Bayesian Neural Networks with Attention Mechanisms. BA U-Net delivers accurate, interpretable results, crucial for reliable pathology screening. Evaluated on BraTS 2020, this model addresses the critical need for confidence estimation in deep learning-based medical imaging.

Translated Abstract:
불확실성 정량화는 의료 이미지 분할 같은 안전이 중요한 딥러닝 응용 프로그램에서 정말 중요해. 우리는 BA U-Net이라는 모델을 소개하는데, 이 모델은 MRI 분할을 위해 베이지안 신경망과 주의 메커니즘을 결합한 불확실성 인식 모델이야.

BA U-Net은 정확하고 해석 가능한 결과를 제공하는데, 이건 신뢰할 수 있는 병리학 검사를 위해서 꼭 필요해. BraTS 2020에서 평가된 이 모델은 딥러닝 기반 의료 이미지에서 신뢰도 추정의 중요한 필요성을 해결해.

================================================================================

URL:
https://arxiv.org/pdf/2312.07128.pdf

Title: MS-Twins: Multi-Scale Deep Self-Attention Networks for Medical Image Segmentation

Original Abstract:
Although transformer is preferred in natural language processing, some studies has only been applied to the field of medical imaging in recent years. For its long-term dependency, the transformer is expected to contribute to unconventional convolution neural net conquer their inherent spatial induction bias. The lately suggested transformer-based segmentation method only uses the transformer as an auxiliary module to help encode the global context into a convolutional representation. How to optimally integrate self-attention with convolution has not been investigated in depth. To solve the problem, this paper proposes MS-Twins (Multi-Scale Twins), which is a powerful segmentation model on account of the bond of self-attention and convolution. MS-Twins can better capture semantic and fine-grained information by combining different scales and cascading features. Compared with the existing network structure, MS-Twins has made progress on the previous method based on the transformer of two in common use data sets, Synapse and ACDC. In particular, the performance of MS-Twins on Synapse is 8% higher than SwinUNet. Even compared with nnUNet, the best entirely convoluted medical image segmentation network, the performance of MS-Twins on Synapse and ACDC still has a bit advantage.

Translated Abstract:
트랜스포머는 자연어 처리에서 선호되지만, 최근 몇 년 동안 의료 영상 분야에만 적용된 연구들이 있어. 트랜스포머는 긴 의존성을 잘 처리할 수 있어서, 기존의 컨볼루션 신경망이 가지고 있는 공간 유도 편향을 극복하는 데 도움이 될 거라고 기대되고 있어. 최근 제안된 트랜스포머 기반 세그멘테이션 방법은 트랜스포머를 보조 모듈로만 사용해서 글로벌 컨텍스트를 컨볼루션 표현에 인코딩하는 데 도움을 줘. 하지만 자기 주의(self-attention)와 컨볼루션을 최적의 방식으로 결합하는 방법은 깊이 있게 연구되지 않았어.

이 문제를 해결하기 위해, 이 논문에서는 MS-Twins(멀티 스케일 트윈스)를 제안해. 이 모델은 자기 주의와 컨볼루션의 결합 덕분에 강력한 세그멘테이션 모델이야. MS-Twins는 서로 다른 스케일을 결합하고 피쳐를 연결함으로써 의미적이고 세밀한 정보를 더 잘 포착할 수 있어. 기존 네트워크 구조와 비교했을 때, MS-Twins는 Synapse와 ACDC라는 두 개의 일반적으로 사용되는 데이터 세트에서 이전의 트랜스포머 기반 방법보다 향상된 성능을 보여줬어. 특히, MS-Twins는 Synapse에서 SwinUNet보다 8% 더 높은 성능을 기록했어. 심지어 완전히 컨볼루션 방식의 의료 영상 세그멘테이션 네트워크인 nnUNet과 비교해도, MS-Twins는 Synapse와 ACDC에서 여전히 약간의 이점을 가지고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2403.16803.pdf

Title: Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View Planning

Original Abstract:
Object reconstruction is relevant for many autonomous robotic tasks that require interaction with the environment. A key challenge in such scenarios is planning view configurations to collect informative measurements for reconstructing an initially unknown object. One-shot view planning enables efficient data collection by predicting view configurations and planning the globally shortest path connecting all views at once. However, prior knowledge about the object is required to conduct one-shot view planning. In this work, we propose a novel one-shot view planning approach that utilizes the powerful 3D generation capabilities of diffusion models as priors. By incorporating such geometric priors into our pipeline, we achieve effective one-shot view planning starting with only a single RGB image of the object to be reconstructed. Our planning experiments in simulation and real-world setups indicate that our approach balances well between object reconstruction quality and movement cost.

Translated Abstract:
물체 재구성은 환경과 상호작용이 필요한 많은 자율 로봇 작업에서 중요해. 이런 상황에서 가장 큰 도전 과제는 처음에 알지 못하는 물체를 재구성하기 위해 유용한 측정을 수집할 수 있는 뷰 구성을 계획하는 거야. 

원샷 뷰 계획은 뷰 구성을 예측하고 모든 뷰를 한 번에 연결하는 최단 경로를 계획함으로써 효율적인 데이터 수집을 가능하게 해. 하지만 원샷 뷰 계획을 하려면 물체에 대한 사전 지식이 필요해. 

이 연구에서는 확산 모델의 강력한 3D 생성 능력을 사전 정보로 활용하는 새로운 원샷 뷰 계획 방법을 제안해. 이런 기하학적 사전 정보를 우리의 파이프라인에 포함시킴으로써, 재구성할 물체의 RGB 이미지 하나만으로도 효과적인 원샷 뷰 계획을 달성할 수 있어. 

시뮬레이션과 실제 환경에서의 계획 실험 결과, 우리의 접근 방식이 물체 재구성 품질과 이동 비용 간의 균형을 잘 맞춘다는 것을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2403.20058.pdf

Title: Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks

Original Abstract:
Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal neuroimaging technique. It provides an unprecedented opportunity for concurrently monitoring and integrating multifaceted brain networks built by spatiotemporally covaried metabolic activity, neural activity, and cerebral blood flow (perfusion). Albeit high scientific/clinical values, short in hardware accessibility of PET/MR hinders its applications, let alone modern AI-based PET/MR fusion models. Our objective is to develop a clinically feasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR data with the power of, during inferencing, allowing single modality input (e.g., PET only) as well as enforcing multimodal-based accuracy. To this end, we propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction Model. It is modality detachable and exchangeable, allocating different multi-layer perceptrons dynamically ("mixture of experts") through learnable weights to learn respective representations from different modalities. Such design will not sacrifice model performance in uni-modal situation. To fully exploit the inherent complex and nonlinear relation among modalities while producing fine-grained representations for uni-modal inference, we subsequently add a modal alignment module to line up a dominant modality (e.g., PET) with representations of auxiliary modalities (MR). We further adopt multimodal reconstruction to promote the quality of learned features. Experiments on precious multimodal sf-PET/MR data for Mild Cognitive Impairment diagnosis showcase the efficacy of our model toward clinically feasible precision medicine.

Translated Abstract:
동시 기능성 PET/MR(sf-PET/MR)은 최신 멀티모달 신경 이미징 기술이야. 이 방법은 공간적, 시간적 변동이 있는 대사 활동, 신경 활동, 뇌 혈류(관류)를 기반으로 한 다양한 뇌 네트워크를 동시에 모니터링하고 통합할 수 있는 기회를 제공해. 과학적/임상적으로 매우 가치가 높지만, PET/MR의 하드웨어 접근성이 낮아서 적용이 어려워. 특히 현대 AI 기반의 PET/MR 융합 모델은 더 말할 것도 없지.

우리는 포괄적인 sf-PET/MR 데이터를 기반으로 훈련된 임상적으로 실현 가능한 AI 기반 질병 진단 모델을 개발하는 게 목표야. 이 모델은 추론할 때 단일 모달리티 입력(예: PET만 사용)도 가능하게 하면서 다중 모달 기반의 정확성을 유지할 수 있어. 이를 위해 MX-ARM이라는 멀티모달 MiXture-of-experts 정렬 및 재구성 모델을 제안해. 이 모델은 모달리티를 분리하고 교환할 수 있어서, 학습 가능한 가중치를 통해 서로 다른 모달리티에서 각각의 표현을 배울 수 있도록 다층 퍼셉트론을 동적으로 할당해. 이런 설계는 단일 모달 상황에서도 모델 성능을 희생하지 않아.

모달리티 간의 복잡하고 비선형적인 관계를 최대한 활용하면서 단일 모달 추론을 위한 세밀한 표현을 만들기 위해, 우리는 주 모달리티(예: PET)와 보조 모달리티(MR)의 표현을 정렬하는 모달 정렬 모듈을 추가해. 그리고 우리는 학습된 특성의 품질을 높이기 위해 멀티모달 재구성을 채택해. 가벼운 인지 장애 진단을 위한 귀중한 멀티모달 sf-PET/MR 데이터에 대한 실험은 우리 모델이 임상적으로 실현 가능한 정밀 의학을 향한 효과성을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2404.03703.pdf

Title: Mitigating analytical variability in fMRI results with style transfer

Original Abstract:
We propose a novel approach to improve the reproducibility of neuroimaging results by converting statistic maps across different functional MRI pipelines. We make the assumption that pipelines used to compute fMRI statistic maps can be considered as a style component and we propose to use different generative models, among which, Generative Adversarial Networks (GAN) and Diffusion Models (DM) to convert statistic maps across different pipelines. We explore the performance of multiple GAN frameworks, and design a new DM framework for unsupervised multi-domain styletransfer. We constrain the generation of 3D fMRI statistic maps using the latent space of an auxiliary classifier that distinguishes statistic maps from different pipelines and extend traditional sampling techniques used in DM to improve the transition performance. Our experiments demonstrate that our proposed methods aresuccessful: pipelines can indeed be transferred as a style component, providing animportant source of data augmentation for future medical studies.

Translated Abstract:
우리는 신경영상 결과의 재현성을 높이기 위해 새로운 접근 방식을 제안해. 이 방법은 서로 다른 기능적 MRI 파이프라인 간의 통계 맵을 변환하는 거야. 

우리는 fMRI 통계 맵을 계산하는 데 사용되는 파이프라인을 스타일 요소로 간주할 수 있다고 가정해. 그래서 GAN(적대적 생성 신경망)과 DM(확산 모델) 같은 다양한 생성 모델을 사용해서 통계 맵을 변환하는 방법을 제안해. 

여러 GAN 프레임워크의 성능을 살펴보고, 비지도 방식으로 다중 도메인 스타일 전송을 위한 새로운 DM 프레임워크도 설계했어. 우리는 보조 분류기의 잠재 공간을 사용해서 3D fMRI 통계 맵 생성을 제한하고, DM에서 전통적인 샘플링 기법을 확장해 전이 성능을 향상시켰어. 

우리 실험 결과, 제안한 방법이 성공적임을 보여주었어. 파이프라인을 스타일 요소로 전송할 수 있다는 것이 밝혀졌고, 이는 향후 의료 연구를 위한 중요한 데이터 증강 원천이 될 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2404.17357.pdf

Title: Simultaneous Tri-Modal Medical Image Fusion and Super-Resolution using Conditional Diffusion Model

Original Abstract:
In clinical practice, tri-modal medical image fusion, compared to the existing dual-modal technique, can provide a more comprehensive view of the lesions, aiding physicians in evaluating the disease's shape, location, and biological activity. However, due to the limitations of imaging equipment and considerations for patient safety, the quality of medical images is usually limited, leading to sub-optimal fusion performance, and affecting the depth of image analysis by the physician. Thus, there is an urgent need for a technology that can both enhance image resolution and integrate multi-modal information. Although current image processing methods can effectively address image fusion and super-resolution individually, solving both problems synchronously remains extremely challenging. In this paper, we propose TFS-Diff, a simultaneously realize tri-modal medical image fusion and super-resolution model. Specially, TFS-Diff is based on the diffusion model generation of a random iterative denoising process. We also develop a simple objective function and the proposed fusion super-resolution loss, effectively evaluates the uncertainty in the fusion and ensures the stability of the optimization process. And the channel attention module is proposed to effectively integrate key information from different modalities for clinical diagnosis, avoiding information loss caused by multiple image processing. Extensive experiments on public Harvard datasets show that TFS-Diff significantly surpass the existing state-of-the-art methods in both quantitative and visual evaluations. Code is available at this https URL}{this https URL.

Translated Abstract:
임상에서 삼중 모드 의료 이미지 융합은 기존의 이중 모드 기술보다 병변을 더 잘 보여줘서 의사가 질병의 형태, 위치, 생물학적 활동을 평가하는 데 도움을 줄 수 있어. 하지만 이미징 장비의 한계와 환자 안전을 고려하다 보니 의료 이미지의 품질이 보통 제한적이어서 융합 성능이 최적화되지 않고, 의사가 이미지 분석을 깊이 있게 하는 데 영향을 미쳐. 그래서 이미지 해상도를 높이고 다중 모드 정보를 통합할 수 있는 기술이 급히 필요해. 

현재 이미지 처리 방법들이 이미지 융합과 초고해상도 문제를 각각 잘 해결할 수 있지만, 두 가지 문제를 동시에 해결하는 건 여전히 매우 어려워. 이 논문에서는 TFS-Diff라는 삼중 모드 의료 이미지 융합과 초고해상도를 동시에 실현하는 모델을 제안해. TFS-Diff는 무작위 반복적인 노이즈 제거 과정을 기반으로 한 확산 모델 생성에 기반해 있어. 

또한, 간단한 목적 함수를 개발하고, 제안된 융합 초고해상도 손실 함수를 통해 융합에서의 불확실성을 효과적으로 평가하고 최적화 과정의 안정성을 보장해. 그리고 서로 다른 모드에서 중요한 정보를 효과적으로 통합하기 위해 채널 주의 모듈을 제안해, 여러 이미지 처리로 인한 정보 손실을 피할 수 있어. 

공공 하버드 데이터셋에 대한 광범위한 실험 결과, TFS-Diff가 기존의 최첨단 방법들을 수치적 평가와 시각적 평가 모두에서 크게 능가함을 보여줘. 코드도 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2405.01673.pdf

Title: ShadowNav: Autonomous Global Localization for Lunar Navigation in Darkness

Original Abstract:
The ability to determine the pose of a rover in an inertial frame autonomously is a crucial capability necessary for the next generation of surface rover missions on other planetary bodies. Currently, most on-going rover missions utilize ground-in-the-loop interventions to manually correct for drift in the pose estimate and this human supervision bottlenecks the distance over which rovers can operate autonomously and carry out scientific measurements. In this paper, we present ShadowNav, an autonomous approach for global localization on the Moon with an emphasis on driving in darkness and at nighttime. Our approach uses the leading edge of Lunar craters as landmarks and a particle filtering approach is used to associate detected craters with known ones on an offboard map. We discuss the key design decisions in developing the ShadowNav framework for use with a Lunar rover concept equipped with a stereo camera and an external illumination source. Finally, we demonstrate the efficacy of our proposed approach in both a Lunar simulation environment and on data collected during a field test at Cinder Lakes, Arizona.

Translated Abstract:
로버의 자세를 자율적으로 결정하는 능력은 다른 행성 표면 탐사에서 다음 세대 로버 미션에 꼭 필요한 기능이야. 현재 대부분의 로버 미션은 자세 추정의 드리프트를 수동으로 수정하기 위해 사람이 개입하는 방식을 사용하고 있어. 이런 인간 감독은 로버가 자율적으로 작동할 수 있는 거리와 과학적 측정을 수행하는 데 제한을 두게 돼.

이번 논문에서는 그림자 내비게이션(ShadowNav)이라는 자율적인 지구 위치 추정 방법을 소개해. 이 방법은 특히 어두운 환경이나 야간에 달에서 주행할 때 유용해. 우리의 접근법은 달의 분화구 가장자리를 랜드마크로 사용하고, 입자 필터링 기법을 통해 탐지된 분화구를 외부 지도에 있는 알려진 분화구와 연결해. 

그림자 내비게이션 프레임워크를 개발하기 위한 주요 설계 결정에 대해서도 이야기할 거야. 이 프레임워크는 스테레오 카메라와 외부 조명 장치를 장착한 달 로버 개념과 함께 사용할 수 있어. 마지막으로, 우리는 이 방법이 달 시뮬레이션 환경과 아리조나의 신더 레이크에서 수집한 데이터에서도 효과적임을 보여줄 거야.

================================================================================

URL:
https://arxiv.org/pdf/2405.07392.pdf

Title: NGD-SLAM: Towards Real-Time Dynamic SLAM without GPU

Original Abstract:
Existing SLAM (Simultaneous Localization and Mapping) algorithms have achieved remarkable localization accuracy in dynamic environments by using deep learning techniques to identify dynamic objects. However, they usually require GPUs to operate in real-time. Therefore, this paper proposes an open-source real-time dynamic SLAM system that runs solely on CPU by incorporating a mask prediction mechanism, which allows the deep learning method and the camera tracking to run entirely in parallel at different frequencies. Our SLAM system further introduces a dual-stage optical flow tracking approach and employs a hybrid usage of optical flow and ORB features, enhancing efficiency and robustness by selectively allocating computational resources to input frames. Compared with previous methods, our system maintains high localization accuracy in dynamic environments while achieving a tracking frame rate of 56 FPS on a laptop CPU, proving that deep learning methods are feasible for dynamic SLAM without GPU support. To the best of our knowledge, this is the first SLAM system to achieve this.

Translated Abstract:
기존의 SLAM(동시 위치 추정 및 맵핑) 알고리즘은 동적 환경에서 딥러닝 기법을 사용해 동적 객체를 인식함으로써 뛰어난 위치 정확도를 달성했어. 하지만 보통 실시간으로 작동하려면 GPU가 필요해. 

그래서 이 논문에서는 마스크 예측 메커니즘을 도입해서 CPU만으로 작동하는 오픈소스 실시간 동적 SLAM 시스템을 제안해. 이 시스템은 딥러닝 방법과 카메라 추적을 서로 다른 주파수로 완전히 병렬로 실행할 수 있어. 

우리 SLAM 시스템은 더 나아가 이중 단계의 광학 흐름 추적 방식을 도입하고, 광학 흐름과 ORB 특징을 혼합해서 사용해. 이렇게 해서 입력 프레임에 컴퓨팅 자원을 선택적으로 할당하면서 효율성과 견고성을 높였어. 

이전 방법들과 비교했을 때, 우리 시스템은 동적 환경에서도 높은 위치 정확도를 유지하면서 노트북 CPU에서 56 FPS의 추적 프레임 속도를 달성했어. 이로써 GPU 지원 없이도 딥러닝 방법이 동적 SLAM에 적용 가능하다는 걸 증명했어. 우리가 아는 한, 이건 첫 번째 SLAM 시스템이야.

================================================================================

URL:
https://arxiv.org/pdf/2407.12792.pdf

Title: Visually Robust Adversarial Imitation Learning from Videos with Contrastive Learning

Original Abstract:
We propose C-LAIfO, a computationally efficient algorithm designed for imitation learning from videos in the presence of visual mismatch between agent and expert domains. We analyze the problem of imitation from expert videos with visual discrepancies, and introduce a solution for robust latent space estimation using contrastive learning and data augmentation. Provided a visually robust latent space, our algorithm performs imitation entirely within this space using off-policy adversarial imitation learning. We conduct a thorough ablation study to justify our design and test C-LAIfO on high-dimensional continuous robotic tasks. Additionally, we demonstrate how C-LAIfO can be combined with other reward signals to facilitate learning on a set of challenging hand manipulation tasks with sparse rewards. Our experiments show improved performance compared to baseline methods, highlighting the effectiveness of C-LAIfO. To ensure reproducibility, we open source our code.

Translated Abstract:
우리는 C-LAIfO라는 알고리즘을 제안해. 이건 비디오에서 모방 학습을 할 때, 에이전트와 전문가 도메인 간의 시각적 불일치가 있을 때도 잘 작동하도록 설계된 효율적인 알고리즘이야. 

전문가 비디오에서 시각적 불일치로 인한 모방 문제를 분석하고, 대조 학습과 데이터 증강을 이용한 강력한 잠재 공간 추정 방법을 소개해. 시각적으로 안정적인 잠재 공간이 주어지면, 우리 알고리즘은 이 공간 내에서 오프 폴리시 적대적 모방 학습을 통해 완전히 모방을 수행해.

우리는 설계를 정당화하기 위해 철저한 제거 연구를 진행했고, C-LAIfO를 고차원 연속 로봇 작업에 테스트했어. 게다가 C-LAIfO가 다른 보상 신호와 결합되어 희소 보상이 있는 도전적인 손 조작 작업에서 학습을 촉진할 수 있는 방법도 보여줬어. 

실험 결과, 기본 방법들에 비해 성능이 향상된 걸 확인했어. C-LAIfO의 효과를 강조하는 거지. 재현성을 보장하기 위해, 우리는 코드도 오픈 소스로 공개했어.

================================================================================

URL:
https://arxiv.org/pdf/2408.01689.pdf

Title: Controllable Unlearning for Image-to-Image Generative Models via $\varepsilon$-Constrained Optimization

Original Abstract:
While generative models have made significant advancements in recent years, they also raise concerns such as privacy breaches and biases. Machine unlearning has emerged as a viable solution, aiming to remove specific training data, e.g., containing private information and bias, from models. In this paper, we study the machine unlearning problem in Image-to-Image (I2I) generative models. Previous studies mainly treat it as a single objective optimization problem, offering a solitary solution, thereby neglecting the varied user expectations towards the trade-off between complete unlearning and model utility. To address this issue, we propose a controllable unlearning framework that uses a control coefficient $\varepsilon$ to control the trade-off. We reformulate the I2I generative model unlearning problem into a $\varepsilon$-constrained optimization problem and solve it with a gradient-based method to find optimal solutions for unlearning boundaries. These boundaries define the valid range for the control coefficient. Within this range, every yielded solution is theoretically guaranteed with Pareto optimality. We also analyze the convergence rate of our framework under various control functions. Extensive experiments on two benchmark datasets across three mainstream I2I models demonstrate the effectiveness of our controllable unlearning framework.

Translated Abstract:
최근 생성 모델들이 큰 발전을 이루었지만, 개인 정보 유출이나 편향 같은 문제도 함께 발생하고 있어요. 그래서 머신 언러닝이 하나의 해결책으로 떠오르고 있는데, 이건 특정한 훈련 데이터를 모델에서 제거하는 걸 목표로 해요. 예를 들어, 개인 정보나 편향이 포함된 데이터를 말하는 거죠.

이 논문에서는 이미지-이미지(I2I) 생성 모델에서 머신 언러닝 문제를 연구해요. 이전 연구들은 주로 이걸 단일 목표 최적화 문제로 다뤘고, 하나의 해결책만 제시했어요. 그래서 완전한 언러닝과 모델 유용성 사이의 다양한 사용자 기대를 무시했죠.

이런 문제를 해결하기 위해 우리는 제어 계수 $\varepsilon$를 사용해서 언러닝의 균형을 조절할 수 있는 제어 가능한 언러닝 프레임워크를 제안해요. I2I 생성 모델의 언러닝 문제를 $\varepsilon$ 제약 최적화 문제로 재구성하고, 경량 기반 방법을 사용해 최적의 언러닝 경계를 찾는 방법을 제시해요. 이 경계는 제어 계수의 유효 범위를 정의해요. 이 범위 내에서는 모든 솔루션이 이론적으로 파레토 최적성을 보장해요. 

우리는 또한 다양한 제어 함수 하에서 우리의 프레임워크의 수렴 속도를 분석해요. 세 가지 주요 I2I 모델에 대해 두 개의 벤치마크 데이터셋에서 많은 실험을 통해 우리의 제어 가능한 언러닝 프레임워크의 효과를 입증했어요.

================================================================================

URL:
https://arxiv.org/pdf/2408.14997.pdf

Title: Depth Restoration of Hand-Held Transparent Objects for Human-to-Robot Handover

Original Abstract:
Transparent objects are common in daily life, while their optical properties pose challenges for RGB-D cameras to capture accurate depth information. This issue is further amplified when these objects are hand-held, as hand occlusions further complicate depth estimation. For assistant robots, however, accurately perceiving hand-held transparent objects is critical to effective human-robot interaction. This paper presents a Hand-Aware Depth Restoration (HADR) method based on creating an implicit neural representation function from a single RGB-D image. The proposed method utilizes hand posture as an important guidance to leverage semantic and geometric information of hand-object interaction. To train and evaluate the proposed method, we create a high-fidelity synthetic dataset named TransHand-14K with a real-to-sim data generation scheme. Experiments show that our method has better performance and generalization ability compared with existing methods. We further develop a real-world human-to-robot handover system based on HADR, demonstrating its potential in human-robot interaction applications.

Translated Abstract:
투명한 물체는 일상생활에서 흔히 볼 수 있지만, 그 광학 특성 때문에 RGB-D 카메라가 정확한 깊이 정보를 캡처하는 데 어려움이 있어. 특히 손으로 들고 있는 경우에는 손이 물체를 가리기 때문에 깊이 추정이 더 복잡해져. 하지만 보조 로봇에게는 손으로 들고 있는 투명한 물체를 정확히 인식하는 게 인간-로봇 상호작용에 정말 중요해.

이 논문에서는 단일 RGB-D 이미지로부터 암묵적인 신경 표현 함수를 만들어내는 '손 인식 깊이 복원(HADR)' 방법을 제안해. 이 방법은 손의 자세를 중요한 가이드로 활용해서 손-물체 상호작용의 의미론적 및 기하학적 정보를 잘 활용해.

제안한 방법을 훈련하고 평가하기 위해 'TransHand-14K'라는 고충실도 합성 데이터셋을 만들었어. 이 데이터셋은 실제 데이터를 기반으로 시뮬레이션 데이터를 생성하는 방식으로 만들어졌지. 실험 결과, 우리의 방법이 기존 방법들보다 성능과 일반화 능력이 더 뛰어난 걸 보여줬어. 그리고 HADR을 기반으로 한 실제 인간-로봇 물체 전달 시스템도 개발해서, 인간-로봇 상호작용 응용에 대한 잠재력을 보였어.

================================================================================

URL:
https://arxiv.org/pdf/2408.15002.pdf

Title: Knowledge Discovery in Optical Music Recognition: Enhancing Information Retrieval with Instance Segmentation

Original Abstract:
Optical Music Recognition (OMR) automates the transcription of musical notation from images into machine-readable formats like MusicXML, MEI, or MIDI, significantly reducing the costs and time of manual transcription. This study explores knowledge discovery in OMR by applying instance segmentation using Mask R-CNN to enhance the detection and delineation of musical symbols in sheet music. Unlike Optical Character Recognition (OCR), OMR must handle the intricate semantics of Common Western Music Notation (CWMN), where symbol meanings depend on shape, position, and context. Our approach leverages instance segmentation to manage the density and overlap of musical symbols, facilitating more precise information retrieval from music scores. Evaluations on the DoReMi and MUSCIMA++ datasets demonstrate substantial improvements, with our method achieving a mean Average Precision (mAP) of up to 59.70\% in dense symbol environments, achieving comparable results to object detection. Furthermore, using traditional computer vision techniques, we add a parallel step for staff detection to infer the pitch for the recognised symbols. This study emphasises the role of pixel-wise segmentation in advancing accurate music symbol recognition, contributing to knowledge discovery in OMR. Our findings indicate that instance segmentation provides more precise representations of musical symbols, particularly in densely populated scores, advancing OMR technology. We make our implementation, pre-processing scripts, trained models, and evaluation results publicly available to support further research and development.

Translated Abstract:
광학 음악 인식(OMR)은 이미지에서 음악 기호를 기계가 읽을 수 있는 형식인 MusicXML, MEI, MIDI 등으로 자동으로 변환해 줘. 이 덕분에 수작업으로 변환하는 비용과 시간을 크게 줄일 수 있어. 

이 연구는 Mask R-CNN을 사용해서 OMR에서 지식 발견을 어떻게 할 수 있는지를 알아보는 거야. 이 방법으로 악보에서 음악 기호를 더 잘 찾고 구분할 수 있게 돼. 광학 문자 인식(OCR)과는 달리, OMR은 일반 서양 음악 표기법(CWMN)의 복잡한 의미를 다뤄야 해. 기호의 의미는 모양, 위치, 맥락에 따라 달라지거든. 

우리의 접근 방식은 인스턴스 세분화를 활용해서 음악 기호의 밀도와 겹침을 관리하고, 음악 점검에서 정보를 더 정확하게 추출할 수 있도록 해. DoReMi와 MUSCIMA++ 데이터셋에서 평가를 해봤는데, 우리의 방법은 밀집된 기호 환경에서 평균 정밀도(mAP) 59.70%까지 달성하면서 물체 탐지와 비슷한 결과를 보여줬어. 

게다가 전통적인 컴퓨터 비전 기법을 사용해서, 인식된 기호의 음높이를 추론하기 위해 오선지 감지라는 병행 단계를 추가했어. 이 연구는 음악 기호 인식을 정확하게 하기 위해 픽셀 단위 세분화의 중요성을 강조하고, OMR에서 지식 발견에 기여하고 있어. 우리의 결과는 인스턴스 세분화가 특히 밀집된 악보에서 음악 기호를 더 정확하게 표현할 수 있다는 걸 보여줘. 

우리는 연구와 개발을 지원하기 위해 우리의 구현, 전처리 스크립트, 훈련된 모델, 평가 결과를 공개했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.01633.pdf

Title: Dreaming is All You Need

Original Abstract:
In classification tasks, achieving a harmonious balance between exploration and precision is of paramount importance. To this end, this research introduces two novel deep learning models, SleepNet and DreamNet, to strike this balance. SleepNet seamlessly integrates supervised learning with unsupervised ``sleep" stages using pre-trained encoder models. Dedicated neurons within SleepNet are embedded in these unsupervised features, forming intermittent ``sleep" blocks that facilitate exploratory learning. Building upon the foundation of SleepNet, DreamNet employs full encoder-decoder frameworks to reconstruct the hidden states, mimicking the human "dreaming" process. This reconstruction process enables further exploration and refinement of the learned representations. Moreover, the principle ideas of our SleepNet and DreamNet are generic and can be applied to both computer vision and natural language processing downstream tasks. Through extensive empirical evaluations on diverse image and text datasets, SleepNet and DreanNet have demonstrated superior performance compared to state-of-the-art models, showcasing the strengths of unsupervised exploration and supervised precision afforded by our innovative approaches.

Translated Abstract:
분류 작업에서는 탐색과 정확성 사이의 균형이 정말 중요해. 이 연구에서는 SleepNet과 DreamNet이라는 두 가지 새로운 딥러닝 모델을 소개해서 이 균형을 맞추려고 해. 

SleepNet은 감독 학습과 비감독 "수면" 단계를 매끄럽게 결합해. 이 모델은 미리 훈련된 인코더 모델을 사용하고, SleepNet 안에는 비감독 피처에 장착된 전용 뉴런이 있어서 간헐적인 "수면" 블록을 만들고 탐색 학습을 도와줘.

SleepNet의 기반 위에 DreamNet이 있는데, 이건 전체 인코더-디코더 프레임워크를 사용해서 숨겨진 상태를 재구성해. 이 과정은 인간의 "꿈"을 흉내 내는 거야. 이렇게 재구성하는 과정 덕분에 더 많은 탐색과 학습된 표현을 다듬을 수 있어. 

게다가, SleepNet과 DreamNet의 기본 아이디어는 일반적이어서 컴퓨터 비전과 자연어 처리 같은 다양한 후속 작업에도 적용할 수 있어. 다양한 이미지와 텍스트 데이터셋에 대한 광범위한 평가를 통해, SleepNet과 DreamNet은 최신 모델들보다 더 우수한 성능을 보여주었어. 이 모델들은 비감독 탐색과 감독 정확성의 장점을 잘 활용하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07914.pdf

Title: InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation

Original Abstract:
We present InterACT: Inter-dependency aware Action Chunking with Hierarchical Attention Transformers, a novel imitation learning framework for bimanual manipulation that integrates hierarchical attention to capture inter-dependencies between dual-arm joint states and visual inputs. InterACT consists of a Hierarchical Attention Encoder and a Multi-arm Decoder, both designed to enhance information aggregation and coordination. The encoder processes multi-modal inputs through segment-wise and cross-segment attention mechanisms, while the decoder leverages synchronization blocks to refine individual action predictions, providing the counterpart's prediction as context. Our experiments on a variety of simulated and real-world bimanual manipulation tasks demonstrate that InterACT significantly outperforms existing methods. Detailed ablation studies validate the contributions of key components of our work, including the impact of CLS tokens, cross-segment encoders, and synchronization blocks.

Translated Abstract:
우리는 InterACT라는 새로운 모방 학습 프레임워크를 소개해. 이건 양손 조작을 위한 것으로, 계층적 주의를 통합해서 두 팔의 관절 상태와 시각 입력 간의 상호 의존성을 잘 포착해. InterACT는 계층적 주의 인코더와 다중 팔 디코더로 구성되어 있는데, 둘 다 정보 집합과 조정을 개선하도록 설계됐어.

인코더는 여러 종류의 입력을 세그먼트별 및 크로스 세그먼트 주의 메커니즘으로 처리해. 반면에 디코더는 동기화 블록을 활용해서 각 행동 예측을 더 정교하게 다듬고, 상대방의 예측을 맥락으로 제공해. 

우리가 다양한 시뮬레이션과 실제 양손 조작 작업에서 실험한 결과, InterACT가 기존 방법들보다 훨씬 더 뛰어난 성능을 보였어. 자세한 제거 연구를 통해 CLS 토큰, 크로스 세그먼트 인코더, 동기화 블록 같은 주요 구성 요소의 기여도를 검증했어.

================================================================================

