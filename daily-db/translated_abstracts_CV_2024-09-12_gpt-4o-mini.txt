URL:
https://arxiv.org/pdf/2409.06705.pdf

Title: HSR-KAN: Efficient Hyperspectral Image Super-Resolution via Kolmogorov-Arnold Networks

Original Abstract:
Hyperspectral images (HSIs) have great potential in various visual tasks due to their rich spectral information. However, obtaining high-resolution hyperspectral images remains challenging due to limitations of physical imaging. Inspired by Kolmogorov-Arnold Networks (KANs), we propose an efficient HSI super-resolution (HSI-SR) model to fuse a low-resolution HSI (LR-HSI) and a high-resolution multispectral image (HR-MSI), yielding a high-resolution HSI (HR-HSI). To achieve the effective integration of spatial information from HR-MSI, we design a fusion module based on KANs, called KAN-Fusion. Further inspired by the channel attention mechanism, we design a spectral channel attention module called KAN Channel Attention Block (KAN-CAB) for post-fusion feature extraction. As a channel attention module integrated with KANs, KAN-CAB not only enhances the fine-grained adjustment ability of deep networks, enabling networks to accurately simulate details of spectral sequences and spatial textures, but also effectively avoid Curse of Dimensionality (COD). Extensive experiments show that, compared to current state-of-the-art (SOTA) HSI-SR methods, proposed HSR-KAN achieves the best performance in terms of both qualitative and quantitative assessments. Our code is available at: this https URL.

Translated Abstract:
하이퍼스펙트럼 이미지(HSI)는 다양한 시각 작업에서 풍부한 스펙트럼 정보를 제공하기 때문에 큰 잠재력을 가지고 있어. 하지만 고해상도 하이퍼스펙트럼 이미지를 얻는 건 물리적 촬영의 한계 때문에 어려워. 

그래서 우리는 Kolmogorov-Arnold Networks (KANs)에서 영감을 받아, 저해상도 하이퍼스펙트럼 이미지(LR-HSI)와 고해상도 멀티스펙트럼 이미지(HR-MSI)를 합쳐서 고해상도 하이퍼스펙트럼 이미지(HR-HSI)를 만드는 효율적인 HSI 슈퍼해상도(HSI-SR) 모델을 제안해. 

HR-MSI의 공간 정보를 효과적으로 통합하기 위해 KAN을 기반으로 한 융합 모듈인 KAN-Fusion을 설계했어. 또한 채널 주의 메커니즘에서 영감을 받아, 후처리 특성 추출을 위한 스펙트럴 채널 주의 모듈인 KAN Channel Attention Block (KAN-CAB)을 만들었어. KAN과 통합된 채널 주의 모듈인 KAN-CAB는 깊은 네트워크의 세밀한 조정 능력을 강화해주고, 네트워크가 스펙트럼 시퀀스와 공간 텍스처의 세부 사항을 정확하게 모사할 수 있게 도와줘. 게다가 차원의 저주(COD)를 효과적으로 피할 수 있어.

많은 실험 결과, 현재의 최신 HSI-SR 방법들과 비교했을 때, 우리가 제안한 HSR-KAN이 질적, 양적 평가 모두에서 최고의 성능을 보여줬어. 우리 코드도 이 링크에서 확인할 수 있어: this https URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.06707.pdf

Title: Gating Syn-to-Real Knowledge for Pedestrian Crossing Prediction in Safe Driving

Original Abstract:
Pedestrian Crossing Prediction (PCP) in driving scenes plays a critical role in ensuring the safe operation of intelligent vehicles. Due to the limited observations of pedestrian crossing behaviors in typical situations, recent studies have begun to leverage synthetic data with flexible variation to boost prediction performance, employing domain adaptation frameworks. However, different domain knowledge has distinct cross-domain distribution gaps, which necessitates suitable domain knowledge adaption ways for PCP tasks. In this work, we propose a Gated Syn-to-Real Knowledge transfer approach for PCP (Gated-S2R-PCP), which has two aims: 1) designing the suitable domain adaptation ways for different kinds of crossing-domain knowledge, and 2) transferring suitable knowledge for specific situations with gated knowledge fusion. Specifically, we design a framework that contains three domain adaption methods including style transfer, distribution approximation, and knowledge distillation for various information, such as visual, semantic, depth, location, etc. A Learnable Gated Unit (LGU) is employed to fuse suitable cross-domain knowledge to boost pedestrian crossing prediction. We construct a new synthetic benchmark S2R-PCP-3181 with 3181 sequences (489,740 frames) which contains the pedestrian locations, RGB frames, semantic images, and depth images. With the synthetic S2R-PCP-3181, we transfer the knowledge to two real challenging datasets of PIE and JAAD, and superior PCP performance is obtained to the state-of-the-art methods.

Translated Abstract:
보행자 신호 예측(PCP)은 자율주행차의 안전한 작동을 위해 정말 중요해. 하지만 일반적인 상황에서 보행자 신호 행동을 관찰할 수 있는 기회가 제한적이어서, 최근 연구들은 예측 성능을 높이기 위해 변화를 줄 수 있는 합성 데이터를 활용하기 시작했어. 여기서 도메인 적응 프레임워크를 사용해.

하지만 각 도메인 지식은 서로 다른 분포 차이를 가지고 있어서, PCP 작업에 맞는 적절한 도메인 지식 적응 방법이 필요해. 그래서 우리는 Gated Syn-to-Real Knowledge transfer 접근법인 Gated-S2R-PCP를 제안해. 이 방법은 두 가지 목표가 있어: 1) 서로 다른 종류의 도메인 지식에 맞는 적절한 도메인 적응 방법을 설계하고, 2) 특정 상황에 적합한 지식을 게이트 지식 융합으로 전달하는 거야.

구체적으로는, 시각 정보, 의미 정보, 깊이 정보, 위치 정보 등 다양한 정보를 위한 세 가지 도메인 적응 방법인 스타일 전이, 분포 근사, 지식 증류를 포함하는 프레임워크를 설계했어. 여기서 Learnable Gated Unit(LGU)를 사용해 적합한 크로스 도메인 지식을 융합해서 보행자 신호 예측을 향상시키는 거지.

우리는 보행자 위치, RGB 프레임, 의미 이미지, 깊이 이미지가 포함된 3181개의 시퀀스를 가진 새로운 합성 벤치마크 S2R-PCP-3181을 만들었어. 이 합성 데이터를 이용해 두 개의 실제 도전 과제인 PIE와 JAAD 데이터셋에 지식을 전달했더니, 최신 방법들보다 훨씬 더 좋은 PCP 성능을 얻었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06710.pdf

Title: McGrids: Monte Carlo-Driven Adaptive Grids for Iso-Surface Extraction

Original Abstract:
Iso-surface extraction from an implicit field is a fundamental process in various applications of computer vision and graphics. When dealing with geometric shapes with complicated geometric details, many existing algorithms suffer from high computational costs and memory usage. This paper proposes McGrids, a novel approach to improve the efficiency of iso-surface extraction. The key idea is to construct adaptive grids for iso-surface extraction rather than using a simple uniform grid as prior art does. Specifically, we formulate the problem of constructing adaptive grids as a probability sampling problem, which is then solved by Monte Carlo process. We demonstrate McGrids' capability with extensive experiments from both analytical SDFs computed from surface meshes and learned implicit fields from real multiview images. The experiment results show that our McGrids can significantly reduce the number of implicit field queries, resulting in significant memory reduction, while producing high-quality meshes with rich geometric details.

Translated Abstract:
암시적 필드에서의 등치면 추출은 컴퓨터 비전과 그래픽스에서 여러 응용 분야에 기본적인 과정이야. 복잡한 기하학적 세부사항을 가진 형상을 다룰 때, 기존의 많은 알고리즘은 높은 계산 비용과 메모리 사용량으로 어려움을 겪어.

이 논문에서는 등치면 추출의 효율성을 개선하기 위한 새로운 접근 방식인 McGrids를 제안해. 핵심 아이디어는 기존의 간단한 균일 격자를 사용하는 대신, 등치면 추출을 위한 적응형 격자를 만드는 거야. 구체적으로, 적응형 격자를 만드는 문제를 확률 샘플링 문제로 설정하고, 몬테 카를로 프로세스를 통해 해결해.

우리는 분석적으로 계산된 SDF와 실제 다중 뷰 이미지에서 얻은 학습된 암시적 필드를 사용해 McGrids의 능력을 폭넓게 실험으로 보여줬어. 실험 결과, 우리 McGrids는 암시적 필드 질의를 크게 줄일 수 있어서 메모리 사용량도 줄이고, 풍부한 기하학적 세부사항을 가진 고품질 메시를 생성할 수 있다는 걸 증명했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06711.pdf

Title: Quantized neural network for complex hologram generation

Original Abstract:
Computer-generated holography (CGH) is a promising technology for augmented reality displays, such as head-mounted or head-up displays. However, its high computational demand makes it impractical for implementation. Recent efforts to integrate neural networks into CGH have successfully accelerated computing speed, demonstrating the potential to overcome the trade-off between computational cost and image quality. Nevertheless, deploying neural network-based CGH algorithms on computationally limited embedded systems requires more efficient models with lower computational cost, memory footprint, and power consumption. In this study, we developed a lightweight model for complex hologram generation by introducing neural network quantization. Specifically, we built a model based on tensor holography and quantized it from 32-bit floating-point precision (FP32) to 8-bit integer precision (INT8). Our performance evaluation shows that the proposed INT8 model achieves hologram quality comparable to that of the FP32 model while reducing the model size by approximately 70% and increasing the speed fourfold. Additionally, we implemented the INT8 model on a system-on-module to demonstrate its deployability on embedded platforms and high power efficiency.

Translated Abstract:
컴퓨터 생성 홀로그램(CG Holography, CGH)은 헤드 마운트 디스플레이나 헤드업 디스플레이 같은 증강 현실 장치에 유망한 기술이야. 하지만 이 기술은 계산 요구량이 높아서 실제로 사용하기가 어려워. 최근에는 신경망을 CGH에 통합하는 노력들이 있었고, 이 덕분에 계산 속도가 빨라졌어. 그래서 계산 비용과 이미지 품질 사이의 균형을 맞출 수 있는 가능성을 보여줬지.

그렇지만, 계산 능력이 제한된 임베디드 시스템에서 신경망 기반 CGH 알고리즘을 사용하려면, 더 효율적인 모델이 필요해. 즉, 계산 비용, 메모리 사용량, 전력 소비가 낮아야 해. 이 연구에서는 신경망 양자화를 통해 복잡한 홀로그램 생성을 위한 경량 모델을 개발했어. 구체적으로는 텐서 홀로그램을 기반으로 한 모델을 32비트 부동소수점(FP32)에서 8비트 정수(INT8)로 양자화했어.

우리 성능 평가 결과, 제안한 INT8 모델은 FP32 모델과 비슷한 홀로그램 품질을 유지하면서 모델 크기를 약 70% 줄이고, 속도는 4배 향상되는 것을 보여줬어. 게다가, INT8 모델을 시스템 온 모듈에 구현해서 임베디드 플랫폼에서 사용할 수 있는 가능성과 높은 전력 효율성을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06726.pdf

Title: Feedback-based Modal Mutual Search for Attacking Vision-Language Pre-training Models

Original Abstract:
Although vision-language pre-training (VLP) models have achieved remarkable progress on cross-modal tasks, they remain vulnerable to adversarial attacks. Using data augmentation and cross-modal interactions to generate transferable adversarial examples on surrogate models, transfer-based black-box attacks have become the mainstream methods in attacking VLP models, as they are more practical in real-world scenarios. However, their transferability may be limited due to the differences on feature representation across different models. To this end, we propose a new attack paradigm called Feedback-based Modal Mutual Search (FMMS). FMMS introduces a novel modal mutual loss (MML), aiming to push away the matched image-text pairs while randomly drawing mismatched pairs closer in feature space, guiding the update directions of the adversarial examples. Additionally, FMMS leverages the target model feedback to iteratively refine adversarial examples, driving them into the adversarial region. To our knowledge, this is the first work to exploit target model feedback to explore multi-modality adversarial boundaries. Extensive empirical evaluations on Flickr30K and MSCOCO datasets for image-text matching tasks show that FMMS significantly outperforms the state-of-the-art baselines.

Translated Abstract:
비전-언어 사전 훈련(VLP) 모델이 크로스 모달 작업에서 꽤 좋은 성과를 내고 있지만, 여전히 적대적 공격에 취약해. 데이터 증강과 크로스 모달 상호작용을 이용해 대체 모델에서 전이 가능한 적대적 예제를 만드는 방식으로, 전이 기반 블랙박스 공격이 VLP 모델 공격의 주류 방법이 되었어. 이런 방법들이 실제 상황에서는 더 실용적이긴 한데, 서로 다른 모델 간의 특징 표현 차이 때문에 전이 가능성이 제한될 수 있어.

그래서 우리는 피드백 기반 모달 상호 검색(Feedback-based Modal Mutual Search, FMMS)이라는 새로운 공격 패러다임을 제안해. FMMS는 새로운 모달 상호 손실(MML)을 도입해서, 일치하는 이미지-텍스트 쌍은 멀어지게 하고, 무작위로 일치하지 않는 쌍은 특징 공간에서 더 가까워지게 해. 이렇게 해서 적대적 예제의 업데이트 방향을 안내해. 게다가 FMMS는 타겟 모델의 피드백을 활용해서 적대적 예제를 반복적으로 다듬고, 이를 적대적 영역으로 밀어넣어.

우리의 지식으로는, 이 연구가 타겟 모델의 피드백을 이용해 다중 모달 적대적 경계를 탐색한 첫 번째 작업이야. Flickr30K와 MSCOCO 데이터셋에서 이미지-텍스트 매칭 작업에 대한 광범위한 실험 평가 결과, FMMS가 최신 기준 모델들보다 훨씬 뛰어난 성능을 보였어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06764.pdf

Title: Modeling Image Tone Dichotomy with the Power Function

Original Abstract:
The primary purpose of this paper is to present the concept of dichotomy in image illumination modeling based on the power function. In particular, we review several mathematical properties of the power function to identify the limitations and propose a new mathematical model capable of abstracting illumination dichotomy. The simplicity of the equation opens new avenues for classical and modern image analysis and processing. The article provides practical and illustrative image examples to explain how the new model manages dichotomy in image perception. The article shows dichotomy image space as a viable way to extract rich information from images despite poor contrast linked to tone, lightness, and color perception. Moreover, a comparison with state-of-the-art methods in image enhancement provides evidence of the method's value.

Translated Abstract:
이 논문의 주된 목적은 파워 함수를 기반으로 한 이미지 조명 모델에서 이분법의 개념을 제시하는 거야. 특히, 우리는 파워 함수의 여러 수학적 성질을 살펴보며 그 한계를 찾고, 조명 이분법을 추상화할 수 있는 새로운 수학 모델을 제안해.

이 공식의 간단함 덕분에 고전적이고 현대적인 이미지 분석 및 처리에 새로운 길이 열릴 수 있어. 이 논문에서는 새로운 모델이 이미지 인식에서 이분법을 어떻게 다루는지를 설명하기 위해 실제 이미지 예시를 제공해.

또한, 이분법 이미지 공간이 톤, 밝기, 색상 인식과 관련된 낮은 대비에도 불구하고 이미지에서 풍부한 정보를 추출할 수 있는 유용한 방법임을 보여줘. 마지막으로, 최신 이미지 향상 방법과 비교해보면 이 방법의 가치를 증명할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06765.pdf

Title: gsplat: An Open-Source Library for Gaussian Splatting

Original Abstract:
gsplat is an open-source library designed for training and developing Gaussian Splatting methods. It features a front-end with Python bindings compatible with the PyTorch library and a back-end with highly optimized CUDA kernels. gsplat offers numerous features that enhance the optimization of Gaussian Splatting models, which include optimization improvements for speed, memory, and convergence times. Experimental results demonstrate that gsplat achieves up to 10% less training time and 4x less memory than the original implementation. Utilized in several research projects, gsplat is actively maintained on GitHub. Source code is available at this https URL under Apache License 2.0. We welcome contributions from the open-source community.

Translated Abstract:
gsplat는 가우시안 스플래팅 방법을 훈련하고 개발하기 위해 만들어진 오픈 소스 라이브러리야. 이 라이브러리는 PyTorch 라이브러리와 호환되는 파이썬 바인딩을 가진 프론트엔드와, 매우 최적화된 CUDA 커널을 가진 백엔드로 구성되어 있어.

gsplat는 가우시안 스플래팅 모델의 최적화를 향상시키는 여러 가지 기능을 제공해. 여기에는 속도, 메모리, 수렴 시간에 대한 최적화 개선이 포함돼. 실험 결과에 따르면, gsplat은 원래 구현보다 훈련 시간을 최대 10% 줄이고 메모리는 4배 덜 사용해.

여러 연구 프로젝트에서 활용되고 있는 gsplat은 GitHub에서 활발히 유지 관리되고 있어. 소스 코드는 Apache License 2.0에 따라 이 https URL에서 확인할 수 있어. 오픈 소스 커뮤니티의 기여를 환영해!

================================================================================

URL:
https://arxiv.org/pdf/2409.06791.pdf

Title: Human Motion Synthesis_ A Diffusion Approach for Motion Stitching and In-Betweening

Original Abstract:
Human motion generation is an important area of research in many fields. In this work, we tackle the problem of motion stitching and in-betweening. Current methods either require manual efforts, or are incapable of handling longer sequences. To address these challenges, we propose a diffusion model with a transformer-based denoiser to generate realistic human motion. Our method demonstrated strong performance in generating in-betweening sequences, transforming a variable number of input poses into smooth and realistic motion sequences consisting of 75 frames at 15 fps, resulting in a total duration of 5 seconds. We present the performance evaluation of our method using quantitative metrics such as Frechet Inception Distance (FID), Diversity, and Multimodality, along with visual assessments of the generated outputs.

Translated Abstract:
인간의 움직임 생성은 여러 분야에서 중요한 연구 주제야. 이번 연구에서는 움직임 스티칭과 중간 프레임 생성 문제를 다뤘어. 현재 방법들은 수작업이 필요하거나 긴 시퀀스를 처리할 수 없는 경우가 많아.

이런 문제를 해결하기 위해 우리는 트랜스포머 기반의 노이즈 제거기를 사용하는 확산 모델을 제안했어. 이 방법은 중간 프레임 시퀀스를 생성하는 데 강력한 성능을 보여줬어. 다양한 입력 포즈를 부드럽고 현실적인 75프레임의 움직임 시퀀스로 변환할 수 있었고, 이건 15fps로 5초 분량이야.

우리는 이 방법의 성능을 Frechet Inception Distance (FID), 다양성, 다중성 같은 수치적 지표를 사용해서 평가하고, 생성된 결과물에 대한 시각적 평가도 함께 제시했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06809.pdf

Title: DetailCLIP: Detail-Oriented CLIP for Fine-Grained Tasks

Original Abstract:
In this paper, we introduce DetailCLIP: A Detail-Oriented CLIP to address the limitations of contrastive learning-based vision-language models, particularly CLIP, in handling detail-oriented and fine-grained tasks like segmentation. While CLIP and its variants excel in the global alignment of image and text representations, they often struggle to capture the fine-grained details necessary for precise segmentation. To overcome these challenges, we propose a novel framework that employs patch-level comparison of self-distillation and pixel-level reconstruction losses, enhanced with an attention-based token removal mechanism. This approach selectively retains semantically relevant tokens, enabling the model to focus on the image's critical regions aligned with the specific functions of our model, including textual information processing, patch comparison, and image reconstruction, ensuring that the model learns high-level semantics and detailed visual features. Our experiments demonstrate that DetailCLIP surpasses existing CLIP-based and traditional self-supervised learning (SSL) models in segmentation accuracy and exhibits superior generalization across diverse datasets. DetailCLIP represents a significant advancement in vision-language modeling, offering a robust solution for tasks that demand high-level semantic understanding and detailed feature extraction. this https URL.

Translated Abstract:
이 논문에서는 DetailCLIP을 소개해. DetailCLIP은 CLIP 같은 대비 학습 기반의 비전-언어 모델이 세부적인 작업, 특히 세분화 같은 것들을 처리하는 데 한계가 있다는 문제를 해결하려고 해. CLIP과 그 변형들은 이미지와 텍스트 표현을 전반적으로 잘 맞추지만, 세밀한 세부사항을 포착하는 데는 어려움을 겪어.

이 문제를 해결하기 위해, 우리는 새로운 프레임워크를 제안해. 이 프레임워크는 패치 수준에서 자기 증류(self-distillation)와 픽셀 수준의 재구성 손실을 비교하고, 주의 기반의 토큰 제거 메커니즘을 추가했어. 이 방법은 의미적으로 관련 있는 토큰만 선택적으로 유지해서 모델이 특정 기능에 맞는 이미지의 중요한 영역에 집중할 수 있게 해. 여기에는 텍스트 정보 처리, 패치 비교, 이미지 재구성이 포함돼. 이렇게 해서 모델이 높은 수준의 의미와 세밀한 시각적 특징을 학습하도록 도와줘.

실험 결과, DetailCLIP은 기존의 CLIP 기반 모델과 전통적인 자기 지도 학습(SSL) 모델보다 세분화 정확도가 더 뛰어나고, 다양한 데이터셋에서 더 나은 일반화 성능을 보여줘. DetailCLIP은 비전-언어 모델링에서 큰 발전을 나타내며, 높은 수준의 의미 이해와 세부 특징 추출이 필요한 작업에 강력한 해결책을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.06815.pdf

Title: Object Modeling from Underwater Forward-Scan Sonar Imagery with Sea-Surface Multipath

Original Abstract:
We propose an optimization technique for 3-D underwater object modeling from 2-D forward-scan sonar images at known poses. A key contribution, for objects imaged in the proximity of the sea surface, is to resolve the multipath artifacts due to the air-water interface. Here, the object image formed by the direct target backscatter is almost always corrupted by the ghost and sometimes by the mirror components (generated by the multipath propagation). Assuming a planar air-water interface, we model, localize, and discard the corrupted object region within each view, thus avoiding the distortion of recovered 3-D shape. Additionally, complementary visual cues from the boundary of the mirror component, distinct at suitable sonar poses, are employed to enhance the 3-D modeling accuracy.
The optimization is implemented as iterative shape adjustment by displacing the vertices of triangular patches in the 3-D surface mesh model, in order to minimize the discrepancy between the data and synthesized views of the 3-D object model. To this end, we first determine 2-D motion fields that align the object regions in the data and synthesized views, then calculate the 3-D motion of triangular patch centers, and finally the model vertices. The 3-D model is initialized with the solution of an earlier space carving method applied to the same data. The same parameters are applied in various experiments with 2 real data sets, mixed real-synthetic data set, and computer-generated data guided by general findings from a real experiment, to explore the impact of non-flat air-water interface. The results confirm the generation of a refined 3-D model in about half-dozen iterations.

Translated Abstract:
우리는 알려진 자세에서 2D 전방 스캔 음파 이미지로부터 3D 수중 물체 모델링을 위한 최적화 기법을 제안해. 주요 기여점은 해수면 근처에서 촬영된 물체의 경우, 공기-물 경계로 인한 다중 경로 아티팩트를 해결하는 거야. 여기서, 직접 목표로부터 반사된 물체 이미지는 거의 항상 유령과 때때로 거울 성분(다중 경로 전파로 생성됨)으로 인해 손상돼.

평면 공기-물 경계를 가정하고, 우리는 각 뷰 내에서 손상된 물체 영역을 모델링, 위치 추적, 그리고 버려서 복구된 3D 형태의 왜곡을 피해. 추가로, 적절한 음파 자세에서 뚜렷하게 구분되는 거울 성분의 경계에서 나오는 보조 시각 신호를 사용해 3D 모델링의 정확도를 높여.

이 최적화는 3D 표면 메쉬 모델의 삼각형 패치 꼭지점을 이동시키면서 데이터와 합성된 3D 객체 모델의 뷰 간의 불일치를 최소화하는 반복적인 형태 조정으로 구현돼. 이를 위해 먼저 데이터와 합성된 뷰에서 물체 영역을 정렬하는 2D 운동장을 결정하고, 그 다음 삼각형 패치 중심의 3D 운동을 계산해, 마지막으로 모델 꼭지점을 조정해.

3D 모델은 같은 데이터에 적용된 이전의 공간 조각화 방법의 솔루션으로 초기화돼. 같은 파라미터를 사용해서 2개의 실제 데이터 세트, 혼합된 실제-합성 데이터 세트, 그리고 실제 실험의 일반적인 발견에 의해 안내된 컴퓨터 생성 데이터를 가지고 여러 실험을 진행해 비평면 공기-물 경계의 영향을 탐구했어. 결과적으로 약 6번의 반복을 통해 정제된 3D 모델이 생성됐다는 걸 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06821.pdf

Title: Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts

Original Abstract:
Foundation models like the segment anything model require high-quality manual prompts for medical image segmentation, which is time-consuming and requires expertise. SAM and its variants often fail to segment structures in ultrasound (US) images due to domain shift.
We propose Sam2Rad, a prompt learning approach to adapt SAM and its variants for US bone segmentation without human prompts. It introduces a prompt predictor network (PPN) with a cross-attention module to predict prompt embeddings from image encoder features. PPN outputs bounding box and mask prompts, and 256-dimensional embeddings for regions of interest. The framework allows optional manual prompting and can be trained end-to-end using parameter-efficient fine-tuning (PEFT).
Sam2Rad was tested on 3 musculoskeletal US datasets: wrist (3822 images), rotator cuff (1605 images), and hip (4849 images). It improved performance across all datasets without manual prompts, increasing Dice scores by 2-7% for hip/wrist and up to 33% for shoulder data. Sam2Rad can be trained with as few as 10 labeled images and is compatible with any SAM architecture for automatic segmentation.

Translated Abstract:
기초 모델인 세그먼트 에니씽 모델(SAM)은 의료 이미지 세분화를 위해 고품질의 수동 프롬프트가 필요해. 이게 시간도 많이 걸리고 전문 지식도 있어야 해. SAM과 그 변형들은 초음파 이미지에서 구조를 세분화하는 데 종종 실패해.

우리는 Sam2Rad라는 프롬프트 학습 방법을 제안해. 이 방법은 SAM과 그 변형들을 초음파 뼈 세분화를 위해 사람의 프롬프트 없이 적응시켜. 여기서는 프롬프트 예측 네트워크(PPN)를 도입해서 이미지 인코더 특징에서 프롬프트 임베딩을 예측해. PPN은 바운딩 박스와 마스크 프롬프트, 그리고 관심 영역에 대한 256차원 임베딩을 출력해. 이 프레임워크는 수동 프롬프트를 선택적으로 사용할 수 있고, 파라미터 효율적인 미세 조정(PEFT)을 이용해 엔드 투 엔드로 훈련할 수 있어.

Sam2Rad는 3개의 근골격계 초음파 데이터셋에서 테스트했어: 손목(3822장), 회전근개(1605장), 그리고 엉덩이(4849장). 수동 프롬프트 없이 모든 데이터셋에서 성능이 향상되었고, 엉덩이와 손목 데이터에서는 Dice 점수가 2-7% 증가했으며, 어깨 데이터에서는 최대 33%까지 증가했어. Sam2Rad는 라벨이 붙은 이미지 10장만으로도 훈련할 수 있고, 자동 세분화를 위해 어떤 SAM 아키텍처와도 호환돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.06827.pdf

Title: Cross-Modal Self-Supervised Learning with Effective Contrastive Units for LiDAR Point Clouds

Original Abstract:
3D perception in LiDAR point clouds is crucial for a self-driving vehicle to properly act in 3D environment. However, manually labeling point clouds is hard and costly. There has been a growing interest in self-supervised pre-training of 3D perception models. Following the success of contrastive learning in images, current methods mostly conduct contrastive pre-training on point clouds only. Yet an autonomous driving vehicle is typically supplied with multiple sensors including cameras and LiDAR. In this context, we systematically study single modality, cross-modality, and multi-modality for contrastive learning of point clouds, and show that cross-modality wins over other alternatives. In addition, considering the huge difference between the training sources in 2D images and 3D point clouds, it remains unclear how to design more effective contrastive units for LiDAR. We therefore propose the instance-aware and similarity-balanced contrastive units that are tailored for self-driving point clouds. Extensive experiments reveal that our approach achieves remarkable performance gains over various point cloud models across the downstream perception tasks of LiDAR based 3D object detection and 3D semantic segmentation on the four popular benchmarks including Waymo Open Dataset, nuScenes, SemanticKITTI and ONCE.

Translated Abstract:
LiDAR 포인트 클라우드에서의 3D 인식은 자율주행 차량이 3D 환경에서 제대로 행동하기 위해 정말 중요해. 그런데 포인트 클라우드를 수동으로 라벨링하는 건 어렵고 비용도 많이 들어. 그래서 최근에는 3D 인식 모델의 자기 지도 학습에 대한 관심이 커지고 있어. 

이미지에서의 대조 학습이 성공한 걸 따라, 현재 방법들은 대부분 포인트 클라우드에 대해서만 대조적 사전 훈련을 하고 있어. 하지만 자율주행 차량은 보통 카메라와 LiDAR 같은 여러 센서를 가지고 있지. 이런 배경에서, 우리는 포인트 클라우드의 대조 학습을 위해 단일 모달리티, 교차 모달리티, 그리고 다중 모달리티를 체계적으로 연구했어. 그 결과, 교차 모달리티가 다른 대안보다 더 좋다는 걸 보여줬어.

게다가 2D 이미지와 3D 포인트 클라우드의 훈련 소스 간에 엄청난 차이가 있기 때문에, LiDAR에 더 효과적인 대조 단위를 어떻게 디자인해야 할지는 아직 불확실해. 그래서 우리는 자율주행 포인트 클라우드에 맞게 조정된 인스턴스 인식 및 유사성 균형 대조 단위를 제안했어. 다양한 실험을 통해 우리의 접근 방식이 Waymo Open Dataset, nuScenes, SemanticKITTI, ONCE 같은 네 가지 인기 있는 벤치마크에서 LiDAR 기반 3D 객체 탐지와 3D 의미론적 분할 같은 하류 인식 작업에 대해 놀라운 성능 향상을 이룬다는 걸 밝혀냈어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06842.pdf

Title: Few-Shot Learning: Expanding ID Cards Presentation Attack Detection to Unknown ID Countries

Original Abstract:
This paper proposes a Few-shot Learning (FSL) approach for detecting Presentation Attacks on ID Cards deployed in a remote verification system and its extension to new countries. Our research analyses the performance of Prototypical Networks across documents from Spain and Chile as a baseline and measures the extension of generalisation capabilities of new ID Card countries such as Argentina and Costa Rica. Specifically targeting the challenge of screen display presentation attacks. By leveraging convolutional architectures and meta-learning principles embodied in Prototypical Networks, we have crafted a model that demonstrates high efficacy with Few-shot examples. This research reveals that competitive performance can be achieved with as Few-shots as five unique identities and with under 100 images per new country added. This opens a new insight for novel generalised Presentation Attack Detection on ID cards to unknown attacks.

Translated Abstract:
이 논문에서는 원격 검증 시스템에서 ID 카드에 대한 프레젠테이션 공격을 탐지하기 위한 Few-shot Learning (FSL) 접근 방식을 제안해. 우리는 스페인과 칠레의 문서들을 기준으로 Prototypical Networks의 성능을 분석하고, 아르헨티나와 코스타리카 같은 새로운 ID 카드 국가에 대한 일반화 능력을 측정했어. 특히 화면 표시 공격에 대한 도전에 초점을 맞췄지.

컨볼루션 아키텍처와 Prototypical Networks에 담긴 메타-러닝 원칙을 활용해서, 우리는 Few-shot 예제에서도 높은 효율성을 보여주는 모델을 만들었어. 이 연구는 단 5개의 고유한 아이디와 100장 이하의 이미지만으로도 경쟁력 있는 성능을 달성할 수 있다는 걸 보여줘. 

이건 미지의 공격에 대한 새로운 일반화된 프레젠테이션 공격 탐지에 대한 새로운 통찰력을 열어주는 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06845.pdf

Title: Face Mask Removal with Region-attentive Face Inpainting

Original Abstract:
During the COVID-19 pandemic, face masks have become ubiquitous in our lives. Face masks can cause some face recognition models to fail since they cover significant portion of a face. In addition, removing face masks from captured images or videos can be desirable, e.g., for better social interaction and for image/video editing and enhancement purposes. Hence, we propose a generative face inpainting method to effectively recover/reconstruct the masked part of a face. Face inpainting is more challenging compared to traditional inpainting, since it requires high fidelity while maintaining the identity at the same time. Our proposed method includes a Multi-scale Channel-Spatial Attention Module (M-CSAM) to mitigate the spatial information loss and learn the inter- and intra-channel correlation. In addition, we introduce an approach enforcing the supervised signal to focus on masked regions instead of the whole image. We also synthesize our own Masked-Faces dataset from the CelebA dataset by incorporating five different types of face masks, including surgical mask, regular mask and scarves, which also cover the neck area. The experimental results show that our proposed method outperforms different baselines in terms of structural similarity index measure, peak signal-to-noise ratio and l1 loss, while also providing better outputs qualitatively. The code will be made publicly available. Code is available at GitHub.

Translated Abstract:
COVID-19 팬데믹 동안, 얼굴 마스크는 우리 생활에서 흔하게 사용되게 되었어. 하지만 얼굴 마스크가 얼굴의 큰 부분을 가리기 때문에, 일부 얼굴 인식 모델이 제대로 작동하지 않을 수 있어. 게다가, 촬영한 이미지나 비디오에서 얼굴 마스크를 제거하는 게 필요할 수도 있어. 예를 들어, 더 나은 사회적 상호작용이나 이미지/비디오 편집 및 향상을 위해서 말이지.

그래서 우리는 마스크로 가려진 얼굴의 부분을 효과적으로 복구하는 생성적 얼굴 인페인팅 방법을 제안해. 얼굴 인페인팅은 전통적인 인페인팅보다 더 어려워. 왜냐하면, 얼굴의 정체성을 유지하면서 높은 정확도를 요구하기 때문이야. 우리가 제안한 방법은 다중 스케일 채널-공간 주의 모듈(M-CSAM)을 포함해서 공간 정보 손실을 줄이고 채널 간의 상관관계를 배우도록 해.

또한, 우리는 전체 이미지 대신 마스크가 가려진 영역에 집중하도록 지도 신호를 강화하는 방법도 도입했어. 우리는 CelebA 데이터셋에서 다섯 가지 다른 종류의 얼굴 마스크(수술 마스크, 일반 마스크, 스카프 등)를 포함해서 나만의 Masked-Faces 데이터셋을 만들었어. 

실험 결과, 우리 방법이 구조적 유사도 지수, 피크 신호 대 잡음 비율, l1 손실 측면에서 다양한 기준선보다 성능이 더 뛰어나고, 질적으로도 더 좋은 결과를 보여줬어. 코드도 공개할 예정이니 GitHub에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06848.pdf

Title: Shadow Removal Refinement via Material-Consistent Shadow Edges

Original Abstract:
Shadow boundaries can be confused with material boundaries as both exhibit sharp changes in luminance or contrast within a scene. However, shadows do not modify the intrinsic color or texture of surfaces. Therefore, on both sides of shadow edges traversing regions with the same material, the original color and textures should be the same if the shadow is removed properly. These shadow/shadow-free pairs are very useful but hard-to-collect supervision signals. The crucial contribution of this paper is to learn how to identify those shadow edges that traverse material-consistent regions and how to use them as self-supervision for shadow removal refinement during test time. To achieve this, we fine-tune SAM, an image segmentation foundation model, to produce a shadow-invariant segmentation and then extract material-consistent shadow edges by comparing the SAM segmentation with the shadow mask. Utilizing these shadow edges, we introduce color and texture-consistency losses to enhance the shadow removal process. We demonstrate the effectiveness of our method in improving shadow removal results on more challenging, in-the-wild images, outperforming the state-of-the-art shadow removal methods. Additionally, we propose a new metric and an annotated dataset for evaluating the performance of shadow removal methods without the need for paired shadow/shadow-free data.

Translated Abstract:
그림자 경계는 장면 내에서 밝기나 대비의 급격한 변화를 보여서 물질 경계와 혼동될 수 있어. 하지만 그림자는 표면의 본래 색깔이나 질감을 바꾸지 않아. 그래서 같은 물질이 있는 지역을 가로지르는 그림자 가장자리 양쪽에서는 그림자를 제대로 제거하면 원래의 색깔과 질감이 같아야 해. 이런 그림자/그림자가 없는 쌍은 유용하지만 수집하기 어려운 감독 신호야.

이 논문의 중요한 기여는 물질이 일관된 지역을 가로지르는 그림자 가장자리를 찾는 방법과, 이걸 테스트할 때 그림자 제거의 정제를 위한 자기 감독으로 활용하는 방법을 배우는 거야. 이를 위해 우리는 이미지 분할 기초 모델인 SAM을 미세 조정해서 그림자와 관계없는 분할을 만들고, SAM 분할과 그림자 마스크를 비교해 물질이 일관된 그림자 가장자리를 추출해.

이 그림자 가장자리를 사용해서 그림자 제거 과정을 향상시키기 위해 색깔과 질감의 일관성 손실을 도입했어. 우리는 야외에서 더 도전적인 이미지에서 그림자 제거 결과를 개선하는 방법의 효과를 보여주었고, 최신 그림자 제거 방법들을 능가했어. 또한, 쌍으로 된 그림자/그림자가 없는 데이터 없이 그림자 제거 방법의 성능을 평가할 수 있는 새로운 메트릭과 주석이 포함된 데이터셋도 제안했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06851.pdf

Title: LIME-M: Less Is More for Evaluation of MLLMs

Original Abstract:
With the remarkable success achieved by Multimodal Large Language Models (MLLMs), numerous benchmarks have been designed to assess MLLMs' ability to guide their development in image perception tasks (e.g., image captioning and visual question answering). However, the existence of numerous benchmarks results in a substantial computational burden when evaluating model performance across all of them. Moreover, these benchmarks contain many overly simple problems or challenging samples, which do not effectively differentiate the capabilities among various MLLMs. To address these challenges, we propose a pipeline to process the existing benchmarks, which consists of two modules: (1) Semi-Automated Screening Process and (2) Eliminating Answer Leakage. The Semi-Automated Screening Process filters out samples that cannot distinguish the model's capabilities by synthesizing various MLLMs and manually evaluating them. The Eliminate Answer Leakage module filters samples whose answers can be inferred without images. Finally, we curate the LIME-M: Less Is More for Evaluation of Multimodal LLMs, a lightweight Multimodal benchmark that can more effectively evaluate the performance of different models. Our experiments demonstrate that: LIME-M can better distinguish the performance of different MLLMs with fewer samples (24% of the original) and reduced time (23% of the original); LIME-M eliminates answer leakage, focusing mainly on the information within images; The current automatic metric (i.e., CIDEr) is insufficient for evaluating MLLMs' capabilities in captioning. Moreover, removing the caption task score when calculating the overall score provides a more accurate reflection of model performance differences. All our codes and data are released at this https URL.

Translated Abstract:
최근 멀티모달 대형 언어 모델(MLLMs)의 성공 덕분에, 이미지 인식 작업(예: 이미지 캡셔닝, 시각적 질문 응답)을 평가하기 위해 다양한 벤치마크가 만들어졌어. 하지만 이렇게 많은 벤치마크가 있다 보니, 모든 모델 성능을 평가하는 데 엄청난 계산 부담이 생겨. 게다가 이 벤치마크들에는 너무 간단한 문제나 너무 어려운 샘플들이 많아서, 여러 MLLM 간의 능력을 제대로 구분하기도 힘들어.

이런 문제를 해결하기 위해, 우리는 기존 벤치마크를 처리하는 파이프라인을 제안해. 이 파이프라인은 두 가지 모듈로 구성돼: (1) 반자동 스크리닝 과정과 (2) 답안 유출 제거. 반자동 스크리닝 과정은 다양한 MLLM을 합성하고 수동으로 평가해서 모델의 능력을 구분할 수 없는 샘플들을 걸러내. 답안 유출 제거 모듈은 이미지 없이도 답을 유추할 수 있는 샘플을 필터링해.

결국, 우리는 LIME-M: 평가를 위한 더 적은 것이 더 많은 멀티모달 벤치마크를 만들었어. 이 벤치마크는 다양한 모델의 성능을 더 효과적으로 평가할 수 있어. 실험 결과에 따르면, LIME-M은 적은 샘플(원본의 24%)로도 서로 다른 MLLM의 성능을 더 잘 구분하고, 시간도 줄여(원본의 23%); LIME-M은 답안 유출을 없애고 주로 이미지 안의 정보에 집중해; 현재의 자동 평가 지표(CIDEr)는 캡셔닝에서 MLLM의 능력을 평가하기에 부족해. 또한, 전체 점수를 계산할 때 캡셔닝 점수를 제거하면 모델 성능 차이를 더 정확하게 반영할 수 있어. 모든 코드와 데이터는 이 URL에서 공개했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06853.pdf

Title: ExIQA: Explainable Image Quality Assessment Using Distortion Attributes

Original Abstract:
Blind Image Quality Assessment (BIQA) aims to develop methods that estimate the quality scores of images in the absence of a reference image. In this paper, we approach BIQA from a distortion identification perspective, where our primary goal is to predict distortion types and strengths using Vision-Language Models (VLMs), such as CLIP, due to their extensive knowledge and generalizability. Based on these predicted distortions, we then estimate the quality score of the image. To achieve this, we propose an explainable approach for distortion identification based on attribute learning. Instead of prompting VLMs with the names of distortions, we prompt them with the attributes or effects of distortions and aggregate this information to infer the distortion strength. Additionally, we consider multiple distortions per image, making our method more scalable. To support this, we generate a dataset consisting of 100,000 images for efficient training. Finally, attribute probabilities are retrieved and fed into a regressor to predict the image quality score. The results show that our approach, besides its explainability and transparency, achieves state-of-the-art (SOTA) performance across multiple datasets in both PLCC and SRCC metrics. Moreover, the zero-shot results demonstrate the generalizability of the proposed approach.

Translated Abstract:
블라인드 이미지 품질 평가(BIQA)는 참조 이미지 없이 이미지의 품질 점수를 추정하는 방법을 개발하는 걸 목표로 해. 이 논문에서는 BIQA를 왜곡 식별 관점에서 접근해. 우리의 주 목표는 CLIP 같은 비전-언어 모델(VLM)을 활용해 왜곡의 종류와 강도를 예측하는 거야. VLM은 광범위한 지식과 일반화 능력이 뛰어나거든.

예측된 왜곡을 바탕으로 이미지의 품질 점수를 추정해. 이를 위해 속성 학습에 기반한 왜곡 식별을 위한 설명 가능한 방법을 제안해. 왜곡의 이름으로 VLM에 질문하는 대신, 왜곡의 속성이나 효과로 질문하고 이 정보를 모아서 왜곡 강도를 추론해. 또한, 이미지마다 여러 개의 왜곡을 고려해서 우리의 방법을 더 확장 가능하게 만들었어.

이를 지원하기 위해 효율적인 학습을 위해 10만 개의 이미지로 구성된 데이터셋을 생성했어. 마지막으로, 속성 확률을 추출해서 회귀 모델에 넣어 이미지 품질 점수를 예측해. 결과적으로, 우리의 접근 방식은 설명 가능성과 투명성을 갖추고 있을 뿐만 아니라, PLCC와 SRCC 메트릭에서 여러 데이터셋에서 최첨단(SOTA) 성능을 달성했어. 게다가 제로샷 결과는 제안한 접근 방식의 일반화 능력을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.06856.pdf

Title: AssistTaxi: A Comprehensive Dataset for Taxiway Analysis and Autonomous Operations

Original Abstract:
The availability of high-quality datasets play a crucial role in advancing research and development especially, for safety critical and autonomous systems. In this paper, we present AssistTaxi, a comprehensive novel dataset which is a collection of images for runway and taxiway analysis. The dataset comprises of more than 300,000 frames of diverse and carefully collected data, gathered from Melbourne (MLB) and Grant-Valkaria (X59) general aviation airports. The importance of AssistTaxi lies in its potential to advance autonomous operations, enabling researchers and developers to train and evaluate algorithms for efficient and safe taxiing. Researchers can utilize AssistTaxi to benchmark their algorithms, assess performance, and explore novel approaches for runway and taxiway analysis. Addition-ally, the dataset serves as a valuable resource for validating and enhancing existing algorithms, facilitating innovation in autonomous operations for aviation. We also propose an initial approach to label the dataset using a contour based detection and line extraction technique.

Translated Abstract:
고품질 데이터셋은 특히 안전이 중요한 자율 시스템의 연구 및 개발에 매우 중요해. 이 논문에서는 AssistTaxi라는 새롭고 포괄적인 데이터셋을 소개할 건데, 이건 활주로와 택시웨이 분석을 위한 이미지 모음이야. 이 데이터셋은 멜버른(MLB)과 그랜트-발카리아(X59) 일반 항공 공항에서 모은 다양한 자료를 포함해서 30만 개 이상의 프레임으로 구성되어 있어.

AssistTaxi의 중요성은 자율작업을 발전시킬 수 있는 가능성에 있어. 연구자와 개발자들은 이 데이터셋을 활용해 알고리즘을 훈련하고 평가할 수 있어, 더 효율적이고 안전한 택싱을 위한 거지. 연구자들은 AssistTaxi를 통해 자신들의 알고리즘을 벤치마킹하고 성능을 평가하며, 활주로와 택시웨이 분석을 위한 새로운 접근 방식을 탐구할 수 있어.

또한, 이 데이터셋은 기존 알고리즘을 검증하고 개선하는 데에도 유용한 자원이 돼서 항공 분야의 자율 운영 혁신을 촉진해. 우린 데이터셋에 레이블을 붙이는 초기 접근 방법도 제안하는데, 이건 윤곽 기반 탐지 및 선 추출 기법을 사용해.

================================================================================

URL:
https://arxiv.org/pdf/2409.06889.pdf

Title: Enhanced Pix2Pix GAN for Visual Defect Removal in UAV-Captured Images

Original Abstract:
This paper presents a neural network that effectively removes visual defects from UAV-captured images. It features an enhanced Pix2Pix GAN, specifically engineered to address visual defects in UAV imagery. The method incorporates advanced modifications to the Pix2Pix architecture, targeting prevalent issues such as mode collapse. The suggested method facilitates significant improvements in the quality of defected UAV images, yielding cleaner and more precise visual results. The effectiveness of the proposed approach is demonstrated through evaluation on a custom dataset of aerial photographs, highlighting its capability to refine and restore UAV imagery effectively.

Translated Abstract:
이 논문에서는 UAV(무인 항공기)로 찍은 이미지에서 시각적 결함을 효과적으로 제거하는 신경망을 소개해. 이건 시각적 결함을 해결하기 위해 특별히 설계된 개선된 Pix2Pix GAN을 사용해.

이 방법은 Pix2Pix 구조에 고급 수정 사항을 추가해서, 흔히 발생하는 문제인 모드 붕괴 같은 것들을 겨냥하고 있어. 제안된 방법은 결함이 있는 UAV 이미지의 품질을 크게 개선해서, 더 깨끗하고 정확한 비주얼 결과를 얻을 수 있게 해.

제안된 접근법의 효과는 맞춤형 항공 사진 데이터셋을 통해 평가되었고, UAV 이미지를 효과적으로 개선하고 복원할 수 있는 능력을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.06923.pdf

Title: Rethinking Directional Parameterization in Neural Implicit Surface Reconstruction

Original Abstract:
Multi-view 3D surface reconstruction using neural implicit representations has made notable progress by modeling the geometry and view-dependent radiance fields within a unified framework. However, their effectiveness in reconstructing objects with specular or complex surfaces is typically biased by the directional parameterization used in their view-dependent radiance network. {\it Viewing direction} and {\it reflection direction} are the two most commonly used directional parameterizations but have their own limitations. Typically, utilizing the viewing direction usually struggles to correctly decouple the geometry and appearance of objects with highly specular surfaces, while using the reflection direction tends to yield overly smooth reconstructions for concave or complex structures. In this paper, we analyze their failed cases in detail and propose a novel hybrid directional parameterization to address their limitations in a unified form. Extensive experiments demonstrate the proposed hybrid directional parameterization consistently delivered satisfactory results in reconstructing objects with a wide variety of materials, geometry and appearance, whereas using other directional parameterizations faces challenges in reconstructing certain objects. Moreover, the proposed hybrid directional parameterization is nearly parameter-free and can be effortlessly applied in any existing neural surface reconstruction method.

Translated Abstract:
다중 시점 3D 표면 재구성에서 신경 임플리시트 표현을 사용하는 방식이 발전해왔어. 이 방법은 기하학과 시점에 따라 달라지는 복사 필드를 하나의 프레임워크 안에서 모델링해. 하지만, 거울처럼 반짝이는 표면이나 복잡한 표면을 가진 물체를 재구성할 때는 방향 파라미터화 때문에 효과가 떨어져. 

주로 사용하는 두 개의 방향 파라미터화는 {\it 보기 방향}과 {\it 반사 방향}인데, 각각의 한계가 있어. 보기 방향을 사용하면 반짝이는 표면을 가진 물체의 기하학과 외관을 잘 분리하지 못하고, 반사 방향을 사용할 때는 오히려 오목하거나 복잡한 구조에 대해 너무 부드러운 재구성이 나와. 

이 논문에서는 이 두 방법의 실패 사례를 자세히 분석하고 그 한계를 해결하기 위해 새로운 하이브리드 방향 파라미터화를 제안해. 여러 실험을 통해 이 하이브리드 방향 파라미터화가 다양한 재질, 기하학, 외관을 가진 물체를 재구성하는 데 일관되게 만족스러운 결과를 보여줬어. 반면, 다른 방향 파라미터화를 사용했을 때는 특정 물체를 재구성하는 데 어려움을 겪었어. 게다가 이 하이브리드 방향 파라미터화는 거의 파라미터가 필요 없고, 기존의 신경 표면 재구성 방법에 쉽게 적용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06928.pdf

Title: Intrapartum Ultrasound Image Segmentation of Pubic Symphysis and Fetal Head Using Dual Student-Teacher Framework with CNN-ViT Collaborative Learning

Original Abstract:
The segmentation of the pubic symphysis and fetal head (PSFH) constitutes a pivotal step in monitoring labor progression and identifying potential delivery complications. Despite the advances in deep learning, the lack of annotated medical images hinders the training of segmentation. Traditional semi-supervised learning approaches primarily utilize a unified network model based on Convolutional Neural Networks (CNNs) and apply consistency regularization to mitigate the reliance on extensive annotated data. However, these methods often fall short in capturing the discriminative features of unlabeled data and in delineating the long-range dependencies inherent in the ambiguous boundaries of PSFH within ultrasound images. To address these limitations, we introduce a novel framework, the Dual-Student and Teacher Combining CNN and Transformer (DSTCT), which synergistically integrates the capabilities of CNNs and Transformers. Our framework comprises a Vision Transformer (ViT) as the teacher and two student mod ls one ViT and one CNN. This dual-student setup enables mutual supervision through the generation of both hard and soft pseudo-labels, with the consistency in their predictions being refined by minimizing the classifier determinacy discrepancy. The teacher model further reinforces learning within this architecture through the imposition of consistency regularization constraints. To augment the generalization abilities of our approach, we employ a blend of data and model perturbation techniques. Comprehensive evaluations on the benchmark dataset of the PSFH Segmentation Grand Challenge at MICCAI 2023 demonstrate our DSTCT framework outperformed ten contemporary semi-supervised segmentation methods. Code available at this https URL.

Translated Abstract:
pubic symphysis와 태아 두부(PSFH) 구분은 분만 진행 상황을 모니터링하고 잠재적인 출산 합병증을 식별하는 데 중요한 단계야. 딥러닝이 발전했지만, 주석이 달린 의료 이미지가 부족해서 세분화 훈련이 어려워. 전통적인 반지도 학습 방식은 주로 CNN 기반의 통합 네트워크 모델을 사용하는데, 이 방식은 많은 주석 데이터에 의존하지 않도록 일관성 정규화를 적용해.

하지만 이런 방법들은 종종 라벨이 없는 데이터의 구별 가능한 특징을 제대로 잡아내지 못하고, 초음파 이미지에서 PSFH의 불확실한 경계에 있는 장기 의존성을 잘 표현하지 못해. 이런 한계를 해결하기 위해 우리는 Dual-Student and Teacher Combining CNN and Transformer(DSTCT)라는 새로운 프레임워크를 도입했어. 이 프레임워크는 CNN과 Transformer의 기능을 함께 활용해.

우리 프레임워크는 비전 트랜스포머(ViT)를 교사로 두고, 두 개의 학생 모델, 하나는 ViT이고 다른 하나는 CNN을 사용해. 이 듀얼 학생 설정은 하드와 소프트 의사 라벨을 생성해 서로 감독할 수 있게 해주고, 그들의 예측 일관성을 최소화하면서 개선해. 교사 모델은 일관성 정규화 제약을 통해 이 구조 내에서 학습을 강화해.

우리 접근 방식의 일반화 능력을 높이기 위해 데이터와 모델 perturbation 기술의 혼합을 사용해. MICCAI 2023의 PSFH 세분화 그랜드 챌린지 벤치마크 데이터셋에서의 종합 평가 결과, 우리의 DSTCT 프레임워크가 10개의 최신 반지도 세분화 방법보다 더 나은 성능을 보여줬어. 코드에 대한 링크는 이 URL에 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06942.pdf

Title: Automated Body Composition Analysis Using DAFS Express on 2D MRI Slices at L3 Vertebral Level

Original Abstract:
Body composition analysis is vital in assessing health conditions such as obesity, sarcopenia, and metabolic syndromes. MRI provides detailed images of skeletal muscle (SKM), visceral adipose tissue (VAT), and subcutaneous adipose tissue (SAT), but their manual segmentation is labor-intensive and limits clinical applicability. This study validates an automated tool for MRI-based 2D body composition analysis- (Data Analysis Facilitation Suite (DAFS) Express), comparing its automated measurements with expert manual segmentations using UK Biobank data. A cohort of 399 participants from the UK Biobank dataset was selected, yielding 423 single L3 slices for analysis. DAFS Express performed automated segmentations of SKM, VAT, and SAT, which were then manually corrected by expert raters for validation. Evaluation metrics included Jaccard coefficients, Dice scores, Intraclass Correlation Coefficients (ICCs), and Bland-Altman Plots to assess segmentation agreement and reliability. High agreements were observed between automated and manual segmentations with mean Jaccard scores: SKM 99.03%, VAT 95.25%, and SAT 99.57%; and mean Dice scores: SKM 99.51%, VAT 97.41%, and SAT 99.78%. Cross-sectional area comparisons showed consistent measurements with automated methods closely matching manual measurements for SKM and SAT, and slightly higher values for VAT (SKM: Auto 132.51 cm^2, Manual 132.36 cm^2; VAT: Auto 137.07 cm^2, Manual 134.46 cm^2; SAT: Auto 203.39 cm^2, Manual 202.85 cm^2). ICCs confirmed strong reliability (SKM: 0.998, VAT: 0.994, SAT: 0.994). Bland-Altman plots revealed minimal biases, and boxplots illustrated distribution similarities across SKM, VAT, and SAT areas. On average DAFS Express took 18 seconds per DICOM. This underscores its potential to streamline image analysis processes in research and clinical settings, enhancing diagnostic accuracy and efficiency.

Translated Abstract:
신체 구성 분석은 비만, 근감소증, 대사 증후군 같은 건강 상태를 평가하는 데 중요해. MRI는 근육, 내장 지방, 피하 지방에 대한 상세한 이미지를 제공하지만, 수동으로 분할하는 건 시간도 많이 걸리고 임상에서 활용하기 힘들어. 이 연구는 MRI 기반 2D 신체 구성 분석을 위한 자동화 도구인 DAFS Express의 유효성을 검증했어. 자동 측정값을 전문가의 수동 분할과 비교했지. 

UK Biobank 데이터에서 399명의 참가자를 선정하고, 423개의 L3 슬라이스를 분석했어. DAFS Express는 SKM, VAT, SAT의 자동 분할을 수행했고, 전문가들이 수동으로 수정해서 검증했어. 평가 지표로는 Jaccard 계수, Dice 점수, Intraclass Correlation Coefficients (ICCs), Bland-Altman 플롯을 사용해서 분할의 일치성과 신뢰성을 평가했어. 자동 분할과 수동 분할 간의 높은 일치도가 관찰되었고, 평균 Jaccard 점수는 SKM 99.03%, VAT 95.25%, SAT 99.57%였어. 평균 Dice 점수는 SKM 99.51%, VAT 97.41%, SAT 99.78%였지.

단면적 비교에서는 자동 방법이 수동 측정과 잘 일치했고, VAT는 약간 더 높은 값을 보였어. (SKM: 자동 132.51 cm², 수동 132.36 cm²; VAT: 자동 137.07 cm², 수동 134.46 cm²; SAT: 자동 203.39 cm², 수동 202.85 cm²). ICCs는 강한 신뢰성을 확인했어 (SKM: 0.998, VAT: 0.994, SAT: 0.994). Bland-Altman 플롯에서는 편향이 거의 없었고, boxplot은 SKM, VAT, SAT 영역 간의 분포 유사성을 보여줬어. 평균적으로 DAFS Express는 DICOM 하나당 18초가 걸렸어. 이건 연구와 임상 환경에서 이미지 분석 과정을 간소화할 수 있는 가능성을 강조하고, 진단 정확성과 효율성을 높일 수 있다는 걸 의미해.

================================================================================

URL:
https://arxiv.org/pdf/2409.06945.pdf

Title: FSMDet: Vision-guided feature diffusion for fully sparse 3D detector

Original Abstract:
Fully sparse 3D detection has attracted an increasing interest in the recent years. However, the sparsity of the features in these frameworks challenges the generation of proposals because of the limited diffusion process. In addition, the quest for efficiency has led to only few work on vision-assisted fully sparse models. In this paper, we propose FSMDet (Fully Sparse Multi-modal Detection), which use visual information to guide the LiDAR feature diffusion process while still maintaining the efficiency of the pipeline. Specifically, most of fully sparse works focus on complex customized center fusion diffusion/regression operators. However, we observed that if the adequate object completion is performed, even the simplest interpolation operator leads to satisfactory results. Inspired by this observation, we split the vision-guided diffusion process into two modules: a Shape Recover Layer (SRLayer) and a Self Diffusion Layer (SDLayer). The former uses RGB information to recover the shape of the visible part of an object, and the latter uses a visual prior to further spread the features to the center region. Experiments demonstrate that our approach successfully improves the performance of previous fully sparse models that use LiDAR only and reaches SOTA performance in multimodal models. At the same time, thanks to the sparse architecture, our method can be up to 5 times more efficient than previous SOTA methods in the inference process.

Translated Abstract:
최근 몇 년 동안 완전히 희소한 3D 탐지가 많은 관심을 받고 있어. 하지만 이런 프레임워크에서 특징들이 희소하기 때문에 제안 생성이 어려워. 그 이유는 확산 과정이 제한적이기 때문이야. 게다가 효율성을 추구하다 보니 비전 지원 완전 희소 모델에 대한 연구는 거의 없어. 

이 논문에서는 FSMDet (Fully Sparse Multi-modal Detection)를 제안해. 이 모델은 시각 정보를 사용해서 LiDAR 특징 확산 과정을 안내하면서도 파이프라인의 효율성을 유지해. 대부분의 완전 희소 연구는 복잡한 맞춤형 중심 융합 확산/회귀 연산자에 초점을 맞추고 있어. 그런데 우리는 적절한 객체 완성이 이루어지면, 가장 간단한 보간 연산자도 만족스러운 결과를 낼 수 있다는 걸 발견했어. 

이 관찰에서 영감을 받아, 우리는 비전 안내 확산 과정을 두 개의 모듈로 나누었어: Shape Recover Layer (SRLayer)와 Self Diffusion Layer (SDLayer). SRLayer는 RGB 정보를 사용해서 객체의 보이는 부분의 형태를 복구하고, SDLayer는 시각적 우선순위를 사용해 특징을 중심 영역으로 더 확산시켜. 

실험 결과, 우리의 접근 방식이 LiDAR만 사용하는 이전의 완전 희소 모델의 성능을 성공적으로 개선하고, 다중 모달 모델에서 SOTA 성능에 도달했어. 동시에 희소한 구조 덕분에, 우리의 방법은 추론 과정에서 이전 SOTA 방법보다 최대 5배 더 효율적일 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06956.pdf

Title: Bridging Domain Gap of Point Cloud Representations via Self-Supervised Geometric Augmentation

Original Abstract:
Recent progress of semantic point clouds analysis is largely driven by synthetic data (e.g., the ModelNet and the ShapeNet), which are typically complete, well-aligned and noisy free. Therefore, representations of those ideal synthetic point clouds have limited variations in the geometric perspective and can gain good performance on a number of 3D vision tasks such as point cloud classification. In the context of unsupervised domain adaptation (UDA), representation learning designed for synthetic point clouds can hardly capture domain invariant geometric patterns from incomplete and noisy point clouds. To address such a problem, we introduce a novel scheme for induced geometric invariance of point cloud representations across domains, via regularizing representation learning with two self-supervised geometric augmentation tasks. On one hand, a novel pretext task of predicting translation distances of augmented samples is proposed to alleviate centroid shift of point clouds due to occlusion and noises. On the other hand, we pioneer an integration of the relational self-supervised learning on geometrically-augmented point clouds in a cascade manner, utilizing the intrinsic relationship of augmented variants and other samples as extra constraints of cross-domain geometric features. Experiments on the PointDA-10 dataset demonstrate the effectiveness of the proposed method, achieving the state-of-the-art performance.

Translated Abstract:
최근의 의미론적 포인트 클라우드 분석은 주로 합성 데이터(예: ModelNet, ShapeNet)에 의해 발전하고 있어. 이 데이터는 일반적으로 완전하고 잘 정렬되어 있으며, 노이즈가 없어. 그래서 이런 이상적인 합성 포인트 클라우드의 표현 방식은 기하학적으로 다양성이 제한적이고, 포인트 클라우드 분류 같은 여러 3D 비전 작업에서 좋은 성능을 내. 

하지만 비지도 도메인 적응(UDA) 맥락에서는, 합성 포인트 클라우드를 위해 설계된 표현 학습이 불완전하고 노이즈가 있는 포인트 클라우드에서 도메인 불변의 기하학적 패턴을 잘 포착하지 못해. 이런 문제를 해결하기 위해, 우리는 도메인 간 포인트 클라우드 표현의 기하학적 불변성을 유도하는 새로운 방식을 제안해. 이 방식은 두 개의 자기 지도 기하학적 증강 작업으로 표현 학습을 정규화하는 거야.

한편으로, 증강 샘플의 변환 거리 예측이라는 새로운 프리텍스트 작업을 제안해. 이 작업은 가림 현상과 노이즈로 인한 포인트 클라우드의 중심 이동을 완화하는 데 도움을 줘. 다른 한편으로, 우리는 기하학적으로 증강된 포인트 클라우드에서 관계적 자기 지도 학습을 통합하는 방식을 처음으로 도입해. 이건 증강된 변형과 다른 샘플 간의 내재적 관계를 활용해서 도메인 간 기하학적 특징에 대한 추가 제약을 제공하는 거야. 

PointDA-10 데이터셋에서 진행한 실험 결과, 제안한 방법의 효과를 보여주고, 최첨단 성능을 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06963.pdf

Title: Brain-Inspired Stepwise Patch Merging for Vision Transformers

Original Abstract:
The hierarchical architecture has become a mainstream design paradigm for Vision Transformers (ViTs), with Patch Merging serving as the pivotal component that transforms a columnar architecture into a hierarchical one. Drawing inspiration from the brain's ability to integrate global and local information for comprehensive visual understanding, we propose a novel technique called Stepwise Patch Merging (SPM), which enhances the subsequent attention mechanism's ability to 'see' better. SPM comprises two critical modules: Multi-Scale Aggregation (MSA) and Guided Local Enhancement (GLE). The MSA module integrates multi-scale features to enrich feature representation, while the GLE module focuses on refining local detail extraction, thus achieving an optimal balance between long-range dependency modeling and local feature enhancement. Extensive experiments conducted on benchmark datasets, including ImageNet-1K, COCO, and ADE20K, demonstrate that SPM significantly improves the performance of various models, particularly in dense prediction tasks such as object detection and semantic segmentation. These results underscore the efficacy of SPM in enhancing model accuracy and robustness across a wide range of computer vision tasks.

Translated Abstract:
계층 구조는 비전 트랜스포머(ViT)의 주류 디자인 패러다임이 되었고, 패치 병합이 열쇠 역할을 해서 열(column) 구조를 계층적으로 바꿉니다. 우리는 뇌가 글로벌 정보와 로컬 정보를 통합해서 전체적인 시각적 이해를 하는 방식에서 영감을 받아, '단계적 패치 병합(Stepwise Patch Merging, SPM)'이라는 새로운 기술을 제안합니다. 이 기술은 후속 주의(attention) 메커니즘이 더 잘 '보기'를 할 수 있게 도와줍니다.

SPM은 두 가지 중요한 모듈로 구성되어 있습니다: 다중 스케일 집계(Multi-Scale Aggregation, MSA)와 안내된 로컬 강화(Guided Local Enhancement, GLE). MSA 모듈은 다중 스케일 특징을 통합해서 특징 표현을 풍부하게 하고, GLE 모듈은 로컬 세부 정보 추출을 개선하는 데 집중합니다. 이렇게 해서 장거리 종속성과 로컬 특징 강화를 최적의 균형으로 맞춥니다.

이미지넷-1K, COCO, ADE20K 같은 기준 데이터셋에서 실시한 광범위한 실험 결과, SPM은 다양한 모델의 성능을 크게 향상시켰고, 특히 물체 탐지와 의미 분할 같은 밀집 예측 작업에서 효과를 보였습니다. 이러한 결과는 SPM이 다양한 컴퓨터 비전 작업에서 모델의 정확성과 강인성을 높이는 데 효과적임을 강조합니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.06980.pdf

Title: PanAdapter: Two-Stage Fine-Tuning with Spatial-Spectral Priors Injecting for Pansharpening

Original Abstract:
Pansharpening is a challenging image fusion task that involves restoring images using two different modalities: low-resolution multispectral images (LRMS) and high-resolution panchromatic (PAN). Many end-to-end specialized models based on deep learning (DL) have been proposed, yet the scale and performance of these models are limited by the size of dataset. Given the superior parameter scales and feature representations of pre-trained models, they exhibit outstanding performance when transferred to downstream tasks with small datasets. Therefore, we propose an efficient fine-tuning method, namely PanAdapter, which utilizes additional advanced semantic information from pre-trained models to alleviate the issue of small-scale datasets in pansharpening tasks. Specifically, targeting the large domain discrepancy between image restoration and pansharpening tasks, the PanAdapter adopts a two-stage training strategy for progressively adapting to the downstream task. In the first stage, we fine-tune the pre-trained CNN model and extract task-specific priors at two scales by proposed Local Prior Extraction (LPE) module. In the second stage, we feed the extracted two-scale priors into two branches of cascaded adapters respectively. At each adapter, we design two parameter-efficient modules for allowing the two branches to interact and be injected into the frozen pre-trained VisionTransformer (ViT) blocks. We demonstrate that by only training the proposed LPE modules and adapters with a small number of parameters, our approach can benefit from pre-trained image restoration models and achieve state-of-the-art performance in several benchmark pansharpening datasets. The code will be available soon.

Translated Abstract:
팬샤프닝은 저해상도 다분광 이미지(LRMS)와 고해상도 팬크로매틱 이미지(PAN)의 두 가지 서로 다른 모달리티를 사용해 이미지를 복원하는 어려운 이미지 융합 작업이야. 딥러닝(DL)을 기반으로 한 여러 전문 모델들이 제안되었지만, 이 모델들의 성능과 규모는 데이터셋의 크기에 제한받아. 

사전 훈련된 모델은 더 뛰어난 파라미터 규모와 특징 표현을 가지고 있어서, 작은 데이터셋으로도 다운스트림 작업에서 우수한 성능을 보여. 그래서 우리는 PanAdapter라는 효율적인 미세 조정 방법을 제안해. 이 방법은 사전 훈련된 모델에서 추가적인 고급 의미 정보를 활용해서 팬샤프닝 작업의 작은 규모 데이터셋 문제를 해결하려는 거야.

특히, 이미지 복원 작업과 팬샤프닝 작업 사이의 큰 도메인 불일치를 해결하기 위해 PanAdapter는 다운스트림 작업에 점진적으로 적응하는 두 단계 훈련 전략을 채택해. 첫 번째 단계에서는 사전 훈련된 CNN 모델을 미세 조정하고, 제안된 지역 선행 추출(Local Prior Extraction, LPE) 모듈을 사용해 두 가지 스케일에서 작업 특정 선행 정보를 추출해. 

두 번째 단계에서는 추출한 두 가지 스케일의 선행 정보를 각각 두 개의 연속 어댑터 브랜치에 넣어. 각 어댑터에서는 두 브랜치가 상호작용하고 고정된 사전 훈련된 비전 트랜스포머(ViT) 블록에 주입될 수 있도록 두 개의 파라미터 효율적인 모듈을 설계했어. 우리는 제안된 LPE 모듈과 어댑터를 소수의 파라미터만으로 훈련시켜서, 사전 훈련된 이미지 복원 모델의 이점을 얻고 여러 기준 팬샤프닝 데이터셋에서 최첨단 성능을 달성할 수 있음을 보여줬어. 코드도 곧 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06991.pdf

Title: 1M-Deepfakes Detection Challenge

Original Abstract:
The detection and localization of deepfake content, particularly when small fake segments are seamlessly mixed with real videos, remains a significant challenge in the field of digital media security. Based on the recently released AV-Deepfake1M dataset, which contains more than 1 million manipulated videos across more than 2,000 subjects, we introduce the 1M-Deepfakes Detection Challenge. This challenge is designed to engage the research community in developing advanced methods for detecting and localizing deepfake manipulations within the large-scale high-realistic audio-visual dataset. The participants can access the AV-Deepfake1M dataset and are required to submit their inference results for evaluation across the metrics for detection or localization tasks. The methodologies developed through the challenge will contribute to the development of next-generation deepfake detection and localization systems. Evaluation scripts, baseline models, and accompanying code will be available on this https URL.

Translated Abstract:
딥페이크 콘텐츠를 찾아내고 위치를 파악하는 것은 여전히 큰 도전이야. 특히 작은 가짜 영상이 진짜 비디오와 자연스럽게 섞여 있을 때 더 어려워. 

최근에 공개된 AV-Deepfake1M 데이터셋을 기반으로 해. 이 데이터셋은 2,000명이 넘는 사람의 100만 개 이상의 조작된 비디오를 포함하고 있어. 우리는 1M-Deepfakes Detection Challenge라는 대회를 소개해. 이 대회는 연구자들이 대규모 고해상도 오디오-비주얼 데이터셋에서 딥페이크 조작을 감지하고 찾기 위한 방법을 개발하도록 유도하는 거야.

참가자들은 AV-Deepfake1M 데이터셋에 접근할 수 있고, 감지나 위치 파악 작업에 대한 평가를 위해 인퍼런스 결과를 제출해야 해. 이 대회를 통해 개발된 방법론은 차세대 딥페이크 탐지 및 위치 파악 시스템의 발전에 기여할 거야. 평가 스크립트, 기준 모델, 그리고 관련 코드도 이 URL에서 제공될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07002.pdf

Title: AdvLogo: Adversarial Patch Attack against Object Detectors based on Diffusion Models

Original Abstract:
With the rapid development of deep learning, object detectors have demonstrated impressive performance; however, vulnerabilities still exist in certain scenarios. Current research exploring the vulnerabilities using adversarial patches often struggles to balance the trade-off between attack effectiveness and visual quality. To address this problem, we propose a novel framework of patch attack from semantic perspective, which we refer to as AdvLogo. Based on the hypothesis that every semantic space contains an adversarial subspace where images can cause detectors to fail in recognizing objects, we leverage the semantic understanding of the diffusion denoising process and drive the process to adversarial subareas by perturbing the latent and unconditional embeddings at the last timestep. To mitigate the distribution shift that exposes a negative impact on image quality, we apply perturbation to the latent in frequency domain with the Fourier Transform. Experimental results demonstrate that AdvLogo achieves strong attack performance while maintaining high visual quality.

Translated Abstract:
딥러닝이 빠르게 발전하면서 객체 탐지기가 인상적인 성능을 보여주고 있어. 하지만 특정 상황에서는 여전히 취약점이 존재해. 현재 연구에서는 적대적 패치를 사용해 취약점을 탐구하고 있는데, 공격 효과와 시각적 품질 사이의 균형을 맞추는 게 쉽지 않아.

이 문제를 해결하기 위해 우리는 "AdvLogo"라는 새로운 패치 공격 프레임워크를 제안해. 이건 의미론적 관점에서 접근한 거야. 모든 의미 공간에는 객체 인식에 실패하게 만드는 적대적 하위 공간이 있다는 가정에 기반하고 있어. 우리는 확산 노이즈 제거 과정의 의미적 이해를 활용하고, 마지막 시간 단계에서 잠재적이고 무조건적인 임베딩을 변형시켜 적대적 하위 영역으로 이끌어. 

이미지 품질에 부정적인 영향을 미치는 분포 변화를 완화하기 위해, 우리는 푸리에 변환을 사용해 주파수 영역에서 잠재적 변형을 적용해. 실험 결과, AdvLogo는 높은 시각적 품질을 유지하면서 강력한 공격 성능을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07003.pdf

Title: ODYSSEE: Oyster Detection Yielded by Sensor Systems on Edge Electronics

Original Abstract:
Oysters are a keystone species in coastal ecosystems, offering significant economic, environmental, and cultural benefits. However, current monitoring systems are often destructive, typically involving dredging to physically collect and count oysters. A nondestructive alternative is manual identification from video footage collected by divers, which is time-consuming and labor-intensive with expert input.
An alternative to human monitoring is the deployment of a system with trained object detection models that performs real-time, on edge oyster detection in the field. One such platform is the Aqua2 robot. Effective training of these models requires extensive high-quality data, which is difficult to obtain in marine settings. To address these complications, we introduce a novel method that leverages stable diffusion to generate high-quality synthetic data for the marine domain. We exploit diffusion models to create photorealistic marine imagery, using ControlNet inputs to ensure consistency with the segmentation ground-truth mask, the geometry of the scene, and the target domain of real underwater images for oysters. The resulting dataset is used to train a YOLOv10-based vision model, achieving a state-of-the-art 0.657 mAP@50 for oyster detection on the Aqua2 platform. The system we introduce not only improves oyster habitat monitoring, but also paves the way to autonomous surveillance for various tasks in marine contexts, improving aquaculture and conservation efforts.

Translated Abstract:
굴은 해안 생태계에서 중요한 역할을 하는 종으로, 경제적, 환경적, 문화적으로 많은 이점을 제공합니다. 하지만 현재의 감시 시스템은 보통 파괴적이며, 굴을 물리적으로 수집하고 세는 과정에서 준설을 포함합니다. 

대안으로 잠수부가 촬영한 비디오 영상을 수작업으로 식별하는 방법이 있지만, 이건 시간이 많이 걸리고 전문가의 도움이 필요해요. 

그래서 사람의 감시 대신, 현장에서 굴을 실시간으로 탐지할 수 있는 훈련된 객체 탐지 모델을 가진 시스템을 도입할 수 있습니다. 그 중 하나가 Aqua2 로봇이에요. 이러한 모델을 효과적으로 훈련시키려면 고품질의 데이터가 많이 필요하지만, 해양 환경에서는 이 데이터를 얻기 어렵습니다. 

이런 문제를 해결하기 위해, 우리는 안정적인 확산을 이용해 해양 분야에 적합한 고품질 합성 데이터를 생성하는 새로운 방법을 소개합니다. 확산 모델을 활용해 포토리얼리스틱한 해양 이미지를 만들고, ControlNet 입력을 사용해 세그멘테이션 진실 마스크와 장면의 기하학, 실제 굴의 수중 이미지와 일치하도록 합니다. 

이렇게 생성된 데이터셋은 YOLOv10 기반 비전 모델을 훈련하는 데 사용되며, Aqua2 플랫폼에서 굴 탐지에 대해 0.657 mAP@50이라는 최신 기술 수준의 성과를 달성했습니다. 우리가 소개하는 시스템은 굴 서식지 모니터링을 개선할 뿐만 아니라, 해양 환경에서 다양한 작업을 위한 자율 감시의 길을 열어주고, 양식업과 보존 노력을 향상시킵니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.07022.pdf

Title: Insight Any Instance: Promptable Instance Segmentation for Remote Sensing Images

Original Abstract:
Instance segmentation of remote sensing images (RSIs) is an essential task for a wide range of applications such as land planning and intelligent transport. Instance segmentation of RSIs is constantly plagued by the unbalanced ratio of foreground and background and limited instance size. And most of the instance segmentation models are based on deep feature learning and contain operations such as multiple downsampling, which is harmful to instance segmentation of RSIs, and thus the performance is still limited. Inspired by the recent superior performance of prompt learning in visual tasks, we propose a new prompt paradigm to address the above issues. Based on the existing instance segmentation model, firstly, a local prompt module is designed to mine local prompt information from original local tokens for specific instances; secondly, a global-to-local prompt module is designed to model the contextual information from the global tokens to the local tokens where the instances are located for specific instances. Finally, a proposal's area loss function is designed to add a decoupling dimension for proposals on the scale to better exploit the potential of the above two prompt modules. It is worth mentioning that our proposed approach can extend the instance segmentation model to a promptable instance segmentation model, i.e., to segment the instances with the specific boxes prompt. The time consumption for each promptable instance segmentation process is only 40 ms. The paper evaluates the effectiveness of our proposed approach based on several existing models in four instance segmentation datasets of RSIs, and thorough experiments prove that our proposed approach is effective for addressing the above issues and is a competitive model for instance segmentation of RSIs.

Translated Abstract:
원격 감지 이미지(RSI)의 인스턴스 분할은 토지 계획과 지능형 교통 같은 다양한 응용 분야에서 중요한 작업이야. 하지만 RSI의 인스턴스 분할은 배경과 전경의 비율이 불균형하고 인스턴스 크기가 제한적이라서 항상 어려움이 있어. 대부분의 인스턴스 분할 모델은 딥러닝 기반의 특징 학습을 사용하고, 여러 번 다운샘플링 같은 작업을 포함하고 있는데, 이게 RSI 인스턴스 분할에 해가 되어서 성능이 아직도 한계가 있어.

최근 비주얼 작업에서 프롬프트 학습이 뛰어난 성과를 내고 있는 데서 영감을 받아서, 우리는 위의 문제를 해결하기 위한 새로운 프롬프트 패러다임을 제안해. 기존의 인스턴스 분할 모델을 기반으로 해서, 먼저 특정 인스턴스를 위해 원래의 로컬 토큰에서 로컬 프롬프트 정보를 추출하는 로컬 프롬프트 모듈을 설계했어. 다음으로는, 특정 인스턴스가 위치한 로컬 토큰으로 글로벌 토큰의 맥락 정보를 모델링하는 글로벌-투-로컬 프롬프트 모듈을 설계했어. 마지막으로, 제안된 영역 손실 함수는 스케일에 대한 제안의 디커플링 차원을 추가해서 위의 두 프롬프트 모듈의 잠재력을 더 잘 활용할 수 있도록 했어.

우리의 접근법은 인스턴스 분할 모델을 프롬프트 가능한 인스턴스 분할 모델로 확장할 수 있다는 점도 중요해. 즉, 특정 박스를 프롬프트로 사용해서 인스턴스를 분할할 수 있다는 거야. 각 프롬프트 가능한 인스턴스 분할 과정의 시간 소모는 단 40ms야. 이 논문은 우리의 접근법의 효과를 네 가지 RSI 인스턴스 분할 데이터셋에서 여러 기존 모델을 바탕으로 평가했어. 실험 결과, 우리의 제안이 위의 문제를 해결하는 데 효과적이라는 것과 RSI 인스턴스 분할을 위한 경쟁력 있는 모델이라는 것을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07024.pdf

Title: SCLNet: A Scale-Robust Complementary Learning Network for Object Detection in UAV Images

Original Abstract:
Most recent UAV (Unmanned Aerial Vehicle) detectors focus primarily on general challenge such as uneven distribution and occlusion. However, the neglect of scale challenges, which encompass scale variation and small objects, continues to hinder object detection in UAV images. Although existing works propose solutions, they are implicitly modeled and have redundant steps, so detection performance remains limited. And one specific work addressing the above scale challenges can help improve the performance of UAV image detectors. Compared to natural scenes, scale challenges in UAV images happen with problems of limited perception in comprehensive scales and poor robustness to small objects. We found that complementary learning is beneficial for the detection model to address the scale challenges. Therefore, the paper introduces it to form our scale-robust complementary learning network (SCLNet) in conjunction with the object detection model. The SCLNet consists of two implementations and a cooperation method. In detail, one implementation is based on our proposed scale-complementary decoder and scale-complementary loss function to explicitly extract complementary information as complement, named comprehensive-scale complementary learning (CSCL). Another implementation is based on our proposed contrastive complement network and contrastive complement loss function to explicitly guide the learning of small objects with the rich texture detail information of the large objects, named inter-scale contrastive complementary learning (ICCL). In addition, an end-to-end cooperation (ECoop) between two implementations and with the detection model is proposed to exploit each potential.

Translated Abstract:
최근 UAV(무인 항공기) 탐지기는 주로 불균형 분포와 가림 같은 일반적인 문제에 중점을 두고 있어. 하지만, 스케일 문제, 즉 스케일 변화와 작은 물체에 대한 문제를 간과하고 있어서 UAV 이미지에서 물체 탐지에 여전히 방해가 되고 있어. 기존 연구들이 이 문제를 해결하려고 하지만, 모델링 방식이 간접적이고 불필요한 단계가 많아서 탐지 성능이 제한적이야. 

이 논문에서는 스케일 문제를 해결하는 한 가지 특정 연구가 UAV 이미지 탐지기의 성능을 향상시키는 데 도움이 된다고 이야기하고 있어. 자연 풍경과 비교했을 때, UAV 이미지에서 스케일 문제는 제한된 인식과 작은 물체에 대한 낮은 강인성으로 발생해. 우리는 보완 학습이 탐지 모델이 스케일 문제를 해결하는 데 유익하다는 것을 발견했어. 그래서 이 논문은 이를 활용해서 스케일 강인 보완 학습 네트워크(SCLNet)를 제안해, 물체 탐지 모델과 함께 사용해.

SCLNet은 두 가지 구현 방식과 협력 방법으로 구성되어 있어. 첫 번째 구현은 우리가 제안한 스케일 보완 디코더와 스케일 보완 손실 함수를 기반으로 해서 보완 정보를 명확하게 추출하는 방법이야. 이건 '종합 스케일 보완 학습(CSCL)'이라고 불려. 두 번째 구현은 우리가 제안한 대조적 보완 네트워크와 대조적 보완 손실 함수를 기반으로 해서 작은 물체의 학습을 대형 물체의 세밀한 텍스처 정보를 활용하여 유도하는 방법이야. 이건 '상호 스케일 대조적 보완 학습(ICCL)'이라고 불려. 

추가로, 두 구현 방식과 탐지 모델 간의 종단 간 협력(ECoop)을 제안해서 각각의 잠재력을 최대한 활용할 수 있도록 했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07040.pdf

Title: Retinex-RAWMamba: Bridging Demosaicing and Denoising for Low-Light RAW Image Enhancement

Original Abstract:
Low-light image enhancement, particularly in cross-domain tasks such as mapping from the raw domain to the sRGB domain, remains a significant challenge. Many deep learning-based methods have been developed to address this issue and have shown promising results in recent years. However, single-stage methods, which attempt to unify the complex mapping across both domains, leading to limited denoising performance. In contrast, two-stage approaches typically decompose a raw image with color filter arrays (CFA) into a four-channel RGGB format before feeding it into a neural network. However, this strategy overlooks the critical role of demosaicing within the Image Signal Processing (ISP) pipeline, leading to color distortions under varying lighting conditions, especially in low-light scenarios. To address these issues, we design a novel Mamba scanning mechanism, called RAWMamba, to effectively handle raw images with different CFAs. Furthermore, we present a Retinex Decomposition Module (RDM) grounded in Retinex prior, which decouples illumination from reflectance to facilitate more effective denoising and automatic non-linear exposure correction. By bridging demosaicing and denoising, better raw image enhancement is achieved. Experimental evaluations conducted on public datasets SID and MCR demonstrate that our proposed RAWMamba achieves state-of-the-art performance on cross-domain mapping.

Translated Abstract:
저조도 이미지 향상은 원시 도메인에서 sRGB 도메인으로의 매핑 같은 크로스 도메인 작업에서 여전히 큰 도전 과제야. 최근 몇 년 동안 이 문제를 해결하기 위해 많은 딥러닝 기반 방법들이 개발됐고, 꽤 괜찮은 결과를 보여줬어. 하지만 단일 단계 방법들은 두 도메인 간의 복잡한 매핑을 통합하려고 하다 보니, 노이즈 제거 성능이 제한적이야.

반면에, 두 단계 접근 방식은 원시 이미지를 색상 필터 배열(CFA)로 나눈 다음 네 개의 채널 RGGB 형식으로 변환해서 신경망에 입력해. 하지만 이 전략은 이미지 신호 처리(ISP) 파이프라인에서의 디모자이킹의 중요한 역할을 간과해서, 조명 조건이 다를 때 색상 왜곡이 발생해, 특히 저조도 상황에서 더 심해져.

이런 문제를 해결하기 위해 우리는 RAWMamba라는 새로운 맘바 스캔 메커니즘을 설계했어. 이 메커니즘은 다양한 CFA를 가진 원시 이미지를 효과적으로 다룰 수 있어. 그리고 우리는 조명과 반사를 분리해 더 효과적인 노이즈 제거와 자동 비선형 노출 보정을 가능하게 하는 Retinex Decomposition Module (RDM)을 제안해. 디모자이킹과 노이즈 제거를 연결함으로써, 더 나은 원시 이미지 향상을 이뤘어.

공공 데이터셋인 SID와 MCR에서 실시한 실험 평가 결과, 우리가 제안한 RAWMamba가 크로스 도메인 매핑에서 최첨단 성능을 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07041.pdf

Title: SoftShadow: Leveraging Penumbra-Aware Soft Masks for Shadow Removal

Original Abstract:
Recent advancements in deep learning have yielded promising results for the image shadow removal task. However, most existing methods rely on binary pre-generated shadow masks. The binary nature of such masks could potentially lead to artifacts near the boundary between shadow and non-shadow areas. In view of this, inspired by the physical model of shadow formation, we introduce novel soft shadow masks specifically designed for shadow removal. To achieve such soft masks, we propose a \textit{SoftShadow} framework by leveraging the prior knowledge of pretrained SAM and integrating physical constraints. Specifically, we jointly tune the SAM and the subsequent shadow removal network using penumbra formation constraint loss and shadow removal loss. This framework enables accurate predictions of penumbra (partially shaded regions) and umbra (fully shaded regions) areas while simultaneously facilitating end-to-end shadow removal. Through extensive experiments on popular datasets, we found that our SoftShadow framework, which generates soft masks, can better restore boundary artifacts, achieve state-of-the-art performance, and demonstrate superior generalizability.

Translated Abstract:
최근 딥러닝의 발전 덕분에 이미지 그림자 제거 작업에서 좋은 결과를 얻고 있어. 하지만 대부분의 기존 방법들은 이진으로 만들어진 그림자 마스크를 사용해. 이런 이진 마스크는 그림자와 비그림자 영역의 경계 근처에서 문제를 일으킬 수 있어.

그래서 우리는 그림자 형성의 물리적 모델에서 영감을 받아 새로운 소프트 그림자 마스크를 만들었어. 이 소프트 마스크를 만들기 위해 \textit{SoftShadow} 프레임워크를 제안하고, 사전 훈련된 SAM의 지식을 활용하고 물리적 제약조건을 통합했어. 구체적으로, 우리는 SAM과 이후의 그림자 제거 네트워크를 함께 조정하면서 반그림자 형성 제약 손실과 그림자 제거 손실을 사용해.

이 프레임워크는 반그림자(부분적으로 그늘진 지역)와 본그림자(완전히 그늘진 지역)의 정확한 예측을 가능하게 하고, 동시에 끝에서 끝까지 그림자 제거를 할 수 있어. 여러 인기 있는 데이터셋에서 실험을 진행했는데, 우리의 SoftShadow 프레임워크가 소프트 마스크를 생성함으로써 경계 문제를 더 잘 복원하고, 최신 성능을 달성하며, 뛰어난 일반화 능력을 보여주었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07048.pdf

Title: Pushing the Limits of Vision-Language Models in Remote Sensing without Human Annotations

Original Abstract:
The prominence of generalized foundation models in vision-language integration has witnessed a surge, given their multifarious applications. Within the natural domain, the procurement of vision-language datasets to construct these foundation models is facilitated by their abundant availability and the ease of web crawling. Conversely, in the remote sensing domain, although vision-language datasets exist, their volume is suboptimal for constructing robust foundation models. This study introduces an approach to curate vision-language datasets by employing an image decoding machine learning model, negating the need for human-annotated labels. Utilizing this methodology, we amassed approximately 9.6 million vision-language paired datasets in VHR imagery. The resultant model outperformed counterparts that did not leverage publicly available vision-language datasets, particularly in downstream tasks such as zero-shot classification, semantic localization, and image-text retrieval. Moreover, in tasks exclusively employing vision encoders, such as linear probing and k-NN classification, our model demonstrated superior efficacy compared to those relying on domain-specific vision-language datasets.

Translated Abstract:
일반화된 기초 모델이 비전-언어 통합에서 점점 더 중요해지고 있는 이유는 다양한 용도로 많이 쓰이기 때문이야. 자연 영역에서는 비전-언어 데이터셋을 쉽게 구할 수 있어서 이런 모델들을 만드는 데 큰 도움이 돼. 하지만 원거리 센싱 분야에서는 비전-언어 데이터셋이 있긴 하지만, 그 양이 충분하지 않아서 강력한 기초 모델을 만드는 데는 부족해.

이 연구에서는 이미지 디코딩 머신러닝 모델을 사용해서 비전-언어 데이터셋을 만드는 방법을 소개해. 이 방법은 사람이 직접 라벨을 달 필요가 없어. 이 방식을 이용해 약 960만 개의 비전-언어 쌍 데이터셋을 VHR 이미지에서 모았어. 그 결과로 만든 모델은 공개된 비전-언어 데이터셋을 이용하지 않은 다른 모델들보다 성능이 더 좋았어. 특히 제로샷 분류, 의미적 위치 파악, 이미지-텍스트 검색 같은 작업에서 두드러진 성과를 보여줬어.

게다가, 비전 인코더만 사용하는 작업인 선형 프로빙이나 k-NN 분류에서도, 우리의 모델은 특정 분야의 비전-언어 데이터셋에 의존하는 모델들보다 더 뛰어난 성과를 나타냈어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07067.pdf

Title: Edge Modeling Activation Free Fourier Network for Spacecraft Image Denoising

Original Abstract:
Spacecraft image denoising is a crucial basic technology closely related to aerospace research. However, the existing deep learning-based image denoising methods lack deep consideration of the characteristics of spacecraft image. To address the aforementioned shortcomings, we analyses spacecraft noise image and identifies two main characteristics. One is that there are a large number of low-light images in the obtained spacecraft noise image dataset. Another is there are a lot of repetitive periodic features in spacecraft image. According to the above mentioned characteristics, we propose a Edge modeling Activation Free Fourier Network (EAFFN), which is an efficient spacecraft image denoising method including Edge Modeling Block (EMB) and Activation Free Fourier Block (AFFB). We present EMB to effectively model edge and extract structural information and better identify the spacecraft components from dark regions in spacecraft noise image. We present AFFB and utilize an improved fast fourier block to extract repetitive periodic features and long-range information in noisy spacecraft image. In addition, Simple Gate is designed in our AFFB to reduce the computational complexity. Extensive experimental results demonstrate our EAFFN performs competitively to the state-of-the-art on spacecraft noise image datasets.

Translated Abstract:
우주선 이미지 노이즈 제거는 우주 연구와 밀접하게 관련된 중요한 기술이야. 하지만 기존의 딥러닝 기반 이미지 노이즈 제거 방법들은 우주선 이미지의 특성을 깊게 고려하지 못하고 있어. 

이런 문제를 해결하기 위해 우리는 우주선 노이즈 이미지 분석을 통해 두 가지 주요 특징을 찾았어. 첫 번째는 얻은 우주선 노이즈 이미지 데이터셋에 많은 저조도 이미지가 포함되어 있다는 거고, 두 번째는 우주선 이미지에서 반복적인 주기적 특성이 많다는 거야. 

이러한 특징을 바탕으로, 우리는 Edge Modeling Activation Free Fourier Network (EAFFN)라는 우주선 이미지 노이즈 제거 방법을 제안해. 이 방법은 Edge Modeling Block (EMB)과 Activation Free Fourier Block (AFFB)을 포함하고 있어. EMB는 엣지를 효과적으로 모델링하고 구조 정보를 추출해서 어두운 영역에서 우주선 구성 요소를 더 잘 파악할 수 있게 해. 

AFFB는 개선된 빠른 푸리에 블록을 사용해서 노이즈가 있는 우주선 이미지에서 반복적인 주기적 특성과 장거리 정보를 추출해. 또한, AFFB 안에 Simple Gate를 설계해서 계산 복잡성을 줄였어. 

많은 실험 결과를 보면, 우리의 EAFFN이 우주선 노이즈 이미지 데이터셋에서 최첨단 기술과 경쟁할 만한 성능을 보인다는 걸 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07078.pdf

Title: Multimodal Emotion Recognition with Vision-language Prompting and Modality Dropout

Original Abstract:
In this paper, we present our solution for the Second Multimodal Emotion Recognition Challenge Track 1(MER2024-SEMI). To enhance the accuracy and generalization performance of emotion recognition, we propose several methods for Multimodal Emotion Recognition. Firstly, we introduce EmoVCLIP, a model fine-tuned based on CLIP using vision-language prompt learning, designed for video-based emotion recognition tasks. By leveraging prompt learning on CLIP, EmoVCLIP improves the performance of pre-trained CLIP on emotional videos. Additionally, to address the issue of modality dependence in multimodal fusion, we employ modality dropout for robust information fusion. Furthermore, to aid Baichuan in better extracting emotional information, we suggest using GPT-4 as the prompt for Baichuan. Lastly, we utilize a self-training strategy to leverage unlabeled videos. In this process, we use unlabeled videos with high-confidence pseudo-labels generated by our model and incorporate them into the training set. Experimental results demonstrate that our model ranks 1st in the MER2024-SEMI track, achieving an accuracy of 90.15% on the test set.

Translated Abstract:
이번 논문에서는 제2회 다중 모달 감정 인식 챌린지 트랙 1(MER2024-SEMI)에 대한 우리의 솔루션을 소개할게. 감정 인식의 정확도와 일반화 성능을 높이기 위해 여러 가지 방법을 제안해.

먼저, 비디오 기반 감정 인식 작업을 위해 CLIP을 기반으로 한 EmoVCLIP 모델을 소개해. 이 모델은 비전-언어 프롬프트 학습을 이용해 CLIP을 미세 조정했어. 프롬프트 학습을 활용함으로써 EmoVCLIP은 감정 비디오에서 사전 훈련된 CLIP의 성능을 향상시켜.

그리고 다중 모달 융합에서 모달 의존성 문제를 해결하기 위해, 강력한 정보 융합을 위해 모달리티 드롭아웃을 사용해. 또, Baichuan이 감정 정보를 더 잘 추출할 수 있도록 GPT-4를 Baichuan의 프롬프트로 사용하는 것을 제안해.

마지막으로, 레이블이 없는 비디오를 활용하기 위해 자기 학습 전략을 사용해. 이 과정에서 우리 모델이 생성한 높은 신뢰도의 유사 레이블이 있는 레이블 없는 비디오를 훈련 세트에 포함시켜.

실험 결과, 우리 모델은 MER2024-SEMI 트랙에서 1위를 차지했고, 테스트 세트에서 90.15%의 정확도를 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07098.pdf

Title: Redundancy-Aware Camera Selection for Indoor Scene Neural Rendering

Original Abstract:
Novel view synthesis of indoor scenes can be achieved by capturing a monocular video sequence of the environment. However, redundant information caused by artificial movements in the input video data reduces the efficiency of scene modeling. In this work, we tackle this challenge from the perspective of camera selection. We begin by constructing a similarity matrix that incorporates both the spatial diversity of the cameras and the semantic variation of the images. Based on this matrix, we use the Intra-List Diversity (ILD) metric to assess camera redundancy, formulating the camera selection task as an optimization problem. Then we apply a diversity-based sampling algorithm to optimize the camera selection. We also develop a new dataset, IndoorTraj, which includes long and complex camera movements captured by humans in virtual indoor environments, closely mimicking real-world scenarios. Experimental results demonstrate that our strategy outperforms other approaches under time and memory constraints. Remarkably, our method achieves performance comparable to models trained on the full dataset, while using only an average of 15% of the frames and 75% of the allotted time.

Translated Abstract:
실내 장면의 새로운 시점을 합성하기 위해서는 환경의 단안 비디오 시퀀스를 캡처하는 방법이 있어. 하지만 입력 비디오 데이터에서 인위적인 움직임으로 인해 중복된 정보가 생기면 장면 모델링의 효율성이 떨어져. 이 문제를 해결하기 위해 우리는 카메라 선택의 관점에서 접근했어.

먼저, 카메라의 공간적 다양성과 이미지의 의미적 변화를 모두 반영하는 유사성 행렬을 만들었어. 이 행렬을 바탕으로 Intra-List Diversity (ILD) 메트릭을 사용해 카메라의 중복성을 평가하고, 카메라 선택 작업을 최적화 문제로 설정했어. 그리고 다양성 기반 샘플링 알고리즘을 적용해 카메라 선택을 최적화했지.

우리는 또한 IndoorTraj라는 새로운 데이터셋을 개발했어. 이 데이터셋은 가상 실내 환경에서 사람들이 촬영한 긴 복잡한 카메라 움직임을 포함하고 있어, 실제 상황을 잘 모방하고 있어. 실험 결과, 우리의 전략이 시간과 메모리 제약 하에서도 다른 방법들보다 뛰어난 성능을 보였어. 놀랍게도, 우리의 방법은 전체 데이터셋으로 훈련된 모델과 유사한 성능을 내면서도 평균 15%의 프레임과 75%의 할당된 시간만 사용했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07129.pdf

Title: MVLLaVA: An Intelligent Agent for Unified and Flexible Novel View Synthesis

Original Abstract:
This paper introduces MVLLaVA, an intelligent agent designed for novel view synthesis tasks. MVLLaVA integrates multiple multi-view diffusion models with a large multimodal model, LLaVA, enabling it to handle a wide range of tasks efficiently. MVLLaVA represents a versatile and unified platform that adapts to diverse input types, including a single image, a descriptive caption, or a specific change in viewing azimuth, guided by language instructions for viewpoint generation. We carefully craft task-specific instruction templates, which are subsequently used to fine-tune LLaVA. As a result, MVLLaVA acquires the capability to generate novel view images based on user instructions, demonstrating its flexibility across diverse tasks. Experiments are conducted to validate the effectiveness of MVLLaVA, demonstrating its robust performance and versatility in tackling diverse novel view synthesis challenges.

Translated Abstract:
이 논문에서는 MVLLaVA라는 새로운 뷰 합성을 위한 스마트 에이전트를 소개해. MVLLaVA는 여러 다중 뷰 확산 모델을 큰 멀티모달 모델인 LLaVA와 통합해서 다양한 작업을 효율적으로 처리할 수 있어. 

MVLLaVA는 다양한 입력 유형에 적응할 수 있는 다재다능하고 통합된 플랫폼이야. 여기에는 단일 이미지, 설명 캡션, 특정 시점 변화가 포함되고, 언어 지침에 따라 뷰포인트를 생성해. 우리는 작업에 맞는 지침 템플릿을 신중하게 만들고, 이를 사용해 LLaVA를 미세 조정해. 

그 결과, MVLLaVA는 사용자 지침에 따라 새로운 뷰 이미지를 생성할 수 있는 능력을 가지게 돼. 이건 다양한 작업에서의 유연성을 보여줘. 실험을 통해 MVLLaVA의 효과를 검증했는데, 다양한 새로운 뷰 합성 문제를 해결하는 데 강력한 성능과 다재다능함을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07172.pdf

Title: Swin-LiteMedSAM: A Lightweight Box-Based Segment Anything Model for Large-Scale Medical Image Datasets

Original Abstract:
Medical imaging is essential for the diagnosis and treatment of diseases, with medical image segmentation as a subtask receiving high attention. However, automatic medical image segmentation models are typically task-specific and struggle to handle multiple scenarios, such as different imaging modalities and regions of interest. With the introduction of the Segment Anything Model (SAM), training a universal model for various clinical scenarios has become feasible. Recently, several Medical SAM (MedSAM) methods have been proposed, but these models often rely on heavy image encoders to achieve high performance, which may not be practical for real-world applications due to their high computational demands and slow inference speed. To address this issue, a lightweight version of the MedSAM (LiteMedSAM) can provide a viable solution, achieving high performance while requiring fewer resources and less time. In this work, we introduce Swin-LiteMedSAM, a new variant of LiteMedSAM. This model integrates the tiny Swin Transformer as the image encoder, incorporates multiple types of prompts, including box-based points and scribble generated from a given bounding box, and establishes skip connections between the image encoder and the mask decoder. In the \textit{Segment Anything in Medical Images on Laptop} challenge (CVPR 2024), our approach strikes a good balance between segmentation performance and speed, demonstrating significantly improved overall results across multiple modalities compared to the LiteMedSAM baseline provided by the challenge organizers. Our proposed model achieved a DSC score of \textbf{0.8678} and an NSD score of \textbf{0.8844} on the validation set. On the final test set, it attained a DSC score of \textbf{0.8193} and an NSD score of \textbf{0.8461}, securing fourth place in the challenge.

Translated Abstract:
의료 이미지는 질병 진단과 치료에 정말 중요해. 그 중에서도 의료 이미지 분할은 많은 주목을 받고 있는 작업이야. 하지만 자동 의료 이미지 분할 모델은 보통 특정 작업에 맞춰져 있어서, 여러 다양한 상황, 예를 들어 다른 이미징 방식이나 관심 영역을 처리하는 데 어려움이 있어.

Segment Anything Model (SAM)이 등장하면서 여러 임상 시나리오에 대해 보편적인 모델을 훈련시키는 것이 가능해졌어. 최근에는 여러 Medical SAM (MedSAM) 방법들이 제안되었지만, 이 모델들은 성능을 높이기 위해 무거운 이미지 인코더에 의존하는 경우가 많아. 그래서 실제 세계에서 활용하기엔 계산 요구량이 크고 추론 속도가 느리다는 문제가 있어.

이 문제를 해결하기 위해 LiteMedSAM이라는 경량 버전이 좋은 대안이 될 수 있어. 이 모델은 적은 자원과 시간을 사용하면서도 높은 성능을 달성할 수 있어. 이번 연구에서는 LiteMedSAM의 새로운 변형인 Swin-LiteMedSAM을 소개해. 이 모델은 작은 Swin Transformer를 이미지 인코더로 사용하고, 박스 기반 포인트와 주어진 경계 상자에서 생성된 낙서 같은 여러 종류의 프롬프트를 포함해. 또, 이미지 인코더와 마스크 디코더 사이에 스킵 연결을 설정했어.

CVPR 2024에서 열린 '노트북에서 의료 이미지의 모든 것을 분할하기' 챌린지에서 우리 접근 방식은 분할 성능과 속도 사이에 좋은 균형을 이루었어. 챌린지 주최 측이 제공한 LiteMedSAM 기준 모델에 비해 여러 모드에서 전체적으로 성능이 크게 개선되었어. 우리가 제안한 모델은 검증 세트에서 DSC 점수 \textbf{0.8678}과 NSD 점수 \textbf{0.8844}를 달성했어. 최종 테스트 세트에서는 DSC 점수 \textbf{0.8193}과 NSD 점수 \textbf{0.8461}을 기록하며 4위를 차지했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07179.pdf

Title: Phy124: Fast Physics-Driven 4D Content Generation from a Single Image

Original Abstract:
4D content generation focuses on creating dynamic 3D objects that change over time. Existing methods primarily rely on pre-trained video diffusion models, utilizing sampling processes or reference videos. However, these approaches face significant challenges. Firstly, the generated 4D content often fails to adhere to real-world physics since video diffusion models do not incorporate physical priors. Secondly, the extensive sampling process and the large number of parameters in diffusion models result in exceedingly time-consuming generation processes. To address these issues, we introduce Phy124, a novel, fast, and physics-driven method for controllable 4D content generation from a single image. Phy124 integrates physical simulation directly into the 4D generation process, ensuring that the resulting 4D content adheres to natural physical laws. Phy124 also eliminates the use of diffusion models during the 4D dynamics generation phase, significantly speeding up the process. Phy124 allows for the control of 4D dynamics, including movement speed and direction, by manipulating external forces. Extensive experiments demonstrate that Phy124 generates high-fidelity 4D content with significantly reduced inference times, achieving stateof-the-art performance. The code and generated 4D content are available at the provided link: https://anonymous.4open.science/r/BBF2/.

Translated Abstract:
4D 콘텐츠 생성은 시간이 지나면서 변화하는 동적인 3D 객체를 만드는 데 초점을 맞추고 있어. 기존 방법들은 주로 미리 훈련된 비디오 확산 모델에 의존하고, 샘플링 과정이나 참조 비디오를 사용해. 하지만 이런 접근 방식은 몇 가지 큰 문제에 직면해 있어.

첫째, 생성된 4D 콘텐츠가 실제 물리 법칙을 잘 따르지 않아. 비디오 확산 모델이 물리적 사전 정보를 고려하지 않기 때문이야. 둘째, 방대한 샘플링 과정과 확산 모델의 많은 매개변수 때문에 생성 과정이 너무 느려져.

이 문제를 해결하기 위해서 우리는 Phy124라는 새로운 방법을 소개해. 이 방법은 단일 이미지에서 제어 가능한 4D 콘텐츠 생성을 빠르고 물리 기반으로 가능하게 해. Phy124는 물리 시뮬레이션을 4D 생성 과정에 직접 통합해서, 결과물인 4D 콘텐츠가 자연의 물리 법칙을 준수하도록 보장해. 그리고 4D 동작 생성 단계에서 확산 모델을 사용하지 않기 때문에, 과정이 훨씬 빨라져.

Phy124는 외부 힘을 조작해서 4D 동작, 즉 이동 속도와 방향을 제어할 수 있게 해. 다양한 실험 결과, Phy124는 고충실도의 4D 콘텐츠를 생성하면서 추론 시간을 크게 줄여서 최첨단 성능을 달성했어. 코드와 생성된 4D 콘텐츠는 제공된 링크에서 확인할 수 있어: https://anonymous.4open.science/r/BBF2/.

================================================================================

URL:
https://arxiv.org/pdf/2409.07186.pdf

Title: Enhancing Angular Resolution via Directionality Encoding and Geometric Constraints in Brain Diffusion Tensor Imaging

Original Abstract:
Diffusion-weighted imaging (DWI) is a type of Magnetic Resonance Imaging (MRI) technique sensitised to the diffusivity of water molecules, offering the capability to inspect tissue microstructures and is the only in-vivo method to reconstruct white matter fiber tracts non-invasively. The DWI signal can be analysed with the diffusion tensor imaging (DTI) model to estimate the directionality of water diffusion within voxels. Several scalar metrics, including axial diffusivity (AD), mean diffusivity (MD), radial diffusivity (RD), and fractional anisotropy (FA), can be further derived from DTI to quantitatively summarise the microstructural integrity of brain tissue. These scalar metrics have played an important role in understanding the organisation and health of brain tissue at a microscopic level in clinical studies. However, reliable DTI metrics rely on DWI acquisitions with high gradient directions, which often go beyond the commonly used clinical protocols. To enhance the utility of clinically acquired DWI and save scanning time for robust DTI analysis, this work proposes DirGeo-DTI, a deep learning-based method to estimate reliable DTI metrics even from a set of DWIs acquired with the minimum theoretical number (6) of gradient directions. DirGeo-DTI leverages directional encoding and geometric constraints to facilitate the training process. Two public DWI datasets were used for evaluation, demonstrating the effectiveness of the proposed method. Extensive experimental results show that the proposed method achieves the best performance compared to existing DTI enhancement methods and potentially reveals further clinical insights with routine clinical DWI scans.

Translated Abstract:
확산강조영상(DWI)은 물 분자의 확산성을 감지하는 MRI 기술의 일종으로, 조직의 미세 구조를 검사할 수 있게 해줘. 이 방법은 비침습적으로 백질 섬유 경로를 재구성할 수 있는 유일한 방법이야. DWI 신호는 확산 텐서 이미징(DTI) 모델로 분석해서 부피 요소 내에서 물의 확산 방향을 추정할 수 있어. 

DTI로부터는 축 방향 확산도(AD), 평균 확산도(MD), 방사 방향 확산도(RD), 분수 이방성(FA) 같은 여러 지표를 도출할 수 있어. 이 지표들은 임상 연구에서 뇌 조직의 미세 구조적 건강과 조직의 조직적 특성을 이해하는 데 중요한 역할을 해. 하지만, 신뢰할 수 있는 DTI 지표는 높은 기울기 방향으로 DWI를 얻어야 하는데, 이건 보통 임상에서 사용하는 프로토콜을 넘어서는 경우가 많아.

이 연구는 임상에서 얻은 DWI의 활용성을 높이고 강력한 DTI 분석을 위해 스캔 시간을 절약하기 위해 DirGeo-DTI라는 방법을 제안해. 이건 최소한의 이론적 기울기 방향 수(6개)로 얻은 DWI 세트에서도 신뢰할 수 있는 DTI 지표를 추정할 수 있는 딥러닝 기반 방법이야. DirGeo-DTI는 방향 인코딩과 기하학적 제약을 활용해서 학습 과정을 돕고 있어.

평가를 위해 두 개의 공개 DWI 데이터 세트를 사용했는데, 제안된 방법의 효과를 보여줬어. 실험 결과는 이 방법이 기존의 DTI 개선 방법들보다 더 나은 성능을 보이며, 임상 DWI 스캔으로부터 추가적인 임상적 통찰을 제공할 가능성이 있음을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.07200.pdf

Title: ThermalGaussian: Thermal 3D Gaussian Splatting

Original Abstract:
Thermography is especially valuable for the military and other users of surveillance cameras. Some recent methods based on Neural Radiance Fields (NeRF) are proposed to reconstruct the thermal scenes in 3D from a set of thermal and RGB images. However, unlike NeRF, 3D Gaussian splatting (3DGS) prevails due to its rapid training and real-time rendering. In this work, we propose ThermalGaussian, the first thermal 3DGS approach capable of rendering high-quality images in RGB and thermal modalities. We first calibrate the RGB camera and the thermal camera to ensure that both modalities are accurately aligned. Subsequently, we use the registered images to learn the multimodal 3D Gaussians. To prevent the overfitting of any single modality, we introduce several multimodal regularization constraints. We also develop smoothing constraints tailored to the physical characteristics of the thermal modality. Besides, we contribute a real-world dataset named RGBT-Scenes, captured by a hand-hold thermal-infrared camera, facilitating future research on thermal scene reconstruction. We conduct comprehensive experiments to show that ThermalGaussian achieves photorealistic rendering of thermal images and improves the rendering quality of RGB images. With the proposed multimodal regularization constraints, we also reduced the model's storage cost by 90\%. The code and dataset will be released.

Translated Abstract:
열화상 촬영은 군대와 감시 카메라를 사용하는 다른 분야에서 특히 유용해. 최근에 신경 방사 필드(NeRF)를 기반으로 한 방법들이 열화상과 RGB 이미지를 사용해 3D 열 장면을 재구성하는 기술이 제안됐어. 하지만 NeRF와는 달리, 3D 가우시안 스플래팅(3DGS)은 빠른 학습 속도와 실시간 렌더링 덕분에 더 많이 사용돼. 

이번 연구에서는 ThermalGaussian이라는 첫 번째 열화상 3DGS 접근법을 제안해. 이 방법은 RGB와 열 모드에서 고품질 이미지를 렌더링할 수 있어. 먼저 RGB 카메라와 열 카메라를 보정해서 두 모드가 정확하게 일치하도록 해. 그 다음, 정렬된 이미지를 사용해 다중 모드 3D 가우시안을 학습해. 특정 모드에 대한 과적합을 방지하기 위해 여러 다중 모드 정규화 제약 조건을 도입했어. 그리고 열 모드의 물리적 특성에 맞춘 스무딩 제약 조건도 개발했어. 

또한, RGBT-Scenes라는 실제 데이터셋을 기여했어. 이 데이터셋은 손에 들 수 있는 열 적외선 카메라로 캡처된 거고, 향후 열 장면 재구성을 위한 연구에 도움이 될 거야. 실험 결과, ThermalGaussian이 열화상의 포토리얼리스틱 렌더링을 달성하고 RGB 이미지의 렌더링 품질도 개선했다는 걸 보여줬어. 제안한 다중 모드 정규화 제약 덕분에 모델의 저장 비용도 90% 줄였어. 코드와 데이터셋은 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07210.pdf

Title: Enhancing CTC-Based Visual Speech Recognition

Original Abstract:
This paper presents LiteVSR2, an enhanced version of our previously introduced efficient approach to Visual Speech Recognition (VSR). Building upon our knowledge distillation framework from a pre-trained Automatic Speech Recognition (ASR) model, we introduce two key improvements: a stabilized video preprocessing technique and feature normalization in the distillation process. These improvements yield substantial performance gains on the LRS2 and LRS3 benchmarks, positioning LiteVSR2 as the current best CTC-based VSR model without increasing the volume of training data or computational resources utilized. Furthermore, we explore the scalability of our approach by examining performance metrics across varying model complexities and training data volumes. LiteVSR2 maintains the efficiency of its predecessor while significantly enhancing accuracy, thereby demonstrating the potential for resource-efficient advancements in VSR technology.

Translated Abstract:
이 논문에서는 LiteVSR2라는, 우리가 이전에 소개한 효율적인 비주얼 스피치 인식(VSR) 접근 방식을 개선한 버전을 소개해. 

우리는 미리 훈련된 자동 음성 인식(ASR) 모델에서 지식을 증류하는 프레임워크를 바탕으로 두 가지 주요 개선점을 추가했어. 첫째, 안정화된 비디오 전처리 기술과 둘째, 증류 과정에서의 특징 정규화야. 이런 개선 덕분에 LRS2와 LRS3 벤치마크에서 성능이 크게 향상되었고, LiteVSR2는 현재 최고의 CTC 기반 VSR 모델로 자리 잡았어. 이 과정에서 훈련 데이터 양이나 컴퓨팅 자원을 늘리지도 않았어.

게다가 우리는 다양한 모델 복잡성과 훈련 데이터 양에 따른 성능 지표를 살펴보면서 우리 접근 방식의 확장 가능성을 탐구했어. LiteVSR2는 이전 모델의 효율성을 유지하면서 정확성을 크게 향상시켰고, 이를 통해 VSR 기술에서 자원 효율적인 발전의 가능성을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07220.pdf

Title: Watchlist Challenge: 3rd Open-set Face Detection and Identification

Original Abstract:
In the current landscape of biometrics and surveillance, the ability to accurately recognize faces in uncontrolled settings is paramount. The Watchlist Challenge addresses this critical need by focusing on face detection and open-set identification in real-world surveillance scenarios. This paper presents a comprehensive evaluation of participating algorithms, using the enhanced UnConstrained College Students (UCCS) dataset with new evaluation protocols. In total, four participants submitted four face detection and nine open-set face recognition systems. The evaluation demonstrates that while detection capabilities are generally robust, closed-set identification performance varies significantly, with models pre-trained on large-scale datasets showing superior performance. However, open-set scenarios require further improvement, especially at higher true positive identification rates, i.e., lower thresholds.

Translated Abstract:
요즘 생체 인식과 감시 분야에서는 통제되지 않은 환경에서 얼굴을 정확하게 인식하는 게 정말 중요해. 그래서 Watchlist Challenge가 이런 필요를 해결하려고 얼굴 탐지와 개방형 식별에 집중하고 있어. 

이 논문에서는 참가 알고리즘에 대한 종합적인 평가를 제공하는데, 새로운 평가 프로토콜을 사용한 향상된 UnConstrained College Students (UCCS) 데이터셋을 활용했어. 총 네 명의 참가자가 네 개의 얼굴 탐지 시스템과 아홉 개의 개방형 얼굴 인식 시스템을 제출했어. 

평가 결과를 보면, 탐지 능력은 대체로 튼튼하지만, 폐쇄형 식별 성능은 크게 차이가 나. 대규모 데이터셋으로 미리 학습된 모델이 더 나은 성능을 보였어. 하지만 개방형 시나리오에서는, 특히 높은 진짜 긍정 식별율 즉, 낮은 임계값에서 더 개선이 필요해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07238.pdf

Title: Diff-VPS: Video Polyp Segmentation via a Multi-task Diffusion Network with Adversarial Temporal Reasoning

Original Abstract:
Diffusion Probabilistic Models have recently attracted significant attention in the community of computer vision due to their outstanding performance. However, while a substantial amount of diffusion-based research has focused on generative tasks, no work introduces diffusion models to advance the results of polyp segmentation in videos, which is frequently challenged by polyps' high camouflage and redundant temporal this http URL this paper, we present a novel diffusion-based network for video polyp segmentation task, dubbed as Diff-VPS. We incorporate multi-task supervision into diffusion models to promote the discrimination of diffusion models on pixel-by-pixel segmentation. This integrates the contextual high-level information achieved by the joint classification and detection tasks. To explore the temporal dependency, Temporal Reasoning Module (TRM) is devised via reasoning and reconstructing the target frame from the previous frames. We further equip TRM with a generative adversarial self-supervised strategy to produce more realistic frames and thus capture better dynamic cues. Extensive experiments are conducted on SUN-SEG, and the results indicate that our proposed Diff-VPS significantly achieves state-of-the-art performance. Code is available at this https URL.

Translated Abstract:
확산 확률 모델(Diffusion Probabilistic Models)이 최근 컴퓨터 비전 분야에서 큰 주목을 받고 있어. 이 모델들이 성능이 뛰어나서 그런 거야. 하지만 많은 확산 기반 연구들이 생성 작업에 집중하는 반면, 비디오에서 폴립 분할 성능을 향상시키기 위한 확산 모델을 다룬 연구는 없었어. 폴립이 잘 숨고 시간적으로 중복된 문제들이 많아서 어려운 작업이거든.

이 논문에서는 비디오 폴립 분할 작업을 위한 새로운 확산 기반 네트워크인 Diff-VPS를 소개해. 우리는 다중 작업 감독 방식을 확산 모델에 적용해서 픽셀 단위 분할에서의 차별성을 높였어. 이 방식은 분류와 탐지 작업을 함께 수행하면서 얻은 고수준의 맥락 정보를 통합해.

시간적 의존성을 탐구하기 위해 Temporal Reasoning Module (TRM)을 만들었어. 이 모듈은 이전 프레임으로부터 목표 프레임을 추리하고 재구성하는 방식이야. 그리고 TRM에 생성적 적대 자기 감독 전략을 추가해서 더 현실적인 프레임을 만들어내고, 그래서 더 나은 동적 신호를 포착할 수 있게 했어. 

SUN-SEG 데이터셋에서 광범위한 실험을 진행했는데, 결과는 우리가 제안한 Diff-VPS가 최신의 성능을 달성한다는 것을 보여줘. 코드도 제공돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.07239.pdf

Title: PiTe: Pixel-Temporal Alignment for Large Video-Language Model

Original Abstract:
Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models (LVLMs) have emerged as a pivotal advancement, bridging the gap between image and text. However, video making it challenging for LVLMs to perform adequately due to the complexity of the relationship between language and spatial-temporal data structure. Recent Large Video-Language Models (LVidLMs) align feature of static visual data like image into latent space of language feature, by general multi-modal tasks to leverage abilities of LLMs sufficiently. In this paper, we explore fine-grained alignment approach via object trajectory for different modalities across both spatial and temporal dimensions simultaneously. Thus, we propose a novel LVidLM by trajectory-guided Pixel-Temporal Alignment, dubbed PiTe, that exhibits promising applicable model property. To achieve fine-grained video-language alignment, we curate a multi-modal pre-training dataset PiTe-143k, the dataset provision of moving trajectories in pixel level for all individual objects, that appear and mention in the video and caption both, by our automatic annotation pipeline. Meanwhile, PiTe demonstrates astounding capabilities on myriad video-related multi-modal tasks through beat the state-of-the-art methods by a large margin.

Translated Abstract:
최근 대형 언어 모델(LLMs)의 발전 덕분에, 대형 비주얼-언어 모델(LVLMs)이 등장했어. 이 모델은 이미지와 텍스트 간의 간극을 메우는 중요한 발전이야. 하지만 비디오의 경우, 언어와 공간-시간 데이터 구조 간의 복잡성 때문에 LVLMs가 제대로 작동하기 어려워.

최근에 나온 대형 비디오-언어 모델(LVidLMs)은 정적인 시각 데이터인 이미지를 언어 특징의 잠재 공간에 맞추는 방법을 사용해. 이 모델은 다양한 멀티모달 작업을 일반화해서 LLMs의 능력을 충분히 활용할 수 있도록 해. 

이 논문에서는 물체의 경로를 통해 다양한 모달리티를 공간적, 시간적 차원에서 동시에 정밀하게 맞추는 방법을 탐구해. 그래서 우리는 PiTe라는 새로운 LVidLM을 제안해. 이 모델은 경로 기반의 픽셀-시간 정렬을 통해 유망한 적용 가능성을 보여줘. 

정밀한 비디오-언어 정렬을 달성하기 위해, 우리는 PiTe-143k라는 멀티모달 사전 훈련 데이터셋을 만들었어. 이 데이터셋은 비디오와 자막에 등장하는 모든 개별 물체의 픽셀 수준의 이동 경로를 제공해. 이건 우리의 자동 주석 파이프라인을 통해 만들어졌어. 

한편, PiTe는 많은 비디오 관련 멀티모달 작업에서 뛰어난 능력을 보여주며, 최신 기술보다 훨씬 더 좋은 성능을 기록하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07245.pdf

Title: Single-View 3D Reconstruction via SO(2)-Equivariant Gaussian Sculpting Networks

Original Abstract:
This paper introduces SO(2)-Equivariant Gaussian Sculpting Networks (GSNs) as an approach for SO(2)-Equivariant 3D object reconstruction from single-view image observations.
GSNs take a single observation as input to generate a Gaussian splat representation describing the observed object's geometry and texture. By using a shared feature extractor before decoding Gaussian colors, covariances, positions, and opacities, GSNs achieve extremely high throughput (>150FPS). Experiments demonstrate that GSNs can be trained efficiently using a multi-view rendering loss and are competitive, in quality, with expensive diffusion-based reconstruction algorithms. The GSN model is validated on multiple benchmark experiments. Moreover, we demonstrate the potential for GSNs to be used within a robotic manipulation pipeline for object-centric grasping.

Translated Abstract:
이 논문에서는 SO(2)-Equivariant Gaussian Sculpting Networks(이하 GSNs)를 소개해. GSNs는 단일 이미지 관찰로부터 SO(2)-Equivariant 3D 객체 재구성을 위한 방법이야.

GSNs는 하나의 관찰 이미지를 입력으로 받아서, 관찰된 객체의 기하학과 질감을 설명하는 가우시안 스플랫 표현을 생성해. 가우시안 색상, 공분산, 위치, 불투명도를 디코딩하기 전에 공유된 특징 추출기를 사용하니까, GSNs는 매우 높은 처리 속도(150FPS 이상)를 달성할 수 있어. 실험 결과, GSNs는 다중 뷰 렌더링 손실을 사용해 효율적으로 훈련될 수 있고, 비싼 확산 기반 재구성 알고리즘과 품질 면에서 경쟁력이 있어.

GSN 모델은 여러 벤치마크 실험에서 검증되었고, 또한 GSNs가 객체 중심의 그랩을 위한 로봇 조작 파이프라인에서도 사용될 가능성을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.07255.pdf

Title: EMOdiffhead: Continuously Emotional Control in Talking Head Generation via Diffusion

Original Abstract:
The task of audio-driven portrait animation involves generating a talking head video using an identity image and an audio track of speech. While many existing approaches focus on lip synchronization and video quality, few tackle the challenge of generating emotion-driven talking head videos. The ability to control and edit emotions is essential for producing expressive and realistic animations. In response to this challenge, we propose EMOdiffhead, a novel method for emotional talking head video generation that not only enables fine-grained control of emotion categories and intensities but also enables one-shot generation. Given the FLAME 3D model's linearity in expression modeling, we utilize the DECA method to extract expression vectors, that are combined with audio to guide a diffusion model in generating videos with precise lip synchronization and rich emotional expressiveness. This approach not only enables the learning of rich facial information from emotion-irrelevant data but also facilitates the generation of emotional videos. It effectively overcomes the limitations of emotional data, such as the lack of diversity in facial and background information, and addresses the absence of emotional details in emotion-irrelevant data. Extensive experiments and user studies demonstrate that our approach achieves state-of-the-art performance compared to other emotion portrait animation methods.

Translated Abstract:
오디오 기반 초상화 애니메이션 작업은 신원 이미지와 음성 트랙을 사용해 말하는 얼굴 비디오를 만드는 거야. 기존의 많은 방법들이 입술 동기화나 비디오 품질에 집중하고 있지만, 감정을 반영한 말하는 얼굴 비디오를 만드는 문제는 거의 다루어지지 않았어. 감정을 조절하고 편집하는 능력은 표현력 있고 사실적인 애니메이션을 만드는 데 필수적이야.

이런 문제에 대응하기 위해 우리는 EMOdiffhead라는 새로운 방법을 제안해. 이 방법은 감정 카테고리와 강도를 세밀하게 조절할 수 있을 뿐만 아니라, 한 번의 시도로도 비디오를 생성할 수 있어. FLAME 3D 모델의 표현 모델링의 선형성을 활용해서 DECA 방법을 사용해 표현 벡터를 추출하고, 이 벡터를 오디오와 결합해 확산 모델을 통해 정확한 입술 동기화와 풍부한 감정 표현을 가진 비디오를 생성해.

이 접근법은 감정과 관련 없는 데이터에서 풍부한 얼굴 정보를 학습할 수 있게 해주고, 감정 비디오 생성도 가능하게 해. 얼굴과 배경 정보의 다양성이 부족한 감정 데이터의 한계를 극복하고, 감정과 관련 없는 데이터에서 감정적인 세부 정보가 부족한 문제도 해결해. 여러 실험과 사용자 연구를 통해 이 방법이 다른 감정 초상화 애니메이션 방법들과 비교했을 때 최첨단 성능을 달성했다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07256.pdf

Title: MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing

Original Abstract:
With the rapid advancements in multimodal generative technology, Affective Computing research has provoked discussion about the potential consequences of AI systems equipped with emotional intelligence. Affective Computing involves the design, evaluation, and implementation of Emotion AI and related technologies aimed at improving people's lives. Designing a computational model in affective computing requires vast amounts of multimodal data, including RGB images, video, audio, text, and physiological signals. Moreover, Affective Computing research is deeply engaged with ethical considerations at various stages-from training emotionally intelligent models on large-scale human data to deploying these models in specific applications. Fundamentally, the development of any AI system must prioritize its impact on humans, aiming to augment and enhance human abilities rather than replace them, while drawing inspiration from human intelligence in a safe and responsible manner. The MRAC 2024 Track 1 workshop seeks to extend these principles from controlled, small-scale lab environments to real-world, large-scale contexts, emphasizing responsible development. The workshop also aims to highlight the potential implications of generative technology, along with the ethical consequences of its use, to researchers and industry professionals. To the best of our knowledge, this is the first workshop series to comprehensively address the full spectrum of multimodal, generative affective computing from a responsible AI perspective, and this is the second iteration of this workshop. Webpage: this https URL

Translated Abstract:
빠르게 발전하는 다중 모달 생성 기술 덕분에, 감정 컴퓨팅 연구가 감정 지능을 가진 AI 시스템의 잠재적 결과에 대한 논의를 촉발하고 있어. 감정 컴퓨팅은 사람들의 삶을 개선하기 위한 감정 AI와 관련 기술의 설계, 평가, 구현을 포함해. 감정 컴퓨팅에서 컴퓨터 모델을 설계하려면 RGB 이미지, 비디오, 오디오, 텍스트, 생리적 신호 같은 다양한 다중 모달 데이터가 필요해.

게다가 감정 컴퓨팅 연구는 감정 지능 모델을 대규모 인간 데이터를 기반으로 훈련하고, 이 모델을 특정 응용 프로그램에 배포하는 과정에서 윤리적 고려와 깊이 연관되어 있어. 기본적으로 어떤 AI 시스템의 개발은 인간에게 미치는 영향을 우선시해야 하고, 인간의 능력을 대체하기보다는 보완하고 향상시키는 방향으로 나아가야 해. 이 과정에서 인간 지능에서 영감을 얻고, 안전하고 책임감 있는 방식으로 접근해야 해.

MRAC 2024 Track 1 워크숍은 이러한 원칙을 통제된 소규모 실험실 환경에서 실제 대규모 맥락으로 확장하려고 해, 책임감 있는 개발을 강조하면서. 이 워크숍은 또한 생성 기술의 잠재적 함의와 그 사용에 따른 윤리적 결과를 연구자와 산업 전문가들에게 알리는 것을 목표로 하고 있어. 우리가 아는 한, 이 워크숍 시리즈는 책임 있는 AI 관점에서 다중 모달 생성 감정 컴퓨팅의 전체 범위를 포괄적으로 다루는 첫 번째 워크숍이야, 그리고 이번이 두 번째 iteration이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07267.pdf

Title: MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving

Original Abstract:
Vision-language models (VLMs) serve as general-purpose end-to-end models in autonomous driving, performing subtasks such as prediction, planning, and perception through question-and-answer interactions. However, most existing methods rely on computationally expensive visual encoders and large language models (LLMs), making them difficult to deploy in real-world scenarios and real-time applications. Meanwhile, most existing VLMs lack the ability to process multiple images, making it difficult to adapt to multi-camera perception in autonomous driving. To address these issues, we propose a novel framework called MiniDrive, which incorporates our proposed Feature Engineering Mixture of Experts (FE-MoE) module and Dynamic Instruction Adapter (DI-Adapter). The FE-MoE effectively maps 2D features into visual token embeddings before being input into the language model. The DI-Adapter enables the visual token embeddings to dynamically change with the instruction text embeddings, resolving the issue of static visual token embeddings for the same image in previous approaches. Compared to previous works, MiniDrive achieves state-of-the-art performance in terms of parameter size, floating point operations, and response efficiency, with the smallest version containing only 83M parameters.

Translated Abstract:
비전-언어 모델(VLMs)은 자율주행에서 예측, 계획, 인식 같은 여러 작업을 질문과 답변 형식으로 수행하는 범용 모델이야. 하지만 기존의 많은 방법들은 계산 비용이 많이 드는 시각 인코더와 대형 언어 모델(LLMs)에 의존하고 있어서 실제 상황이나 실시간 애플리케이션에 적용하기가 어려워.

게다가, 대부분의 기존 VLM은 여러 이미지를 처리하는 능력이 부족해서 자율주행에서 다중 카메라 인식에 적응하기 힘들어. 이런 문제를 해결하기 위해, 우리는 MiniDrive라는 새로운 프레임워크를 제안해. 이 프레임워크는 우리가 제안한 특징 엔지니어링 전문가 혼합(FE-MoE) 모듈과 동적 지침 어댑터(DI-Adapter)를 포함하고 있어.

FE-MoE는 2D 특징을 언어 모델에 입력하기 전에 시각 토큰 임베딩으로 효과적으로 변환해. DI-Adapter는 시각 토큰 임베딩이 지침 텍스트 임베딩에 따라 동적으로 변화하도록 해줘. 이로 인해 이전 접근 방식에서 같은 이미지에 대한 정적인 시각 토큰 임베딩 문제를 해결할 수 있어.

MiniDrive는 이전 연구들에 비해 파라미터 크기, 부동 소수점 연산, 응답 효율성 면에서 최첨단 성능을 보여줘. 가장 작은 버전은 단 83M 파라미터만 가지고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07269.pdf

Title: Realistic and Efficient Face Swapping: A Unified Approach with Diffusion Models

Original Abstract:
Despite promising progress in face swapping task, realistic swapped images remain elusive, often marred by artifacts, particularly in scenarios involving high pose variation, color differences, and occlusion. To address these issues, we propose a novel approach that better harnesses diffusion models for face-swapping by making following core contributions. (a) We propose to re-frame the face-swapping task as a self-supervised, train-time inpainting problem, enhancing the identity transfer while blending with the target image. (b) We introduce a multi-step Denoising Diffusion Implicit Model (DDIM) sampling during training, reinforcing identity and perceptual similarities. (c) Third, we introduce CLIP feature disentanglement to extract pose, expression, and lighting information from the target image, improving fidelity. (d) Further, we introduce a mask shuffling technique during inpainting training, which allows us to create a so-called universal model for swapping, with an additional feature of head swapping. Ours can swap hair and even accessories, beyond traditional face swapping. Unlike prior works reliant on multiple off-the-shelf models, ours is a relatively unified approach and so it is resilient to errors in other off-the-shelf models. Extensive experiments on FFHQ and CelebA datasets validate the efficacy and robustness of our approach, showcasing high-fidelity, realistic face-swapping with minimal inference time. Our code is available at this https URL.

Translated Abstract:
얼굴 교환 작업에서 좋은 발전이 있었지만, 실제로 바꾼 이미지는 여전히 어려운 문제야. 특히 자세가 많이 변하거나 색깔 차이, 가려짐 같은 상황에서는 더 그렇지. 이런 문제를 해결하기 위해, 우리는 얼굴 교환을 더 잘 활용할 수 있는 새로운 접근 방식을 제안해. 

첫 번째로, 얼굴 교환 작업을 자기 지도 학습 방식으로 재구성했어. 이건 대상을 잘 섞으면서도 정체성을 잘 전달할 수 있게 해. 

두 번째로, 훈련 중에 다단계 Denoising Diffusion Implicit Model(DDIM) 샘플링을 도입해서 정체성과 인식 유사성을 강화했어. 

세 번째로, CLIP 특징 분리를 통해 대상 이미지에서 자세, 표정, 조명 정보를 추출해 신뢰성을 높였어. 

네 번째로, 인페인팅 훈련 중에 마스크 셔플링 기법을 도입해서, 머리 교환 기능도 추가된 범용 모델을 만들 수 있었어. 이제 우리는 전통적인 얼굴 교환을 넘어서 머리카락이나 액세서리도 교환할 수 있어. 

기존 연구들은 여러 개의 모델에 의존했지만, 우리의 방법은 비교적 통합된 접근 방식이라 다른 모델의 오류에도 강해. FFHQ와 CelebA 데이터셋에서 여러 실험을 통해 우리의 방법의 효과와 강건성을 입증했어. 결과적으로 높은 신뢰도의 실제적인 얼굴 교환이 가능했고, 추론 시간도 최소화했어. 코드도 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07271.pdf

Title: CCFExp: Facial Image Synthesis with Cycle Cross-Fusion Diffusion Model for Facial Paralysis Individuals

Original Abstract:
Facial paralysis is a debilitating condition that affects the movement of facial muscles, leading to a significant loss of facial expressions. Currently, the diagnosis of facial paralysis remains a challenging task, often relying heavily on the subjective judgment and experience of clinicians, which can introduce variability and uncertainty in the assessment process. One promising application in real-life situations is the automatic estimation of facial paralysis. However, the scarcity of facial paralysis datasets limits the development of robust machine learning models for automated diagnosis and therapeutic interventions. To this end, this study aims to synthesize a high-quality facial paralysis dataset to address this gap, enabling more accurate and efficient algorithm training. Specifically, a novel Cycle Cross-Fusion Expression Generative Model (CCFExp) based on the diffusion model is proposed to combine different features of facial information and enhance the visual details of facial appearance and texture in facial regions, thus creating synthetic facial images that accurately represent various degrees and types of facial paralysis. We have qualitatively and quantitatively evaluated the proposed method on the commonly used public clinical datasets of facial paralysis to demonstrate its effectiveness. Experimental results indicate that the proposed method surpasses state-of-the-art methods, generating more realistic facial images and maintaining identity consistency.

Translated Abstract:
안면 마비는 얼굴 근육의 움직임에 영향을 주는 힘든 상태로, 얼굴 표정의 큰 손실을 가져와. 현재 안면 마비 진단은 꽤 어려운 작업인데, 주로 의사들의 주관적인 판단과 경험에 의존해서 평가 과정에서 변동성과 불확실성이 생겨. 실제 상황에서 유망한 응용 중 하나는 안면 마비의 자동 추정이야. 하지만, 안면 마비 데이터셋이 부족해서 자동 진단과 치료 개입을 위한 강력한 머신러닝 모델 개발이 제한되고 있어.

그래서 이 연구는 이런 문제를 해결하기 위해 고품질의 안면 마비 데이터셋을 합성하는 것을 목표로 해. 이 데이터셋은 더 정확하고 효율적인 알고리즘 훈련을 가능하게 해. 구체적으로는, 확산 모델을 기반으로 한 새로운 Cycle Cross-Fusion Expression Generative Model (CCFExp)을 제안해. 이 모델은 얼굴 정보의 다양한 특성을 결합하고 얼굴의 외관과 질감의 시각적 세부 사항을 향상시켜서, 다양한 정도와 유형의 안면 마비를 정확히 나타내는 합성 얼굴 이미지를 만들어.

우리는 제안한 방법을 일반적으로 사용되는 공개 임상 데이터셋에 대해 정성적 및 정량적으로 평가해서 효과를 입증했어. 실험 결과는 제안한 방법이 최신 기술을 초월하여 더 현실적인 얼굴 이미지를 생성하고, 정체성 일관성을 유지한다는 것을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.07284.pdf

Title: TLD-READY: Traffic Light Detection -- Relevance Estimation and Deployment Analysis

Original Abstract:
Effective traffic light detection is a critical component of the perception stack in autonomous vehicles. This work introduces a novel deep-learning detection system while addressing the challenges of previous work. Utilizing a comprehensive dataset amalgamation, including the Bosch Small Traffic Lights Dataset, LISA, the DriveU Traffic Light Dataset, and a proprietary dataset from Karlsruhe, we ensure a robust evaluation across varied scenarios. Furthermore, we propose a relevance estimation system that innovatively uses directional arrow markings on the road, eliminating the need for prior map creation. On the DriveU dataset, this approach results in 96% accuracy in relevance estimation. Finally, a real-world evaluation is performed to evaluate the deployment and generalizing abilities of these models. For reproducibility and to facilitate further research, we provide the model weights and code: this https URL.

Translated Abstract:
자율주행차의 인식 시스템에서 신호등을 제대로 감지하는 건 정말 중요해. 이 연구는 이전의 문제들을 해결하면서 새로운 딥러닝 기반 감지 시스템을 소개해. 

우리는 Bosch Small Traffic Lights Dataset, LISA, DriveU Traffic Light Dataset, 그리고 Karlsruhe의 독점 데이터셋을 포함한 다양한 데이터셋을 활용해서 여러 상황에서 강력한 평가를 할 수 있도록 했어. 

게다가, 도로 위의 방향 화살표 마크를 활용한 관련성 추정 시스템도 제안해. 이 방법은 기존에 지도를 만들 필요가 없어. DriveU 데이터셋에서는 이 접근법이 96%의 정확도로 관련성을 추정할 수 있게 해줬어. 

마지막으로, 이 모델들이 실제 환경에서 어떻게 잘 작동하는지 평가하기 위한 실험도 진행했어. 다른 연구자들이 쉽게 재현할 수 있도록 모델 가중치와 코드를 제공할 예정이야: 이 https URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.07295.pdf

Title: PaveSAM Segment Anything for Pavement Distress

Original Abstract:
Automated pavement monitoring using computer vision can analyze pavement conditions more efficiently and accurately than manual methods. Accurate segmentation is essential for quantifying the severity and extent of pavement defects and consequently, the overall condition index used for prioritizing rehabilitation and maintenance activities. Deep learning-based segmentation models are however, often supervised and require pixel-level annotations, which can be costly and time-consuming. While the recent evolution of zero-shot segmentation models can generate pixel-wise labels for unseen classes without any training data, they struggle with irregularities of cracks and textured pavement backgrounds. This research proposes a zero-shot segmentation model, PaveSAM, that can segment pavement distresses using bounding box prompts. By retraining SAM's mask decoder with just 180 images, pavement distress segmentation is revolutionized, enabling efficient distress segmentation using bounding box prompts, a capability not found in current segmentation models. This not only drastically reduces labeling efforts and costs but also showcases our model's high performance with minimal input, establishing the pioneering use of SAM in pavement distress segmentation. Furthermore, researchers can use existing open-source pavement distress images annotated with bounding boxes to create segmentation masks, which increases the availability and diversity of segmentation pavement distress datasets.

Translated Abstract:
자동화된 포장 상태 모니터링은 컴퓨터 비전을 활용해 수동 방법보다 더 효율적이고 정확하게 도로 상태를 분석할 수 있어. 정확한 세분화는 포장 결함의 심각성과 범위를 파악하는 데 필수적이고, 이 정보는 재활 및 유지보수 활동의 우선순위를 정하는 데 중요해. 

하지만 딥러닝 기반의 세분화 모델은 일반적으로 감독 학습을 필요로 하고, 픽셀 수준의 주석이 필요해. 이 과정은 비용이 많이 들고 시간이 오래 걸리지. 최근에는 제로샷 세분화 모델이 등장했는데, 이 모델은 훈련 데이터 없이도 보지 못한 클래스에 대한 픽셀 단위의 레이블을 생성할 수 있어. 하지만 이 모델은 균일하지 않은 균열이나 질감이 있는 포장 배경에서는 성능이 떨어져.

이 연구에서는 PaveSAM이라는 제로샷 세분화 모델을 제안해. 이 모델은 바운딩 박스 프롬프트를 사용해서 포장 결함을 세분화할 수 있어. 단 180장의 이미지로 SAM의 마스크 디코더를 재훈련함으로써, 포장 결함 세분화가 혁신적으로 변화했어. 이제 바운딩 박스 프롬프트를 이용해서 효율적으로 결함 세분화를 할 수 있게 되었고, 이는 기존 세분화 모델에선 찾아볼 수 없는 기능이야. 

이렇게 하면 레이블 작업의 노력과 비용이 대폭 줄어들고, 최소한의 입력으로도 높은 성능을 보여줘. 이는 SAM을 포장 결함 세분화에 처음으로 사용하는 사례를 만들었어. 게다가 연구자들은 바운딩 박스로 주석이 달린 기존의 오픈소스 포장 결함 이미지를 사용해서 세분화 마스크를 만들 수 있으니, 포장 결함 데이터셋의 가용성과 다양성을 높이는 데도 도움이 돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.07307.pdf

Title: Data Augmentation via Latent Diffusion for Saliency Prediction

Original Abstract:
Saliency prediction models are constrained by the limited diversity and quantity of labeled data. Standard data augmentation techniques such as rotating and cropping alter scene composition, affecting saliency. We propose a novel data augmentation method for deep saliency prediction that edits natural images while preserving the complexity and variability of real-world scenes. Since saliency depends on high-level and low-level features, our approach involves learning both by incorporating photometric and semantic attributes such as color, contrast, brightness, and class. To that end, we introduce a saliency-guided cross-attention mechanism that enables targeted edits on the photometric properties, thereby enhancing saliency within specific image regions. Experimental results show that our data augmentation method consistently improves the performance of various saliency models. Moreover, leveraging the augmentation features for saliency prediction yields superior performance on publicly available saliency benchmarks. Our predictions align closely with human visual attention patterns in the edited images, as validated by a user study.

Translated Abstract:
주목도 예측 모델은 라벨이 붙은 데이터의 다양성과 양이 제한되어 있어서 어려움이 있어. 일반적인 데이터 증강 기법인 회전이나 크롭은 장면 구성을 바꿔서 주목도에 영향을 미치지. 그래서 우리는 실제 세계의 복잡성과 다양성을 유지하면서 자연 이미지를 편집하는 새로운 데이터 증강 방법을 제안해.

주목도는 고수준과 저수준 특성에 따라 달라지기 때문에, 우리는 색상, 대비, 밝기, 클래스 같은 광학적 및 의미적 속성을 포함해서 둘 다 배우는 방식을 사용해. 이를 위해 우리는 주목도에 기반한 크로스-어텐션 메커니즘을 도입했어. 이 메커니즘은 광학적 속성에 대해 목표 지향적인 편집을 가능하게 해서 특정 이미지 영역에서 주목도를 높여줘.

실험 결과, 우리의 데이터 증강 방법이 다양한 주목도 모델의 성능을 일관되게 향상시키는 걸 보여줬어. 게다가, 이 증강 기능을 활용한 주목도 예측은 공개된 주목도 벤치마크에서 더 뛰어난 성능을 발휘했어. 우리의 예측은 편집된 이미지에서 인간의 시각적 주의 패턴과 밀접하게 일치한다는 걸 사용자 연구로 검증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07321.pdf

Title: Module-wise Adaptive Adversarial Training for End-to-end Autonomous Driving

Original Abstract:
Recent advances in deep learning have markedly improved autonomous driving (AD) models, particularly end-to-end systems that integrate perception, prediction, and planning stages, achieving state-of-the-art performance. However, these models remain vulnerable to adversarial attacks, where human-imperceptible perturbations can disrupt decision-making processes. While adversarial training is an effective method for enhancing model robustness against such attacks, no prior studies have focused on its application to end-to-end AD models. In this paper, we take the first step in adversarial training for end-to-end AD models and present a novel Module-wise Adaptive Adversarial Training (MA2T). However, extending conventional adversarial training to this context is highly non-trivial, as different stages within the model have distinct objectives and are strongly interconnected. To address these challenges, MA2T first introduces Module-wise Noise Injection, which injects noise before the input of different modules, targeting training models with the guidance of overall objectives rather than each independent module loss. Additionally, we introduce Dynamic Weight Accumulation Adaptation, which incorporates accumulated weight changes to adaptively learn and adjust the loss weights of each module based on their contributions (accumulated reduction rates) for better balance and robust training. To demonstrate the efficacy of our defense, we conduct extensive experiments on the widely-used nuScenes dataset across several end-to-end AD models under both white-box and black-box attacks, where our method outperforms other baselines by large margins (+5-10%). Moreover, we validate the robustness of our defense through closed-loop evaluation in the CARLA simulation environment, showing improved resilience even against natural corruption.

Translated Abstract:
최근 딥러닝의 발전 덕분에 자율주행(AD) 모델이 크게 향상됐어. 특히, 인지, 예측, 계획 단계를 통합한 엔드 투 엔드 시스템들이 최첨단 성능을 달성하고 있어. 하지만 이런 모델들은 적대적 공격에 취약해. 인간이 알아차리지 못하는 작은 변화가 의사결정 과정을 방해할 수 있거든.

적대적 훈련은 이런 공격에 대한 모델의 강인함을 높이는 좋은 방법인데, 지금까지 엔드 투 엔드 AD 모델에 적용된 연구는 없어. 그래서 이 논문에서는 엔드 투 엔드 AD 모델을 위한 적대적 훈련의 첫걸음을 내딛고, 새로운 방법인 모듈-wise 적응형 적대적 훈련(MA2T)을 소개해.

하지만 기존의 적대적 훈련을 이런 맥락에 적용하는 건 쉽지 않아. 모델의 각 단계가 서로 다른 목표를 가지고 있고, 강하게 연결돼 있기 때문이야. 그래서 MA2T는 먼저 모듈-wise 노이즈 주입을 도입해. 이 방법은 다양한 모듈의 입력 전에 노이즈를 주입해서, 각 모듈의 손실이 아니라 전체 목표를 기준으로 모델을 훈련하도록 돕는 거야.

또한, 동적 가중치 누적 적응법을 도입해서, 각 모듈의 기여도(누적 감소율)에 따라 손실 가중치를 적응적으로 배우고 조정할 수 있어. 이렇게 해서 더 균형 잡히고 강인한 훈련이 가능해.

우리 방어 방법의 효과를 보여주기 위해, 다양한 엔드 투 엔드 AD 모델에 대해 널리 사용되는 nuScenes 데이터셋에서 화이트박스와 블랙박스 공격 아래에서 광범위한 실험을 했어. 그 결과, 우리 방법이 다른 기준선보다 훨씬 더 높은 성능을 보였어(+5-10%). 더불어, CARLA 시뮬레이션 환경에서 폐쇄 루프 평가를 통해 방어의 강인성을 검증했어. 자연적인 변형에 대해서도 개선된 저항성을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07327.pdf

Title: Current Symmetry Group Equivariant Convolution Frameworks for Representation Learning

Original Abstract:
Euclidean deep learning is often inadequate for addressing real-world signals where the representation space is irregular and curved with complex topologies. Interpreting the geometric properties of such feature spaces has become paramount in obtaining robust and compact feature representations that remain unaffected by nontrivial geometric transformations, which vanilla CNNs cannot effectively handle. Recognizing rotation, translation, permutation, or scale symmetries can lead to equivariance properties in the learned representations. This has led to notable advancements in computer vision and machine learning tasks under the framework of geometric deep learning, as compared to their invariant counterparts. In this report, we emphasize the importance of symmetry group equivariant deep learning models and their realization of convolution-like operations on graphs, 3D shapes, and non-Euclidean spaces by leveraging group theory and symmetry. We categorize them as regular, steerable, and PDE-based convolutions and thoroughly examine the inherent symmetries of their input spaces and ensuing representations. We also outline the mathematical link between group convolutions or message aggregation operations and the concept of equivariance. The report also highlights various datasets, their application scopes, limitations, and insightful observations on future directions to serve as a valuable reference and stimulate further research in this emerging discipline.

Translated Abstract:
유클리드 딥러닝은 실제 신호를 다루는데 부족할 때가 많아. 왜냐하면 실제 신호는 불규칙하고 복잡한 형태를 가지거든. 이런 특성 공간의 기하학적 속성을 이해하는 게 중요해. 그래야 비틀림 같은 복잡한 기하학적 변환에도 영향을 받지 않는 강력하고 간결한 특징 표현을 얻을 수 있어. 일반적인 CNN은 이런 걸 잘 처리하지 못해.

회전, 이동, 순열, 스케일 대칭을 인식하면 학습된 표현에서 동등성이 생길 수 있어. 이런 점 때문에 기하학적 딥러닝 프레임워크 아래에서 컴퓨터 비전과 머신러닝 작업에 큰 발전이 있었어. 일반적인 불변 표현과 비교할 때 말이야.

이 보고서에서는 대칭 그룹 동등한 딥러닝 모델의 중요성을 강조해. 그리고 그룹 이론과 대칭을 활용해서 그래프, 3D 형태, 비유클리드 공간에서 컨볼루션 같은 작업을 어떻게 구현하는지를 설명해. 우리는 이를 정규, 조향 가능, PDE 기반 컨볼루션으로 분류하고, 입력 공간과 생성된 표현의 고유한 대칭성을 자세히 살펴봐.

또한, 그룹 컨볼루션이나 메시지 집계 작업과 동등성 개념 간의 수학적 연결도 설명할 거야. 이 보고서는 다양한 데이터셋, 그 적용 범위, 한계점, 그리고 앞으로의 방향에 대한 통찰도 담고 있어서 이 새로운 분야를 연구하는 데 유용한 참고자료가 될 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07331.pdf

Title: Learning to Compress Contexts for Efficient Knowledge-based Visual Question Answering

Original Abstract:
Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot performance on visual question answering (VQA). However, when it comes to knowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized domain knowledge to answer such questions and require obtaining necessary information from external knowledge sources. Previous works like Retrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input information, such as image-based textual descriptions and retrieved knowledge, as possible to improve performance, but they all overlook the issue that with the number of input tokens increasing, inference efficiency significantly decreases, which contradicts the demands of practical applications. To address this issue, we propose Retrieval-Augmented MLLM with Compressed Contexts (RACC). RACC learns to compress and aggregate retrieved contexts, from which it generates a compact modulation in the form of Key-Value (KV) cache. This modulation is then used to adapt the downstream frozen MLLM, thereby achieving effective and efficient inference. RACC achieves a state-of-the-art (SOTA) performance of 62.9% on OK-VQA. Moreover, it significantly reduces inference latency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments show RACC's broad applicability. It is compatible with various off-the-shelf MLLMs and can also handle different knowledge sources including textual and multimodal documents.

Translated Abstract:
멀티모달 대형 언어 모델(MLLMs)은 시각적 질문 응답(VQA)에서 매우 뛰어난 제로샷 성능을 보여줬어. 하지만 지식 기반 VQA(KB-VQA)에서는 MLLMs가 인간의 상식이나 특정 분야의 지식을 부족하게 가질 수 있어서, 이런 질문에 답하기 위해 외부 지식 소스에서 필요한 정보를 얻어야 해.

이전 연구들인 Retrieval-Augmented VQA-v2(RAVQA-v2)는 이미지 기반의 텍스트 설명과 검색된 지식 같은 입력 정보를 최대한 활용해서 성능을 향상시키는 데 집중했지만, 입력 토큰 수가 증가하면서 추론 효율성이 크게 떨어지는 문제를 간과했어. 이건 실제 응용의 요구와는 반대되는 상황이야.

이 문제를 해결하기 위해 우리는 Retrieval-Augmented MLLM with Compressed Contexts(RACC)를 제안해. RACC는 검색된 맥락을 압축하고 집계하는 방법을 배우고, 이로부터 Key-Value(KV) 캐시 형태의 간결한 조정을 생성해. 이 조정은 이후 다운스트림 얼어붙은 MLLM에 적응하는 데 사용돼서 효과적이고 효율적인 추론을 가능하게 해.

RACC는 OK-VQA에서 62.9%의 최첨단(SOTA) 성능을 달성했어. 게다가, 유명한 RAVQA-v2와 비교했을 때 추론 지연 시간을 22.0%-59.7%나 줄여줬어. 다양한 실험 결과들이 RACC의 광범위한 적용 가능성을 보여주고 있어. RACC는 여러 기존 MLLM과 호환되고, 텍스트와 멀티모달 문서 같은 다양한 지식 소스도 처리할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07337.pdf

Title: Benchmarking 2D Egocentric Hand Pose Datasets

Original Abstract:
Hand pose estimation from egocentric video has broad implications across various domains, including human-computer interaction, assistive technologies, activity recognition, and robotics, making it a topic of significant research interest. The efficacy of modern machine learning models depends on the quality of data used for their training. Thus, this work is devoted to the analysis of state-of-the-art egocentric datasets suitable for 2D hand pose estimation. We propose a novel protocol for dataset evaluation, which encompasses not only the analysis of stated dataset characteristics and assessment of data quality, but also the identification of dataset shortcomings through the evaluation of state-of-the-art hand pose estimation models. Our study reveals that despite the availability of numerous egocentric databases intended for 2D hand pose estimation, the majority are tailored for specific use cases. There is no ideal benchmark dataset yet; however, H2O and GANerated Hands datasets emerge as the most promising real and synthetic datasets, respectively.

Translated Abstract:
자기중심 비디오에서 손 자세 추정은 인간-컴퓨터 상호작용, 보조 기술, 활동 인식, 로봇 공학 등 여러 분야에 큰 영향을 미쳐. 그래서 이건 연구자들이 관심을 가지는 주제야. 

현대 머신러닝 모델의 효과는 훈련에 사용되는 데이터의 품질에 달려 있어. 그래서 이 연구는 2D 손 자세 추정에 적합한 최신 자기중심 데이터셋을 분석하는 데 집중해. 우리는 데이터셋 평가를 위한 새로운 프로토콜을 제안해. 이 프로토콜은 데이터셋의 특성과 품질을 분석하는 것뿐만 아니라, 최신 손 자세 추정 모델을 평가해서 데이터셋의 단점을 파악하는 것도 포함해. 

우리 연구 결과에 따르면, 2D 손 자세 추정을 위한 여러 자기중심 데이터베이스가 있지만, 대부분은 특정 용도에 맞춰져 있어. 아직 완벽한 기준 데이터셋은 없지만, H2O와 GANerated Hands 데이터셋이 각각 가장 유망한 실제 및 합성 데이터셋으로 떠오르고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07353.pdf

Title: Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks

Original Abstract:
Large Vision-Language Models (LVLMs), trained on multimodal big datasets, have significantly advanced AI by excelling in vision-language tasks. However, these models remain vulnerable to adversarial attacks, particularly jailbreak attacks, which bypass safety protocols and cause the model to generate misleading or harmful responses. This vulnerability stems from both the inherent susceptibilities of LLMs and the expanded attack surface introduced by the visual modality. We propose Sim-CLIP+, a novel defense mechanism that adversarially fine-tunes the CLIP vision encoder by leveraging a Siamese architecture. This approach maximizes cosine similarity between perturbed and clean samples, facilitating resilience against adversarial manipulations. Sim-CLIP+ offers a plug-and-play solution, allowing seamless integration into existing LVLM architectures as a robust vision encoder. Unlike previous defenses, our method requires no structural modifications to the LVLM and incurs minimal computational overhead. Sim-CLIP+ demonstrates effectiveness against both gradient-based adversarial attacks and various jailbreak techniques. We evaluate Sim-CLIP+ against three distinct jailbreak attack strategies and perform clean evaluations using standard downstream datasets, including COCO for image captioning and OKVQA for visual question answering. Extensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracy while substantially improving robustness against both gradient-based adversarial attacks and jailbreak techniques. Our code and robust vision encoders are available at this https URL.

Translated Abstract:
대규모 비전-언어 모델(LVLMs)은 다양한 데이터셋을 바탕으로 훈련돼서 비전-언어 작업에서 좋은 성과를 내고 있어요. 하지만 이 모델들은 특히 '탈옥 공격' 같은 적대적 공격에 취약해요. 이런 공격은 안전 프로토콜을 우회해서 모델이 잘못된 정보나 해로운 응답을 생성하게 만들죠. 이러한 취약점은 LLM의 본질적인 약점과 시각적 요소로 인해 공격 표면이 넓어지는 데서 발생해요.

우리는 Sim-CLIP+라는 새로운 방어 메커니즘을 제안해요. 이 방법은 시암 구조를 이용해 CLIP 비전 인코더를 적대적으로 미세 조정해요. 이렇게 하면 변형된 샘플과 깨끗한 샘플 간의 코사인 유사성을 극대화해서 적대적 조작에 대한 저항력을 높일 수 있어요. Sim-CLIP+는 기존 LVLM 아키텍처에 쉽게 통합할 수 있는 플러그 앤 플레이 솔루션이에요. 이전 방어 방법들과 달리, 우리의 방법은 LVLM의 구조를 변경할 필요가 없고 계산 오버헤드도 최소화해요.

Sim-CLIP+는 그래디언트 기반의 적대적 공격과 다양한 탈옥 기술에 대해 효과적임을 보여줘요. 우리는 Sim-CLIP+를 세 가지 다른 탈옥 공격 전략에 대해 평가하고, 이미지 캡셔닝을 위한 COCO와 시각적 질문 응답을 위한 OKVQA 같은 표준 다운스트림 데이터셋을 사용해 깨끗한 평가를 진행했어요. 광범위한 실험 결과, Sim-CLIP+는 높은 정확도를 유지하면서 그래디언트 기반의 적대적 공격과 탈옥 기술에 대해 상당한 강인성을 개선하는 것을 보여줬어요. 우리의 코드와 강력한 비전 인코더는 이 링크에서 확인할 수 있어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.07365.pdf

Title: Event-based Mosaicing Bundle Adjustment

Original Abstract:
We tackle the problem of mosaicing bundle adjustment (i.e., simultaneous refinement of camera orientations and scene map) for a purely rotating event camera. We formulate the problem as a regularized non-linear least squares optimization. The objective function is defined using the linearized event generation model in the camera orientations and the panoramic gradient map of the scene. We show that this BA optimization has an exploitable block-diagonal sparsity structure, so that the problem can be solved efficiently. To the best of our knowledge, this is the first work to leverage such sparsity to speed up the optimization in the context of event-based cameras, without the need to convert events into image-like representations. We evaluate our method, called EMBA, on both synthetic and real-world datasets to show its effectiveness (50% photometric error decrease), yielding results of unprecedented quality. In addition, we demonstrate EMBA using high spatial resolution event cameras, yielding delicate panoramas in the wild, even without an initial map. Project page: this https URL

Translated Abstract:
우리는 순수하게 회전하는 이벤트 카메라를 위한 모자이크 번들 조정 문제를 다룹니다. 이건 카메라 방향과 장면 맵을 동시에 개선하는 작업이에요. 문제를 정규화된 비선형 최소 제곱 최적화로 설정했습니다. 

목표 함수는 카메라 방향에서의 선형화된 이벤트 생성 모델과 장면의 파노라마 기울기 맵을 사용해 정의돼요. 이 BA 최적화가 활용 가능한 블록-대각선 희소 구조를 가지고 있어서 문제를 효율적으로 해결할 수 있음을 보여줬어요. 우리가 아는 한, 이 연구는 이벤트 기반 카메라에서 최적화를 빠르게 하기 위해 이런 희소성을 이용한 첫 번째 작업이에요. 이벤트를 이미지 같은 형태로 변환할 필요 없이요.

우리는 EMBA라는 방법을 합성 데이터와 실제 데이터 세트에서 평가해서 효과성을 보여줬어요. 결과적으로 50%의 광학 오류 감소를 달성했고, 품질이 예전에는 없던 수준으로 향상된 결과를 얻었어요. 게다가, EMBA를 고해상도 이벤트 카메라를 사용해서 시연했는데, 초기 맵 없이도 자연에서 섬세한 파노라마를 생성할 수 있었어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.07414.pdf

Title: NVRC: Neural Video Representation Compression

Original Abstract:
Recent advances in implicit neural representation (INR)-based video coding have demonstrated its potential to compete with both conventional and other learning-based approaches. With INR methods, a neural network is trained to overfit a video sequence, with its parameters compressed to obtain a compact representation of the video content. However, although promising results have been achieved, the best INR-based methods are still out-performed by the latest standard codecs, such as VVC VTM, partially due to the simple model compression techniques employed. In this paper, rather than focusing on representation architectures as in many existing works, we propose a novel INR-based video compression framework, Neural Video Representation Compression (NVRC), targeting compression of the representation. Based on the novel entropy coding and quantization models proposed, NVRC, for the first time, is able to optimize an INR-based video codec in a fully end-to-end manner. To further minimize the additional bitrate overhead introduced by the entropy models, we have also proposed a new model compression framework for coding all the network, quantization and entropy model parameters hierarchically. Our experiments show that NVRC outperforms many conventional and learning-based benchmark codecs, with a 24% average coding gain over VVC VTM (Random Access) on the UVG dataset, measured in PSNR. As far as we are aware, this is the first time an INR-based video codec achieving such performance. The implementation of NVRC will be released at this http URL.

Translated Abstract:
최근 암시적 신경 표현(INR) 기반 비디오 코딩의 발전이 기존 방식과 다른 학습 기반 접근법과 경쟁할 수 있는 가능성을 보여주고 있어. INR 방법에서는 신경망이 비디오 시퀀스에 과적합되도록 훈련되고, 그 파라미터가 압축되어 비디오 내용의 간결한 표현을 얻는 방식이야. 하지만, 좋은 결과에도 불구하고, 최신 표준 코덱인 VVC VTM 같은 것들보다 여전히 성능이 떨어지는 경우가 많아. 이는 주로 사용되는 간단한 모델 압축 기술 때문이야.

이 논문에서는 기존 연구처럼 표현 아키텍처에 초점을 맞추기보다는, 표현 압축을 목표로 하는 새로운 INR 기반 비디오 압축 프레임워크인 Neural Video Representation Compression(NVRC)을 제안해. 제안된 새로운 엔트로피 코딩과 양자화 모델 덕분에, NVRC는 최초로 INR 기반 비디오 코덱을 완전한 엔드 투 엔드 방식으로 최적화할 수 있어. 추가적인 비트레이트 오버헤드를 최소화하기 위해 모든 네트워크, 양자화 및 엔트로피 모델 파라미터를 계층적으로 코딩하는 새로운 모델 압축 프레임워크도 제안했어.

실험 결과, NVRC는 많은 기존 방식과 학습 기반 벤치마크 코덱보다 성능이 뛰어나고, UVG 데이터셋에서 PSNR 기준으로 VVC VTM(랜덤 접근)에 비해 평균 24%의 코딩 이득을 보여줬어. 우리가 알기로, INR 기반 비디오 코덱이 이렇게 좋은 성능을 달성한 것은 처음이야. NVRC의 구현은 이 URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07426.pdf

Title: Deep Neural Network-Based Sign Language Recognition: A Comprehensive Approach Using Transfer Learning with Explainability

Original Abstract:
To promote inclusion and ensuring effective communication for those who rely on sign language as their main form of communication, sign language recognition (SLR) is crucial. Sign language recognition (SLR) seamlessly incorporates with diverse technology, enhancing accessibility for the deaf community by facilitating their use of digital platforms, video calls, and communication devices. To effectively solve this problem, we suggest a novel solution that uses a deep neural network to fully automate sign language recognition. This methodology integrates sophisticated preprocessing methodologies to optimise the overall performance. The architectures resnet, inception, xception, and vgg are utilised to selectively categorise images of sign language. We prepared a DNN architecture and merged it with the pre-processing architectures. In the post-processing phase, we utilised the SHAP deep explainer, which is based on cooperative game theory, to quantify the influence of specific features on the output of a machine learning model. Bhutanese-Sign-Language (BSL) dataset was used for training and testing the suggested technique. While training on Bhutanese-Sign-Language (BSL) dataset, overall ResNet50 with the DNN model performed better accuracy which is 98.90%. Our model's ability to provide informational clarity was assessed using the SHAP (SHapley Additive exPlanations) method. In part to its considerable robustness and reliability, the proposed methodological approach can be used to develop a fully automated system for sign language recognition.

Translated Abstract:
수화 인식(SLR)은 수화를 주 소통 수단으로 사용하는 사람들을 위해 포용성을 높이고 효과적인 소통을 보장하는 데 아주 중요해. SLR은 다양한 기술과 잘 어우러져서, 청각 장애인들이 디지털 플랫폼, 영상 통화, 그리고 소통 기기를 더 쉽게 사용할 수 있도록 도와줘.

이 문제를 효과적으로 해결하기 위해, 우리는 딥 뉴럴 네트워크를 사용해 수화 인식을 완전히 자동화하는 새로운 솔루션을 제안해. 이 방법론은 전체 성능을 최적화하기 위해 정교한 전처리 방법들을 통합하고 있어. ResNet, Inception, Xception, VGG 같은 아키텍처를 활용해 수화 이미지를 선택적으로 분류해.

우리는 DNN 아키텍처를 준비하고 전처리 아키텍처와 결합했어. 후처리 단계에서는 협력 게임 이론에 기반한 SHAP 딥 설명기를 사용해서 특정 특징이 머신러닝 모델의 출력에 미치는 영향을 정량화했어. 부탄 수화(BSL) 데이터셋을 사용해서 제안한 기술을 훈련하고 테스트했어. BSL 데이터셋에서 훈련할 때, 전체적으로 ResNet50과 DNN 모델이 98.90%라는 더 나은 정확도를 보여줬어.

우리 모델이 제공하는 정보의 명확성은 SHAP 방법을 사용해서 평가했어. 이 방법론은 상당한 강력함과 신뢰성을 가지고 있어서, 수화 인식을 위한 완전 자동화 시스템을 개발하는 데 사용될 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07447.pdf

Title: StereoCrafter: Diffusion-based Generation of Long and High-fidelity Stereoscopic 3D from Monocular Videos

Original Abstract:
This paper presents a novel framework for converting 2D videos to immersive stereoscopic 3D, addressing the growing demand for 3D content in immersive experience. Leveraging foundation models as priors, our approach overcomes the limitations of traditional methods and boosts the performance to ensure the high-fidelity generation required by the display devices. The proposed system consists of two main steps: depth-based video splatting for warping and extracting occlusion mask, and stereo video inpainting. We utilize pre-trained stable video diffusion as the backbone and introduce a fine-tuning protocol for the stereo video inpainting task. To handle input video with varying lengths and resolutions, we explore auto-regressive strategies and tiled processing. Finally, a sophisticated data processing pipeline has been developed to reconstruct a large-scale and high-quality dataset to support our training. Our framework demonstrates significant improvements in 2D-to-3D video conversion, offering a practical solution for creating immersive content for 3D devices like Apple Vision Pro and 3D displays. In summary, this work contributes to the field by presenting an effective method for generating high-quality stereoscopic videos from monocular input, potentially transforming how we experience digital media.

Translated Abstract:
이 논문은 2D 비디오를 몰입감 있는 스테레오 3D로 변환하는 새로운 프레임워크를 제안해. 요즘 3D 콘텐츠에 대한 수요가 커지고 있어서 이 연구가 필요했어. 우리가 사용한 방법은 기존의 기술적인 한계를 극복하고, 디스플레이 장치에서 요구하는 높은 품질의 3D 영상을 만들어내는 데 성능을 높였어.

제안된 시스템은 두 가지 주요 단계로 이루어져 있어. 첫 번째는 깊이 기반 비디오 스플래팅을 통해 왜곡을 처리하고 가려진 부분의 마스크를 추출하는 거고, 두 번째는 스테레오 비디오 인페인팅이야. 우리는 사전 훈련된 안정적인 비디오 확산 모델을 기본으로 사용하고, 스테레오 비디오 인페인팅 작업을 위해 세부 조정 프로토콜을 도입했어.

입력 비디오의 길이와 해상도가 다양할 때는 자동 회귀 전략과 타일 처리 방법을 탐색했어. 마지막으로, 대규모 고품질 데이터셋을 재구성하기 위한 복잡한 데이터 처리 파이프라인도 개발했어. 우리 프레임워크는 2D에서 3D 비디오 변환에서 상당한 개선을 보여주고, Apple Vision Pro 같은 3D 디바이스를 위한 몰입형 콘텐츠를 만드는 실제적인 해결책을 제공해.

결론적으로, 이 연구는 단안 입력으로부터 고품질 스테레오 비디오를 생성하는 효과적인 방법을 제시하면서, 디지털 미디어를 경험하는 방식을 변화시킬 가능성이 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07451.pdf

Title: FreeEnhance: Tuning-Free Image Enhancement via Content-Consistent Noising-and-Denoising Process

Original Abstract:
The emergence of text-to-image generation models has led to the recognition that image enhancement, performed as post-processing, would significantly improve the visual quality of the generated images. Exploring diffusion models to enhance the generated images nevertheless is not trivial and necessitates to delicately enrich plentiful details while preserving the visual appearance of key content in the original image. In this paper, we propose a novel framework, namely FreeEnhance, for content-consistent image enhancement using the off-the-shelf image diffusion models. Technically, FreeEnhance is a two-stage process that firstly adds random noise to the input image and then capitalizes on a pre-trained image diffusion model (i.e., Latent Diffusion Models) to denoise and enhance the image details. In the noising stage, FreeEnhance is devised to add lighter noise to the region with higher frequency to preserve the high-frequent patterns (e.g., edge, corner) in the original image. In the denoising stage, we present three target properties as constraints to regularize the predicted noise, enhancing images with high acutance and high visual quality. Extensive experiments conducted on the HPDv2 dataset demonstrate that our FreeEnhance outperforms the state-of-the-art image enhancement models in terms of quantitative metrics and human preference. More remarkably, FreeEnhance also shows higher human preference compared to the commercial image enhancement solution of Magnific AI.

Translated Abstract:
텍스트-이미지 생성 모델의 발전으로 후처리로서의 이미지 향상이 생성된 이미지의 시각적 품질을 크게 향상시킬 수 있다는 점이 인식되었어. 하지만 생성된 이미지를 향상시키기 위해 확산 모델을 탐색하는 건 쉽지 않아. 원본 이미지의 주요 내용을 보존하면서 풍부한 디테일을 섬세하게 추가해야 하거든.

이 논문에서는 오프더셀프 이미지 확산 모델을 사용해 콘텐츠 일관성을 유지하는 이미지 향상을 위한 새로운 프레임워크인 FreeEnhance를 제안해. 기술적으로 FreeEnhance는 두 단계로 이루어져 있어. 첫 번째 단계에서는 입력 이미지에 무작위 노이즈를 추가하고, 두 번째 단계에서는 미리 훈련된 이미지 확산 모델(예: 잠재 확산 모델)을 사용해 노이즈를 제거하고 이미지 디테일을 향상시켜.

노이징 단계에서는 FreeEnhance가 원본 이미지의 고주파 패턴(예: 가장자리, 모서리)을 보존하기 위해 고주파 영역에 더 가벼운 노이즈를 추가하도록 설계되었어. 디노이징 단계에서는 세 가지 목표 속성을 제시해 예측된 노이즈를 규제하고, 고명료성과 높은 시각적 품질을 가진 이미지를 생성해.

HPDv2 데이터셋에서 진행한 광범위한 실험 결과, 우리의 FreeEnhance가 정량적 지표와 인간 선호도 모두에서 최신 이미지 향상 모델보다 더 뛰어난 성능을 보였어. 더 놀라운 건, FreeEnhance가 상업적 이미지 향상 솔루션인 Magnific AI보다도 더 높은 인간 선호도를 나타냈다는 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07452.pdf

Title: Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models

Original Abstract:
Despite having tremendous progress in image-to-3D generation, existing methods still struggle to produce multi-view consistent images with high-resolution textures in detail, especially in the paradigm of 2D diffusion that lacks 3D awareness. In this work, we present High-resolution Image-to-3D model (Hi3D), a new video diffusion based paradigm that redefines a single image to multi-view images as 3D-aware sequential image generation (i.e., orbital video generation). This methodology delves into the underlying temporal consistency knowledge in video diffusion model that generalizes well to geometry consistency across multiple views in 3D generation. Technically, Hi3D first empowers the pre-trained video diffusion model with 3D-aware prior (camera pose condition), yielding multi-view images with low-resolution texture details. A 3D-aware video-to-video refiner is learnt to further scale up the multi-view images with high-resolution texture details. Such high-resolution multi-view images are further augmented with novel views through 3D Gaussian Splatting, which are finally leveraged to obtain high-fidelity meshes via 3D reconstruction. Extensive experiments on both novel view synthesis and single view reconstruction demonstrate that our Hi3D manages to produce superior multi-view consistency images with highly-detailed textures. Source code and data are available at \url{this https URL}.

Translated Abstract:
이미지에서 3D로 변환하는 기술이 많이 발전했지만, 기존 방법들은 여전히 다중 시점에서 일관된 이미지를 고해상도의 세부 텍스처로 만드는 데 어려움을 겪고 있어. 특히 2D 확산 방식에서는 3D에 대한 인식이 부족해. 

이번 연구에서는 Hi3D라는 새로운 비디오 확산 기반 모델을 소개해. 이 모델은 단일 이미지를 다중 시점 이미지로 변환하는 3D 인식 순차적 이미지 생성(즉, 궤도 비디오 생성) 방식으로 다시 정의해. 이 방법은 비디오 확산 모델의 시간적 일관성 지식을 깊이 파고들어 여러 시점에서의 기하학적 일관성에 잘 적용될 수 있어. 

기술적으로 Hi3D는 먼저 사전 훈련된 비디오 확산 모델에 3D 인식 선행 정보를 추가해(카메라 자세 조건), 저해상도 텍스처 세부 정보가 있는 다중 시점 이미지를 생성해. 이후에는 3D 인식 비디오-비디오 정제 모델을 학습시켜 다중 시점 이미지를 고해상도 텍스처 세부 정보로 더욱 확장해. 이렇게 생성된 고해상도 다중 시점 이미지는 3D 가우시안 스플래팅을 통해 새로운 시점으로 보강되어, 최종적으로 3D 재구성을 통해 고충실도 메시를 얻는 데 활용돼. 

새로운 시점 합성과 단일 시점 재구성에 대한 광범위한 실험 결과, 우리의 Hi3D가 매우 세부적인 텍스처를 가진 우수한 다중 시점 일관성 이미지를 생성하는 데 성공했다는 것을 보여줘. 소스 코드와 데이터는 제공된 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07454.pdf

Title: DreamMesh: Jointly Manipulating and Texturing Triangle Meshes for Text-to-3D Generation

Original Abstract:
Learning radiance fields (NeRF) with powerful 2D diffusion models has garnered popularity for text-to-3D generation. Nevertheless, the implicit 3D representations of NeRF lack explicit modeling of meshes and textures over surfaces, and such surface-undefined way may suffer from the issues, e.g., noisy surfaces with ambiguous texture details or cross-view inconsistency. To alleviate this, we present DreamMesh, a novel text-to-3D architecture that pivots on well-defined surfaces (triangle meshes) to generate high-fidelity explicit 3D model. Technically, DreamMesh capitalizes on a distinctive coarse-to-fine scheme. In the coarse stage, the mesh is first deformed by text-guided Jacobians and then DreamMesh textures the mesh with an interlaced use of 2D diffusion models in a tuning free manner from multiple viewpoints. In the fine stage, DreamMesh jointly manipulates the mesh and refines the texture map, leading to high-quality triangle meshes with high-fidelity textured materials. Extensive experiments demonstrate that DreamMesh significantly outperforms state-of-the-art text-to-3D methods in faithfully generating 3D content with richer textual details and enhanced geometry. Our project page is available at this https URL.

Translated Abstract:
텍스트를 3D로 변환하는 데 강력한 2D 확산 모델을 활용한 방사장 학습(NeRF)이 인기를 끌고 있어. 하지만 NeRF의 암묵적인 3D 표현은 표면에서 메시와 텍스처를 명시적으로 모델링하지 않아서, 텍스처 세부 사항이 모호한 시끄러운 표면이나 시점 간 불일치 같은 문제를 겪을 수 있어.

이런 문제를 해결하기 위해, 우리는 DreamMesh라는 새로운 텍스트-투-3D 아키텍처를 제안해. DreamMesh는 잘 정의된 표면(삼각형 메시)을 기반으로 고품질의 명시적인 3D 모델을 생성해. 기술적으로, DreamMesh는 독특한 거칠게-세밀하게 변형하는 방식을 사용해. 거친 단계에서는, 메시가 텍스트에 따라 변형되고, 그 다음에 DreamMesh가 2D 확산 모델을 사용해서 여러 시점에서 메시를 텍스처링해.

세밀한 단계에서는, DreamMesh가 메시와 텍스처 맵을 함께 조작하고 개선해서 고품질의 삼각형 메시와 고충실도의 텍스처를 만들어내. 실험 결과, DreamMesh는 텍스트 세부 사항이 풍부하고 지오메트리가 개선된 3D 콘텐츠를 신뢰성 있게 생성하는 데 있어 기존의 최첨단 텍스트-투-3D 방법들보다 훨씬 뛰어난 성능을 보여줬어. 우리 프로젝트 페이지는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07456.pdf

Title: Self-Evolving Depth-Supervised 3D Gaussian Splatting from Rendered Stereo Pairs

Original Abstract:
3D Gaussian Splatting (GS) significantly struggles to accurately represent the underlying 3D scene geometry, resulting in inaccuracies and floating artifacts when rendering depth maps. In this paper, we address this limitation, undertaking a comprehensive analysis of the integration of depth priors throughout the optimization process of Gaussian primitives, and present a novel strategy for this purpose. This latter dynamically exploits depth cues from a readily available stereo network, processing virtual stereo pairs rendered by the GS model itself during training and achieving consistent self-improvement of the scene representation. Experimental results on three popular datasets, breaking ground as the first to assess depth accuracy for these models, validate our findings.

Translated Abstract:
3D 가우시안 스플래팅(GS)은 3D 장면의 기하학을 정확하게 표현하는 데 큰 어려움을 겪어서, 깊이 맵을 렌더링할 때 부정확성과 떠다니는 아티팩트가 생기는 문제가 있어. 

이 논문에서는 이런 한계를 해결하기 위해 깊이 우선 정보(depth priors)를 가우시안 원시의 최적화 과정에 통합하는 방법을 자세히 분석하고, 이를 위한 새로운 전략을 제시해. 이 전략은 쉽게 구할 수 있는 스테레오 네트워크에서 깊이 정보를 동적으로 활용해서, GS 모델이 훈련 중에 자체적으로 렌더링한 가상 스테레오 쌍을 처리하며 장면 표현을 지속적으로 개선해.

세 가지 인기 있는 데이터셋에서 실험한 결과, 이 모델들의 깊이 정확성을 평가한 첫 번째 사례로서 우리의 발견을 검증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06714.pdf

Title: FCDM: Sparse-view Sinogram Inpainting with Frequency Domain Convolution Enhanced Diffusion Models

Original Abstract:
Reducing the radiation dose in computed tomography (CT) is crucial, but it often results in sparse-view CT, where the number of available projections is significantly reduced. This reduction in projection data makes it challenging to accurately reconstruct high-quality CT images. In this condition, a sinogram, which is a collection of these projections, becomes incomplete. Sinogram inpainting then becomes essential because it enables accurate image reconstruction with limited projections. Existing models performing well on conventional RGB images for inpainting mostly fail in the case of sinograms. Further, these models usually do not make full use of unique properties, e.g., frequency features and absorption characteristics in the sinogram, and cannot handle large-area masks and complex real-world projections well.
To address these limitations, we propose a novel model called the Frequency Convolution Diffusion Model (FCDM). It employs frequency domain convolutions to extract frequency information from various angles and capture the intricate relationships between these angles, which is essential for high-quality CT reconstruction. We also design a specific loss function based on the unique properties of a sinogram to maintain the consistency in physical properties, which allows the model to learn more effectively even in larger mask areas. We compare FCDM using both simulations and real data with nine inpainting models examples, among which two are designed for sinogram and seven for RGB. The results indicate that our model significantly improves the quality of the inpainted sinograms in terms of both visually and quantitatively, with an SSIM of more than 0.95 and PSNR of more than 30, achieving up to a 33% improvement in SSIM and a 29% improvement in PSNR compared to the baseline.

Translated Abstract:
CT(컴퓨터 단층 촬영)에서 방사선량을 줄이는 건 중요하지만, 이로 인해 사용할 수 있는 투영 수가 많이 줄어드는 희소 투영 CT가 되는 경우가 많아. 이렇게 투영 데이터가 줄어들면 고품질 CT 이미지를 정확하게 복원하기가 어려워져. 이럴 때, 여러 투영을 모은 신호그램(sinogram)이 불완전해지는데, 그래서 신호그램 보간(sinogram inpainting)이 매우 중요해져. 이 방법은 제한된 투영으로도 정확한 이미지 복원을 가능하게 해줘.

기존의 보간 모델들은 일반적인 RGB 이미지에선 잘 작동하지만 신호그램에선 잘 안 돼. 게다가, 이 모델들은 신호그램의 독특한 특성인 주파수 정보나 흡수 특성을 제대로 활용하지 못하고, 넓은 마스크나 복잡한 현실 투영을 잘 처리하지 못해.

이런 문제를 해결하기 위해, 우리는 주파수 컨볼루션 확산 모델(Frequency Convolution Diffusion Model, FCDM)을 제안해. 이 모델은 주파수 영역에서 컨볼루션을 사용해 다양한 각도에서 주파수 정보를 추출하고, 이 각도들 간의 복잡한 관계를 포착해. 이건 고품질 CT 복원에 필수적이야. 또한, 우리는 신호그램의 독특한 특성을 바탕으로 특정 손실 함수도 설계했어. 이 손실 함수는 물리적 특성의 일관성을 유지하게 해줘, 그래서 모델이 더 큰 마스크 영역에서도 효과적으로 학습할 수 있어.

우리는 FCDM을 아홉 가지 보간 모델과 함께 시뮬레이션과 실제 데이터로 비교했어. 그 중 두 개는 신호그램을 위해 설계됐고, 일곱 개는 RGB를 위해 설계됐어. 결과는 우리 모델이 시각적으로나 수치적으로 보간된 신호그램의 품질을 상당히 향상시킨다는 걸 보여줘. SSIM(구조 유사성 지수)은 0.95 이상, PSNR(피크 신호 대 잡음비)은 30 이상을 기록했어. 기본 모델에 비해 SSIM은 최대 33% 개선, PSNR은 29% 개선된 결과야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06716.pdf

Title: Detailed delineation of the fetal brain in diffusion MRI via multi-task learning

Original Abstract:
Diffusion-weighted MRI is increasingly used to study the normal and abnormal development of fetal brain in-utero. Recent studies have shown that dMRI can offer invaluable insights into the neurodevelopmental processes in the fetal stage. However, because of the low data quality and rapid brain development, reliable analysis of fetal dMRI data requires dedicated computational methods that are currently unavailable. The lack of automated methods for fast, accurate, and reproducible data analysis has seriously limited our ability to tap the potential of fetal brain dMRI for medical and scientific applications. In this work, we developed and validated a unified computational framework to (1) segment the brain tissue into white matter, cortical/subcortical gray matter, and cerebrospinal fluid, (2) segment 31 distinct white matter tracts, and (3) parcellate the brain's cortex and delineate the deep gray nuclei and white matter structures into 96 anatomically meaningful regions. We utilized a set of manual, semi-automatic, and automatic approaches to annotate 97 fetal brains. Using these labels, we developed and validated a multi-task deep learning method to perform the three computations. Our evaluations show that the new method can accurately carry out all three tasks, achieving a mean Dice similarity coefficient of 0.865 on tissue segmentation, 0.825 on white matter tract segmentation, and 0.819 on parcellation. The proposed method can greatly advance the field of fetal neuroimaging as it can lead to substantial improvements in fetal brain tractography, tract-specific analysis, and structural connectivity assessment.

Translated Abstract:
확산 강조 MRI는 태아 뇌의 정상적이고 비정상적인 발달을 연구하는 데 점점 더 많이 사용되고 있어. 최근 연구들은 dMRI가 태아 단계에서 신경 발달 과정에 대한 귀중한 통찰을 제공할 수 있다는 걸 보여줬어. 하지만 데이터 품질이 낮고 뇌가 빠르게 발달하기 때문에, 태아 dMRI 데이터를 신뢰성 있게 분석하려면 현재로서는 특별한 컴퓨터 방법이 필요해.

자동화된 방법이 부족해서 빠르고 정확하며 재현 가능한 데이터 분석이 어려운 상황이야. 이로 인해 태아 뇌 dMRI를 의료 및 과학적 응용에 활용하는 데 큰 제약이 있어. 이 연구에서는 (1) 뇌 조직을 백질, 피질/피질 하 회색질, 그리고 뇌척수액으로 나누고, (2) 31개의 서로 다른 백질 경로를 분리하고, (3) 뇌의 피질을 구획화하고 깊은 회색 핵과 백질 구조를 96개의 해부학적으로 의미 있는 영역으로 나누는 통합된 컴퓨터 프레임워크를 개발하고 검증했어.

97개의 태아 뇌를 주석하기 위해 수작업, 반자동, 자동화된 다양한 방법을 사용했어. 이러한 레이블을 바탕으로 세 가지 작업을 수행할 다중 작업 딥러닝 방법을 개발하고 검증했어. 평가 결과, 새로운 방법이 모든 작업을 정확하게 수행할 수 있다는 걸 보여줬고, 조직 분할에서는 평균 Dice 유사도 계수 0.865, 백질 경로 분할에서는 0.825, 구획화에서는 0.819를 달성했어. 제안된 방법은 태아 신경영상 분야를 크게 발전시킬 수 있고, 태아 뇌 경로 추적, 경로별 분석, 구조적 연결성 평가에서 큰 개선을 이끌어낼 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06722.pdf

Title: Automated Quantification of White Blood Cells in Light Microscopic Images of Injured Skeletal Muscle

Original Abstract:
White blood cells (WBCs) are the most diverse cell types observed in the healing process of injured skeletal muscles. In the course of healing, WBCs exhibit dynamic cellular response and undergo multiple protein expression changes. The progress of healing can be analyzed by quantifying the number of WBCs or the amount of specific proteins in light microscopic images obtained at different time points after injury. In this paper, we propose an automated quantifying and analysis framework to analyze WBCs using light microscopic images of uninjured and injured muscles. The proposed framework is based on the Localized Iterative Otsu's threshold method with muscle edge detection and region of interest extraction. Compared with the threshold methods used in ImageJ, the LI Otsu's threshold method has high resistance to background area and achieves better accuracy. The CD68-positive cell results are presented for demonstrating the effectiveness of the proposed work.

Translated Abstract:
백혈구(WBC)는 부상당한 골격근의 치유 과정에서 가장 다양한 세포 유형이야. 치유가 진행되면서 WBC는 동적인 세포 반응을 보이고 여러 단백질 발현 변화가 일어나. 치유의 진행 상황은 부상 후 다른 시간대에 얻은 빛 현미경 이미지를 통해 WBC의 수나 특정 단백질의 양을 정량화함으로써 분석할 수 있어.

이 논문에서는 부상당한 근육과 건강한 근육의 빛 현미경 이미지를 이용해 WBC를 자동으로 정량화하고 분석하는 프레임워크를 제안해. 이 프레임워크는 근육 경계 감지와 관심 영역 추출을 포함한 지역 반복 Otsu의 임계값 방법을 기반으로 해. ImageJ에서 사용되는 임계값 방법들과 비교했을 때, LI Otsu의 임계값 방법은 배경 영역에 대한 저항력이 높고 더 나은 정확성을 보여줘. 제안된 연구의 효과를 보여주기 위해 CD68 양성 세포 결과를 제시했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06817.pdf

Title: Bifurcation Identification for Ultrasound-driven Robotic Cannulation

Original Abstract:
In trauma and critical care settings, rapid and precise intravascular access is key to patients' survival. Our research aims at ensuring this access, even when skilled medical personnel are not readily available. Vessel bifurcations are anatomical landmarks that can guide the safe placement of catheters or needles during medical procedures. Although ultrasound is advantageous in navigating anatomical landmarks in emergency scenarios due to its portability and safety, to our knowledge no existing algorithm can autonomously extract vessel bifurcations using ultrasound images. This is primarily due to the limited availability of ground truth data, in particular, data from live subjects, needed for training and validating reliable models. Researchers often resort to using data from anatomical phantoms or simulations. We introduce BIFURC, Bifurcation Identification for Ultrasound-driven Robot Cannulation, a novel algorithm that identifies vessel bifurcations and provides optimal needle insertion sites for an autonomous robotic cannulation system. BIFURC integrates expert knowledge with deep learning techniques to efficiently detect vessel bifurcations within the femoral region and can be trained on a limited amount of in-vivo data. We evaluated our algorithm using a medical phantom as well as real-world experiments involving live pigs. In all cases, BIFURC consistently identified bifurcation points and needle insertion locations in alignment with those identified by expert clinicians.

Translated Abstract:
외상 및 중환자 치료 환경에서 빠르고 정확한 혈관 접근은 환자의 생존에 매우 중요해. 우리 연구는 숙련된 의료 인력이 항상 있는 것이 아니더라도 이 접근을 보장하는 걸 목표로 하고 있어. 

혈관의 분기점은 의료 절차 중 카테터나 바늘을 안전하게 놓는 데 도움이 되는 해부학적 기준점이야. 초음파는 이동성이 좋고 안전하기 때문에 응급 상황에서 해부학적 기준점을 찾는 데 유리하지만, 우리가 아는 한 초음파 이미지를 사용해 혈관 분기점을 자동으로 추출할 수 있는 알고리즘은 아직 없어. 그 이유는 신뢰할 수 있는 모델을 훈련하고 검증하는 데 필요한 실제 데이터가 부족하기 때문이야. 연구자들은 보통 해부학적 팬텀이나 시뮬레이션 데이터를 사용하는 경우가 많아.

우리는 BIFURC, 즉 초음파 기반 로봇 카뉼레이션을 위한 분기점 식별 알고리즘을 소개해. 이 알고리즘은 혈관 분기점을 식별하고 자율 로봇 카뉼레이션 시스템을 위한 최적의 바늘 삽입 위치를 제공해. BIFURC는 전문가의 지식과 딥러닝 기술을 결합해 대퇴부 영역 내에서 혈관 분기점을 효율적으로 탐지할 수 있도록 해. 그리고 제한된 양의 생체 데이터를 가지고도 훈련할 수 있어.

우리는 의료 팬텀과 실제 생돼지를 대상으로 한 실험을 통해 알고리즘을 평가했어. 모든 경우에 BIFURC는 전문가들이 식별한 것과 일치하게 분기점과 바늘 삽입 위치를 꾸준히 잘 찾아냈어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06887.pdf

Title: Ordinal Learning: Longitudinal Attention Alignment Model for Predicting Time to Future Breast Cancer Events from Mammograms

Original Abstract:
Precision breast cancer (BC) risk assessment is crucial for developing individualized screening and prevention. Despite the promising potential of recent mammogram (MG) based deep learning models in predicting BC risk, they mostly overlook the 'time-to-future-event' ordering among patients and exhibit limited explorations into how they track history changes in breast tissue, thereby limiting their clinical application. In this work, we propose a novel method, named OA-BreaCR, to precisely model the ordinal relationship of the time to and between BC events while incorporating longitudinal breast tissue changes in a more explainable manner. We validate our method on public EMBED and inhouse datasets, comparing with existing BC risk prediction and time prediction methods. Our ordinal learning method OA-BreaCR outperforms existing methods in both BC risk and time-to-future-event prediction tasks. Additionally, ordinal heatmap visualizations show the model's attention over time. Our findings underscore the importance of interpretable and precise risk assessment for enhancing BC screening and prevention efforts. The code will be accessible to the public.

Translated Abstract:
정밀 유방암(BC) 위험 평가가 개인 맞춤형 검진과 예방을 개발하는 데 정말 중요해. 최근의 유방촬영술(MG) 기반 딥러닝 모델들이 BC 위험을 예측하는 데 가능성이 있지만, 대부분 환자들 사이의 '미래 사건까지의 시간' 순서를 무시하고, 유방 조직의 변화 기록을 다루는 데 한계가 있어. 이 때문에 임상에서 활용하기 어려운 거지.

그래서 우리는 OA-BreaCR이라는 새로운 방법을 제안해. 이 방법은 BC 사건까지의 시간과 사건 간의 순서 관계를 정밀하게 모델링하면서, 유방 조직의 변화도 더 설명 가능하게 포함해. 우리는 이 방법을 EMBED와 내부 데이터셋에서 검증했어. 기존의 BC 위험 예측과 시간 예측 방법과 비교해봤고, 우리의 순서 학습 방법인 OA-BreaCR이 두 가지 작업 모두에서 기존 방법보다 더 성과가 좋았어.

게다가, 순서 히트맵 시각화는 시간에 따른 모델의 주의 집중을 보여줘. 우리의 연구 결과는 BC 검진과 예방 노력을 향상시키기 위해 해석 가능하고 정확한 위험 평가의 중요성을 강조해. 코드도 공개할 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06993.pdf

Title: RICAU-Net: Residual-block Inspired Coordinate Attention U-Net for Segmentation of Small and Sparse Calcium Lesions in Cardiac CT

Original Abstract:
The Agatston score, which is the sum of the calcification in the four main coronary arteries, has been widely used in the diagnosis of coronary artery disease (CAD). However, many studies have emphasized the importance of the vessel-specific Agatston score, as calcification in a specific vessel is significantly correlated with the occurrence of coronary heart disease (CHD). In this paper, we propose the Residual-block Inspired Coordinate Attention U-Net (RICAU-Net), which incorporates coordinate attention in two distinct manners and a customized combo loss function for lesion-specific coronary artery calcium (CAC) segmentation. This approach aims to tackle the high class-imbalance issue associated with small and sparse lesions, particularly for CAC in the left main coronary artery (LM) which is generally small and the scarcest in the dataset due to its anatomical structure. The proposed method was compared with six different methods using Dice score, precision, and recall. Our approach achieved the highest per-lesion Dice scores for all four lesions, especially for CAC in LM compared to other methods. The ablation studies demonstrated the significance of positional information from the coordinate attention and the customized loss function in segmenting small and sparse lesions with a high class-imbalance problem.

Translated Abstract:
아가트스톤 점수는 네 개의 주요 관상 동맥에서의 석회화 총합으로, 관상 동맥 질환(CAD) 진단에 널리 사용되고 있어. 하지만 많은 연구에서 특정 혈관의 아가트스톤 점수가 중요하다고 강조하고 있어. 특정 혈관의 석회화가 관상 심장 질환(CHD) 발생과 크게 관련이 있거든.

이 논문에서는 Residual-block Inspired Coordinate Attention U-Net (RICAU-Net)을 제안해. 이 모델은 두 가지 다른 방식으로 좌표 주의를 포함하고, 병변 특정 관상 동맥 칼슘(CAC) 분할을 위한 맞춤형 조합 손실 함수를 사용해. 이 접근법은 작고 드문 병변과 관련된 높은 클래스 불균형 문제를 해결하는 데 목표를 두고 있어. 특히 좌측 주관상동맥(LM)의 CAC는 일반적으로 작고 데이터셋에서 가장 드물기 때문에 더욱 그렇지.

제안된 방법은 Dice 점수, 정밀도, 재현율을 이용해 여섯 가지 다른 방법과 비교했어. 우리의 접근법은 네 가지 병변 모두에서, 특히 LM의 CAC에 대해 다른 방법들보다 더 높은 병변당 Dice 점수를 기록했어. 또한, 소규모 및 희소 병변을 분할하는 데 있어 좌표 주의에서 얻은 위치 정보와 맞춤형 손실 함수의 중요성을 보여주는 소거 연구도 진행했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07004.pdf

Title: Performance Assessment of Feature Detection Methods for 2-D FS Sonar Imagery

Original Abstract:
Underwater robot perception is crucial in scientific subsea exploration and commercial operations. The key challenges include non-uniform lighting and poor visibility in turbid environments. High-frequency forward-look sonar cameras address these issues, by providing high-resolution imagery at maximum range of tens of meters, despite complexities posed by high degree of speckle noise, and lack of color and texture. In particular, robust feature detection is an essential initial step for automated object recognition, localization, navigation, and 3-D mapping. Various local feature detectors developed for RGB images are not well-suited for sonar data. To assess their performances, we evaluate a number of feature detectors using real sonar images from five different sonar devices. Performance metrics such as detection accuracy, false positives, and robustness to variations in target characteristics and sonar devices are applied to analyze the experimental results. The study would provide a deeper insight into the bottlenecks of feature detection for sonar data, and developing more effective methods

Translated Abstract:
수중 로봇의 인식 능력은 과학적인 해양 탐사와 상업적인 작업에서 매우 중요해. 주요 도전 과제는 조명이 고르지 않거나 탁한 환경에서 가시성이 떨어지는 거야. 고주파 전방 탐지 소나 카메라는 이런 문제를 해결하는데, 최대 수십 미터 거리에서 고해상도 이미지를 제공해. 하지만 스펙클 노이즈가 심하고 색상이나 질감이 부족한 복잡한 상황에서도 잘 작동해.

특히, 튼튼한 특징 감지는 자동 물체 인식, 위치 파악, 내비게이션, 그리고 3D 맵핑을 위한 중요한 첫 단계야. RGB 이미지에 대해 개발된 다양한 지역 특징 감지기는 소나 데이터에는 잘 맞지 않아. 그래서 우리는 다섯 개의 다른 소나 장치에서 실제 소나 이미지를 사용해 여러 특징 감지기의 성능을 평가했어.

검출 정확도, 허위 긍정률, 그리고 목표 특성과 소나 장치의 변화에 대한 강인성과 같은 성능 지표를 적용해서 실험 결과를 분석했어. 이 연구는 소나 데이터의 특징 감지에서의 병목 현상에 대한 깊은 통찰을 제공하고, 더 효과적인 방법을 개발하는 데 도움이 될 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07012.pdf

Title: Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records

Original Abstract:
Chest X-ray imaging (CXR) is an important diagnostic tool used in hospitals to assess patient conditions and monitor changes over time. Generative models, specifically diffusion-based models, have shown promise in generating realistic synthetic X-rays. However, these models mainly focus on conditional generation using single-time-point data, i.e., typically CXRs taken at a specific time with their corresponding reports, limiting their clinical utility, particularly for capturing temporal changes. To address this limitation, we propose a novel framework, EHRXDiff, which predicts future CXR images by integrating previous CXRs with subsequent medical events, e.g., prescriptions, lab measures, etc. Our framework dynamically tracks and predicts disease progression based on a latent diffusion model, conditioned on the previous CXR image and a history of medical events. We comprehensively evaluate the performance of our framework across three key aspects, including clinical consistency, demographic consistency, and visual realism. We demonstrate that our framework generates high-quality, realistic future images that capture potential temporal changes, suggesting its potential for further development as a clinical simulation tool. This could offer valuable insights for patient monitoring and treatment planning in the medical field.

Translated Abstract:
흉부 X선 영상(CXR)은 병원에서 환자 상태를 평가하고 시간에 따른 변화를 모니터링하는 데 중요한 진단 도구야. 생성 모델, 특히 확산 기반 모델은 현실적인 합성 X선을 생성하는 데 가능성을 보여줬어. 하지만 이런 모델들은 주로 단일 시간점 데이터만 사용해서 조건부 생성을 하는데, 보통 특정 시간에 찍힌 CXR과 그에 맞는 보고서를 사용하거든. 이게 임상에서 사용할 때 제한적이고, 특히 시간에 따른 변화를 잡는 데 어려움이 있어.

그래서 우리는 EHRXDiff라는 새로운 프레임워크를 제안해. 이건 이전 CXR과 이후의 의료 이벤트(예: 처방전, 실험실 측정 등)를 통합해 미래의 CXR 이미지를 예측하는 거야. 우리 프레임워크는 이전 CXR 이미지와 의료 이벤트의 역사에 기반해서 질병 진행 상황을 동적으로 추적하고 예측해.

우리는 이 프레임워크의 성능을 임상 일관성, 인구 통계 일관성, 시각적 현실성 등 세 가지 주요 측면에서 종합적으로 평가했어. 결과적으로 우리의 프레임워크는 잠재적인 시간 변화를 담고 있는 고품질의 현실적인 미래 이미지를 생성한다는 걸 보여줬어. 이게 임상 시뮬레이션 도구로서 더 발전할 가능성을 제시하고, 환자 모니터링과 치료 계획에 귀중한 통찰력을 제공할 수 있을 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07020.pdf

Title: EVENet: Evidence-based Ensemble Learning for Uncertainty-aware Brain Parcellation Using Diffusion MRI

Original Abstract:
In this study, we developed an Evidence-based Ensemble Neural Network, namely EVENet, for anatomical brain parcellation using diffusion MRI. The key innovation of EVENet is the design of an evidential deep learning framework to quantify predictive uncertainty at each voxel during a single inference. Using EVENet, we obtained accurate parcellation and uncertainty estimates across different datasets from healthy and clinical populations and with different imaging acquisitions. The overall network includes five parallel subnetworks, where each is dedicated to learning the FreeSurfer parcellation for a certain diffusion MRI parameter. An evidence-based ensemble methodology is then proposed to fuse the individual outputs. We perform experimental evaluations on large-scale datasets from multiple imaging sources, including high-quality diffusion MRI data from healthy adults and clinically diffusion MRI data from participants with various brain diseases (schizophrenia, bipolar disorder, attention-deficit/hyperactivity disorder, Parkinson's disease, cerebral small vessel disease, and neurosurgical patients with brain tumors). Compared to several state-of-the-art methods, our experimental results demonstrate highly improved parcellation accuracy across the multiple testing datasets despite the differences in dMRI acquisition protocols and health conditions. Furthermore, thanks to the uncertainty estimation, our EVENet approach demonstrates a good ability to detect abnormal brain regions in patients with lesions, enhancing the interpretability and reliability of the segmentation results.

Translated Abstract:
이번 연구에서는 확산 MRI를 사용해 해부학적 뇌 분할을 위한 증거 기반 앙상블 신경망, 즉 EVENet을 개발했어. EVENet의 주요 혁신은 단일 추론 중 각 복셀에서 예측 불확실성을 정량화할 수 있는 증거 기반 딥러닝 프레임워크를 설계한 거야.

EVENet을 사용해서 건강한 사람과 임상 집단에서 다양한 데이터셋을 통해 정확한 분할과 불확실성 추정치를 얻었어. 전체 네트워크는 다섯 개의 병렬 서브네트워크로 구성되어 있고, 각 서브네트워크는 특정 확산 MRI 파라미터에 대한 FreeSurfer 분할을 학습하는 데 전념하고 있어. 그런 다음, 개별 출력을 융합하기 위한 증거 기반 앙상블 방법론을 제안했어.

우리는 건강한 성인과 다양한 뇌 질환이 있는 참가자들(조현병, 양극성 장애, 주의력 결핍/과잉행동장애, 파킨슨병, 뇌 소혈관 질환, 뇌종양 환자들)로부터 수집한 고품질 확산 MRI 데이터를 포함한 대규모 데이터셋에 대해 실험 평가를 진행했어. 여러 최신 방법들과 비교했을 때, 우리의 실험 결과는 dMRI 획득 프로토콜과 건강 상태의 차이에도 불구하고 다양한 테스트 데이터셋에서 분할 정확도가 크게 향상됐음을 보여줘.

게다가 불확실성 추정 덕분에 EVENet 접근법은 병변이 있는 환자들의 비정상적인 뇌 영역을 잘 감지할 수 있어, 분할 결과의 해석 가능성과 신뢰성을 높이는 데 도움이 돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.07092.pdf

Title: CWT-Net: Super-resolution of Histopathology Images Using a Cross-scale Wavelet-based Transformer

Original Abstract:
Super-resolution (SR) aims to enhance the quality of low-resolution images and has been widely applied in medical imaging. We found that the design principles of most existing methods are influenced by SR tasks based on real-world images and do not take into account the significance of the multi-level structure in pathological images, even if they can achieve respectable objective metric evaluations. In this work, we delve into two super-resolution working paradigms and propose a novel network called CWT-Net, which leverages cross-scale image wavelet transform and Transformer architecture. Our network consists of two branches: one dedicated to learning super-resolution and the other to high-frequency wavelet features. To generate high-resolution histopathology images, the Transformer module shares and fuses features from both branches at various stages. Notably, we have designed a specialized wavelet reconstruction module to effectively enhance the wavelet domain features and enable the network to operate in different modes, allowing for the introduction of additional relevant information from cross-scale images. Our experimental results demonstrate that our model significantly outperforms state-of-the-art methods in both performance and visualization evaluations and can substantially boost the accuracy of image diagnostic networks.

Translated Abstract:
슈퍼 해상도(SR)는 저해상도 이미지를 고해상도로 개선하는 기술로, 의료 영상에서 많이 사용되고 있어. 그런데 대부분의 기존 방법들이 실제 이미지에 기반한 SR 작업의 영향을 받아서 병리 이미지의 다층 구조의 중요성을 잘 고려하지 않고 있어. 비록 일정한 객관적인 평가를 달성할 수 있지만 말이야.

이번 연구에서는 두 가지 슈퍼 해상도 작업 패러다임을 살펴보고, CWT-Net이라는 새로운 네트워크를 제안했어. 이 네트워크는 크로스 스케일 이미지 웨이브렛 변환과 트랜스포머 구조를 활용해. 네트워크는 두 개의 가지로 구성되어 있어: 하나는 슈퍼 해상도를 학습하는 데 집중하고, 다른 하나는 고주파 웨이브렛 특징을 학습해.

고해상도 병리 이미지를 생성하기 위해, 트랜스포머 모듈은 각 단계에서 두 가지 가지의 특징을 공유하고 융합해. 특히, 웨이브렛 도메인 특징을 효과적으로 향상시키고 네트워크가 다양한 모드로 작동할 수 있도록 특별한 웨이브렛 재구성 모듈을 설계했어. 이렇게 하면 크로스 스케일 이미지에서 추가적인 관련 정보를 도입할 수 있어.

실험 결과를 보면, 우리의 모델이 성능과 시각화 평가 모두에서 최첨단 방법들보다 훨씬 뛰어난 성능을 보였고, 이미지 진단 네트워크의 정확성을 크게 향상시킬 수 있다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.07094.pdf

Title: Deep intra-operative illumination calibration of hyperspectral cameras

Original Abstract:
Hyperspectral imaging (HSI) is emerging as a promising novel imaging modality with various potential surgical applications. Currently available cameras, however, suffer from poor integration into the clinical workflow because they require the lights to be switched off, or the camera to be manually recalibrated as soon as lighting conditions change. Given this critical bottleneck, the contribution of this paper is threefold: (1) We demonstrate that dynamically changing lighting conditions in the operating room dramatically affect the performance of HSI applications, namely physiological parameter estimation, and surgical scene segmentation. (2) We propose a novel learning-based approach to automatically recalibrating hyperspectral images during surgery and show that it is sufficiently accurate to replace the tedious process of white reference-based recalibration. (3) Based on a total of 742 HSI cubes from a phantom, porcine models, and rats we show that our recalibration method not only outperforms previously proposed methods, but also generalizes across species, lighting conditions, and image processing tasks. Due to its simple workflow integration as well as high accuracy, speed, and generalization capabilities, our method could evolve as a central component in clinical surgical HSI.

Translated Abstract:
하이퍼스펙트럴 이미징(HSI)은 다양한 수술에 활용될 수 있는 새로운 이미징 기술로 주목받고 있어. 하지만 현재 사용되는 카메라들은 조명을 꺼야 하거나 조명 조건이 바뀔 때마다 수동으로 재교정해야 해서 임상 작업 흐름에 잘 통합되지 못하고 있어.

이 논문에서는 세 가지 중요한 기여를 해. 

첫 번째, 수술실에서 조명이 동적으로 변화할 때 HSI 애플리케이션의 성능, 특히 생리적 파라미터 추정과 수술 장면 분할에 큰 영향을 미친다는 걸 보여줘. 

두 번째, 수술 중 하이퍼스펙트럴 이미지를 자동으로 재교정하는 새로운 학습 기반 접근 방식을 제안해. 이 방법이 기존의 번거로운 백색 기준 재교정 과정을 대체할 만큼 정확하다는 걸 입증했어.

세 번째, 742개의 HSI 큐브를 사용해 우리의 재교정 방법이 이전에 제안된 방법들보다 성능이 뛰어나고, 다양한 종, 조명 조건, 이미지 처리 작업에서도 잘 작동한다는 걸 보여줬어. 

이 방법은 간단한 작업 흐름 통합, 높은 정확도, 속도, 일반화 능력 덕분에 임상 수술 하이퍼스펙트럴 이미징의 핵심 요소로 발전할 가능성이 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07100.pdf

Title: Fast Medical Shape Reconstruction via Meta-learned Implicit Neural Representations

Original Abstract:
Efficient and fast reconstruction of anatomical structures plays a crucial role in clinical practice. Minimizing retrieval and processing times not only potentially enhances swift response and decision-making in critical scenarios but also supports interactive surgical planning and navigation. Recent methods attempt to solve the medical shape reconstruction problem by utilizing implicit neural functions. However, their performance suffers in terms of generalization and computation time, a critical metric for real-time applications. To address these challenges, we propose to leverage meta-learning to improve the network parameters initialization, reducing inference time by an order of magnitude while maintaining high accuracy. We evaluate our approach on three public datasets covering different anatomical shapes and modalities, namely CT and MRI. Our experimental results show that our model can handle various input configurations, such as sparse slices with different orientations and spacings. Additionally, we demonstrate that our method exhibits strong transferable capabilities in generalizing to shape domains unobserved at training time.

Translated Abstract:
해부학적 구조를 효율적이고 빠르게 재구성하는 것은 임상에서 정말 중요한 역할을 해. 정보를 검색하고 처리하는 시간을 최소화하면, 위기 상황에서 빠른 반응과 의사 결정을 도와줄 뿐만 아니라, 상호작용이 가능한 수술 계획과 내비게이션에도 도움이 돼.

최근 방법들은 암묵적인 신경 함수를 이용해 의료 형태 재구성 문제를 해결하려고 해. 하지만 이 방법들은 일반화와 계산 시간에서 성능이 떨어져. 특히 실시간 애플리케이션에선 계산 시간이 중요한데, 이 부분이 아쉬워.

그래서 우리는 메타 학습을 활용해서 네트워크 파라미터 초기화를 개선하려고 해. 이렇게 하면 추론 시간을 10배 줄이면서도 높은 정확도를 유지할 수 있어. 우리는 CT와 MRI를 포함한 다양한 해부학적 형태와 모달리티를 다룬 세 개의 공개 데이터셋에서 우리의 접근 방식을 평가했어.

실험 결과, 우리 모델은 다양한 입력 구성에서도 잘 작동하는 걸 보여줬어. 예를 들어, 다른 방향과 간격을 가진 희박한 슬라이스 같은 입력도 잘 처리해. 그리고 우리는 우리의 방법이 훈련 시간에 관찰되지 않은 형태 도메인으로 일반화하는 강력한 전이 가능성을 가지고 있다는 것도 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07115.pdf

Title: Attention Down-Sampling Transformer, Relative Ranking and Self-Consistency for Blind Image Quality Assessment

Original Abstract:
The no-reference image quality assessment is a challenging domain that addresses estimating image quality without the original reference. We introduce an improved mechanism to extract local and non-local information from images via different transformer encoders and CNNs. The utilization of Transformer encoders aims to mitigate locality bias and generate a non-local representation by sequentially processing CNN features, which inherently capture local visual structures. Establishing a stronger connection between subjective and objective assessments is achieved through sorting within batches of images based on relative distance information. A self-consistency approach to self-supervision is presented, explicitly addressing the degradation of no-reference image quality assessment (NR-IQA) models under equivariant transformations. Our approach ensures model robustness by maintaining consistency between an image and its horizontally flipped equivalent. Through empirical evaluation of five popular image quality assessment datasets, the proposed model outperforms alternative algorithms in the context of no-reference image quality assessment datasets, especially on smaller datasets. Codes are available at \href{this https URL}{this https URL}

Translated Abstract:
노레퍼런스 이미지 품질 평가(No-Reference Image Quality Assessment)는 원본 참조 없이 이미지 품질을 추정하는 어려운 분야야. 우리는 다양한 트랜스포머 인코더와 CNN을 이용해 이미지에서 지역 정보와 비지역 정보를 추출하는 개선된 메커니즘을 소개해.

트랜스포머 인코더를 사용하면 지역 편향을 줄이고 CNN 특성을 순차적으로 처리해서 비지역 표현을 생성할 수 있어. CNN은 본질적으로 지역 시각 구조를 포착하거든. 주관적 평가와 객관적 평가 사이의 더 강한 연결을 위해 이미지 배치 내에서 상대 거리 정보를 기준으로 정렬하는 방법을 사용했어.

자기 일관성(self-consistency) 접근 방식을 통해 자기 감독(self-supervision)을 제시했는데, 이는 동등 변환(equivariant transformations) 하에서 노레퍼런스 이미지 품질 평가(NR-IQA) 모델의 저하 문제를 명확하게 다루고 있어. 우리 접근 방식은 이미지를 수평으로 뒤집은 것과의 일관성을 유지함으로써 모델의 강인함을 보장해.

다섯 가지 인기 있는 이미지 품질 평가 데이터셋에 대한 실증 평가를 통해, 제안된 모델이 노레퍼런스 이미지 품질 평가 데이터셋에서 다른 알고리즘보다 더 나은 성능을 보였어. 특히 작은 데이터셋에서 더 잘 작동했지. 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07128.pdf

Title: Deep Learning Techniques for Hand Vein Biometrics: A Comprehensive Review

Original Abstract:
Biometric authentication has garnered significant attention as a secure and efficient method of identity verification. Among the various modalities, hand vein biometrics, including finger vein, palm vein, and dorsal hand vein recognition, offer unique advantages due to their high accuracy, low susceptibility to forgery, and non-intrusiveness. The vein patterns within the hand are highly complex and distinct for each individual, making them an ideal biometric identifier. Additionally, hand vein recognition is contactless, enhancing user convenience and hygiene compared to other modalities such as fingerprint or iris recognition. Furthermore, the veins are internally located, rendering them less susceptible to damage or alteration, thus enhancing the security and reliability of the biometric system. The combination of these factors makes hand vein biometrics a highly effective and secure method for identity verification. This review paper delves into the latest advancements in deep learning techniques applied to finger vein, palm vein, and dorsal hand vein recognition. It encompasses all essential fundamentals of hand vein biometrics, summarizes publicly available datasets, and discusses state-of-the-art metrics used for evaluating the three modes. Moreover, it provides a comprehensive overview of suggested approaches for finger, palm, dorsal, and multimodal vein techniques, offering insights into the best performance achieved, data augmentation techniques, and effective transfer learning methods, along with associated pretrained deep learning models. Additionally, the review addresses research challenges faced and outlines future directions and perspectives, encouraging researchers to enhance existing methods and propose innovative techniques.

Translated Abstract:
생체 인증은 안전하고 효율적인 신원 확인 방법으로 큰 주목을 받고 있어. 여러 방식 중에서 손 정맥 생체 인식, 즉 손가락 정맥, 손바닥 정맥, 손등 정맥 인식은 높은 정확도, 위조에 대한 낮은 저항성, 그리고 비접촉성 덕분에 독특한 장점이 있어. 손 안의 정맥 패턴은 매우 복잡하고 개인마다 다르기 때문에, 생체 인식에 적합해.

또한, 손 정맥 인식은 비접촉식이라서 사용자 편의성과 위생이 좋아. 지문이나 홍채 인식 같은 다른 방법과 비교했을 때 장점이지. 게다가 정맥은 내부에 위치해 있어서 손상되거나 변형될 가능성이 적어, 생체 시스템의 보안성과 신뢰성을 높이는 데 도움을 줘. 이런 모든 요소들이 결합되어 손 정맥 생체 인식이 신원 확인에 매우 효과적이고 안전한 방법이 되는 거야.

이 논문 리뷰는 손가락 정맥, 손바닥 정맥, 손등 정맥 인식에 적용된 최신 딥러닝 기술의 발전을 다뤄. 손 정맥 생체 인식의 기본 사항을 모두 포함하고 공개된 데이터셋을 요약하며, 세 가지 방식의 평가에 사용되는 최신 메트릭에 대해서도 논의해. 게다가 손가락, 손바닥, 손등, 그리고 다중 모드 정맥 기술에 대한 제안된 접근 방식에 대한 포괄적인 개요를 제공하며, 최고의 성능, 데이터 증강 기법, 효과적인 전이 학습 방법과 관련된 사전 훈련된 딥러닝 모델에 대한 통찰도 담고 있어.

마지막으로, 이 리뷰는 연구자들이 직면한 도전 과제를 다루고, 앞으로의 방향과 전망을 제시하면서 기존 방법을 개선하고 혁신적인 기술을 제안할 수 있도록 독려하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07163.pdf

Title: Mamba Policy: Towards Efficient 3D Diffusion Policy with Hybrid Selective State Models

Original Abstract:
Diffusion models have been widely employed in the field of 3D manipulation due to their efficient capability to learn distributions, allowing for precise prediction of action trajectories. However, diffusion models typically rely on large parameter UNet backbones as policy networks, which can be challenging to deploy on resource-constrained devices. Recently, the Mamba model has emerged as a promising solution for efficient modeling, offering low computational complexity and strong performance in sequence modeling. In this work, we propose the Mamba Policy, a lighter but stronger policy that reduces the parameter count by over 80% compared to the original policy network while achieving superior performance. Specifically, we introduce the XMamba Block, which effectively integrates input information with conditional features and leverages a combination of Mamba and Attention mechanisms for deep feature extraction. Extensive experiments demonstrate that the Mamba Policy excels on the Adroit, Dexart, and MetaWorld datasets, requiring significantly fewer computational resources. Additionally, we highlight the Mamba Policy's enhanced robustness in long-horizon scenarios compared to baseline methods and explore the performance of various Mamba variants within the Mamba Policy framework. Our project page is in this https URL.

Translated Abstract:
확산 모델은 3D 조작 분야에서 널리 사용되고 있어. 이 모델들은 분포를 배우는 데 뛰어나서 행동 경로를 정확하게 예측할 수 있어. 하지만, 보통 확산 모델은 큰 파라미터를 가진 UNet 백본을 정책 네트워크로 사용해서, 자원이 제한된 기기에서는 사용하기 어려운 경우가 많아.

최근에 Mamba 모델이 효율적인 모델링을 위한 유망한 솔루션으로 떠올랐어. 이 모델은 계산 복잡성이 낮고, 시퀀스 모델링에서 강력한 성능을 보여줘. 이번 연구에서는 Mamba Policy를 제안하는데, 이건 원래 정책 네트워크에 비해 파라미터 수를 80% 이상 줄이면서도 성능은 더 뛰어나.

특히 XMamba Block을 도입했는데, 이 블록은 입력 정보를 조건부 특성과 잘 결합하고 Mamba와 Attention 메커니즘의 조합을 활용해 깊이 있는 특성을 추출해. 다양한 실험을 통해 Mamba Policy가 Adroit, Dexart, MetaWorld 데이터셋에서 뛰어난 성능을 보이며, 필요한 계산 자원도 훨씬 적다는 걸 보여줬어.

또한 Mamba Policy가 기존 방법들에 비해 긴 시간 동안의 시나리오에서도 더 강한 내구성을 가진다는 점도 강조하고, Mamba Policy 프레임워크 내에서 다양한 Mamba 변형의 성능도 살펴봤어. 프로젝트 페이지는 이 https URL에 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07171.pdf

Title: AC-IND: Sparse CT reconstruction based on attenuation coefficient estimation and implicit neural distribution

Original Abstract:
Computed tomography (CT) reconstruction plays a crucial role in industrial nondestructive testing and medical diagnosis. Sparse view CT reconstruction aims to reconstruct high-quality CT images while only using a small number of projections, which helps to improve the detection speed of industrial assembly lines and is also meaningful for reducing radiation in medical scenarios. Sparse CT reconstruction methods based on implicit neural representations (INRs) have recently shown promising performance, but still produce artifacts because of the difficulty of obtaining useful prior information. In this work, we incorporate a powerful prior: the total number of material categories of objects. To utilize the prior, we design AC-IND, a self-supervised method based on Attenuation Coefficient Estimation and Implicit Neural Distribution. Specifically, our method first transforms the traditional INR from scalar mapping to probability distribution mapping. Then we design a compact attenuation coefficient estimator initialized with values from a rough reconstruction and fast segmentation. Finally, our algorithm finishes the CT reconstruction by jointly optimizing the estimator and the generated distribution. Through experiments, we find that our method not only outperforms the comparative methods in sparse CT reconstruction but also can automatically generate semantic segmentation maps.

Translated Abstract:
계산 단층 촬영(CT) 재구성은 산업 비파괴 검사와 의료 진단에서 매우 중요한 역할을 해. 희소 뷰 CT 재구성은 적은 수의 투사만 사용해서 고품질 CT 이미지를 재구성하려고 하는데, 이게 산업 조립 라인의 검출 속도를 높이고 의료 환경에서 방사선 노출을 줄이는데 도움이 돼. 

최근에 암시적 신경 표현(INR)을 기반으로 한 희소 CT 재구성 방법들이 좋은 성과를 보였지만, 유용한 사전 정보를 얻기 어렵기 때문에 여전히 아티팩트가 생기는 문제가 있어. 이 연구에서는 강력한 사전 정보를 도입해: 물체의 재료 카테고리 총 개수야. 이 사전 정보를 활용하기 위해 AC-IND라는 자가 감독 방식의 방법을 설계했어. 

먼저, 우리 방법은 전통적인 INR을 스칼라 매핑에서 확률 분포 매핑으로 변환해. 그 다음에, 대략적인 재구성 값과 빠른 분할에서 초기화된 컴팩트 감쇠 계수 추정기를 설계해. 마지막으로, 우리의 알고리즘은 추정기와 생성된 분포를 함께 최적화하면서 CT 재구성을 마무리해. 실험을 통해, 우리 방법이 희소 CT 재구성에서 비교 방법들보다 더 뛰어난 성능을 보이는 것뿐만 아니라 자동으로 의미론적 분할 맵도 생성할 수 있음을 발견했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07218.pdf

Title: Behavioral Cloning Models Reality Check for Autonomous Driving

Original Abstract:
How effective are recent advancements in autonomous vehicle perception systems when applied to real-world autonomous vehicle control? While numerous vision-based autonomous vehicle systems have been trained and evaluated in simulated environments, there is a notable lack of real-world validation for these systems. This paper addresses this gap by presenting the real-world validation of state-of-the-art perception systems that utilize Behavior Cloning (BC) for lateral control, processing raw image data to predict steering commands. The dataset was collected using a scaled research vehicle and tested on various track setups. Experimental results demonstrate that these methods predict steering angles with low error margins in real-time, indicating promising potential for real-world applications.

Translated Abstract:
최근 자율주행차 인식 시스템의 발전이 실제 자율주행차 제어에 얼마나 효과적인지에 대한 연구가 필요해. 많은 비전 기반 자율주행차 시스템이 시뮬레이션 환경에서 훈련되고 평가되었지만, 실제 환경에서의 검증은 부족한 상황이야. 

이 논문은 이 문제를 해결하기 위해 최신 인식 시스템의 실제 검증 결과를 보여줘. 이 시스템은 행동 복제(Behavior Cloning, BC)를 사용해서 측면 제어를 하고, 원본 이미지 데이터를 처리해 조향 명령을 예측해. 데이터셋은 축소된 연구 차량을 사용해서 수집했으며, 다양한 트랙 설정에서 테스트했어. 

실험 결과에 따르면, 이 방법이 조향 각도를 실시간으로 낮은 오차 범위로 예측할 수 있어서, 실제 응용 가능성이 높다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.07236.pdf

Title: 3DGCQA: A Quality Assessment Database for 3D AI-Generated Contents

Original Abstract:
Although 3D generated content (3DGC) offers advantages in reducing production costs and accelerating design timelines, its quality often falls short when compared to 3D professionally generated content. Common quality issues frequently affect 3DGC, highlighting the importance of timely and effective quality assessment. Such evaluations not only ensure a higher standard of 3DGCs for end-users but also provide critical insights for advancing generative technologies. To address existing gaps in this domain, this paper introduces a novel 3DGC quality assessment dataset, 3DGCQA, built using 7 representative Text-to-3D generation methods. During the dataset's construction, 50 fixed prompts are utilized to generate contents across all methods, resulting in the creation of 313 textured meshes that constitute the 3DGCQA dataset. The visualization intuitively reveals the presence of 6 common distortion categories in the generated 3DGCs. To further explore the quality of the 3DGCs, subjective quality assessment is conducted by evaluators, whose ratings reveal significant variation in quality across different generation methods. Additionally, several objective quality assessment algorithms are tested on the 3DGCQA dataset. The results expose limitations in the performance of existing algorithms and underscore the need for developing more specialized quality assessment methods. To provide a valuable resource for future research and development in 3D content generation and quality assessment, the dataset has been open-sourced in this https URL.

Translated Abstract:
3D 생성 콘텐츠(3DGC)는 제작 비용을 줄이고 디자인 속도를 높이는 장점이 있지만, 전문적으로 생성된 3D 콘텐츠와 비교했을 때 품질이 떨어지는 경우가 많아. 이런 3DGC는 자주 품질 문제가 발생하는데, 그래서 적시적절하고 효과적인 품질 평가가 중요해. 이러한 평가가 이루어지면 최종 사용자에게 더 높은 품질의 3DGC를 제공할 수 있고, 생성 기술 발전에도 중요한 통찰력을 줄 수 있어.

이 논문은 이 분야의 기존 문제를 해결하기 위해 3DGC 품질 평가 데이터셋인 3DGCQA를 소개해. 이 데이터셋은 7개의 대표적인 텍스트-투-3D 생성 방법을 사용해서 만들어졌어. 데이터셋을 구성하는 동안 50개의 고정된 프롬프트를 이용해 모든 방법으로 콘텐츠를 생성했고, 그 결과 313개의 텍스처가 있는 메쉬가 만들어졌어. 시각적으로 보면 생성된 3DGC에서 6개의 일반적인 왜곡 종류가 나타나는 걸 알 수 있어.

3DGC의 품질을 더 탐구하기 위해 평가자들이 주관적인 품질 평가를 진행했는데, 이 평가에서 생성 방법마다 품질에 큰 차이가 있다는 걸 보여줬어. 게다가 여러 객관적인 품질 평가 알고리즘도 3DGCQA 데이터셋에서 테스트했어. 그 결과 기존 알고리즘의 성능 한계가 드러났고, 더 전문화된 품질 평가 방법의 개발 필요성이 강조됐어. 

미래의 3D 콘텐츠 생성 및 품질 평가 연구와 개발에 유용한 자원이 될 수 있도록, 이 데이터셋은 오픈소스로 제공되고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07253.pdf

Title: Alignment of Diffusion Models: Fundamentals, Challenges, and Future

Original Abstract:
Diffusion models have emerged as the leading paradigm in generative modeling, excelling in various applications. Despite their success, these models often misalign with human intentions, generating outputs that may not match text prompts or possess desired properties. Inspired by the success of alignment in tuning large language models, recent studies have investigated aligning diffusion models with human expectations and preferences. This work mainly reviews alignment of diffusion models, covering advancements in fundamentals of alignment, alignment techniques of diffusion models, preference benchmarks, and evaluation for diffusion models. Moreover, we discuss key perspectives on current challenges and promising future directions on solving the remaining challenges in alignment of diffusion models. To the best of our knowledge, our work is the first comprehensive review paper for researchers and engineers to comprehend, practice, and research alignment of diffusion models.

Translated Abstract:
확산 모델은 생성 모델링에서 가장 중요한 방식으로 떠올랐고, 여러 응용 분야에서 뛰어난 성과를 내고 있어. 하지만 이런 모델들은 종종 사람의 의도와 맞지 않아서, 텍스트 프롬프트나 원하는 속성과 일치하지 않는 결과를 만들어내기도 해. 

그래서 최근 연구에서는 대형 언어 모델을 조정할 때의 성공을 바탕으로 확산 모델을 사람의 기대와 선호에 맞추는 방법을 조사해왔어. 이 연구는 주로 확산 모델의 정렬(alignment)에 대한 내용을 다루고, 정렬의 기본 원리, 확산 모델의 정렬 기법, 선호 기준, 그리고 확산 모델 평가에 대한 발전을 설명해.

또한 현재의 도전 과제와 확산 모델의 정렬 문제를 해결하기 위한 유망한 미래 방향에 대한 주요 관점도 논의해. 우리가 아는 한, 이 연구는 확산 모델의 정렬을 이해하고 실践하며 연구할 수 있도록 돕는 첫 번째 종합적인 리뷰 논문이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07257.pdf

Title: TopoMap++: A faster and more space efficient technique to compute projections with topological guarantees

Original Abstract:
High-dimensional data, characterized by many features, can be difficult to visualize effectively. Dimensionality reduction techniques, such as PCA, UMAP, and t-SNE, address this challenge by projecting the data into a lower-dimensional space while preserving important relationships. TopoMap is another technique that excels at preserving the underlying structure of the data, leading to interpretable visualizations. In particular, TopoMap maps the high-dimensional data into a visual space, guaranteeing that the 0-dimensional persistence diagram of the Rips filtration of the visual space matches the one from the high-dimensional data. However, the original TopoMap algorithm can be slow and its layout can be too sparse for large and complex datasets. In this paper, we propose three improvements to TopoMap: 1) a more space-efficient layout, 2) a significantly faster implementation, and 3) a novel TreeMap-based representation that makes use of the topological hierarchy to aid the exploration of the projections. These advancements make TopoMap, now referred to as TopoMap++, a more powerful tool for visualizing high-dimensional data which we demonstrate through different use case scenarios.

Translated Abstract:
고차원 데이터는 많은 특징을 가지고 있어서 효과적으로 시각화하기가 어려워. PCA, UMAP, t-SNE 같은 차원 축소 기법들이 이 문제를 해결하려고 하는데, 데이터의 중요한 관계를 유지하면서 저차원 공간으로 데이터를 투영해. 

TopoMap은 데이터의 기본 구조를 잘 보존하는 기술로, 해석 가능한 시각화를 만들어내는데 강점을 가지고 있어. 특히, TopoMap은 고차원 데이터를 시각적 공간으로 매핑하는데, 이때 시각적 공간의 Rips 필터레이션의 0차 지속 다이어그램이 고차원 데이터의 것과 일치하도록 보장해. 

하지만 원래 TopoMap 알고리즘은 속도가 느리고, 큰 복잡한 데이터셋에 대해 레이아웃이 너무 희박할 수 있어. 이 논문에서는 TopoMap을 개선하기 위한 세 가지 방법을 제안해: 1) 더 공간 효율적인 레이아웃, 2) 훨씬 빠른 구현, 3) 위상 계층 구조를 활용한 새로운 TreeMap 기반 표현. 

이런 발전 덕분에 TopoMap을 TopoMap++라고 부르게 되었고, 이제 고차원 데이터를 시각화하는 데 더 강력한 도구가 되었어. 우리는 다양한 사례를 통해 이걸 보여줄 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07275.pdf

Title: Tuning-Free Online Robust Principal Component Analysis through Implicit Regularization

Original Abstract:
The performance of the standard Online Robust Principal Component Analysis (OR-PCA) technique depends on the optimum tuning of the explicit regularizers and this tuning is dataset sensitive. We aim to remove the dependency on these tuning parameters by using implicit regularization. We propose to use the implicit regularization effect of various modified gradient descents to make OR-PCA tuning free. Our method incorporates three different versions of modified gradient descent that separately but naturally encourage sparsity and low-rank structures in the data. The proposed method performs comparable or better than the tuned OR-PCA for both simulated and real-world datasets. Tuning-free ORPCA makes it more scalable for large datasets since we do not require dataset-dependent parameter tuning.

Translated Abstract:
기존의 온라인 강건 주성분 분석(OR-PCA) 기술의 성능은 명시적인 정규화 조정에 따라 달라지는데, 이 조정은 데이터셋에 따라 달라져. 우리는 이 조정 파라미터에 대한 의존성을 없애고 싶어서 암묵적 정규화를 사용하려고 해.

우리는 다양한 수정된 경량 하강법의 암묵적 정규화 효과를 활용해서 OR-PCA가 조정 없이도 작동하도록 만드는 방법을 제안해. 우리 방법은 데이터에서 희소성과 저랭크 구조를 자연스럽게 각각 촉진하는 세 가지 다른 수정된 경량 하강법 버전을 포함하고 있어.

제안한 방법은 시뮬레이션된 데이터셋과 실제 데이터셋 모두에서 조정된 OR-PCA와 비슷하거나 더 나은 성능을 보여줘. 조정이 필요 없는 OR-PCA는 데이터셋에 따라 조정할 필요가 없어서 큰 데이터셋에서도 더 확장성이 좋아.

================================================================================

URL:
https://arxiv.org/pdf/2409.07291.pdf

Title: Exploring User-level Gradient Inversion with a Diffusion Prior

Original Abstract:
We explore user-level gradient inversion as a new attack surface in distributed learning. We first investigate existing attacks on their ability to make inferences about private information beyond training data reconstruction. Motivated by the low reconstruction quality of existing methods, we propose a novel gradient inversion attack that applies a denoising diffusion model as a strong image prior in order to enhance recovery in the large batch setting. Unlike traditional attacks, which aim to reconstruct individual samples and suffer at large batch and image sizes, our approach instead aims to recover a representative image that captures the sensitive shared semantic information corresponding to the underlying user. Our experiments with face images demonstrate the ability of our methods to recover realistic facial images along with private user attributes.

Translated Abstract:
우리는 분산 학습에서 사용자 수준의 그래디언트 반전을 새로운 공격 경로로 탐구해. 먼저, 기존의 공격들이 훈련 데이터 재구성을 넘어서서 개인 정보에 대한 추론을 얼마나 잘 하는지 조사했어. 

기존 방법들의 재구성 품질이 낮아서, 우리는 새로운 그래디언트 반전 공격을 제안해. 이 공격은 강력한 이미지 prior로서 디노이징 확산 모델을 사용하는데, 이 덕분에 대량 배치 상황에서 복구 성능이 향상돼. 

전통적인 공격들은 개별 샘플을 재구성하는 것을 목표로 하고, 큰 배치와 이미지 크기에서는 성능이 떨어지는 반면, 우리의 접근법은 사용자와 관련된 민감한 공유 의미 정보를 포착하는 대표 이미지를 복구하는 데 초점을 맞춰. 

얼굴 이미지에 대한 실험을 통해, 우리의 방법이 현실적인 얼굴 이미지와 개인 사용자 속성을 복구하는 능력을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07292.pdf

Title: A Unified Contrastive Loss for Self-Training

Original Abstract:
Self-training methods have proven to be effective in exploiting abundant unlabeled data in semi-supervised learning, particularly when labeled data is scarce. While many of these approaches rely on a cross-entropy loss function (CE), recent advances have shown that the supervised contrastive loss function (SupCon) can be more effective. Additionally, unsupervised contrastive learning approaches have also been shown to capture high quality data representations in the unsupervised setting. To benefit from these advantages in a semi-supervised setting, we propose a general framework to enhance self-training methods, which replaces all instances of CE losses with a unique contrastive loss. By using class prototypes, which are a set of class-wise trainable parameters, we recover the probability distributions of the CE setting and show a theoretical equivalence with it. Our framework, when applied to popular self-training methods, results in significant performance improvements across three different datasets with a limited number of labeled data. Additionally, we demonstrate further improvements in convergence speed, transfer ability, and hyperparameter stability. The code is available at \url{this https URL}.

Translated Abstract:
자기 학습 방법은 반지도 학습에서 많은 라벨이 없는 데이터를 효과적으로 활용할 수 있는 것으로 입증되었어. 특히 라벨 데이터가 부족할 때 더 그렇지. 많은 방법들이 교차 엔트로피 손실 함수(CE)를 사용하지만, 최근 연구에 따르면 감독 대조 손실 함수(SupCon)가 더 효과적일 수 있다고 해. 게다가 비지도 대조 학습 방법도 비지도 환경에서 고품질 데이터 표현을 잘 포착할 수 있는 것으로 나타났어.

이러한 장점을 반지도 학습 환경에서 활용하기 위해, 우리는 자기 학습 방법을 향상시키는 일반적인 프레임워크를 제안해. 이 프레임워크는 CE 손실을 독특한 대조 손실로 바꾸는 방식으로 작동해. 클래스 프로토타입을 사용해서 클래스별 학습 가능한 파라미터 세트를 만들고, CE 설정의 확률 분포를 복원하며 이와 이론적으로 동등함을 보여줘.

우리의 프레임워크를 인기 있는 자기 학습 방법에 적용했을 때, 라벨 데이터가 제한적인 세 가지 다른 데이터셋에서 성능이 크게 개선됐어. 그리고 수렴 속도, 전이 능력, 하이퍼파라미터 안정성에서도 추가적인 개선을 보여줬어. 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07304.pdf

Title: BLS-GAN: A Deep Layer Separation Framework for Eliminating Bone Overlap in Conventional Radiographs

Original Abstract:
Conventional radiography is the widely used imaging technology in diagnosing, monitoring, and prognosticating musculoskeletal (MSK) diseases because of its easy availability, versatility, and cost-effectiveness. In conventional radiographs, bone overlaps are prevalent, and can impede the accurate assessment of bone characteristics by radiologists or algorithms, posing significant challenges to conventional and computer-aided diagnoses. This work initiated the study of a challenging scenario - bone layer separation in conventional radiographs, in which separate overlapped bone regions enable the independent assessment of the bone characteristics of each bone layer and lay the groundwork for MSK disease diagnosis and its automation. This work proposed a Bone Layer Separation GAN (BLS-GAN) framework that can produce high-quality bone layer images with reasonable bone characteristics and texture. This framework introduced a reconstructor based on conventional radiography imaging principles, which achieved efficient reconstruction and mitigates the recurrent calculations and training instability issues caused by soft tissue in the overlapped regions. Additionally, pre-training with synthetic images was implemented to enhance the stability of both the training process and the results. The generated images passed the visual Turing test, and improved performance in downstream tasks. This work affirms the feasibility of extracting bone layer images from conventional radiographs, which holds promise for leveraging bone layer separation technology to facilitate more comprehensive analytical research in MSK diagnosis, monitoring, and prognosis. Code and dataset will be made available.

Translated Abstract:
전통적인 방사선 촬영은 근골격계(MSK) 질환을 진단하고 모니터링하며 예측하는 데 널리 사용되는 이미지 기술이야. 이 기술은 쉽게 구할 수 있고, 다양한 용도로 쓸 수 있으며, 비용도 저렴해. 

하지만 전통적인 방사선 사진에서는 뼈가 겹치는 경우가 많아서, 방사선 전문의나 알고리즘이 뼈의 특성을 정확하게 평가하기 어렵게 만들지. 이로 인해 전통적인 진단이나 컴퓨터 기반 진단에서 큰 어려움이 생겨. 

이 연구는 전통적인 방사선 사진에서 겹쳐진 뼈 층을 분리하는 어려운 상황을 다루기 시작했어. 이렇게 분리된 뼈 영역은 각 뼈 층의 특성을 독립적으로 평가할 수 있게 해주고, MSK 질환 진단과 자동화의 기초를 마련해. 

연구진은 뼈 층 이미지를 고품질로 생성할 수 있는 'BLS-GAN'이라는 프레임워크를 제안했어. 이 프레임워크는 전통적인 방사선 촬영 원리에 기반한 재구성기를 도입하여, 효율적인 재구성을 이루고 겹친 영역의 연조직으로 인한 반복 계산과 훈련 불안정성 문제를 줄였어. 또한 인공 이미지를 이용한 사전 훈련을 통해 훈련 과정과 결과의 안정성을 높였지. 

생성된 이미지는 시각적 튜링 테스트를 통과했고, 후속 작업에서 성능이 향상되었어. 이 연구는 전통적인 방사선 사진에서 뼈 층 이미지를 추출하는 것이 가능하다는 것을 확인했어. 이는 뼈 층 분리 기술을 활용해 MSK 진단, 모니터링, 예측에 대한 더 포괄적인 분석 연구를 촉진할 수 있는 가능성을 보여줘. 코드와 데이터셋은 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07351.pdf

Title: Federated Impression for Learning with Distributed Heterogeneous Data

Original Abstract:
Standard deep learning-based classification approaches may not always be practical in real-world clinical applications, as they require a centralized collection of all samples. Federated learning (FL) provides a paradigm that can learn from distributed datasets across clients without requiring them to share data, which can help mitigate privacy and data ownership issues. In FL, sub-optimal convergence caused by data heterogeneity is common among data from different health centers due to the variety in data collection protocols and patient demographics across centers. Through experimentation in this study, we show that data heterogeneity leads to the phenomenon of catastrophic forgetting during local training. We propose FedImpres which alleviates catastrophic forgetting by restoring synthetic data that represents the global information as federated impression. To achieve this, we distill the global model resulting from each communication round. Subsequently, we use the synthetic data alongside the local data to enhance the generalization of local training. Extensive experiments show that the proposed method achieves state-of-the-art performance on both the BloodMNIST and Retina datasets, which contain label imbalance and domain shift, with an improvement in classification accuracy of up to 20%.

Translated Abstract:
표준 딥러닝 기반 분류 방법은 실제 임상에서 항상 실용적이지 않을 수 있어. 왜냐하면 모든 샘플을 중앙에서 모아야 하거든. 그래서 연합 학습(Federated Learning, FL)이 등장했어. FL은 데이터 공유 없이 여러 클라이언트에서 분산된 데이터셋을 학습할 수 있게 해줘. 이렇게 하면 개인 정보 보호와 데이터 소유 문제를 줄일 수 있어.

하지만 FL에서는 데이터의 이질성 때문에 최적의 수렴이 잘 안 되는 경우가 많아. 다양한 건강 센터의 데이터 수집 방법과 환자 인구 통계가 다르기 때문이야. 이 연구에서는 데이터 이질성이 지역 훈련 중 치명적인 잊음 현상을 초래한다는 것을 실험을 통해 보여줬어.

우리는 FedImpres라는 방법을 제안해. 이 방법은 연합 인상을 통해 전역 정보를 나타내는 합성 데이터를 복원함으로써 치명적인 잊음 현상을 완화해. 이를 위해 각 통신 라운드에서 나온 글로벌 모델을 증류해. 그 후, 합성 데이터와 지역 데이터를 함께 사용해서 지역 훈련의 일반화를 높여.

광범위한 실험 결과, 제안한 방법이 BloodMNIST와 Retina 데이터셋에서 최첨단 성능을 달성했어. 이 데이터셋들은 레이블 불균형과 도메인 이동이 있는데, 분류 정확도가 최대 20% 개선된 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07361.pdf

Title: Quantifying Knee Cartilage Shape and Lesion: From Image to Metrics

Original Abstract:
Imaging features of knee articular cartilage have been shown to be potential imaging biomarkers for knee osteoarthritis. Despite recent methodological advancements in image analysis techniques like image segmentation, registration, and domain-specific image computing algorithms, only a few works focus on building fully automated pipelines for imaging feature extraction. In this study, we developed a deep-learning-based medical image analysis application for knee cartilage morphometrics, CartiMorph Toolbox (CMT). We proposed a 2-stage joint template learning and registration network, CMT-reg. We trained the model using the OAI-ZIB dataset and assessed its performance in template-to-image registration. The CMT-reg demonstrated competitive results compared to other state-of-the-art models. We integrated the proposed model into an automated pipeline for the quantification of cartilage shape and lesion (full-thickness cartilage loss, specifically). The toolbox provides a comprehensive, user-friendly solution for medical image analysis and data visualization. The software and models are available at this https URL .

Translated Abstract:
무릎 관절 연골의 이미징 특징이 무릎 골관절염의 잠재적인 이미징 바이오마커로 알려져 있어. 최근 이미지 분석 기술이 발전했지만, 이미지 특징을 자동으로 추출하는 파이프라인을 만드는 데 집중한 연구는 많지 않아.

이 연구에서는 무릎 연골의 형태 측정을 위한 딥러닝 기반의 의료 이미지 분석 애플리케이션인 CartiMorph Toolbox (CMT)를 개발했어. 우리는 두 단계의 공동 템플릿 학습 및 등록 네트워크인 CMT-reg를 제안했어. OAI-ZIB 데이터셋을 사용해 모델을 훈련시키고 템플릿-이미지 등록에서 성능을 평가했지. CMT-reg는 다른 최신 모델들과 비교했을 때 경쟁력 있는 결과를 보여줬어.

우리는 제안한 모델을 연골 형태와 병변(특히, 전체 두께 연골 손실)을 정량화하는 자동화된 파이프라인에 통합했어. 이 툴박스는 의료 이미지 분석과 데이터 시각화를 위한 포괄적이고 사용하기 쉬운 솔루션을 제공해. 소프트웨어와 모델은 이 https URL에서 사용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07379.pdf

Title: FIRAL: An Active Learning Algorithm for Multinomial Logistic Regression

Original Abstract:
We investigate theory and algorithms for pool-based active learning for multiclass classification using multinomial logistic regression. Using finite sample analysis, we prove that the Fisher Information Ratio (FIR) lower and upper bounds the excess risk. Based on our theoretical analysis, we propose an active learning algorithm that employs regret minimization to minimize the FIR. To verify our derived excess risk bounds, we conduct experiments on synthetic datasets. Furthermore, we compare FIRAL with five other methods and found that our scheme outperforms them: it consistently produces the smallest classification error in the multiclass logistic regression setting, as demonstrated through experiments on MNIST, CIFAR-10, and 50-class ImageNet.

Translated Abstract:
우리는 다중 클래스 분류를 위한 풀 기반 능동 학습 이론과 알고리즘을 다루고, 다항 로지스틱 회귀를 사용해 연구했어. 유한 샘플 분석을 통해 피셔 정보 비율(FIR)이 초과 위험의 하한과 상한을 제시한다는 것을 증명했어.

이 이론적 분석을 바탕으로, FIR을 최소화하기 위해 후회 최소화(regret minimization)를 사용하는 능동 학습 알고리즘을 제안했어. 우리가 도출한 초과 위험 경계를 검증하기 위해 합성 데이터셋에서 실험을 진행했어. 

또한, FIRAL을 다른 다섯 가지 방법과 비교했는데, 우리 방법이 더 뛰어난 성능을 보여줬어. MNIST, CIFAR-10, 50 클래스 ImageNet 실험을 통해 다중 클래스 로지스틱 회귀 설정에서 항상 가장 작은 분류 오류를 기록했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07402.pdf

Title: What to align in multimodal contrastive learning?

Original Abstract:
Humans perceive the world through multisensory integration, blending the information of different modalities to adapt their behavior. Contrastive learning offers an appealing solution for multimodal self-supervised learning. Indeed, by considering each modality as a different view of the same entity, it learns to align features of different modalities in a shared representation space. However, this approach is intrinsically limited as it only learns shared or redundant information between modalities, while multimodal interactions can arise in other ways. In this work, we introduce CoMM, a Contrastive MultiModal learning strategy that enables the communication between modalities in a single multimodal space. Instead of imposing cross- or intra- modality constraints, we propose to align multimodal representations by maximizing the mutual information between augmented versions of these multimodal features. Our theoretical analysis shows that shared, synergistic and unique terms of information naturally emerge from this formulation, allowing us to estimate multimodal interactions beyond redundancy. We test CoMM both in a controlled and in a series of real-world settings: in the former, we demonstrate that CoMM effectively captures redundant, unique and synergistic information between modalities. In the latter, CoMM learns complex multimodal interactions and achieves state-of-the-art results on the six multimodal benchmarks.

Translated Abstract:
사람들은 여러 감각을 통합해 세상을 인지해. 서로 다른 방식의 정보를 섞어서 행동을 조절하는 거지. 대조 학습(Contrastive learning)은 다중 모달 자기 지도 학습에 좋은 해결책을 제시해. 각 모달을 같은 객체에 대한 다른 시점으로 생각하면서, 서로 다른 모달의 특징들을 공유된 표현 공간에서 맞추도록 학습해. 

하지만 이 방법은 본질적으로 한계가 있어. 왜냐면 모달 간의 공유되거나 중복된 정보만 배우기 때문이야. 다중 모달 상호작용은 다른 방식으로도 일어날 수 있거든. 그래서 우리는 CoMM이라는 대조적 다중 모달 학습 전략을 소개해. 이 전략은 모든 모달이 하나의 다중 모달 공간에서 소통할 수 있게 해줘. 

우리는 서로 다른 모달 제약을 두는 대신, 이 모달 특징들의 증강 버전 간의 상호 정보를 최대화해서 모달 표현을 맞추는 방법을 제안해. 우리의 이론적 분석에서는 공유된 정보, 시너지 있는 정보, 그리고 독특한 정보가 자연스럽게 나타나면서 중복을 넘어서는 다중 모달 상호작용을 추정할 수 있게 돼. 

CoMM을 통제된 환경과 여러 실제 환경에서 테스트해봤어. 통제된 환경에서는 CoMM이 모달 간의 중복, 독특, 시너지 정보를 효과적으로 포착한다는 걸 보여줬고, 실제 환경에서는 CoMM이 복잡한 다중 모달 상호작용을 배워서 여섯 가지 다중 모달 기준에서 최신 기술 수준의 결과를 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07417.pdf

Title: Efficient One-Step Diffusion Refinement for Snapshot Compressive Imaging

Original Abstract:
Coded Aperture Snapshot Spectral Imaging (CASSI) is a crucial technique for capturing three-dimensional multispectral images (MSIs) through the complex inverse task of reconstructing these images from coded two-dimensional measurements. Current state-of-the-art methods, predominantly end-to-end, face limitations in reconstructing high-frequency details and often rely on constrained datasets like KAIST and CAVE, resulting in models with poor generalizability. In response to these challenges, this paper introduces a novel one-step Diffusion Probabilistic Model within a self-supervised adaptation framework for Snapshot Compressive Imaging (SCI). Our approach leverages a pretrained SCI reconstruction network to generate initial predictions from two-dimensional measurements. Subsequently, a one-step diffusion model produces high-frequency residuals to enhance these initial predictions. Additionally, acknowledging the high costs associated with collecting MSIs, we develop a self-supervised paradigm based on the Equivariant Imaging (EI) framework. Experimental results validate the superiority of our model compared to previous methods, showcasing its simplicity and adaptability to various end-to-end or unfolding techniques.

Translated Abstract:
코드 아퍼처 스냅샷 스펙트럴 이미징(CASSI)은 복잡한 역 문제를 통해 3차원 다중 스펙트럼 이미지(MSI)를 캡처하는 데 중요한 기술이야. 현재 최첨단 방법들은 주로 엔드 투 엔드 방식인데, 고주파 세부 정보를 재구성하는 데 한계가 있어. 또한 KAIST나 CAVE 같은 제한된 데이터셋에 의존하는 경우가 많아서 모델의 일반화 능력이 떨어져.

이런 문제들에 대응하기 위해, 이 논문에서는 스냅샷 압축 이미징(SCI)용으로 새로운 원스텝 확산 확률 모델을 소개해. 이 방법은 미리 훈련된 SCI 재구성 네트워크를 활용해서 2차원 측정값으로부터 초기 예측을 생성해. 그 다음, 원스텝 확산 모델이 고주파 잔차를 생성해서 이 초기 예측을 향상시켜.

또한, MSI 수집에 드는 높은 비용을 고려해서, 우리는 변환 불변 이미징(EI) 프레임워크에 기반한 자가 감독 패러다임을 개발했어. 실험 결과는 우리의 모델이 이전 방법들에 비해 우수하다는 것을 보여주고, 간단하면서도 여러 엔드 투 엔드 방식이나 전개 기법에 잘 적응할 수 있음을 증명해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07422.pdf

Title: Controllable retinal image synthesis using conditional StyleGAN and latent space manipulation for improved diagnosis and grading of diabetic retinopathy

Original Abstract:
Diabetic retinopathy (DR) is a consequence of diabetes mellitus characterized by vascular damage within the retinal tissue. Timely detection is paramount to mitigate the risk of vision loss. However, training robust grading models is hindered by a shortage of annotated data, particularly for severe cases. This paper proposes a framework for controllably generating high-fidelity and diverse DR fundus images, thereby improving classifier performance in DR grading and detection. We achieve comprehensive control over DR severity and visual features (optic disc, vessel structure, lesion areas) within generated images solely through a conditional StyleGAN, eliminating the need for feature masks or auxiliary networks. Specifically, leveraging the SeFa algorithm to identify meaningful semantics within the latent space, we manipulate the DR images generated conditionally on grades, further enhancing the dataset diversity. Additionally, we propose a novel, effective SeFa-based data augmentation strategy, helping the classifier focus on discriminative regions while ignoring redundant features. Using this approach, a ResNet50 model trained for DR detection achieves 98.09% accuracy, 99.44% specificity, 99.45% precision, and an F1-score of 98.09%. Moreover, incorporating synthetic images generated by conditional StyleGAN into ResNet50 training for DR grading yields 83.33% accuracy, a quadratic kappa score of 87.64%, 95.67% specificity, and 72.24% precision. Extensive experiments conducted on the APTOS 2019 dataset demonstrate the exceptional realism of the generated images and the superior performance of our classifier compared to recent studies.

Translated Abstract:
당뇨병성 망막병증(DR)은 당뇨병의 결과로 발생하며, 망막 조직의 혈관 손상이 특징이야. 시력을 잃을 위험을 줄이기 위해서는 조기 발견이 매우 중요해. 하지만, 심각한 경우에 대한 주석이 달린 데이터가 부족해서 강력한 평가 모델을 훈련하는 데 어려움이 있어.

이 논문에서는 높은 품질과 다양한 DR 망막 사진을 조절 가능하게 생성하는 프레임워크를 제안해. 이렇게 해서 DR 등급 및 탐지에서 분류기 성능을 향상시키려고 해. 우리는 조건부 StyleGAN을 통해 생성된 이미지에서 DR의 심각도와 시각적 특징(시신경, 혈관 구조, 병변 영역)에 대해 포괄적인 제어를 할 수 있어. 이 과정에서 특징 마스크나 보조 네트워크는 필요 없어.

특히, SeFa 알고리즘을 활용해 잠재 공간 내에서 의미 있는 의미를 식별하고, 이를 바탕으로 DR 이미지의 등급에 따라 이미지를 조작해 데이터셋의 다양성을 더욱 향상시켜. 또, 새로운 SeFa 기반 데이터 증강 전략도 제안해, 이렇게 하면 분류기가 구별 가능한 영역에 집중하고 중복된 특징은 무시할 수 있어.

이 방법을 사용해서 DR 탐지용으로 훈련된 ResNet50 모델은 98.09%의 정확도, 99.44%의 특이도, 99.45%의 정밀도, 그리고 98.09%의 F1 점수를 달성했어. 게다가, 조건부 StyleGAN으로 생성된 합성 이미지를 ResNet50 훈련에 포함시키면 DR 등급에서 83.33%의 정확도, 87.64%의 이차 카파 점수, 95.67%의 특이도, 그리고 72.24%의 정밀도를 얻을 수 있어.

APTOS 2019 데이터셋을 활용한 광범위한 실험 결과, 생성된 이미지의 뛰어난 현실성과 우리의 분류기가 최근 연구들에 비해 우수한 성능을 보임을 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07446.pdf

Title: Adaptive Adapter Routing for Long-Tailed Class-Incremental Learning

Original Abstract:
In our ever-evolving world, new data exhibits a long-tailed distribution, such as e-commerce platform reviews. This necessitates continuous model learning imbalanced data without forgetting, addressing the challenge of long-tailed class-incremental learning (LTCIL). Existing methods often rely on retraining linear classifiers with former data, which is impractical in real-world settings. In this paper, we harness the potent representation capabilities of pre-trained models and introduce AdaPtive Adapter RouTing (APART) as an exemplar-free solution for LTCIL. To counteract forgetting, we train inserted adapters with frozen pre-trained weights for deeper adaptation and maintain a pool of adapters for selection during sequential model updates. Additionally, we present an auxiliary adapter pool designed for effective generalization, especially on minority classes. Adaptive instance routing across these pools captures crucial correlations, facilitating a comprehensive representation of all classes. Consequently, APART tackles the imbalance problem as well as catastrophic forgetting in a unified framework. Extensive benchmark experiments validate the effectiveness of APART. Code is available at: this https URL

Translated Abstract:
우리의 변화하는 세상에서, 새로운 데이터는 긴 꼬리를 가진 분포를 보여줘. 예를 들어, 전자상거래 플랫폼의 리뷰 같은 것들이 그렇지. 그래서 우리는 잊지 않고 불균형한 데이터를 계속 학습할 수 있는 모델이 필요해. 이걸 긴 꼬리 클래스 점진적 학습(LTCIL)이라고 해. 기존 방법들은 보통 예전 데이터를 가지고 선형 분류기를 다시 훈련하는데, 이건 실제 상황에서 실용적이지 않아.

그래서 우리는 미리 훈련된 모델의 강력한 표현 능력을 활용하고, LTCIL을 위한 예시 없는 솔루션인 AdaPtive Adapter RouTing(APART)를 소개해. 잊어버리는 걸 막기 위해 우리는 고정된 미리 훈련된 가중치로 삽입된 어댑터를 훈련시키고, 순차적으로 모델 업데이트할 때 선택할 수 있도록 어댑터 풀을 유지해.

또한, 우리는 소수 클래스에 대해 효과적인 일반화를 위해 설계된 보조 어댑터 풀도 제안해. 이 풀들 간의 적응형 인스턴스 라우팅은 중요한 상관관계를 포착해서 모든 클래스에 대한 포괄적인 표현을 가능하게 해. 결과적으로 APART는 불균형 문제와 끔찍한 잊음 문제를 통합된 프레임워크로 해결해. 다양한 벤치마크 실험을 통해 APART의 효과가 검증됐어. 코드도 이 링크에서 확인할 수 있어: this https URL

================================================================================

URL:
https://arxiv.org/pdf/2409.07450.pdf

Title: VMAS: Video-to-Music Generation via Semantic Alignment in Web Music Videos

Original Abstract:
We present a framework for learning to generate background music from video inputs. Unlike existing works that rely on symbolic musical annotations, which are limited in quantity and diversity, our method leverages large-scale web videos accompanied by background music. This enables our model to learn to generate realistic and diverse music. To accomplish this goal, we develop a generative video-music Transformer with a novel semantic video-music alignment scheme. Our model uses a joint autoregressive and contrastive learning objective, which encourages the generation of music aligned with high-level video content. We also introduce a novel video-beat alignment scheme to match the generated music beats with the low-level motions in the video. Lastly, to capture fine-grained visual cues in a video needed for realistic background music generation, we introduce a new temporal video encoder architecture, allowing us to efficiently process videos consisting of many densely sampled frames. We train our framework on our newly curated DISCO-MV dataset, consisting of 2.2M video-music samples, which is orders of magnitude larger than any prior datasets used for video music generation. Our method outperforms existing approaches on the DISCO-MV and MusicCaps datasets according to various music generation evaluation metrics, including human evaluation. Results are available at this https URL

Translated Abstract:
우리는 비디오 입력으로부터 배경 음악을 생성하는 학습 프레임워크를 제안해. 기존의 연구들은 기호적인 음악 주석에 의존하는데, 이건 수량과 다양성이 제한적이야. 우리 방법은 대규모 웹 비디오와 배경 음악을 활용해서, 모델이 현실적이고 다양한 음악을 생성하도록 배워. 

이를 위해, 우리는 새로운 시멘틱 비디오-음악 정렬 방식을 가진 생성적 비디오-음악 트랜스포머를 개발했어. 이 모델은 고수준 비디오 콘텐츠와 잘 맞는 음악 생성을 유도하는 공동 자율 회귀 및 대조 학습 목표를 사용해. 또한, 생성된 음악 비트를 비디오의 저수준 동작과 맞추기 위해 새로운 비디오-비트 정렬 방식을 도입했어. 

마지막으로, 현실적인 배경 음악 생성을 위해 필요한 비디오의 세부적인 시각적 단서를 포착하기 위해, 새로운 시간적 비디오 인코더 아키텍처를 도입했어. 이렇게 해서 많은 밀집 샘플링된 프레임으로 구성된 비디오를 효율적으로 처리할 수 있어. 

우리는 새로운 DISCO-MV 데이터셋에서 이 프레임워크를 훈련했는데, 여기엔 220만 개의 비디오-음악 샘플이 들어 있어. 이건 비디오 음악 생성에 사용된 어떤 이전 데이터셋보다 훨씬 더 큰 규모야. 우리의 방법은 DISCO-MV와 MusicCaps 데이터셋에서 다양한 음악 생성 평가 지표, 사람 평가를 포함해서 기존 접근법보다 성능이 뛰어나. 결과는 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2109.04569.pdf

Title: Open-World Distributed Robot Self-Localization with Transferable Visual Vocabulary and Both Absolute and Relative Features

Original Abstract:
Visual robot self-localization is a fundamental problem in visual robot navigation and has been studied across various problem settings, including monocular and sequential localization. However, many existing studies focus primarily on single-robot scenarios, with limited exploration into general settings involving diverse robots connected through wireless networks with constrained communication capacities, such as open-world distributed robot systems. In particular, issues related to the transfer and sharing of key knowledge, such as visual descriptions and visual vocabulary, between robots have been largely neglected. This work introduces a new self-localization framework designed for open-world distributed robot systems that maintains state-of-the-art performance while offering two key advantages: (1) it employs an unsupervised visual vocabulary model that maps to multimodal, lightweight, and transferable visual features, and (2) the visual vocabulary itself is a lightweight and communication-friendly model. Although the primary focus is on encoding monocular view images, the framework can be easily extended to sequential localization applications. By utilizing complementary similarity-preserving features -- both absolute and relative -- the framework meets the requirements for being unsupervised, multimodal, lightweight, and transferable. All features are learned and recognized using a lightweight graph neural network and scene graph. The effectiveness of the proposed method is validated in both passive and active self-localization scenarios.

Translated Abstract:
비주얼 로봇 자기 위치 추정은 비주얼 로봇 내비게이션에서 아주 중요한 문제야. 이건 단안 카메라와 순차적 위치 추정 같은 다양한 문제 설정에서 연구돼 왔어. 그런데 많은 기존 연구는 주로 단일 로봇 상황에만 집중하고 있어서, 다양한 로봇들이 무선 네트워크로 연결돼서 통신 용량이 제한된 일반적인 환경인 오픈 월드 분산 로봇 시스템에 대한 탐색은 부족해. 특히 로봇들 간에 비주얼 설명이나 비주얼 어휘 같은 중요한 지식의 전송과 공유 문제는 크게 간과됐어.

이 연구는 오픈 월드 분산 로봇 시스템을 위한 새로운 자기 위치 추정 프레임워크를 소개해. 이 프레임워크는 최신 성능을 유지하면서 두 가지 주요 장점을 제공해: (1) 다양한 시각적 특징을 가볍고 전이 가능한 형태로 매핑하는 비지도 학습 비주얼 어휘 모델을 사용하고, (2) 비주얼 어휘 자체가 가볍고 통신 친화적인 모델이야. 주로 단안 카메라 뷰 이미지를 인코딩하는 데 초점을 맞추지만, 이 프레임워크는 순차적 위치 추정 애플리케이션으로 쉽게 확장할 수 있어.

보완적인 유사성 보존 특징을 활용해서, 절대적이고 상대적인 특징 모두를 사용해. 이 프레임워크는 비지도 학습, 다중 모달, 경량화, 전이 가능하다는 요구 사항을 충족해. 모든 특징은 가벼운 그래프 신경망과 장면 그래프를 이용해 학습하고 인식해. 제안된 방법의 효과는 수동 및 능동 자기 위치 추정 상황에서 검증됐어.

================================================================================

URL:
https://arxiv.org/pdf/2303.13430.pdf

Title: Medical diffusion on a budget: Textual Inversion for medical image generation

Original Abstract:
Diffusion models for text-to-image generation, known for their efficiency, accessibility, and quality, have gained popularity. While inference with these systems on consumer-grade GPUs is increasingly feasible, training from scratch requires large captioned datasets and significant computational resources. In medical image generation, the limited availability of large, publicly accessible datasets with text reports poses challenges due to legal and ethical concerns. This work shows that adapting pre-trained Stable Diffusion models to medical imaging modalities is achievable by training text embeddings using Textual Inversion. In this study, we experimented with small medical datasets (100 samples each from three modalities) and trained within hours to generate diagnostically accurate images, as judged by an expert radiologist. Experiments with Textual Inversion training and inference parameters reveal the necessity of larger embeddings and more examples in the medical domain. Classification experiments show an increase in diagnostic accuracy (AUC) for detecting prostate cancer on MRI, from 0.78 to 0.80. Further experiments demonstrate embedding flexibility through disease interpolation, combining pathologies, and inpainting for precise disease appearance control. The trained embeddings are compact (less than 1 MB), enabling easy data sharing with reduced privacy concerns.

Translated Abstract:
텍스트-이미지 생성에 대한 확산 모델은 효율성, 접근성, 그리고 품질 덕분에 인기를 끌고 있어. 소비자용 GPU에서 이 시스템을 사용하는 것은 점점 더 가능해지고 있지만, 처음부터 훈련하려면 큰 캡션 데이터셋과 많은 계산 자원이 필요해. 의료 이미지 생성에서는 텍스트 보고서가 포함된 큰 공개 데이터셋이 부족해서 법적과 윤리적 문제로 어려움이 있어.

이 연구에서는 사전 훈련된 Stable Diffusion 모델을 의료 이미징 방식에 맞게 조정할 수 있다는 걸 보여줘. Textual Inversion을 사용해서 텍스트 임베딩을 훈련했거든. 우리는 세 가지 방식에서 각 100개의 샘플로 구성된 작은 의료 데이터셋을 가지고 실험했는데, 몇 시간 만에 전문가 방사선 의사가 진단 정확성을 평가한 이미지를 생성할 수 있었어.

Textual Inversion의 훈련과 추론 파라미터를 가지고 실험한 결과, 의료 분야에서는 더 큰 임베딩과 더 많은 예제가 필요하다는 걸 알게 되었어. 분류 실험에서는 MRI에서 전립선암을 탐지하는 진단 정확도(AUC)가 0.78에서 0.80으로 증가했어. 추가 실험에서는 질병 보간, 병리학 결합, 그리고 정밀한 질병 외관 조정을 위한 인페인팅을 통해 임베딩의 유연성을 보여줬어. 훈련된 임베딩은 크기가 작아서(1MB 미만) 데이터 공유가 쉽고 개인정보 보호 문제를 줄일 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2309.01487.pdf

Title: GenSelfDiff-HIS: Generative Self-Supervision Using Diffusion for Histopathological Image Segmentation

Original Abstract:
Histopathological image segmentation is a laborious and time-intensive task, often requiring analysis from experienced pathologists for accurate examinations. To reduce this burden, supervised machine-learning approaches have been adopted using large-scale annotated datasets for histopathological image analysis. However, in several scenarios, the availability of large-scale annotated data is a bottleneck while training such models. Self-supervised learning (SSL) is an alternative paradigm that provides some respite by constructing models utilizing only the unannotated data which is often abundant. The basic idea of SSL is to train a network to perform one or many pseudo or pretext tasks on unannotated data and use it subsequently as the basis for a variety of downstream tasks. It is seen that the success of SSL depends critically on the considered pretext task. While there have been many efforts in designing pretext tasks for classification problems, there haven't been many attempts on SSL for histopathological segmentation. Motivated by this, we propose an SSL approach for segmenting histopathological images via generative diffusion models in this paper. Our method is based on the observation that diffusion models effectively solve an image-to-image translation task akin to a segmentation task. Hence, we propose generative diffusion as the pretext task for histopathological image segmentation. We also propose a multi-loss function-based fine-tuning for the downstream task. We validate our method using several metrics on two publically available datasets along with a newly proposed head and neck (HN) cancer dataset containing hematoxylin and eosin (H\&E) stained images along with annotations. Codes will be made public at this https URL.

Translated Abstract:
조직병리학 이미지 분할은 힘들고 시간이 많이 걸리는 작업이야. 보통 정확한 검사를 위해서는 경험이 많은 병리학자들의 분석이 필요해. 이런 부담을 줄이기 위해, 대규모 주석이 달린 데이터셋을 이용한 감독 기계 학습 방법이 사용되고 있어. 하지만 대규모 주석 데이터가 부족한 경우가 많아 모델 훈련에 어려움이 생겨.

자기 지도 학습(SSL)은 주석이 없는 데이터만으로 모델을 만드는 방법이야. 이런 데이터는 흔하게 구할 수 있어서 도움이 돼. SSL의 기본 아이디어는 주석이 없는 데이터에서 하나 이상의 가짜 작업(pretext task)을 수행하도록 네트워크를 훈련시키고, 그 결과를 다양한 후속 작업에 사용하는 거야. SSL의 성공은 선택한 가짜 작업에 크게 의존해. 분류 문제를 위한 가짜 작업 설계에는 여러 노력이 있었지만, 조직병리학 분할을 위한 SSL 시도는 많지 않았어.

그래서 우리는 이 논문에서 생성적 확산 모델을 이용한 조직병리학 이미지 분할을 위한 SSL 접근 방식을 제안해. 우리의 방법은 확산 모델이 이미지-이미지 변환 작업, 즉 분할 작업을 효과적으로 해결한다는 관찰에 기반해. 그래서 우리는 조직병리학 이미지 분할을 위해 생성적 확산을 가짜 작업으로 제안해. 또한 후속 작업을 위한 다중 손실 함수 기반의 미세 조정 방법도 제안해.

우리는 두 개의 공개 데이터셋과 함께 주석이 포함된 헤마톡실린과 에오신(H&E) 염색 이미지로 구성된 새로운 두경부(HN) 암 데이터셋을 사용해서 방법을 검증했어. 코드는 이 https URL에서 공개될 거야.

================================================================================

URL:
https://arxiv.org/pdf/2309.07891.pdf

Title: HandNeRF: Learning to Reconstruct Hand-Object Interaction Scene from a Single RGB Image

Original Abstract:
This paper presents a method to learn hand-object interaction prior for reconstructing a 3D hand-object scene from a single RGB image. The inference as well as training-data generation for 3D hand-object scene reconstruction is challenging due to the depth ambiguity of a single image and occlusions by the hand and object. We turn this challenge into an opportunity by utilizing the hand shape to constrain the possible relative configuration of the hand and object geometry. We design a generalizable implicit function, HandNeRF, that explicitly encodes the correlation of the 3D hand shape features and 2D object features to predict the hand and object scene geometry. With experiments on real-world datasets, we show that HandNeRF is able to reconstruct hand-object scenes of novel grasp configurations more accurately than comparable methods. Moreover, we demonstrate that object reconstruction from HandNeRF ensures more accurate execution of downstream tasks, such as grasping and motion planning for robotic hand-over and manipulation. Homepage: this https URL

Translated Abstract:
이 논문에서는 단일 RGB 이미지에서 3D 손-객체 장면을 재구성하기 위해 손-객체 상호작용 정보를 학습하는 방법을 제안해. 단일 이미지의 깊이 모호성과 손과 객체의 가림 때문에 3D 손-객체 장면 재구성이 어렵거든. 우리는 이 문제를 손 모양을 활용해서 손과 객체의 상대적인 구성을 제한하는 기회로 바꿨어.

우리는 HandNeRF라는 일반화 가능한 암묵적 함수를 설계했어. 이 함수는 3D 손 모양 특징과 2D 객체 특징의 상관관계를 명시적으로 인코딩해서 손과 객체의 장면 기하학을 예측해. 실제 데이터셋을 가지고 실험해본 결과, HandNeRF가 새로운 그립 구성의 손-객체 장면을 기존 방법들보다 더 정확하게 재구성할 수 있음을 보여줬어.

게다가, HandNeRF로부터의 객체 재구성이 로봇 손 이동 및 조작을 위한 그립 및 동작 계획 같은 후속 작업의 실행을 더 정확하게 해준다는 것도 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2311.15994.pdf

Title: Adversarial Doodles: Interpretable and Human-drawable Attacks Provide Describable Insights

Original Abstract:
DNN-based image classifiers are susceptible to adversarial attacks. Most previous adversarial attacks do not have clear patterns, making it difficult to interpret attacks' results and gain insights into classifiers' mechanisms. Therefore, we propose Adversarial Doodles, which have interpretable shapes. We optimize black bezier curves to fool the classifier by overlaying them onto the input image. By introducing random affine transformation and regularizing the doodled area, we obtain small-sized attacks that cause misclassification even when humans replicate them by hand. Adversarial doodles provide describable insights into the relationship between the human-drawn doodle's shape and the classifier's output, such as "When we add three small circles on a helicopter image, the ResNet-50 classifier mistakenly classifies it as an airplane."

Translated Abstract:
DNN 기반 이미지 분류기는 적대적 공격에 취약해. 이전의 많은 적대적 공격들은 뚜렷한 패턴이 없어서 공격 결과를 해석하고 분류기의 작동 방식을 이해하기가 어려워. 그래서 우리는 해석 가능한 형태를 가진 '적대적 낙서(Adversarial Doodles)'를 제안해.

우리는 검정색 베지어 곡선을 최적화해서 입력 이미지 위에 겹쳐 놓아 분류기를 속이도록 했어. 무작위 아핀 변환을 도입하고 낙서 영역을 규제해서, 사람이 손으로 따라 그리더라도 잘못 분류하게 만드는 작은 크기의 공격을 얻었어. 

적대적 낙서는 사람이 그린 낙서의 형태와 분류기의 출력 사이의 관계를 설명할 수 있는 통찰을 제공해. 예를 들어 "헬리콥터 이미지에 작은 원 세 개를 추가하면 ResNet-50 분류기가 이를 비행기로 잘못 분류해." 같은 식이야.

================================================================================

URL:
https://arxiv.org/pdf/2311.18402.pdf

Title: MV-CLIP: Multi-View CLIP for Zero-shot 3D Shape Recognition

Original Abstract:
Large-scale pre-trained models have demonstrated impressive performance in vision and language tasks within open-world scenarios. Due to the lack of comparable pre-trained models for 3D shapes, recent methods utilize language-image pre-training to realize zero-shot 3D shape recognition. However, due to the modality gap, pretrained language-image models are not confident enough in the generalization to 3D shape recognition. Consequently, this paper aims to improve the confidence with view selection and hierarchical prompts. Leveraging the CLIP model as an example, we employ view selection on the vision side by identifying views with high prediction confidence from multiple rendered views of a 3D shape. On the textual side, the strategy of hierarchical prompts is proposed for the first time. The first layer prompts several classification candidates with traditional class-level descriptions, while the second layer refines the prediction based on function-level descriptions or further distinctions between the candidates. Remarkably, without the need for additional training, our proposed method achieves impressive zero-shot 3D classification accuracies of 84.44%, 91.51%, and 66.17% on ModelNet40, ModelNet10, and ShapeNet Core55, respectively. Furthermore, we will make the code publicly available to facilitate reproducibility and further research in this area.

Translated Abstract:
대규모로 사전 훈련된 모델들은 오픈 월드 시나리오에서 비전과 언어 작업에서 뛰어난 성능을 보여줬어. 하지만 3D 형태에 대한 비교 가능한 사전 훈련 모델이 부족해서, 최근 방법들은 언어-이미지 사전 훈련을 이용해 제로샷 3D 형태 인식을 구현하고 있어. 하지만 모드 간의 차이 때문에 사전 훈련된 언어-이미지 모델이 3D 형태 인식에 대해 자신감이 부족해. 

그래서 이 논문은 뷰 선택과 계층적 프롬프트를 통해 자신감을 높이는 걸 목표로 해. CLIP 모델을 예로 들면, 우리는 3D 형태의 여러 렌더링 뷰 중에서 높은 예측 신뢰도를 가진 뷰를 선택해. 텍스트 측면에서는 계층적 프롬프트 전략을 처음으로 제안해. 첫 번째 레이어는 전통적인 클래스 레벨 설명으로 여러 분류 후보를 제시하고, 두 번째 레이어는 기능 레벨 설명이나 후보 간의 추가 구분을 기반으로 예측을 다듬어. 

놀랍게도, 추가 훈련 없이도 우리가 제안한 방법은 ModelNet40, ModelNet10, ShapeNet Core55에서 각각 84.44%, 91.51%, 66.17%의 인상적인 제로샷 3D 분류 정확도를 달성했어. 게다가, 우리는 코드도 공개해서 재현성과 이 분야의 추가 연구를 촉진할 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2312.10813.pdf

Title: Towards Efficient Vision-Language Tuning: More Information Density, More Generalizability

Original Abstract:
With the advancement of large pre-trained vision-language models, effectively transferring the knowledge embedded within these foundational models to downstream tasks has become a pivotal topic, particularly in data-scarce environments. Recently, parameter-efficient fine-tuning approaches, especially prompt tuning, have garnered considerable attention. To better understand the nature of prompt tuning, we propose the concept of ``Information Density'' (ID) to indicate whether a matrix strongly belongs to certain feature spaces rather than being evenly distributed across various feature spaces. We suppose a higher ID with strong bias across some feature spaces naturally leads to excellent robustness and stability. Our research, inspired by the observation that generalizability is closely linked to the information density of the prompt matrix, introduces the Dense Information Prompt (DIP). DIP aims to enhance information density to improve generalization. Furthermore, DIP significantly reduces the number of tunable parameters and the requisite storage space, making it particularly advantageous in resource-constrained settings. Comprehensive experiments substantiate the superiority of DIP. Notably, DIP surpasses the latest state-of-the-art methods by a substantial margin with an exceptionally small parameter count. Across a range of tasks spanning 11 datasets, DIP improves the average downstream accuracy of classic prompt tuning by up to 5.76% using merely 0.5K parameters.

Translated Abstract:
대규모 사전 학습된 비전-언어 모델이 발전하면서, 이 모델에 내재된 지식을 실제 작업으로 잘 전이하는 방법이 중요한 주제가 되었어. 특히 데이터가 부족한 환경에서 말이지. 최근에는 파라미터 효율적인 미세 조정 방법, 특히 프롬프트 튜닝이 많은 관심을 받고 있어.

프롬프트 튜닝의 본질을 더 잘 이해하기 위해, 우리는 "정보 밀도"라는 개념을 제안해. 이건 어떤 행렬이 특정 특성 공간에 강하게 속하는지, 아니면 여러 특성 공간에 고르게 분포되어 있는지를 나타내는 거야. 우리는 정보 밀도가 높고 특정 특성 공간에 강한 편향이 있으면, 자연스럽게 더 뛰어난 강인성과 안정성을 가져온다고 생각해.

우리 연구는 일반화 가능성이 프롬프트 행렬의 정보 밀도와 밀접하게 연관되어 있다는 점에서 영감을 얻었고, Dense Information Prompt (DIP)를 소개해. DIP는 정보 밀도를 높여서 일반화를 개선하는 게 목표야. 게다가 DIP는 조정해야 할 파라미터 수와 필요한 저장 공간을 크게 줄여서 자원이 제한된 환경에서 특히 유리해.

종합적인 실험 결과는 DIP의 우수성을 뒷받침해. 특히 DIP는 최신 최첨단 방법들을 상당한 차이로 초월하면서도 매우 적은 파라미터 수로 성능을 발휘해. 11개의 데이터셋을 아우르는 다양한 작업에서, DIP는 고전적인 프롬프트 튜닝의 평균 하위 정확도를 최대 5.76% 개선했어. 단 0.5K 파라미터만 사용해서 말이지.

================================================================================

URL:
https://arxiv.org/pdf/2402.03003.pdf

Title: [Citation needed] Data usage and citation practices in medical imaging conferences

Original Abstract:
Medical imaging papers often focus on methodology, but the quality of the algorithms and the validity of the conclusions are highly dependent on the datasets used. As creating datasets requires a lot of effort, researchers often use publicly available datasets, there is however no adopted standard for citing the datasets used in scientific papers, leading to difficulty in tracking dataset usage. In this work, we present two open-source tools we created that could help with the detection of dataset usage, a pipeline \url{this https URL} using OpenAlex and full-text analysis, and a PDF annotation software \url{this https URL} used in our study to manually label the presence of datasets. We applied both tools on a study of the usage of 20 publicly available medical datasets in papers from MICCAI and MIDL. We compute the proportion and the evolution between 2013 and 2023 of 3 types of presence in a paper: cited, mentioned in the full text, cited and mentioned. Our findings demonstrate the concentration of the usage of a limited set of datasets. We also highlight different citing practices, making the automation of tracking difficult.

Translated Abstract:
의료 이미징 논문은 보통 방법론에 집중하지만, 알고리즘의 품질과 결론의 유효성은 사용되는 데이터셋에 크게 의존해. 데이터셋을 만드는 데는 많은 노력이 필요하기 때문에 연구자들은 종종 공개된 데이터셋을 사용해. 그런데 과학 논문에서 데이터셋을 인용하는 데에 대한 표준이 없어서 데이터셋 사용 추적이 어려워.

이번 연구에서는 데이터셋 사용을 탐지하는 데 도움이 될 수 있는 두 개의 오픈 소스 도구를 소개해. 하나는 OpenAlex와 전체 텍스트 분석을 사용하는 파이프라인이고, 다른 하나는 우리 연구에서 데이터셋의 존재를 수동으로 라벨링하기 위해 사용한 PDF 주석 소프트웨어야. 

우리는 MICCAI와 MIDL에서 발표된 논문들에 대해 20개의 공개 의료 데이터셋 사용을 조사하면서 이 두 도구를 적용했어. 2013년부터 2023년까지 논문에서의 세 가지 존재 유형: 인용된 것, 전체 텍스트에서 언급된 것, 인용 및 언급된 것의 비율과 변화를 계산했어. 

우리의 결과는 제한된 데이터셋 세트의 사용이 집중되어 있음을 보여줘. 또한, 서로 다른 인용 관행을 강조해서 추적 자동화가 어렵다는 점도 밝혀냈어.

================================================================================

URL:
https://arxiv.org/pdf/2402.04013.pdf

Title: Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and Defenses

Original Abstract:
Deep Neural Networks (DNNs) have revolutionized various domains with their exceptional performance across numerous applications. However, Model Inversion (MI) attacks, which disclose private information about the training dataset by abusing access to the trained models, have emerged as a formidable privacy threat. Given a trained network, these attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training samples, posing significant privacy concerns. Despite the rapid advances in the field, we lack a comprehensive and systematic overview of existing MI attacks and defenses. To fill this gap, this paper thoroughly investigates this realm and presents a holistic survey. Firstly, our work briefly reviews early MI studies on traditional machine learning scenarios. We then elaborately analyze and compare numerous recent attacks and defenses on Deep Neural Networks (DNNs) across multiple modalities and learning tasks. By meticulously analyzing their distinctive features, we summarize and classify these methods into different categories and provide a novel taxonomy. Finally, this paper discusses promising research directions and presents potential solutions to open issues. To facilitate further study on MI attacks and defenses, we have implemented an open-source model inversion toolbox on GitHub (this https URL).

Translated Abstract:
딥 뉴럴 네트워크(DNN)는 다양한 분야에서 뛰어난 성능 덕분에 큰 변화를 일으켰어. 하지만 모델 역전(Model Inversion, MI) 공격이 생겨나면서 훈련 데이터셋에 대한 개인 정보를 유출하는 심각한 프라이버시 위협이 되고 있어. 훈련된 네트워크를 이용하면 공격자가 개인 훈련 샘플과 비슷한 고충실도 데이터를 재구성할 수 있어서 큰 문제가 되고 있어.

이 분야는 빠르게 발전하고 있는데, 기존 MI 공격과 방어에 대한 종합적이고 체계적인 개요가 부족해. 그래서 이 논문은 이 영역을 자세히 조사하고 포괄적인 서베이를 제공해. 먼저, 우리는 전통적인 머신러닝 시나리오에서의 초기 MI 연구를 간단히 리뷰해. 그 다음, 여러 최근의 공격과 방어 방법을 DNN에 대해 다양한 양식과 학습 과제를 통해 자세히 분석하고 비교해.

이런 방법들의 독특한 특징을 면밀히 분석해서, 우리는 이들을 여러 카테고리로 요약하고 분류하는 새로운 세분화를 제시해. 마지막으로, 이 논문은 유망한 연구 방향에 대해 논의하고 해결해야 할 문제들에 대한 잠재적 해결책을 제시해. MI 공격과 방어에 대한 추가 연구를 지원하기 위해, 우리는 GitHub에 오픈 소스 모델 역전 툴박스를 구현했어(이 URL).

================================================================================

URL:
https://arxiv.org/pdf/2402.12185.pdf

Title: ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning

Original Abstract:
Recently, many versatile Multi-modal Large Language Models (MLLMs) have emerged continuously. However, their capacity to query information depicted in visual charts and engage in reasoning based on the queried contents remains under-explored. In this paper, to comprehensively and rigorously benchmark the ability of the off-the-shelf MLLMs in the chart domain, we construct ChartX, a multi-modal evaluation set covering 18 chart types, 7 chart tasks, 22 disciplinary topics, and high-quality chart data. Besides, we develop ChartVLM to offer a new perspective on handling multi-modal tasks that strongly depend on interpretable patterns, such as reasoning tasks in the field of charts or geometric images. We evaluate the chart-related ability of mainstream MLLMs and our ChartVLM on the proposed ChartX evaluation set. Extensive experiments demonstrate that ChartVLM surpasses both versatile and chart-related large models, achieving results comparable to GPT-4V. We believe that our study can pave the way for further exploration in creating a more comprehensive chart evaluation set and developing more interpretable multi-modal models. Both ChartX and ChartVLM are available at: this https URL

Translated Abstract:
최근에 다양한 멀티모달 대형 언어 모델(MLLMs)이 계속해서 등장하고 있어. 하지만 이 모델들이 시각적인 차트에서 정보를 조회하고, 그 내용을 기반으로 추론하는 능력은 아직 충분히 연구되지 않았어.

이 논문에서는 차트 분야에서 기존 MLLM의 능력을 종합적이고 철저하게 평가하기 위해 ChartX라는 멀티모달 평가 세트를 만들었어. 이 세트는 18가지 차트 유형, 7가지 차트 작업, 22가지 분야 주제, 그리고 고품질 차트 데이터를 포함하고 있어.

또한, ChartVLM을 개발해서 해석 가능한 패턴에 의존하는 멀티모달 작업, 예를 들어 차트나 기하학적 이미지에서의 추론 작업을 다루는 새로운 관점을 제공하고 있어. 우리는 제안된 ChartX 평가 세트에서 주요 MLLM과 ChartVLM의 차트 관련 능력을 평가했어.

광범위한 실험 결과, ChartVLM이 다재다능한 모델과 차트 관련 대형 모델들보다 더 나은 성과를 내고, GPT-4V와 비슷한 결과를 얻었어. 우리의 연구가 더 포괄적인 차트 평가 세트를 만들고, 더 해석 가능한 멀티모달 모델을 개발하는 데 기여할 수 있을 거라고 믿어. ChartX와 ChartVLM은 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2403.12886.pdf

Title: EmoVOCA: Speech-Driven Emotional 3D Talking Heads

Original Abstract:
The domain of 3D talking head generation has witnessed significant progress in recent years. A notable challenge in this field consists in blending speech-related motions with expression dynamics, which is primarily caused by the lack of comprehensive 3D datasets that combine diversity in spoken sentences with a variety of facial expressions. Whereas literature works attempted to exploit 2D video data and parametric 3D models as a workaround, these still show limitations when jointly modeling the two motions. In this work, we address this problem from a different perspective, and propose an innovative data-driven technique that we used for creating a synthetic dataset, called EmoVOCA, obtained by combining a collection of inexpressive 3D talking heads and a set of 3D expressive sequences. To demonstrate the advantages of this approach, and the quality of the dataset, we then designed and trained an emotional 3D talking head generator that accepts a 3D face, an audio file, an emotion label, and an intensity value as inputs, and learns to animate the audio-synchronized lip movements with expressive traits of the face. Comprehensive experiments, both quantitative and qualitative, using our data and generator evidence superior ability in synthesizing convincing animations, when compared with the best performing methods in the literature. Our code and pre-trained model will be made available.

Translated Abstract:
3D 말하는 얼굴 생성 분야는 최근 몇 년 동안 큰 발전을 이뤘어. 하지만 이 분야에서 큰 문제는 말하는 동작과 표정 변화를 잘 섞는 건데, 이건 다양한 문장을 말할 때의 3D 데이터셋이 부족해서 발생해. 기존 연구들은 2D 비디오 데이터와 매개변수화된 3D 모델을 활용하려고 했지만, 두 가지 동작을 함께 모델링하는 데는 한계가 있어.

이 연구에서는 다른 시각에서 문제를 해결하려고 했고, EmoVOCA라는 합성 데이터셋을 만드는 혁신적인 데이터 기반 기술을 제안했어. 이 데이터셋은 무표정 3D 말하는 얼굴과 다양한 3D 표정 시퀀스를 결합해서 얻은 거야. 

이 접근 방식의 장점과 데이터셋의 품질을 보여주기 위해, 우리는 3D 얼굴, 오디오 파일, 감정 레이블, 강도 값을 입력으로 받아서 오디오와 동기화된 입술 움직임을 애니메이션화하는 감정 3D 말하는 얼굴 생성기를 설계하고 훈련했어. 

우리의 데이터와 생성기를 사용한 종합적인 실험 결과, 기존의 최상위 방법들에 비해 설득력 있는 애니메이션을 합성하는 데 더 뛰어난 능력을 입증했어. 우리의 코드와 미리 훈련된 모델은 곧 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2404.09259.pdf

Title: FedCCL: Federated Dual-Clustered Feature Contrast Under Domain Heterogeneity

Original Abstract:
Federated learning (FL) facilitates a privacy-preserving neural network training paradigm through collaboration between edge clients and a central server. One significant challenge is that the distributed data is not independently and identically distributed (non-IID), typically including both intra-domain and inter-domain heterogeneity. However, recent research is limited to simply using averaged signals as a form of regularization and only focusing on one aspect of these non-IID challenges. Given these limitations, this paper clarifies these two non-IID challenges and attempts to introduce cluster representation to address them from both local and global perspectives. Specifically, we propose a dual-clustered feature contrast-based FL framework with dual focuses. First, we employ clustering on the local representations of each client, aiming to capture intra-class information based on these local clusters at a high level of granularity. Then, we facilitate cross-client knowledge sharing by pulling the local representation closer to clusters shared by clients with similar semantics while pushing them away from clusters with dissimilar semantics. Second, since the sizes of local clusters belonging to the same class may differ for each client, we further utilize clustering on the global side and conduct averaging to create a consistent global signal for guiding each local training in a contrastive manner. Experimental results on multiple datasets demonstrate that our proposal achieves comparable or superior performance gain under intra-domain and inter-domain heterogeneity.

Translated Abstract:
연합 학습(FL)은 엣지 클라이언트와 중앙 서버 간의 협력을 통해 개인 정보를 보호하면서 신경망을 훈련하는 방법이야. 하지만 큰 문제는 분산된 데이터가 독립적이고 동일하게 분포되지 않아서(non-IID) 도메인 내외에서 이질성이 생긴다는 거야.

최근 연구들은 보통 평균 신호를 사용해서 문제를 해결하려고 하고, 이 비슷한 문제의 한 측면만 집중해서 다루는 경우가 많았어. 그래서 이 논문은 이 두 가지 비독립적 분포 문제를 명확히 하고, 이들을 지역적이고 전세계적인 관점에서 해결하기 위해 클러스터 표현을 도입하려고 해.

구체적으로, 우리는 이중 클러스터링된 특징 대비 기반의 FL 프레임워크를 제안해. 첫 번째로, 각 클라이언트의 지역 표현에 클러스터링을 적용해서, 이 지역 클러스터를 바탕으로 높은 세부 정보의 클래스 내 정보를 잡으려고 해. 그리고 비슷한 의미를 가진 클라이언트들 사이에서 지역 표현을 클러스터에 더 가깝게 만들고, 반대로 의미가 다른 클러스터에서 멀리 밀어내는 방식으로 지식 공유를 촉진해.

두 번째로, 같은 클래스의 지역 클러스터 크기가 클라이언트마다 다를 수 있기 때문에, 우리는 전세계적인 측면에서도 클러스터링을 활용하고 평균을 내서 각 지역 훈련을 안내할 수 있는 일관된 글로벌 신호를 만들었어. 여러 데이터셋에서 실험한 결과, 우리의 제안이 도메인 내외의 이질성에 대해 비슷하거나 더 나은 성능 향상을 보여준다는 걸 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2404.09376.pdf

Title: $\textit{sweet}$- An Open Source Modular Platform for Contactless Hand Vascular Biometric Experiments

Original Abstract:
Current finger-vein or palm-vein recognition systems usually require direct contact of the subject with the apparatus. This can be problematic in environments where hygiene is of primary importance. In this work we present a contactless vascular biometrics sensor platform named \sweet which can be used for hand vascular biometrics studies (wrist, palm, and finger-vein) and surface features such as palmprint. It supports several acquisition modalities such as multi-spectral Near-Infrared (NIR), RGB-color, Stereo Vision (SV) and Photometric Stereo (PS). Using this platform we collect a dataset consisting of the fingers, palm and wrist vascular data of 120 subjects and develop a powerful 3D pipeline for the pre-processing of this data. We then present biometric experimental results, focusing on Finger-Vein Recognition (FVR). Finally, we discuss fusion of multiple modalities, such palm-vein combined with palm-print biometrics. The acquisition software, parts of the hardware design, the new FV dataset, as well as source-code for our experiments are publicly available for research purposes.

Translated Abstract:
현재 손가락 정맥이나 손바닥 정맥 인식 시스템은 보통 사용자가 장비와 직접 접촉해야 해. 그런데 이게 위생이 중요한 환경에서는 문제가 될 수 있어. 

이번 연구에서는 \sweet 이라는 비접촉식 혈관 생체 인식 센서 플랫폼을 소개해. 이 플랫폼은 손의 혈관 생체 인식 연구(손목, 손바닥, 손가락 정맥)와 손바닥 인쇄 같은 표면 특징을 연구하는 데 사용할 수 있어. 여러 가지 데이터 수집 방법을 지원하는데, 멀티 스펙트럼 근적외선(NIR), RGB 색상, 스테레오 비전(SV)과 포토메트릭 스테레오(PS) 등이 있어.

이 플랫폼을 사용해서 120명의 손가락, 손바닥, 손목 혈관 데이터를 포함한 데이터셋을 수집했어. 그리고 이 데이터를 전처리하기 위한 강력한 3D 파이프라인을 개발했어. 그 다음으로는 손가락 정맥 인식(FVR)에 초점을 맞춘 생체 인식 실험 결과를 발표했어. 

마지막으로, 손바닥 정맥과 손바닥 인쇄 생체 인식을 결합하는 것 같은 여러 모달리티의 융합에 대해서도 논의했어. 데이터 수집 소프트웨어, 하드웨어 디자인의 일부, 새로운 FV 데이터셋, 그리고 실험에 대한 소스 코드는 연구 목적으로 공개되어 있어.

================================================================================

URL:
https://arxiv.org/pdf/2404.10772.pdf

Title: Gaussian Opacity Fields: Efficient Adaptive Surface Reconstruction in Unbounded Scenes

Original Abstract:
Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time. However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and adaptive surface reconstruction in unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work. We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry. Furthermore, we develop an efficient geometry extraction method utilizing Marching Tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis. Further, it compares favorably to or even outperforms, neural implicit methods in both quality and speed.

Translated Abstract:
최근 3D 가우시안 스플래팅(3DGS)은 새로운 시점 합성에서 놀라운 결과를 보여주면서 고해상도 이미지를 실시간으로 렌더링할 수 있게 되었습니다. 하지만 3D 가우시안을 이용한 표면 재구성은 3D 가우시안의 명시적이고 분리된 특성 때문에 큰 도전 과제가 됩니다.

이 연구에서는 Gaussian Opacity Fields (GOF)라는 새로운 접근 방식을 소개합니다. 이 방법은 무한한 장면에서 효율적이고 고품질의 적응형 표면 재구성을 가능하게 합니다. GOF는 3D 가우시안의 레이 트레이싱 기반 볼륨 렌더링에서 파생되어, 이전 연구에서 사용된 포아송 재구성이나 TSDF 융합에 의존하지 않고 3D 가우시안의 레벨셋을 식별함으로써 직접적으로 기하학을 추출할 수 있게 해줍니다.

또한, 우리는 가우시안의 표면 법선을 레이-가우시안 교차 평면의 법선으로 근사하여 기하학을 크게 향상시키는 정규화를 적용할 수 있습니다. 더불어, 3D 가우시안에서 유도된 사면체 격자를 활용하는 효율적인 기하학 추출 방법인 Marching Tetrahedra를 개발했습니다. 이 방법은 장면의 복잡성에 맞게 적응합니다.

평가 결과, GOF는 표면 재구성과 새로운 시점 합성에서 기존의 3DGS 기반 방법들을 초월하며, 품질과 속도 모두에서 신경 암시적 방법들과 비교해도 유리하거나 심지어 더 우수한 성능을 보였습니다.

================================================================================

URL:
https://arxiv.org/pdf/2404.16017.pdf

Title: RetinaRegNet: A Zero-Shot Approach for Retinal Image Registration

Original Abstract:
We introduce RetinaRegNet, a zero-shot image registration model designed to register retinal images with minimal overlap, large deformations, and varying image quality. RetinaRegNet addresses these challenges and achieves robust and accurate registration through the following steps. First, we extract features from the moving and fixed images using latent diffusion models. We then sample feature points from the fixed image using a combination of the SIFT algorithm and random point sampling. For each sampled point, we identify its corresponding point in the moving image using a 2D correlation map, which computes the cosine similarity between the diffusion feature vectors of the point in the fixed image and all pixels in the moving image. Second, we eliminate most incorrectly detected point correspondences (outliers) by enforcing an inverse consistency constraint, ensuring that correspondences are consistent in both forward and backward directions. We further remove outliers with large distances between corresponding points using a global transformation based outlier detector. Finally, we implement a two-stage registration framework to handle large deformations. The first stage estimates a homography transformation to achieve global alignment between the images, while the second stage uses a third-order polynomial transformation to estimate local deformations. We evaluated RetinaRegNet on three retinal image registration datasets: color fundus images, fluorescein angiography images, and laser speckle flowgraphy images. Our model consistently outperformed state-of-the-art methods across all datasets. The accurate registration achieved by RetinaRegNet enables the tracking of eye disease progression, enhances surgical planning, and facilitates the evaluation of treatment efficacy. Our code is publicly available at: this https URL.

Translated Abstract:
RetinaRegNet을 소개할게. 이건 망막 이미지를 겹침이 적고, 큰 변형이 있으며, 다양한 화질을 가진 상태에서도 등록할 수 있도록 설계된 제로샷 이미지 등록 모델이야. RetinaRegNet은 이런 문제를 해결하고, 다음 단계들을 통해 강력하고 정확한 등록을 달성해.

먼저, 우리는 이동 이미지와 고정 이미지에서 특징을 추출해. 이때 잠재적 확산 모델을 사용해. 그리고 고정 이미지에서 SIFT 알고리즘과 랜덤 포인트 샘플링을 결합해서 특징 점들을 샘플링해. 각 샘플링된 점에 대해, 이동 이미지에서의 해당 점을 2D 상관 맵을 사용해 찾아. 이 상관 맵은 고정 이미지의 점과 이동 이미지의 모든 픽셀 간의 코사인 유사도를 계산해.

두 번째로, 역 일관성 제약 조건을 적용해서 잘못 감지된 점의 대응(아웃라이어)을 대부분 제거해. 이는 대응 관계가 앞뒤 모두에서 일관되도록 보장해. 그리고 대응 점 간의 거리가 큰 아웃라이어는 전역 변환 기반 아웃라이어 탐지기를 사용해 추가로 제거해.

마지막으로, 큰 변형을 처리하기 위해 두 단계 등록 프레임워크를 구현해. 첫 번째 단계에서는 호모그래피 변환을 추정해서 이미지 간의 전반적인 정렬을 달성하고, 두 번째 단계에서는 3차 다항식 변환을 사용해 지역 변형을 추정해.

우리는 RetinaRegNet을 세 가지 망막 이미지 등록 데이터셋에서 평가했어: 컬러 망막 이미지, 형광 혈관조영 이미지, 레이저 스펙클 흐름 그래픽 이미지. 우리 모델은 모든 데이터셋에서 최신 기술보다 항상 더 뛰어난 성능을 보였어. RetinaRegNet이 달성한 정확한 등록은 눈 질병 진행 추적, 수술 계획 향상, 치료 효과 평가를 가능하게 해. 우리의 코드는 공개적으로 사용 가능해: 이 링크를 확인해.

================================================================================

URL:
https://arxiv.org/pdf/2404.17170.pdf

Title: Image Quality Assessment With Compressed Sampling

Original Abstract:
No-Reference Image Quality Assessment (NR-IQA) aims at estimating image quality in accordance with subjective human perception. However, most methods focus on exploring increasingly complex networks to improve the final performance,accompanied by limitations on input images. Especially when applied to high-resolution (HR) images, these methods offen have to adjust the size of original image to meet model this http URL further alleviate the aforementioned issue, we propose two networks for NR-IQA with Compressive Sampling (dubbed CL-IQA and CS-IQA). They consist of four components: (1) The Compressed Sampling Module (CSM) to sample the image (2)The Adaptive Embedding Module (AEM). The measurements are embedded by AEM to extract high-level features. (3) The Vision Transformer and Scale Swin TranBlocksformer Moudle(SSTM) to extract deep features. (4) The Dual Branch (DB) to get final quality score. Experiments show that our proposed methods outperform other methods on various datasets with less data usage.

Translated Abstract:
비참조 이미지 품질 평가(No-Reference Image Quality Assessment, NR-IQA)는 사람의 주관적인 인식에 따라 이미지 품질을 추정하는 걸 목표로 해. 하지만 대부분의 방법들은 최종 성능을 개선하기 위해 점점 더 복잡한 네트워크를 탐색하는 데 집중하고, 입력 이미지에 대한 제한이 있어. 특히 고해상도(High-Resolution, HR) 이미지에 적용할 때는 원본 이미지의 크기를 조정해야 해서 문제가 생기곤 해.

이런 문제를 해결하기 위해, 우리는 압축 샘플링(Compressive Sampling)을 이용한 NR-IQA를 위한 두 가지 네트워크(CL-IQA와 CS-IQA)를 제안해. 이 네트워크들은 네 가지 구성 요소로 이루어져 있어: 

(1) 압축 샘플링 모듈(Compressed Sampling Module, CSM)로 이미지를 샘플링하고,  
(2) 적응형 임베딩 모듈(Adaptive Embedding Module, AEM)을 통해 측정값을 임베딩해 고급 특징을 추출해.  
(3) 비전 트랜스포머와 스케일 스윈 트랜스포머 모듈(Scale Swin Transformer Module, SSTM)을 통해 깊은 특징을 추출하고,  
(4) 듀얼 브랜치(Dual Branch, DB)를 통해 최종 품질 점수를 얻어. 

실험 결과, 우리가 제안한 방법들이 다양한 데이터셋에서 다른 방법들보다 더 나은 성능을 보이며 데이터 사용량도 적다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2405.15757.pdf

Title: Looking Backward: Streaming Video-to-Video Translation with Feature Banks

Original Abstract:
This paper introduces StreamV2V, a diffusion model that achieves real-time streaming video-to-video (V2V) translation with user prompts. Unlike prior V2V methods using batches to process limited frames, we opt to process frames in a streaming fashion, to support unlimited frames. At the heart of StreamV2V lies a backward-looking principle that relates the present to the past. This is realized by maintaining a feature bank, which archives information from past frames. For incoming frames, StreamV2V extends self-attention to include banked keys and values and directly fuses similar past features into the output. The feature bank is continually updated by merging stored and new features, making it compact but informative. StreamV2V stands out for its adaptability and efficiency, seamlessly integrating with image diffusion models without fine-tuning. It can run 20 FPS on one A100 GPU, being 15x, 46x, 108x, and 158x faster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively. Quantitative metrics and user studies confirm StreamV2V's exceptional ability to maintain temporal consistency.

Translated Abstract:
이 논문은 StreamV2V라는 확산 모델을 소개해. 이 모델은 사용자 프롬프트를 이용해서 실시간으로 비디오-투-비디오(V2V) 변환을 가능하게 해. 이전의 V2V 방법들이 한정된 프레임을 처리하기 위해 배치 처리를 사용했던 것과는 달리, 우리는 프레임을 스트리밍 방식으로 처리해서 무한한 프레임을 지원할 수 있어.

StreamV2V의 핵심은 현재와 과거를 연결하는 후향 원리에 있어. 이건 과거 프레임의 정보를 보관하는 피처 뱅크를 유지함으로써 이루어져. 들어오는 프레임에 대해, StreamV2V는 자기 주의(self-attention)를 확장해서 저장된 키와 값을 포함하고, 비슷한 과거 피처를 직접 출력에 융합해.

피처 뱅크는 저장된 피처와 새로운 피처를 병합함으로써 지속적으로 업데이트돼, 그래서 간결하면서도 유용한 정보를 유지해. StreamV2V는 적응성과 효율성이 뛰어나서, 이미지 확산 모델과 매끄럽게 통합될 수 있고, 파인튜닝 없이도 작동해. A100 GPU에서 20 FPS로 실행할 수 있고, FlowVid, CoDeF, Rerender, TokenFlow보다 각각 15배, 46배, 108배, 158배 더 빨라. 정량적 지표와 사용자 연구 결과는 StreamV2V가 시간 일관성을 유지하는 데 뛰어난 능력이 있음을 확인해.

================================================================================

URL:
https://arxiv.org/pdf/2406.01592.pdf

Title: Text-guided Controllable Mesh Refinement for Interactive 3D Modeling

Original Abstract:
We propose a novel technique for adding geometric details to an input coarse 3D mesh guided by a text prompt. Our method is composed of three stages. First, we generate a single-view RGB image conditioned on the input coarse geometry and the input text prompt. This single-view image generation step allows the user to pre-visualize the result and offers stronger conditioning for subsequent multi-view generation. Second, we use our novel multi-view normal generation architecture to jointly generate six different views of the normal images. The joint view generation reduces inconsistencies and leads to sharper details. Third, we optimize our mesh with respect to all views and generate a fine, detailed geometry as output. The resulting method produces an output within seconds and offers explicit user control over the coarse structure, pose, and desired details of the resulting 3D mesh.

Translated Abstract:
우리는 텍스트 프롬프트에 의해 안내받아 입력된 거친 3D 메시에 기하학적 디테일을 추가하는 새로운 기술을 제안해. 이 방법은 세 단계로 이루어져 있어.

첫 번째로, 입력된 거친 기하학과 텍스트 프롬프트를 바탕으로 단일 시점 RGB 이미지를 생성해. 이 단계는 사용자가 결과를 미리 시각화할 수 있게 해주고, 이후 여러 시점 생성을 위한 더 강력한 조건을 제공해.

두 번째로, 우리는 새로운 다중 시점 노멀 생성 아키텍처를 사용해 여섯 가지 다른 시점의 노멀 이미지를 동시에 생성해. 이렇게 동시에 생성하면 불일치를 줄이고 더 선명한 디테일을 얻을 수 있어.

세 번째로, 우리는 모든 시점을 기준으로 메시를 최적화하고 세밀하고 정교한 기하학을 출력해. 이 방법은 몇 초 안에 결과를 만들어내고, 사용자가 거친 구조, 자세, 원하는 디테일을 명확하게 제어할 수 있게 해줘.

================================================================================

URL:
https://arxiv.org/pdf/2406.11633.pdf

Title: DocGenome: An Open Large-scale Scientific Document Benchmark for Training and Testing Multi-modal Large Language Models

Original Abstract:
Scientific documents record research findings and valuable human knowledge, comprising a vast corpus of high-quality data. Leveraging multi-modality data extracted from these documents and assessing large models' abilities to handle scientific document-oriented tasks is therefore meaningful. Despite promising advancements, large models still perform poorly on multi-page scientific document extraction and understanding tasks, and their capacity to process within-document data formats such as charts and equations remains under-explored. To address these issues, we present DocGenome, a structured document benchmark constructed by annotating 500K scientific documents from 153 disciplines in the arXiv open-access community, using our custom auto-labeling pipeline. DocGenome features four key characteristics: 1) Completeness: It is the first dataset to structure data from all modalities including 13 layout attributes along with their LaTeX source codes. 2) Logicality: It provides 6 logical relationships between different entities within each scientific document. 3) Diversity: It covers various document-oriented tasks, including document classification, visual grounding, document layout detection, document transformation, open-ended single-page QA and multi-page QA. 4) Correctness: It undergoes rigorous quality control checks conducted by a specialized team. We conduct extensive experiments to demonstrate the advantages of DocGenome and objectively evaluate the performance of large models on our benchmark.

Translated Abstract:
과학 문서는 연구 결과와 소중한 인간 지식을 기록한 것으로, 고품질 데이터의 방대한 모음을 형성해. 이러한 문서에서 추출한 다중 모달리티 데이터를 활용하고, 대형 모델들이 과학 문서 관련 작업을 처리하는 능력을 평가하는 건 의미 있는 일이야. 

하지만, 대형 모델들이 다중 페이지 과학 문서를 추출하고 이해하는 작업에서는 여전히 성과가 저조해. 또, 차트나 방정식 같은 문서 내 데이터 형식을 처리하는 능력도 잘 연구되지 않았어. 이 문제를 해결하기 위해, 우리는 DocGenome이라는 구조화된 문서 벤치마크를 제안해. 이 벤치마크는 arXiv 오픈 액세스 커뮤니티의 153개 분야에서 500K 과학 문서를 주석 처리해 만든 거야. 우리 맞춤형 자동 라벨링 파이프라인을 사용했어.

DocGenome은 네 가지 주요 특징이 있어: 
1) 완전성: 13개의 레이아웃 속성과 그에 해당하는 LaTeX 소스 코드를 포함한 모든 모달리티의 데이터를 구조화한 첫 번째 데이터셋이야. 
2) 논리성: 각 과학 문서 내에서 서로 다른 엔티티 간의 6가지 논리적 관계를 제공해.
3) 다양성: 문서 분류, 시각적 기초, 문서 레이아웃 탐지, 문서 변환, 개방형 단일 페이지 QA 및 다중 페이지 QA 등 다양한 문서 관련 작업을 다뤄.
4) 정확성: 전문 팀이 수행한 엄격한 품질 관리 검사를 거쳐.

우리는 DocGenome의 장점을 보여주고 대형 모델의 성능을 객관적으로 평가하기 위해 광범위한 실험을 진행해.

================================================================================

URL:
https://arxiv.org/pdf/2406.17109.pdf

Title: GMT: Guided Mask Transformer for Leaf Instance Segmentation

Original Abstract:
Leaf instance segmentation is a challenging multi-instance segmentation task, aiming to separate and delineate each leaf in an image of a plant. Accurate segmentation of each leaf is crucial for plant-related applications such as the fine-grained monitoring of plant growth and crop yield estimation. This task is challenging because of the high similarity (in shape and colour), great size variation, and heavy occlusions among leaf instances. Furthermore, the typically small size of annotated leaf datasets makes it more difficult to learn the distinctive features needed for precise segmentation. We hypothesise that the key to overcoming the these challenges lies in the specific spatial patterns of leaf distribution. In this paper, we propose the Guided Mask Transformer (GMT), which leverages and integrates leaf spatial distribution priors into a Transformer-based segmentor. These spatial priors are embedded in a set of guide functions that map leaves at different positions into a more separable embedding space. Our GMT consistently outperforms the state-of-the-art on three public plant datasets.

Translated Abstract:
잎 인스턴스 분할은 식물의 이미지에서 각 잎을 구분하고 윤곽을 그리는 어려운 다중 인스턴스 분할 작업이야. 각 잎을 정확하게 분할하는 것은 식물 성장 모니터링이나 작물 수확량 추정 같은 식물 관련 응용에 매우 중요해. 

이 작업이 어렵게 느껴지는 이유는 잎들이 모양과 색상에서 매우 비슷하고, 크기 차이가 크며, 잎 인스턴스들 사이에 가려진 부분이 많기 때문이야. 게다가 일반적으로 주석이 달린 잎 데이터셋이 작아서, 정밀한 분할을 위해 필요한 특징을 배우는 게 더 어려워.

우리는 이러한 도전 과제를 극복하는 열쇠가 잎 분포의 특정 공간 패턴에 있다고 가정해. 이 논문에서는 Guided Mask Transformer (GMT)를 제안하는데, 이건 잎의 공간 분포 정보를 활용하고 이를 Transformer 기반의 분할기와 통합하는 방법이야. 

이 공간 정보는 서로 다른 위치의 잎을 더 잘 분리할 수 있는 임베딩 공간으로 매핑하는 가이드 함수 세트에 포함돼. 우리의 GMT는 세 개의 공공 식물 데이터셋에서 최신 기술보다 항상 뛰어난 성능을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2406.18140.pdf

Title: Exclusive Style Removal for Cross Domain Novel Class Discovery

Original Abstract:
As a promising field in open-world learning, \textit{Novel Class Discovery} (NCD) is usually a task to cluster unseen novel classes in an unlabeled set based on the prior knowledge of labeled data within the same domain. However, the performance of existing NCD methods could be severely compromised when novel classes are sampled from a different distribution with the labeled ones. In this paper, we explore and establish the solvability of NCD in cross domain setting with the necessary condition that style information must be removed. Based on the theoretical analysis, we introduce an exclusive style removal module for extracting style information that is distinctive from the baseline features, thereby facilitating inference. Moreover, this module is easy to integrate with other NCD methods, acting as a plug-in to improve performance on novel classes with different distributions compared to the seen labeled set. Additionally, recognizing the non-negligible influence of different backbones and pre-training strategies on the performance of the NCD methods, we build a fair benchmark for future NCD research. Extensive experiments on three common datasets demonstrate the effectiveness of our proposed module.

Translated Abstract:
오픈 월드 학습의 유망한 분야인 \textit{새로운 클래스 발견} (NCD)은 일반적으로 라벨이 없는 데이터셋에서 보지 못한 새로운 클래스를 군집화하는 작업이에요. 이때 라벨이 있는 데이터의 사전 지식을 기반으로 해요. 하지만 기존 NCD 방법들은 새로운 클래스가 라벨이 있는 데이터와 다른 분포에서 샘플링될 경우 성능이 크게 저하될 수 있어요.

이 논문에서는 스타일 정보를 제거해야 하는 필수 조건을 가지고 크로스 도메인 설정에서 NCD의 해결 가능성을 탐구하고 확립해요. 이론적 분석을 바탕으로, 기본 특징과 구별되는 스타일 정보를 추출하는 전용 스타일 제거 모듈을 소개해요. 이 모듈은 추론을 더 쉽게 해주고, 다른 NCD 방법들과 쉽게 통합될 수 있어요. 즉, 보지 못한 클래스의 성능을 향상시키는 플러그인 역할을 해요.

또한, NCD 방법의 성능에 미치는 다양한 백본과 사전 훈련 전략의 영향을 무시할 수 없음을 인식하고, 향후 NCD 연구를 위한 공정한 벤치마크를 구축했어요. 세 가지 일반적인 데이터셋에서 진행한 광범위한 실험을 통해, 우리가 제안한 모듈의 효과를 입증했어요.

================================================================================

URL:
https://arxiv.org/pdf/2406.18197.pdf

Title: Human-Free Automated Prompting for Vision-Language Anomaly Detection: Prompt Optimization with Meta-guiding Prompt Scheme

Original Abstract:
Pre-trained vision-language models (VLMs) are highly adaptable to various downstream tasks through few-shot learning, making prompt-based anomaly detection a promising approach. Traditional methods depend on human-crafted prompts that require prior knowledge of specific anomaly types. Our goal is to develop a human-free prompt-based anomaly detection framework that optimally learns prompts through data-driven methods, eliminating the need for human intervention. The primary challenge in this approach is the lack of anomalous samples during the training phase. Additionally, the Vision Transformer (ViT)-based image encoder in VLMs is not ideal for pixel-wise anomaly segmentation due to a locality feature mismatch between the original image and the output feature map. To tackle the first challenge, we have developed the Object-Attention Anomaly Generation Module (OAGM) to synthesize anomaly samples for training. Furthermore, our Meta-Guiding Prompt-Tuning Scheme (MPTS) iteratively adjusts the gradient-based optimization direction of learnable prompts to avoid overfitting to the synthesized anomalies. For the second challenge, we propose Locality-Aware Attention, which ensures that each local patch feature attends only to nearby patch features, preserving the locality features corresponding to their original locations. This framework allows for the optimal prompt embeddings by searching in the continuous latent space via backpropagation, free from human semantic constraints. Additionally, the modified locality-aware attention improves the precision of pixel-wise anomaly segmentation.

Translated Abstract:
사전 훈련된 비전-언어 모델(VLMs)은 적은 데이터로도 다양한 작업에 잘 적응할 수 있어서, 프롬프트 기반의 이상 탐지가 유망한 접근 방식이야. 전통적인 방법은 인간이 만든 프롬프트에 의존하는데, 이건 특정한 이상 유형에 대한 사전 지식이 필요해.

우리의 목표는 인간의 개입 없이 데이터 기반 방법으로 프롬프트를 최적 학습하는 프롬프트 기반 이상 탐지 프레임워크를 개발하는 거야. 이 방법의 주요 도전 과제는 훈련 단계에서 이상 샘플이 부족하다는 점이야. 게다가, VLMs의 비전 트랜스포머(ViT) 기반 이미지 인코더는 원본 이미지와 출력 피처 맵 간의 지역성 특성 불일치 때문에 픽셀 단위의 이상 세분화에 적합하지 않아.

첫 번째 도전 과제를 해결하기 위해 우리는 훈련을 위한 이상 샘플을 합성하는 객체 주의 이상 생성 모듈(OAGM)을 개발했어. 또한, 우리의 메타 가이딩 프롬프트 튜닝 방식(MPTS)은 학습 가능한 프롬프트의 그래디언트 기반 최적화 방향을 반복적으로 조정해서 합성된 이상에 과적합되지 않도록 해.

두 번째 도전 과제를 위해 우리는 지역성을 고려한 주의(attention) 방법을 제안해. 이 방법은 각 지역 패치 피처가 근처 패치 피처에만 주의를 기울이도록 해서, 원래 위치에 해당하는 지역성 특성을 보존해. 이 프레임워크는 인간의 의미적 제약 없이 역전파를 통해 연속 잠재 공간에서 최적의 프롬프트 임베딩을 찾을 수 있게 해. 게다가 수정된 지역성 고려 주의는 픽셀 단위의 이상 세분화의 정밀도를 향상시켜.

================================================================================

URL:
https://arxiv.org/pdf/2406.18717.pdf

Title: Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos

Original Abstract:
Gaussian splatting has become a popular representation for novel-view synthesis, exhibiting clear strengths in efficiency, photometric quality, and compositional edibility. Following its success, many works have extended Gaussians to 4D, showing that dynamic Gaussians maintain these benefits while also tracking scene geometry far better than alternative representations. Yet, these methods assume dense multi-view videos as supervision. In this work, we are interested in extending the capability of Gaussian scene representations to casually captured monocular videos. We show that existing 4D Gaussian methods dramatically fail in this setup because the monocular setting is underconstrained. Building off this finding, we propose a method we call Dynamic Gaussian Marbles, which consist of three core modifications that target the difficulties of the monocular setting. First, we use isotropic Gaussian "marbles'', reducing the degrees of freedom of each Gaussian. Second, we employ a hierarchical divide and-conquer learning strategy to efficiently guide the optimization towards solutions with globally coherent motion. Finally, we add image-level and geometry-level priors into the optimization, including a tracking loss that takes advantage of recent progress in point tracking. By constraining the optimization, Dynamic Gaussian Marbles learns Gaussian trajectories that enable novel-view rendering and accurately capture the 3D motion of the scene elements. We evaluate on the Nvidia Dynamic Scenes dataset and the DyCheck iPhone dataset, and show that Gaussian Marbles significantly outperforms other Gaussian baselines in quality, and is on-par with non-Gaussian representations, all while maintaining the efficiency, compositionality, editability, and tracking benefits of Gaussians. Our project page can be found here this https URL.

Translated Abstract:
가우시안 스플래팅은 새로운 시점 합성에 인기 있는 표현 방식으로, 효율성, 사진 품질, 조합 가능성에서 뚜렷한 강점을 보여주고 있어. 이 방식이 성공한 뒤, 많은 연구들이 가우시안을 4D로 확장했는데, 동적 가우시안이 이런 장점을 유지하면서도 장면 기하학을 훨씬 더 잘 추적한다는 걸 보여줬어. 하지만 이 방법들은 밀집된 다중 시점 비디오를 감독으로 가정하고 있어.

이번 연구에서는 가우시안 장면 표현의 능력을 일상적으로 촬영한 단안 비디오에 확장하는 데 관심을 가졌어. 기존의 4D 가우시안 방법들이 이 설정에서 극적으로 실패하는 이유는 단안 설정이 제약이 적기 때문이야. 이 발견을 바탕으로 우리는 "동적 가우시안 구슬"이라는 방법을 제안했어. 이 방법은 단안 설정의 어려움을 해결하기 위해 세 가지 핵심 수정사항으로 구성되어 있어.

첫 번째로, 우리는 등방성 가우시안 "구슬"을 사용해서 각 가우시안의 자유도를 줄였어. 두 번째로, 효율적으로 최적화를 안내하기 위해 계층적 분할 정복 학습 전략을 사용했어. 마지막으로, 최근 포인트 추적의 발전을 활용한 추적 손실을 포함해서 이미지 수준과 기하학 수준의 사전 정보를 최적화에 추가했어. 이렇게 최적화를 제약함으로써 동적 가우시안 구슬은 새로운 시점 렌더링을 가능하게 하는 가우시안 궤적을 학습하고, 장면 요소의 3D 움직임을 정확하게 포착할 수 있어.

우리는 Nvidia 동적 장면 데이터셋과 DyCheck 아이폰 데이터셋에서 평가를 했고, 가우시안 구슬이 품질 면에서 다른 가우시안 기준선보다 훨씬 우수하며 비가우시안 표현과도 동등한 성능을 보인다는 걸 보여줬어. 이 모든 과정에서 가우시안의 효율성, 조합 가능성, 편집 가능성, 추적 장점을 유지했어.

================================================================================

URL:
https://arxiv.org/pdf/2407.02174.pdf

Title: BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event Stream

Original Abstract:
Neural implicit representation of visual scenes has attracted a lot of attention in recent research of computer vision and graphics. Most prior methods focus on how to reconstruct 3D scene representation from a set of images. In this work, we demonstrate the possibility to recover the neural radiance fields (NeRF) from a single blurry image and its corresponding event stream. We model the camera motion with a cubic B-Spline in SE(3) space. Both the blurry image and the brightness change within a time interval, can then be synthesized from the 3D scene representation given the 6-DoF poses interpolated from the cubic B-Spline. Our method can jointly learn both the implicit neural scene representation and recover the camera motion by minimizing the differences between the synthesized data and the real measurements without pre-computed camera poses from COLMAP. We evaluate the proposed method with both synthetic and real datasets. The experimental results demonstrate that we are able to render view-consistent latent sharp images from the learned NeRF and bring a blurry image alive in high quality. Code and data are available at this https URL.

Translated Abstract:
시각 장면의 신경 암묵적 표현은 최근 컴퓨터 비전과 그래픽스 연구에서 많은 주목을 받고 있어. 대부분의 기존 방법들은 여러 이미지를 가지고 3D 장면 표현을 어떻게 재구성할지를 집중적으로 다뤘어. 

이번 연구에서는 흐릿한 이미지 하나와 그에 해당하는 이벤트 스트림을 이용해 신경 방사장(NeRF)을 복원할 수 있는 가능성을 보여줘. 카메라 움직임은 SE(3) 공간에서 큐빅 B-스플라인으로 모델링했어. 흐릿한 이미지와 시간 간격 내의 밝기 변화는 큐빅 B-스플라인으로 보간된 6자유도(6-DoF) 포즈를 통해 3D 장면 표현으로부터 합성할 수 있어. 

우리 방법은 암묵적 신경 장면 표현과 카메라 움직임을 함께 학습할 수 있는데, 이는 합성된 데이터와 실제 측정값 간의 차이를 최소화하면서 진행돼. COLMAP에서 미리 계산된 카메라 포즈 없이도 가능해. 

제안된 방법은 합성 데이터와 실제 데이터셋 모두에서 평가했어. 실험 결과, 학습된 NeRF로부터 일관된 뷰의 선명한 이미지를 렌더링할 수 있었고, 흐릿한 이미지를 고품질로 되살릴 수 있었어. 코드와 데이터는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2407.11691.pdf

Title: VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models

Original Abstract:
We present VLMEvalKit: an open-source toolkit for evaluating large multi-modality models based on PyTorch. The toolkit aims to provide a user-friendly and comprehensive framework for researchers and developers to evaluate existing multi-modality models and publish reproducible evaluation results. In VLMEvalKit, we implement over 70 different large multi-modality models, including both proprietary APIs and open-source models, as well as more than 20 different multi-modal benchmarks. By implementing a single interface, new models can be easily added to the toolkit, while the toolkit automatically handles the remaining workloads, including data preparation, distributed inference, prediction post-processing, and metric calculation. Although the toolkit is currently mainly used for evaluating large vision-language models, its design is compatible with future updates that incorporate additional modalities, such as audio and video. Based on the evaluation results obtained with the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to track the progress of multi-modality learning research. The toolkit is released at this https URL and is actively maintained.

Translated Abstract:
우리는 VLMEvalKit이라는 오픈소스 툴킷을 소개해. 이 툴킷은 PyTorch 기반의 대형 멀티모달 모델을 평가하는 데 도움을 주기 위해 만들어졌어. 연구자들과 개발자들이 기존의 멀티모달 모델을 평가하고 재현 가능한 평가 결과를 발표할 수 있도록 사용자 친화적이고 포괄적인 프레임워크를 제공하는 게 목표야.

VLMEvalKit에서는 70개 이상의 다양한 대형 멀티모달 모델을 구현했어. 여기에는 상용 API와 오픈소스 모델이 모두 포함돼 있고, 20개 이상의 다양한 멀티모달 벤치마크도 있어. 단일 인터페이스를 통해 새로운 모델을 쉽게 추가할 수 있고, 툴킷이 데이터 준비, 분산 추론, 예측 후처리, 그리고 지표 계산 같은 나머지 작업을 자동으로 처리해.

현재 이 툴킷은 주로 대형 비전-언어 모델을 평가하는 데 사용되지만, 디자인이 오디오나 비디오 같은 추가 모달리티를 포함하는 미래 업데이트와도 호환돼. 이 툴킷을 사용해 얻은 평가 결과를 바탕으로, 멀티모달 학습 연구의 진행 상황을 추적하는 OpenVLM 리더보드를 운영하고 있어. 툴킷은 이 https URL에서 배포되고 있으며, 활발하게 유지 관리되고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2407.12371.pdf

Title: HIMO: A New Benchmark for Full-Body Human Interacting with Multiple Objects

Original Abstract:
Generating human-object interactions (HOIs) is critical with the tremendous advances of digital avatars. Existing datasets are typically limited to humans interacting with a single object while neglecting the ubiquitous manipulation of multiple objects. Thus, we propose HIMO, a large-scale MoCap dataset of full-body human interacting with multiple objects, containing 3.3K 4D HOI sequences and 4.08M 3D HOI frames. We also annotate HIMO with detailed textual descriptions and temporal segments, benchmarking two novel tasks of HOI synthesis conditioned on either the whole text prompt or the segmented text prompts as fine-grained timeline control. To address these novel tasks, we propose a dual-branch conditional diffusion model with a mutual interaction module for HOI synthesis. Besides, an auto-regressive generation pipeline is also designed to obtain smooth transitions between HOI segments. Experimental results demonstrate the generalization ability to unseen object geometries and temporal compositions.

Translated Abstract:
디지털 아바타의 발전 덕분에 인간-객체 상호작용(HOI)을 생성하는 것이 정말 중요해졌어. 기존 데이터셋은 보통 사람이 하나의 객체와 상호작용하는 것에만 제한되어 있어서 여러 객체를 동시에 다루는 경우는 간과하고 있어. 그래서 우리는 HIMO라는 대규모 모션 캡처 데이터셋을 제안해. 이 데이터셋은 3.3K개의 4D HOI 시퀀스와 4.08M개의 3D HOI 프레임으로 구성되어 있어. 

HIMO에는 자세한 텍스트 설명과 시간 구간으로 주석이 달려 있고, 전체 텍스트 프롬프트나 세분화된 텍스트 프롬프트에 따라 HOI를 합성하는 두 가지 새로운 작업을 벤치마킹하고 있어. 이 새로운 작업을 해결하기 위해 우리는 HOI 합성을 위한 이중 분기 조건부 확산 모델과 상호작용 모듈을 제안해. 게다가 HOI 세그먼트 간 매끄러운 전환을 얻기 위한 자동 회귀 생성 파이프라인도 설계했어. 실험 결과는 보지 못한 객체의 형태와 시간적 조합에 대한 일반화 능력을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2407.19698.pdf

Title: Classification Matters: Improving Video Action Detection with Class-Specific Attention

Original Abstract:
Video action detection (VAD) aims to detect actors and classify their actions in a video. We figure that VAD suffers more from classification rather than localization of actors. Hence, we analyze how prevailing methods form features for classification and find that they prioritize actor regions, yet often overlooking the essential contextual information necessary for accurate classification. Accordingly, we propose to reduce the bias toward actor and encourage paying attention to the context that is relevant to each action class. By assigning a class-dedicated query to each action class, our model can dynamically determine where to focus for effective classification. The proposed model demonstrates superior performance on three challenging benchmarks with significantly fewer parameters and less computation.

Translated Abstract:
비디오 행동 탐지(VAD)는 비디오에서 배우를 찾고 그들의 행동을 분류하는 걸 목표로 해. 우리는 VAD가 배우의 위치를 찾는 것보다 분류에서 더 많은 문제를 겪고 있다는 걸 알게 됐어. 그래서 기존 방법들이 분류를 위해 특징을 어떻게 만드는지 분석해봤고, 배우의 영역에 집중하지만, 정확한 분류를 위해 필요한 중요한 맥락 정보를 자주 간과한다는 걸 발견했어.

그래서 우리는 배우에 대한 편향을 줄이고 각 행동 클래스와 관련된 맥락에 주의를 기울이도록 하자는 제안을 해. 각 행동 클래스에 맞는 쿼리를 할당함으로써, 우리 모델은 효과적인 분류를 위해 어디에 집중해야 할지를 동적으로 결정할 수 있어. 제안한 모델은 세 가지 어려운 벤치마크에서 훨씬 적은 파라미터와 계산으로 우수한 성능을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2407.21773.pdf

Title: RainMamba: Enhanced Locality Learning with State Space Models for Video Deraining

Original Abstract:
The outdoor vision systems are frequently contaminated by rain streaks and raindrops, which significantly degenerate the performance of visual tasks and multimedia applications. The nature of videos exhibits redundant temporal cues for rain removal with higher stability. Traditional video deraining methods heavily rely on optical flow estimation and kernel-based manners, which have a limited receptive field. Yet, transformer architectures, while enabling long-term dependencies, bring about a significant increase in computational complexity. Recently, the linear-complexity operator of the state space models (SSMs) has contrarily facilitated efficient long-term temporal modeling, which is crucial for rain streaks and raindrops removal in videos. Unexpectedly, its uni-dimensional sequential process on videos destroys the local correlations across the spatio-temporal dimension by distancing adjacent pixels. To address this, we present an improved SSMs-based video deraining network (RainMamba) with a novel Hilbert scanning mechanism to better capture sequence-level local information. We also introduce a difference-guided dynamic contrastive locality learning strategy to enhance the patch-level self-similarity learning ability of the proposed network. Extensive experiments on four synthesized video deraining datasets and real-world rainy videos demonstrate the effectiveness and efficiency of our network in the removal of rain streaks and raindrops. Our code and results are available at this https URL.

Translated Abstract:
야외 비전 시스템은 비가 내릴 때 생기는 비 자국과 빗방울 때문에 성능이 많이 떨어져. 이런 비와 관련된 장면을 처리하는 데 있어 비디오에서 시간이 지나도 안정적인 정보를 많이 활용할 수 있어. 전통적인 비디오 제습 방법은 광학 흐름 추정이나 커널 기반 방식을 많이 사용하는데, 이 방법들은 한정된 범위의 정보만 처리할 수 있어. 

반면에 트랜스포머 구조는 장기적인 의존성을 가능하게 하지만, 계산 복잡성이 많이 증가해. 최근에는 상태 공간 모델(SSM)의 선형 복잡도 연산자가 비디오에서 비 자국과 빗방울을 제거하는 데 필요한 효율적인 장기 시간 모델링을 도와주고 있어. 하지만 이 방식은 비디오의 순차적인 처리 과정에서 인접한 픽셀 간의 지역적 상관관계를 해치게 돼. 

그래서 우리는 새로운 힐버트 스캔 메커니즘을 활용한 SSM 기반 비디오 제습 네트워크인 RainMamba를 제안해. 이 네트워크는 시퀀스 수준의 지역 정보를 더 잘 잡아낼 수 있어. 또한, 패치 수준의 자기 유사성 학습 능력을 높이기 위해 차이 기반의 동적 대조 지역 학습 전략도 도입했어. 

네 개의 합성 비디오 제습 데이터셋과 실제 비 오는 비디오에 대한 광범위한 실험을 통해, 우리 네트워크가 비 자국과 빗방울 제거에서 효과적이고 효율적임을 입증했어. 우리의 코드와 결과는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2408.11196.pdf

Title: Robust Long-Range Perception Against Sensor Misalignment in Autonomous Vehicles

Original Abstract:
Advances in machine learning algorithms for sensor fusion have significantly improved the detection and prediction of other road users, thereby enhancing safety. However, even a small angular displacement in the sensor's placement can cause significant degradation in output, especially at long range. In this paper, we demonstrate a simple yet generic and efficient multi-task learning approach that not only detects misalignment between different sensor modalities but is also robust against them for long-range perception. Along with the amount of misalignment, our method also predicts calibrated uncertainty, which can be useful for filtering and fusing predicted misalignment values over time. In addition, we show that the predicted misalignment parameters can be used for self-correcting input sensor data, further improving the perception performance under sensor misalignment.

Translated Abstract:
센서 융합을 위한 머신러닝 알고리즘의 발전 덕분에 도로 위 다른 사용자들을 더 잘 감지하고 예측할 수 있게 되었고, 이로 인해 안전성이 향상됐어. 하지만 센서의 위치가 조금만 틀어져도 출력이 크게 나빠질 수 있는데, 특히 멀리 있는 물체를 감지할 때 더 그래.

이 논문에서는 간단하면서도 일반적이고 효율적인 다중 작업 학습 접근법을 보여줘. 이 방법은 서로 다른 센서 모달리티 간의 정렬 불량을 감지할 뿐만 아니라, 장거리 인식에 대해 강인한 성능을 보여줘. 정렬 불량의 정도와 함께, 우리 방법은 보정된 불확실성도 예측하는데, 이건 시간에 따라 예측된 정렬 불량 값을 필터링하고 융합하는 데 유용할 수 있어.

또한, 예측된 정렬 불량 매개변수를 사용해 입력 센서 데이터를 스스로 수정할 수 있다는 점도 보여줘. 이렇게 하면 센서 정렬 불량이 있을 때 인식 성능이 더 향상될 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2408.12528.pdf

Title: Show-o: One Single Transformer to Unify Multimodal Understanding and Generation

Original Abstract:
We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at this https URL.

Translated Abstract:
우리는 Show-o라는 통합 트랜스포머를 소개해. 이 모델은 여러 가지 모달리티를 이해하고 생성하는 걸 하나로 합쳐줘. 완전히 자율적인 모델과는 달리, Show-o는 자율적 모델링과 (이산) 확산 모델링을 통합해서 다양한 입력과 출력을 유연하게 처리할 수 있어.

이 통합 모델은 시각-언어 작업을 다양하게 지원하는데, 예를 들어 시각적 질문-답변, 텍스트에서 이미지 생성, 텍스트 안내에 의한 인페인팅/엑스트라폴레이션, 그리고 혼합 모달리티 생성 같은 작업들이 있어. 여러 벤치마크에서 이 모델은 기존의 개별 모델들과 비교해서 비슷하거나 더 나은 성능을 보여주고, 파라미터 수도 같거나 더 많아서 이해나 생성에 맞춰져 있어. 

이건 차세대 기초 모델로서의 가능성을 확실히 강조해. 코드와 모델은 이 URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2408.13243.pdf

Title: MCTR: Multi Camera Tracking Transformer

Original Abstract:
Multi-camera tracking plays a pivotal role in various real-world applications. While end-to-end methods have gained significant interest in single-camera tracking, multi-camera tracking remains predominantly reliant on heuristic techniques. In response to this gap, this paper introduces Multi-Camera Tracking tRansformer (MCTR), a novel end-to-end approach tailored for multi-object detection and tracking across multiple cameras with overlapping fields of view. MCTR leverages end-to-end detectors like DEtector TRansformer (DETR) to produce detections and detection embeddings independently for each camera view. The framework maintains set of track embeddings that encaplusate global information about the tracked objects, and updates them at every frame by integrating the local information from the view-specific detection embeddings. The track embeddings are probabilistically associated with detections in every camera view and frame to generate consistent object tracks. The soft probabilistic association facilitates the design of differentiable losses that enable end-to-end training of the entire system. To validate our approach, we conduct experiments on MMPTrack and AI City Challenge, two recently introduced large-scale multi-camera multi-object tracking datasets.

Translated Abstract:
다중 카메라 추적은 여러 실제 응용 프로그램에서 매우 중요해. 단일 카메라 추적에서는 엔드 투 엔드 방식이 많이 주목받고 있지만, 다중 카메라 추적은 여전히 경험적 기법에 의존하고 있어. 이런 문제를 해결하기 위해, 이 논문에서는 Multi-Camera Tracking tRansformer (MCTR)라는 새로운 엔드 투 엔드 방식을 소개해. 이 방식은 여러 카메라에서 겹치는 시야를 가진 다중 객체를 탐지하고 추적하는 데 맞춰져 있어.

MCTR는 DEtector TRansformer (DETR) 같은 엔드 투 엔드 탐지기를 활용해서 각 카메라 뷰에 대해 독립적으로 탐지 결과와 탐지 임베딩을 생성해. 이 프레임워크는 추적되는 객체에 대한 전반적인 정보를 담고 있는 추적 임베딩 세트를 유지하고, 각 프레임마다 뷰별 탐지 임베딩에서 지역 정보를 통합해서 업데이트해. 추적 임베딩은 모든 카메라 뷰와 프레임에서 탐지 결과와 확률적으로 연결되어 일관된 객체 추적을 생성해.

이 부드러운 확률적 연관성 덕분에 전체 시스템의 엔드 투 엔드 훈련을 가능하게 하는 미분 가능한 손실 설계가 가능해. 우리의 접근 방식을 검증하기 위해, 최근에 소개된 대규모 다중 카메라 다중 객체 추적 데이터셋인 MMPTrack과 AI City Challenge에서 실험을 진행했어.

================================================================================

URL:
https://arxiv.org/pdf/2408.13257.pdf

Title: MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?

Original Abstract:
Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has recently garnered widespread attention in the research community. However, we observe that existing benchmarks present several common barriers that make it difficult to measure the significant challenges that models face in the real world, including: 1) small data scale leads to a large performance variance; 2) reliance on model-based annotations results in restricted data quality; 3) insufficient task difficulty, especially caused by the limited image resolution. To tackle these issues, we introduce MME-RealWorld. Specifically, we collect more than $300$K images from public datasets and the Internet, filtering $13,366$ high-quality images for annotation. This involves the efforts of professional $25$ annotators and $7$ experts in MLLMs, contributing to $29,429$ question-answer pairs that cover $43$ subtasks across $5$ real-world scenarios, extremely challenging even for humans. As far as we know, MME-RealWorld is the largest manually annotated benchmark to date, featuring the highest resolution and a targeted focus on real-world applications. We further conduct a thorough evaluation involving $28$ prominent MLLMs, such as GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the most advanced models struggle with our benchmarks, where none of them reach $60\%$ accuracy. The challenges of perceiving high-resolution images and understanding complex real-world scenarios remain urgent issues to be addressed. The data and evaluation code are released at this https URL .

Translated Abstract:
최근에 다중 모달 대형 언어 모델(MLLM)에 대한 포괄적인 평가가 연구 커뮤니티에서 큰 관심을 받고 있어. 하지만 기존의 벤치마크에는 모델들이 실제 세계에서 직면하는 주요 문제를 측정하기 어렵게 만드는 몇 가지 공통적인 장애물이 있어. 여기에는 1) 작은 데이터 규모로 인해 성능 변동이 크고, 2) 모델 기반 주석에 의존하다 보니 데이터 품질이 제한적이고, 3) 이미지 해상도가 낮아서 작업 난이도가 충분하지 않은 문제가 포함돼.

이런 문제를 해결하기 위해 MME-RealWorld를 소개할게. 우리는 공공 데이터셋과 인터넷에서 30만 개 이상의 이미지를 수집하고, 13,366개의 고품질 이미지를 필터링해서 주석을 달았어. 이 과정에는 25명의 전문 주석자와 7명의 MLLM 전문가가 참여했고, 총 29,429개의 질문-답변 쌍을 만들었어. 이 쌍은 5개의 실제 시나리오에서 43개의 하위 작업을 다루고 있어. 이건 인간에게도 매우 도전적인 작업이야. 우리가 아는 한, MME-RealWorld는 지금까지 가장 큰 수작업 주석 벤치마크로, 최고 해상도를 자랑하며 실제 응용 프로그램에 초점을 맞추고 있어.

또한 우리는 GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet 등 28개의 주요 MLLM을 포함한 철저한 평가를 수행했어. 결과적으로, 가장 발전된 모델조차도 우리의 벤치마크에서 고전하고 있으며, 어떤 모델도 60% 정확도에 도달하지 못했어. 고해상도 이미지를 인식하고 복잡한 실제 시나리오를 이해하는 문제는 여전히 시급하게 해결해야 할 과제야. 데이터와 평가 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2408.14672.pdf

Title: Physically Feasible Semantic Segmentation

Original Abstract:
State-of-the-art semantic segmentation models are typically optimized in a data-driven fashion, minimizing solely per-pixel classification objectives on their training data. This purely data-driven paradigm often leads to absurd segmentations, especially when the domain of input images is shifted from the one encountered during training. For instance, state-of-the-art models may assign the label ``road'' to a segment which is located above a segment that is respectively labeled as ``sky'', although our knowledge of the physical world dictates that such a configuration is not feasible for images captured by forward-facing upright cameras. Our method, Physically Feasible Semantic Segmentation (PhyFea), extracts explicit physical constraints that govern spatial class relations from the training sets of semantic segmentation datasets and enforces a differentiable loss function that penalizes violations of these constraints to promote prediction feasibility. PhyFea yields significant performance improvements in mIoU over each state-of-the-art network we use as baseline across ADE20K, Cityscapes and ACDC, notably a $1.5\%$ improvement on ADE20K and a $2.1\%$ improvement on ACDC.

Translated Abstract:
최신의 의미 분할 모델들은 보통 데이터 기반으로 최적화되며, 훈련 데이터에서 픽셀별 분류 목표만 최소화해. 이렇게 데이터에만 의존하는 방식은 훈련 중에 접한 이미지와 다른 도메인의 입력 이미지가 들어오면 엉뚱한 분할 결과를 초래할 수 있어. 예를 들어, 최신 모델이 "하늘"로 레이블이 붙은 영역 위에 있는 영역을 "도로"라고 잘못 레이블링할 수 있어. 물리적인 세계에서 이런 구성이 가능하지 않다는 건 모두가 아는 사실이니까.

우리의 방법인 물리적으로 가능한 의미 분할(PhyFea)은 의미 분할 데이터셋의 훈련 세트에서 공간적 클래스 관계를 지배하는 명확한 물리적 제약을 추출해. 그리고 이러한 제약을 위반할 경우 벌점을 주는 미분 가능한 손실 함수를 적용해 예측의 가능성을 높여. PhyFea는 ADE20K, Cityscapes, ACDC에서 우리가 기준으로 삼은 최신 네트워크에 비해 mIoU에서 상당한 성능 향상을 보여줘. 특히 ADE20K에서 1.5% 향상, ACDC에서 2.1% 향상된 결과를 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.01491.pdf

Title: EarthGen: Generating the World from Top-Down Views

Original Abstract:
In this work, we present a novel method for extensive multi-scale generative terrain modeling. At the core of our model is a cascade of superresolution diffusion models that can be combined to produce consistent images across multiple resolutions. Pairing this concept with a tiled generation method yields a scalable system that can generate thousands of square kilometers of realistic Earth surfaces at high resolution. We evaluate our method on a dataset collected from Bing Maps and show that it outperforms super-resolution baselines on the extreme super-resolution task of 1024x zoom. We also demonstrate its ability to create diverse and coherent scenes via an interactive gigapixel-scale generated map. Finally, we demonstrate how our system can be extended to enable novel content creation applications including controllable world generation and 3D scene generation.

Translated Abstract:
이번 연구에서는 광범위한 다중 스케일 생성 지형 모델링을 위한 새로운 방법을 제시해. 우리 모델의 핵심은 여러 해상도에서 일관된 이미지를 만들어낼 수 있는 초해상도 확산 모델의 연속적인 조합이야. 이 개념을 타일 생성 방식과 결합하면, 수천 평방킬로미터의 현실적인 지구 표면을 고해상도로 생성할 수 있는 확장 가능한 시스템이 만들어져.

우리는 이 방법을 Bing Maps에서 수집한 데이터셋으로 평가했어. 그 결과, 1024배 줌의 극단적인 초해상도 작업에서 기존 초해상도 기준보다 더 뛰어난 성능을 보여줬어. 또한, 인터랙티브 기가픽셀 규모의 생성된 지도를 통해 다양한 장면을 만들 수 있는 능력도 입증했어.

마지막으로, 우리의 시스템이 제어 가능한 세계 생성이나 3D 장면 생성과 같은 새로운 콘텐츠 제작 응용 프로그램으로 확장될 수 있는 방법도 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.02374.pdf

Title: Exploring Low-Dimensional Subspaces in Diffusion Models for Controllable Image Editing

Original Abstract:
Recently, diffusion models have emerged as a powerful class of generative models. Despite their success, there is still limited understanding of their semantic spaces. This makes it challenging to achieve precise and disentangled image generation without additional training, especially in an unsupervised way. In this work, we improve the understanding of their semantic spaces from intriguing observations: among a certain range of noise levels, (1) the learned posterior mean predictor (PMP) in the diffusion model is locally linear, and (2) the singular vectors of its Jacobian lie in low-dimensional semantic subspaces. We provide a solid theoretical basis to justify the linearity and low-rankness in the PMP. These insights allow us to propose an unsupervised, single-step, training-free LOw-rank COntrollable image editing (LOCO Edit) method for precise local editing in diffusion models. LOCO Edit identified editing directions with nice properties: homogeneity, transferability, composability, and linearity. These properties of LOCO Edit benefit greatly from the low-dimensional semantic subspace. Our method can further be extended to unsupervised or text-supervised editing in various text-to-image diffusion models (T-LOCO Edit). Finally, extensive empirical experiments demonstrate the effectiveness and efficiency of LOCO Edit. The codes will be released at this https URL.

Translated Abstract:
최근에 확산 모델이 강력한 생성 모델의 한 종류로 주목받고 있어. 하지만 아직 그 의미 공간에 대한 이해는 부족해. 이 덕분에 추가적인 훈련 없이도 정확하고 분리된 이미지 생성을 하는 게 어려워, 특히 비지도 학습에서는 말이지.

이번 연구에서는 흥미로운 관찰을 통해 이 의미 공간에 대한 이해를 높였어. 특정한 노이즈 수준 범위 내에서, (1) 확산 모델의 학습된 후방 평균 예측기(PMP)가 지역적으로 선형적이라는 것과, (2) 그 야코비안의 특이 벡터가 저차원 의미 하위 공간에 위치한다는 걸 발견했어. 우리는 이 PMP의 선형성과 저차원성에 대한 견고한 이론적 근거를 제공해.

이런 통찰을 바탕으로, 우리는 확산 모델에서 정확한 지역 수정을 위한 비지도식의 단일 단계 훈련 없는 저차원 제어 이미지 편집(LOCO Edit) 방법을 제안했어. LOCO Edit는 균일성, 전이 가능성, 조합 가능성, 선형성과 같은 좋은 특성을 가진 편집 방향을 찾았어. LOCO Edit의 이러한 특성은 저차원 의미 하위 공간 덕분에 크게 향상돼.

우리 방법은 다양한 텍스트-이미지 확산 모델에서 비지도 또는 텍스트-지도 편집으로도 확장할 수 있어(T-LOCO Edit). 마지막으로, 광범위한 실험을 통해 LOCO Edit의 효과성과 효율성을 입증했어. 코드는 이 https URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.02979.pdf

Title: Vec2Face: Scaling Face Dataset Generation with Loosely Constrained Vectors

Original Abstract:
This paper studies how to synthesize face images of non-existent persons, to create a dataset that allows effective training of face recognition (FR) models. Two important goals are (1) the ability to generate a large number of distinct identities (inter-class separation) with (2) a wide variation in appearance of each identity (intra-class variation). However, existing works 1) are typically limited in how many well-separated identities can be generated and 2) either neglect or use a separate editing model for attribute augmentation. We propose Vec2Face, a holistic model that uses only a sampled vector as input and can flexibly generate and control face images and their attributes. Composed of a feature masked autoencoder and a decoder, Vec2Face is supervised by face image reconstruction and can be conveniently used in inference. Using vectors with low similarity among themselves as inputs, Vec2Face generates well-separated identities. Randomly perturbing an input identity vector within a small range allows Vec2Face to generate faces of the same identity with robust variation in face attributes. It is also possible to generate images with designated attributes by adjusting vector values with a gradient descent method. Vec2Face has efficiently synthesized as many as 300K identities with 15 million total images, whereas 60K is the largest number of identities created in the previous works. FR models trained with the generated HSFace datasets, from 10k to 300k identities, achieve state-of-the-art accuracy, from 92% to 93.52%, on five real-world test sets. For the first time, our model created using a synthetic training set achieves higher accuracy than the model created using a same-scale training set of real face images (on the CALFW test set).

Translated Abstract:
이 논문은 존재하지 않는 사람들의 얼굴 이미지를 합성하는 방법을 연구해서, 얼굴 인식(FR) 모델을 효과적으로 훈련할 수 있는 데이터셋을 만드는 걸 목표로 하고 있어. 두 가지 중요한 목표가 있어: 

1) 서로 잘 구분되는 여러 개의 독특한 정체성을 생성하는 것 (클래스 간 분리) 
2) 각 정체성의 외모에 넓은 변화를 주는 것 (클래스 내 변화)

하지만 기존 연구들은 1) 생성할 수 있는 잘 구분된 정체성의 수가 제한적이고, 2) 속성 증가를 위해 별도의 편집 모델을 사용하거나 아예 무시하는 경우가 많아. 그래서 우리는 Vec2Face라는 모델을 제안해. 이 모델은 샘플링된 벡터만 입력으로 사용해서 얼굴 이미지와 그 속성을 유연하게 생성하고 조정할 수 있어.

Vec2Face는 특징 마스크 자동 인코더와 디코더로 구성되어 있고, 얼굴 이미지 재구성을 통해 지도 학습을 받아. 추론할 때도 쉽게 사용할 수 있어. 서로 비슷하지 않은 벡터로 입력을 주면, Vec2Face는 잘 구분된 정체성을 생성해. 입력 정체성 벡터를 작은 범위 내에서 랜덤하게 변형하면, 같은 정체성의 얼굴을 다양한 속성으로 생성할 수 있어. 벡터 값을 그래디언트 하강법으로 조정하면 특정 속성을 가진 이미지를 생성하는 것도 가능해.

Vec2Face는 30만 개의 정체성과 총 1500만 개의 이미지를 효율적으로 합성했어. 이전 연구에서 생성된 최대 정체성 수가 6만 개였던 것과 비교하면 엄청나게 많은 수야. Vec2Face로 생성된 HSFace 데이터셋으로 훈련된 FR 모델은 1만에서 30만 정체성까지, 다섯 개의 실제 테스트 세트에서 92%에서 93.52%까지의 최신 정확도를 달성했어. 이번에 생성한 합성 훈련 세트를 사용한 모델이 실제 얼굴 이미지의 동일 규모 훈련 세트를 사용한 모델보다 더 높은 정확도를 기록한 건 처음이야 (CALFW 테스트 세트에서).

================================================================================

URL:
https://arxiv.org/pdf/2409.03553.pdf

Title: Organized Grouped Discrete Representation for Object-Centric Learning

Original Abstract:
Object-Centric Learning (OCL) represents dense image or video pixels as sparse object features. Representative methods utilize discrete representation composed of Variational Autoencoder (VAE) template features to suppress pixel-level information redundancy and guide object-level feature aggregation. The most recent advancement, Grouped Discrete Representation (GDR), further decomposes these template features into attributes. However, its naive channel grouping as decomposition may erroneously group channels belonging to different attributes together and discretize them as sub-optimal template attributes, which losses information and harms expressivity. We propose Organized GDR (OGDR) to organize channels belonging to the same attributes together for correct decomposition from features into attributes. In unsupervised segmentation experiments, OGDR is fully superior to GDR in augmentating classical transformer-based OCL methods; it even improves state-of-the-art diffusion-based ones. Codebook PCA and representation similarity analyses show that compared with GDR, our OGDR eliminates redundancy and preserves information better for guiding object representation learning. The source code is available in the supplementary material.

Translated Abstract:
객체 중심 학습(Object-Centric Learning, OCL)은 이미지나 비디오의 밀집 픽셀을 희소 객체 특징으로 표현하는 방법이야. 대표적인 방법은 변분 오토인코더(Variational Autoencoder, VAE) 템플릿 특징으로 구성된 이산 표현을 사용해 픽셀 수준의 정보 중복을 줄이고, 객체 수준의 특징 집합을 유도해.

최근에 발전된 방법인 그룹화된 이산 표현(Grouped Discrete Representation, GDR)은 이러한 템플릿 특징을 속성으로 더 세분화했어. 하지만 이 방법은 채널 그룹화가 잘못되면 서로 다른 속성에 속한 채널들이 함께 묶여버려서 최적이 아닌 템플릿 속성으로 이산화될 수 있어. 이러면 정보가 손실되고 표현력이 떨어지는 문제가 생겨.

그래서 우리는 조직화된 GDR(Organized GDR, OGDR)을 제안해. OGDR은 같은 속성에 속한 채널들을 올바르게 모아주어 특징을 속성으로 정확하게 분해할 수 있게 해. 비지도 분할 실험에서 OGDR은 GDR보다 클래식한 트랜스포머 기반 OCL 방법을 더 잘 보완해; 최신 확산 기반 방법도 개선해.

코드북 PCA와 표현 유사성 분석에서도 GDR과 비교했을 때, OGDR은 중복을 줄이고 정보를 더 잘 보존해 객체 표현 학습을 더 잘 안내해. 소스 코드는 보충 자료에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.04004.pdf

Title: One-Shot Diffusion Mimicker for Handwritten Text Generation

Original Abstract:
Existing handwritten text generation methods often require more than ten handwriting samples as style references. However, in practical applications, users tend to prefer a handwriting generation model that operates with just a single reference sample for its convenience and efficiency. This approach, known as "one-shot generation", significantly simplifies the process but poses a significant challenge due to the difficulty of accurately capturing a writer's style from a single sample, especially when extracting fine details from the characters' edges amidst sparse foreground and undesired background noise. To address this problem, we propose a One-shot Diffusion Mimicker (One-DM) to generate handwritten text that can mimic any calligraphic style with only one reference sample. Inspired by the fact that high-frequency information of the individual sample often contains distinct style patterns (e.g., character slant and letter joining), we develop a novel style-enhanced module to improve the style extraction by incorporating high-frequency components from a single sample. We then fuse the style features with the text content as a merged condition for guiding the diffusion model to produce high-quality handwritten text images. Extensive experiments demonstrate that our method can successfully generate handwriting scripts with just one sample reference in multiple languages, even outperforming previous methods using over ten samples. Our source code is available at this https URL.

Translated Abstract:
기존의 손글씨 생성 방법은 보통 스타일 참조로 10개 이상의 손글씨 샘플이 필요해. 하지만 실제로는 사용자들이 편리함과 효율성을 위해 단 하나의 샘플만으로 작동하는 손글씨 생성 모델을 선호해. 이런 방식은 "원샷 생성"이라고 불리는데, 과정을 많이 간소화하지만, 한 개의 샘플만으로 작가의 스타일을 정확히 포착하기가 정말 어려워. 특히, 글자 끝의 세밀한 부분을 배경 소음이 많은 상황에서 뽑아내는 게 쉽지 않아.

이 문제를 해결하기 위해 우리는 One-shot Diffusion Mimicker (One-DM)라는 방법을 제안해. 이 방법은 단 하나의 참조 샘플로 어떤 서예 스타일이라도 흉내낼 수 있는 손글씨 텍스트를 생성해. 개별 샘플의 고주파 정보가 독특한 스타일 패턴(예: 글자 기울기, 글자 연결)을 포함하고 있다는 점에서 영감을 받아, 우리는 고주파 성분을 활용한 스타일 향상 모듈을 개발했어. 이렇게 스타일 특징을 텍스트 내용과 결합해 확산 모델이 고품질 손글씨 이미지로 이어지도록 돕는 조건으로 사용해.

많은 실험 결과, 우리의 방법이 여러 언어로 단 한 개의 샘플만으로도 손글씨 스크립트를 성공적으로 생성할 수 있음을 보여주었어. 심지어 10개 이상의 샘플을 사용하는 이전 방법들보다 더 우수한 성능을 보였고. 우리 소스 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.04559.pdf

Title: Thinking Outside the BBox: Unconstrained Generative Object Compositing

Original Abstract:
Compositing an object into an image involves multiple non-trivial sub-tasks such as object placement and scaling, color/lighting harmonization, viewpoint/geometry adjustment, and shadow/reflection generation. Recent generative image compositing methods leverage diffusion models to handle multiple sub-tasks at once. However, existing models face limitations due to their reliance on masking the original object during training, which constrains their generation to the input mask. Furthermore, obtaining an accurate input mask specifying the location and scale of the object in a new image can be highly challenging. To overcome such limitations, we define a novel problem of unconstrained generative object compositing, i.e., the generation is not bounded by the mask, and train a diffusion-based model on a synthesized paired dataset. Our first-of-its-kind model is able to generate object effects such as shadows and reflections that go beyond the mask, enhancing image realism. Additionally, if an empty mask is provided, our model automatically places the object in diverse natural locations and scales, accelerating the compositing workflow. Our model outperforms existing object placement and compositing models in various quality metrics and user studies.

Translated Abstract:
이미지에 물체를 합성하는 과정에는 물체 배치, 크기 조정, 색상/조명 조화, 시점/기하학 조정, 그림자/반사 생성 같은 여러 복잡한 작업이 필요해. 최근의 생성 이미지 합성 방법은 확산 모델을 사용해서 이런 여러 작업을 동시에 처리하고 있어. 하지만 기존 모델들은 훈련 시 원래 물체를 마스킹하는 데 의존하기 때문에, 생성이 입력된 마스크에 제한되는 문제가 있어. 게다가 새로운 이미지에서 물체의 위치와 크기를 정확하게 지정하는 마스크를 얻는 것도 정말 어려워.

이런 한계를 극복하기 위해 우리는 '제약 없는 생성 물체 합성'이라는 새로운 문제를 정의했어. 즉, 생성 과정이 마스크에 제한되지 않는 거야. 그리고 합성된 쌍 데이터셋을 기반으로 확산 모델을 훈련했어. 우리 모델은 마스크를 넘어서는 그림자와 반사 같은 물체 효과를 생성할 수 있어서 이미지의 사실성을 높여줘. 또한 빈 마스크가 주어지면 우리 모델이 자연스러운 다양한 위치와 크기에 물체를 자동으로 배치해줘, 그래서 합성 작업이 더 빨라져.

우리 모델은 여러 품질 지표와 사용자 연구에서 기존의 물체 배치 및 합성 모델들을 능가했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.04747.pdf

Title: Explicit Mutual Information Maximization for Self-Supervised Learning

Original Abstract:
Recently, self-supervised learning (SSL) has been extensively studied. Theoretically, mutual information maximization (MIM) is an optimal criterion for SSL, with a strong theoretical foundation in information theory. However, it is difficult to directly apply MIM in SSL since the data distribution is not analytically available in applications. In practice, many existing methods can be viewed as approximate implementations of the MIM criterion. This work shows that, based on the invariance property of MI, explicit MI maximization can be applied to SSL under a generic distribution assumption, i.e., a relaxed condition of the data distribution. We further illustrate this by analyzing the generalized Gaussian distribution. Based on this result, we derive a loss function based on the MIM criterion using only second-order statistics. We implement the new loss for SSL and demonstrate its effectiveness via extensive experiments.

Translated Abstract:
최근에 자기 지도 학습(self-supervised learning, SSL)이 많이 연구되고 있어. 이론적으로 상호 정보 최대화(mutual information maximization, MIM)는 SSL에 대한 최적 기준으로, 정보 이론에서 튼튼한 이론적 기반을 가지고 있어. 하지만 실제로는 데이터 분포를 직접적으로 적용하기가 어렵거든. 그래서 많은 기존 방법들은 MIM 기준의 근사적 구현으로 볼 수 있어.

이 논문에서는 MI의 불변성 속성을 바탕으로, 일반적인 분포 가정 하에서 명시적인 MI 최대화를 SSL에 적용할 수 있다고 보여줘. 즉, 데이터 분포의 완화된 조건을 의미해. 우리는 일반화된 가우시안 분포를 분석해서 이 내용을 좀 더 설명해. 

이 결과를 바탕으로, 우리는 2차 통계량만 사용해서 MIM 기준에 기반한 손실 함수(loss function)를 도출했어. 그리고 이 새로운 손실 함수를 SSL에 적용해 봤고, 여러 실험을 통해 그 효과를 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.04751.pdf

Title: Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for Fisheye Cameras

Original Abstract:
Recently, 3D Gaussian Splatting (3DGS) has garnered attention for its high fidelity and real-time rendering. However, adapting 3DGS to different camera models, particularly fisheye lenses, poses challenges due to the unique 3D to 2D projection calculation. Additionally, there are inefficiencies in the tile-based splatting, especially for the extreme curvature and wide field of view of fisheye lenses, which are crucial for its broader real-life applications. To tackle these challenges, we introduce Fisheye-GS.This innovative method recalculates the projection transformation and its gradients for fisheye cameras. Our approach can be seamlessly integrated as a module into other efficient 3D rendering methods, emphasizing its extensibility, lightweight nature, and modular design. Since we only modified the projection component, it can also be easily adapted for use with different camera models. Compared to methods that train after undistortion, our approach demonstrates a clear improvement in visual quality.

Translated Abstract:
최근 3D 가우시안 스플래팅(3DGS)이 높은 품질과 실시간 렌더링 덕분에 주목받고 있어. 하지만 3DGS를 다양한 카메라 모델, 특히 어안 렌즈에 맞추는 건 좀 어려워. 그 이유는 어안 렌즈의 독특한 3D에서 2D로의 투영 계산 때문이야. 게다가 타일 기반 스플래팅은 어안 렌즈의 극단적인 곡률과 넓은 시야각 때문에 비효율적이어서, 이걸 해결하는 게 실제 응용에 중요해.

이런 문제를 해결하기 위해 우리는 Fisheye-GS라는 새로운 방법을 소개해. 이 방법은 어안 카메라에 맞게 투영 변환과 그 기울기를 다시 계산해. 우리의 접근법은 다른 효율적인 3D 렌더링 방법에 모듈로 쉽게 통합할 수 있어서, 확장성이 좋고 가벼우며 모듈형 디자인이야. 우리가 투영 구성 요소만 수정했기 때문에, 다른 카메라 모델에 맞춰서도 쉽게 사용할 수 있어. 왜냐하면 왜곡을 제거한 후에 훈련하는 방법과 비교했을 때, 우리의 접근법이 시각적 품질에서 뚜렷한 개선을 보여주거든.

================================================================================

URL:
https://arxiv.org/pdf/2409.04760.pdf

Title: Training-Free Point Cloud Recognition Based on Geometric and Semantic Information Fusion

Original Abstract:
The trend of employing training-free methods for point cloud recognition is becoming increasingly popular due to its significant reduction in computational resources and time costs. However, existing approaches are limited as they typically extract either geometric or semantic features. To address this limitation, we are the first to propose a novel training-free method that integrates both geometric and semantic features. For the geometric branch, we adopt a non-parametric strategy to extract geometric features. In the semantic branch, we leverage a model aligned with text features to obtain semantic features. Additionally, we introduce the GFE module to complement the geometric information of point clouds and the MFF module to improve performance in few-shot settings. Experimental results demonstrate that our method outperforms existing state-of-the-art training-free approaches on mainstream benchmark datasets, including ModelNet and ScanObiectNN.

Translated Abstract:
점 구름 인식을 위한 훈련 없는 방법을 사용하는 추세가 점점 더 인기를 얻고 있어. 이 방법은 계산 자원과 시간 비용을 크게 줄일 수 있어서 좋아. 하지만 기존의 방법들은 일반적으로 기하학적 특징이나 의미론적 특징 중 하나만 추출하는 한계가 있어. 

이 한계를 해결하기 위해, 우리는 기하학적 특징과 의미론적 특징을 모두 통합한 새로운 훈련 없는 방법을 처음으로 제안해. 기하학적 부분에서는 비모수적 전략을 사용해서 기하학적 특징을 추출하고, 의미론적 부분에서는 텍스트 특징과 일치하는 모델을 활용해 의미론적 특징을 얻어. 

또한, 우리는 점 구름의 기하학적 정보를 보완하기 위한 GFE 모듈과, 적은 샷 환경에서 성능을 향상시키기 위한 MFF 모듈을 소개해. 실험 결과, 우리의 방법이 ModelNet과 ScanObjectNN 같은 주요 벤치마크 데이터셋에서 기존의 최첨단 훈련 없는 접근 방식보다 더 뛰어난 성능을 보였다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.05099.pdf

Title: DreamMapping: High-Fidelity Text-to-3D Generation via Variational Distribution Mapping

Original Abstract:
Score Distillation Sampling (SDS) has emerged as a prevalent technique for text-to-3D generation, enabling 3D content creation by distilling view-dependent information from text-to-2D guidance. However, they frequently exhibit shortcomings such as over-saturated color and excess smoothness. In this paper, we conduct a thorough analysis of SDS and refine its formulation, finding that the core design is to model the distribution of rendered images. Following this insight, we introduce a novel strategy called Variational Distribution Mapping (VDM), which expedites the distribution modeling process by regarding the rendered images as instances of degradation from diffusion-based generation. This special design enables the efficient training of variational distribution by skipping the calculations of the Jacobians in the diffusion U-Net. We also introduce timestep-dependent Distribution Coefficient Annealing (DCA) to further improve distilling precision. Leveraging VDM and DCA, we use Gaussian Splatting as the 3D representation and build a text-to-3D generation framework. Extensive experiments and evaluations demonstrate the capability of VDM and DCA to generate high-fidelity and realistic assets with optimization efficiency.

Translated Abstract:
Score Distillation Sampling (SDS)는 텍스트에서 3D로 생성하는 데 많이 쓰이는 방법으로, 텍스트에서 2D 가이드를 통해 3D 콘텐츠를 만드는 거예요. 하지만 이 방법은 색상이 너무 과하게 바래거나 부드럽게 나오는 문제가 자주 발생해요.

이번 논문에서는 SDS를 자세히 분석하고 그 방식을 개선했어요. 핵심 디자인은 렌더링된 이미지의 분포를 모델링하는 거라는 걸 알게 되었죠. 이 인사이트를 바탕으로 Variational Distribution Mapping (VDM)이라는 새로운 전략을 소개했어요. 이 방법은 렌더링된 이미지를 확산 기반 생성에서의 저하 사례로 보고 분포 모델링 과정을 빠르게 해줘요. 이 특별한 디자인 덕분에 확산 U-Net에서 Jacobian 계산을 건너뛰면서 변별 분포를 효율적으로 훈련할 수 있어요.

또한, Distillation Precision을 더 높이기 위해 timestep에 따라 달라지는 Distribution Coefficient Annealing (DCA)을 도입했어요. VDM과 DCA를 활용해서 Gaussian Splatting을 3D 표현으로 사용하고 텍스트에서 3D로 생성하는 프레임워크를 만들었어요. 여러 실험과 평가를 통해 VDM과 DCA가 고품질의 사실적인 자산을 효율적으로 생성할 수 있다는 걸 보여줬어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.05352.pdf

Title: Driving with Prior Maps: Unified Vector Prior Encoding for Autonomous Vehicle Mapping

Original Abstract:
High-Definition Maps (HD maps) are essential for the precise navigation and decision-making of autonomous vehicles, yet their creation and upkeep present significant cost and timeliness challenges. The online construction of HD maps using on-board sensors has emerged as a promising solution; however, these methods can be impeded by incomplete data due to occlusions and inclement weather. This paper proposes the PriorDrive framework to addresses these limitations by harnessing the power of prior maps, significantly enhancing the robustness and accuracy of online HD map construction. Our approach integrates a variety of prior maps, such as OpenStreetMap's Standard Definition Maps (SD maps), outdated HD maps from vendors, and locally constructed maps from historical vehicle data. To effectively encode this prior information into online mapping models, we introduce a Hybrid Prior Representation (HPQuery) that standardizes the representation of diverse map elements. At the core of PriorDrive is the Unified Vector Encoder (UVE), which employs a dual encoding mechanism to process vector data. The intra-vector encoder captures fine-grained local features, while the inter-vector encoder integrates global context. Furthermore, we propose a segment-level and point-level pre-training strategy that enables the UVE to learn the prior distribution of vector data, thereby improving the encoder's generalizability and performance. Through extensive testing on the nuScenes dataset, we demonstrate that PriorDrive is highly compatible with various online mapping models and substantially improves map prediction capabilities. The integration of prior maps through the PriorDrive framework offers a robust solution to the challenges of single-perception data, paving the way for more reliable autonomous vehicle navigation.

Translated Abstract:
고해상도 지도(HD 지도)는 자율주행차의 정확한 내비게이션과 의사결정에 필수적이지만, 이런 지도를 만드는 것과 유지하는 데는 큰 비용과 시간 문제가 있어. 차량에 장착된 센서를 이용해 온라인으로 HD 지도를 만드는 방법이 유망한 해결책으로 떠오르고 있지만, 가림 현상이나 악천후로 인해 데이터가 불완전해지는 경우가 많아.

이 논문에서는 PriorDrive라는 프레임워크를 제안해 이러한 한계를 극복하려고 해. PriorDrive는 기존의 지도의 힘을 활용해 온라인 HD 지도 구축의 견고성과 정확성을 크게 향상시켜. 우리의 접근 방식은 OpenStreetMap의 표준 정의 지도(SD 지도), 오래된 HD 지도, 그리고 과거 차량 데이터로부터 만든 지역 지도 같은 다양한 기존 지도를 통합해.

기존 정보를 온라인 매핑 모델에 효과적으로 인코딩하기 위해, 우리는 다양한 지도 요소의 표현을 표준화하는 하이브리드 프라이어 표현(HPQuery)을 도입했어. PriorDrive의 핵심은 통합 벡터 인코더(UVE)로, 이건 벡터 데이터를 처리하는 이중 인코딩 메커니즘을 사용해. 내부 벡터 인코더는 세세한 지역 특징을 잡아내고, 외부 벡터 인코더는 전반적인 맥락을 통합해.

또한, 우리는 UVE가 벡터 데이터의 기존 분포를 학습할 수 있도록 세그먼트 수준과 포인트 수준의 사전 훈련 전략을 제안했어. 이로 인해 인코더의 일반화 능력과 성능이 향상돼. nuScenes 데이터셋에 대한 광범위한 테스트를 통해 PriorDrive가 다양한 온라인 매핑 모델과 호환성이 높고, 지도 예측 능력을 크게 개선한다는 것을 보여줬어. PriorDrive 프레임워크를 통해 기존 지도를 통합하는 것은 단일 감지 데이터의 문제를 해결하는 강력한 방법을 제공하고, 더 신뢰할 수 있는 자율주행차 내비게이션을 위한 길을 열어줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.05405.pdf

Title: A Survey of Multimodal Composite Editing and Retrieval

Original Abstract:
In the real world, where information is abundant and diverse across different modalities, understanding and utilizing various data types to improve retrieval systems is a key focus of research. Multimodal composite retrieval integrates diverse modalities such as text, image and audio, etc. to provide more accurate, personalized, and contextually relevant results. To facilitate a deeper understanding of this promising direction, this survey explores multimodal composite editing and retrieval in depth, covering image-text composite editing, image-text composite retrieval, and other multimodal composite retrieval. In this survey, we systematically organize the application scenarios, methods, benchmarks, experiments, and future directions. Multimodal learning is a hot topic in large model era, and have also witnessed some surveys in multimodal learning and vision-language models with transformers published in the PAMI journal. To the best of our knowledge, this survey is the first comprehensive review of the literature on multimodal composite retrieval, which is a timely complement of multimodal fusion to existing reviews. To help readers' quickly track this field, we build the project page for this survey, which can be found at this https URL.

Translated Abstract:
현실 세계에서는 정보가 많고 다양한 형태로 존재해. 그래서 여러 데이터 타입을 이해하고 활용해서 검색 시스템을 개선하는 게 연구의 중요한 목표야. 

멀티모달 복합 검색은 텍스트, 이미지, 오디오 같은 다양한 형태의 데이터를 통합해서 더 정확하고 개인화된, 상황에 맞는 결과를 제공해. 이 흥미로운 분야에 대한 깊은 이해를 돕기 위해, 이 조사는 멀티모달 복합 편집과 검색을 자세히 다뤄. 여기에는 이미지-텍스트 복합 편집, 이미지-텍스트 복합 검색, 그리고 다른 멀티모달 복합 검색이 포함돼.

이 조사는 응용 시나리오, 방법, 기준, 실험, 그리고 미래 방향을 체계적으로 정리했어. 멀티모달 학습은 대형 모델 시대에서 인기 있는 주제고, PAMI 저널에서는 멀티모달 학습과 비전-언어 모델에 관한 여러 조사도 발표됐어. 우리가 아는 한, 이 조사는 멀티모달 복합 검색에 대한 첫 번째 포괄적인 리뷰야. 이건 멀티모달 융합에 대한 기존 리뷰를 보완하는 시의적절한 자료야.

독자들이 이 분야를 빠르게 따라잡을 수 있도록, 이 조사에 대한 프로젝트 페이지도 만들었어. 이 페이지는 이 URL에서 찾아볼 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.05425.pdf

Title: Distribution Discrepancy and Feature Heterogeneity for Active 3D Object Detection

Original Abstract:
LiDAR-based 3D object detection is a critical technology for the development of autonomous driving and robotics. However, the high cost of data annotation limits its advancement. We propose a novel and effective active learning (AL) method called Distribution Discrepancy and Feature Heterogeneity (DDFH), which simultaneously considers geometric features and model embeddings, assessing information from both the instance-level and frame-level perspectives. Distribution Discrepancy evaluates the difference and novelty of instances within the unlabeled and labeled distributions, enabling the model to learn efficiently with limited data. Feature Heterogeneity ensures the heterogeneity of intra-frame instance features, maintaining feature diversity while avoiding redundant or similar instances, thus minimizing annotation costs. Finally, multiple indicators are efficiently aggregated using Quantile Transform, providing a unified measure of informativeness. Extensive experiments demonstrate that DDFH outperforms the current state-of-the-art (SOTA) methods on the KITTI and Waymo datasets, effectively reducing the bounding box annotation cost by 56.3% and showing robustness when working with both one-stage and two-stage models.

Translated Abstract:
LiDAR 기반 3D 물체 탐지는 자율주행과 로봇 기술에 정말 중요한 기술이야. 하지만 데이터 주석 비용이 높아서 발전에 한계가 있어. 그래서 우리는 Distribution Discrepancy and Feature Heterogeneity (DDFH)라는 새로운 능동 학습(AL) 방법을 제안해. 이 방법은 기하학적 특징과 모델 임베딩을 동시에 고려하면서 인스턴스 수준과 프레임 수준에서 정보를 평가해.

Distribution Discrepancy는 라벨이 없는 데이터와 라벨이 있는 데이터의 차이와 새로움을 평가해. 이걸 통해 모델이 제한된 데이터로도 효율적으로 학습할 수 있도록 도와줘. Feature Heterogeneity는 프레임 내 인스턴스 특징의 다양성을 보장해서, 중복되거나 비슷한 인스턴스를 피하면서 주석 비용을 최소화해.

마지막으로, 여러 지표를 Quantile Transform을 사용해 효율적으로 집계해서 정보의 유용성을 통합적으로 평가해. 다양한 실험 결과를 보면, DDFH가 KITTI와 Waymo 데이터셋에서 현재 가장 뛰어난(SOTA) 방법들보다 성능이 더 좋고, 바운딩 박스 주석 비용을 56.3% 줄일 수 있음을 보여줘. 또한, 한 단계와 두 단계 모델 모두에서 강건성을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.05442.pdf

Title: EndoOmni: Zero-Shot Cross-Dataset Depth Estimation in Endoscopy by Robust Self-Learning from Noisy Labels

Original Abstract:
Single-image depth estimation is essential for endoscopy tasks such as localization, reconstruction, and augmented reality. Most existing methods in surgical scenes focus on in-domain depth estimation, limiting their real-world applicability. This constraint stems from the scarcity and inferior labeling quality of medical data for training. In this work, we present EndoOmni, the first foundation model for zero-shot cross-domain depth estimation for endoscopy. To harness the potential of diverse training data, we refine the advanced self-learning paradigm that employs a teacher model to generate pseudo-labels, guiding a student model trained on large-scale labeled and unlabeled data. To address training disturbance caused by inherent noise in depth labels, we propose a robust training framework that leverages both depth labels and estimated confidence from the teacher model to jointly guide the student model training. Moreover, we propose a weighted scale-and-shift invariant loss to adaptively adjust learning weights based on label confidence, thus imposing learning bias towards cleaner label pixels while reducing the influence of highly noisy pixels. Experiments on zero-shot relative depth estimation show that our EndoOmni improves state-of-the-art methods in medical imaging for 41\% and existing foundation models for 25\% in terms of absolute relative error on specific dataset. Furthermore, our model provides strong initialization for fine-tuning to metric depth estimation, maintaining superior performance in both in-domain and out-of-domain scenarios. The source code will be publicly available.

Translated Abstract:
단일 이미지 깊이 추정은 내시경 작업에서 위치 파악, 재구성, 증강 현실 등 여러 가지에 필요해. 기존의 방법들은 주로 수술 장면에서 깊이 추정에 집중하고 있어서 실제 상황에서는 적용하기 어려워. 이런 제약은 훈련을 위한 의료 데이터가 부족하고 레이블 품질이 낮기 때문이야.

이번 연구에서는 EndoOmni를 소개할 건데, 이건 내시경을 위한 제로샷 크로스 도메인 깊이 추정을 위한 첫 번째 기본 모델이야. 다양한 훈련 데이터를 활용하기 위해, 우리는 교사 모델이 가짜 레이블을 생성하고, 이를 바탕으로 대규모 레이블과 비레이블 데이터를 훈련하는 학생 모델을 안내하는 고급 자기 학습 패러다임을 개선했어.

깊이 레이블의 고유한 노이즈로 인해 훈련에 방해가 되는 문제를 해결하기 위해, 우리는 깊이 레이블과 교사 모델에서 추정한 신뢰도를 활용해서 학생 모델 훈련을 동시에 안내하는 강력한 훈련 프레임워크를 제안했어. 게다가, 레이블 신뢰도에 따라 학습 가중치를 적절히 조정하는 가중치 스케일-시프트 불변 손실을 제안해서, 깨끗한 레이블 픽셀에 더 많은 학습 편향을 두고, 노이즈가 심한 픽셀의 영향을 줄였어.

제로샷 상대 깊이 추정 실험에서, EndoOmni가 특정 데이터셋에서 의료 이미징의 최신 방법보다 41% 개선되었고, 기존의 기본 모델보다 25% 개선된 것을 보여줬어. 게다가, 우리 모델은 메트릭 깊이 추정을 위한 파인튜닝에 강력한 초기화를 제공하고, 도메인 내외에서 모두 뛰어난 성능을 유지해. 소스 코드는 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.05463.pdf

Title: DriveScape: Towards High-Resolution Controllable Multi-View Driving Video Generation

Original Abstract:
Recent advancements in generative models have provided promising solutions for synthesizing realistic driving videos, which are crucial for training autonomous driving perception models. However, existing approaches often struggle with multi-view video generation due to the challenges of integrating 3D information while maintaining spatial-temporal consistency and effectively learning from a unified model. In this paper, we propose an end-to-end framework named DriveScape for multi-view, 3D condition-guided video generation. DriveScape not only streamlines the process by integrating camera data to ensure comprehensive spatial-temporal coverage, but also introduces a Bi-Directional Modulated Transformer module to effectively align 3D road structural information. As a result, our approach enables precise control over video generation, significantly enhancing realism and providing a robust solution for generating multi-view driving videos. Our framework achieves state-of-the-art results on the nuScenes dataset, demonstrating impressive generative quality metrics with an FID score of 8.34 and an FVD score of 76.39, as well as superior performance across various perception tasks. This paves the way for more accurate environmental simulations in autonomous driving. Our project homepage: this https URL

Translated Abstract:
최근 생성 모델의 발전 덕분에 현실적인 주행 비디오를 합성하는 데 유망한 해결책이 생겼어. 이런 비디오는 자율주행 인식 모델을 훈련하는 데 필요해. 하지만 기존 방법들은 3D 정보를 통합하면서 공간-시간 일관성을 유지하고 통합 모델에서 효과적으로 학습하는 데 어려움을 겪고 있어.

그래서 이 논문에서는 DriveScape라는 종단 간 프레임워크를 제안해. 이건 다중 시점 3D 조건 기반 비디오 생성을 위한 거야. DriveScape는 카메라 데이터를 통합해 공간-시간 범위를 포괄하도록 과정이 간소화되는데, Bi-Directional Modulated Transformer 모듈을 도입해서 3D 도로 구조 정보를 효과적으로 정렬해.

결과적으로, 이 접근 방식은 비디오 생성을 정밀하게 제어할 수 있게 해주고, 사실감을 크게 향상시켜서 다중 시점 주행 비디오 생성에 강력한 해결책을 제공해. 우리의 프레임워크는 nuScenes 데이터셋에서 최첨단 결과를 달성했고, FID 점수 8.34, FVD 점수 76.39로 놀라운 생성 품질 지표를 보여줬어. 또한 다양한 인식 작업에서도 우수한 성능을 보였고, 자율주행에서 더 정확한 환경 시뮬레이션을 가능하게 해. 프로젝트 홈페이지: this https URL

================================================================================

URL:
https://arxiv.org/pdf/2409.05606.pdf

Title: CustomContrast: A Multilevel Contrastive Perspective For Subject-Driven Text-to-Image Customization

Original Abstract:
Subject-driven text-to-image (T2I) customization has drawn significant interest in academia and industry. This task enables pre-trained models to generate novel images based on unique subjects. Existing studies adopt a self-reconstructive perspective, focusing on capturing all details of a single image, which will misconstrue the specific image's irrelevant attributes (e.g., view, pose, and background) as the subject intrinsic attributes. This misconstruction leads to both overfitting or underfitting of irrelevant and intrinsic attributes of the subject, i.e., these attributes are over-represented or under-represented simultaneously, causing a trade-off between similarity and controllability. In this study, we argue an ideal subject representation can be achieved by a cross-differential perspective, i.e., decoupling subject intrinsic attributes from irrelevant attributes via contrastive learning, which allows the model to focus more on intrinsic attributes through intra-consistency (features of the same subject are spatially closer) and inter-distinctiveness (features of different subjects have distinguished differences). Specifically, we propose CustomContrast, a novel framework, which includes a Multilevel Contrastive Learning (MCL) paradigm and a Multimodal Feature Injection (MFI) Encoder. The MCL paradigm is used to extract intrinsic features of subjects from high-level semantics to low-level appearance through crossmodal semantic contrastive learning and multiscale appearance contrastive learning. To facilitate contrastive learning, we introduce the MFI encoder to capture cross-modal representations. Extensive experiments show the effectiveness of CustomContrast in subject similarity and text controllability.

Translated Abstract:
주제 기반 텍스트-이미지(T2I) 맞춤화는 학계와 산업에서 큰 관심을 받고 있어. 이 작업은 사전 훈련된 모델이 특정 주제를 바탕으로 새로운 이미지를 생성할 수 있게 해줘. 기존 연구들은 자기 재구성 관점에서 접근하는데, 이는 하나의 이미지의 모든 세부 사항을 포착하려고 하다 보니 그 이미지와 관련 없는 특성(예: 시점, 자세, 배경)을 주제의 고유 특성으로 오해하게 돼. 이런 오해로 인해 주제의 관련 없는 특성과 고유 특성이 모두 과도하거나 부족하게 표현되면서 유사성과 제어 가능성 사이에 균형을 잃게 돼.

이 연구에서는 이상적인 주제 표현이 교차 차별적 관점을 통해 이루어질 수 있다고 주장해. 즉, 대조 학습을 통해 주제의 고유 특성을 관련 없는 특성과 분리하면 모델이 고유 특성에 더 집중할 수 있게 돼. 여기서 내부 일관성(같은 주제의 특징이 공간적으로 가까움)과 외부 구별성(다른 주제의 특징이 뚜렷한 차이를 가짐)을 활용하는 거야.

구체적으로, 우리는 CustomContrast라는 새로운 프레임워크를 제안해. 이 프레임워크에는 다단계 대조 학습(MCL) 패러다임과 다중 모달 특징 주입(MFI) 인코더가 포함돼. MCL 패러다임은 고수준 의미에서 저수준 외관까지 주제의 고유 특징을 추출하는 데 사용돼. 이를 위해 우리는 MFI 인코더를 도입해서 교차 모달 표현을 캡처해.

광범위한 실험을 통해 CustomContrast가 주제의 유사성과 텍스트 제어 가능성에서 효과적임을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.05662.pdf

Title: Real-Time Human Action Recognition on Embedded Platforms

Original Abstract:
With advancements in computer vision and deep learning, video-based human action recognition (HAR) has become practical. However, due to the complexity of the computation pipeline, running HAR on live video streams incurs excessive delays on embedded platforms. This work tackles the real-time performance challenges of HAR with four contributions: 1) an experimental study identifying a standard Optical Flow (OF) extraction technique as the latency bottleneck in a state-of-the-art HAR pipeline, 2) an exploration of the latency-accuracy tradeoff between the standard and deep learning approaches to OF extraction, which highlights the need for a novel, efficient motion feature extractor, 3) the design of Integrated Motion Feature Extractor (IMFE), a novel single-shot neural network architecture for motion feature extraction with drastic improvement in latency, 4) the development of RT-HARE, a real-time HAR system tailored for embedded platforms. Experimental results on an Nvidia Jetson Xavier NX platform demonstrated that RT-HARE realizes real-time HAR at a video frame rate of 30 frames per second while delivering high levels of recognition accuracy.

Translated Abstract:
컴퓨터 비전과 딥러닝의 발전 덕분에 비디오 기반의 인간 행동 인식(HAR)이 현실적으로 가능해졌어. 하지만 계산 과정이 복잡해서, 실시간 비디오 스트림에서 HAR을 실행하면 임베디드 플랫폼에서 너무 많은 지연이 발생해.

이 연구는 HAR의 실시간 성능 문제를 해결하기 위해 네 가지 기여를 해: 

1) 최신 HAR 파이프라인에서 표준 광 흐름(OF) 추출 기법이 지연의 병목 현상이라는 걸 밝혀낸 실험 연구, 
2) 표준 OF 추출법과 딥러닝 접근법 간의 지연-정확도 트레이드오프를 탐구한 부분이야. 이걸 통해 새로운 효율적인 움직임 특징 추출기가 필요하다는 걸 강조했어,
3) Integrated Motion Feature Extractor (IMFE)를 설계했어. 이건 움직임 특징 추출을 위한 새로운 단일 샷 신경망 아키텍처로, 지연 시간을 크게 개선했어,
4) 임베디드 플랫폼에 맞춘 실시간 HAR 시스템인 RT-HARE를 개발했어. 

Nvidia Jetson Xavier NX 플랫폼에서 실험한 결과, RT-HARE는 초당 30프레임의 비디오 프레임 속도로 실시간 HAR을 실현하면서도 높은 인식 정확도를 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06189.pdf

Title: MyGo: Consistent and Controllable Multi-View Driving Video Generation with Camera Control

Original Abstract:
High-quality driving video generation is crucial for providing training data for autonomous driving models. However, current generative models rarely focus on enhancing camera motion control under multi-view tasks, which is essential for driving video generation. Therefore, we propose MyGo, an end-to-end framework for video generation, introducing motion of onboard cameras as conditions to make progress in camera controllability and multi-view consistency. MyGo employs additional plug-in modules to inject camera parameters into the pre-trained video diffusion model, which retains the extensive knowledge of the pre-trained model as much as possible. Furthermore, we use epipolar constraints and neighbor view information during the generation process of each view to enhance spatial-temporal consistency. Experimental results show that MyGo has achieved state-of-the-art results in both general camera-controlled video generation and multi-view driving video generation tasks, which lays the foundation for more accurate environment simulation in autonomous driving. Project page: this https URL

Translated Abstract:
고품질의 운전 비디오 생성은 자율 주행 모델을 위한 훈련 데이터 제공에 매우 중요해. 그런데 현재의 생성 모델들은 다중 시점 작업에서 카메라 모션 제어를 강화하는 데 별로 신경 쓰지 않고 있어. 이건 운전 비디오 생성에 정말 필요하거든. 그래서 우리는 MyGo라는 비디오 생성 프레임워크를 제안해. 이 프레임워크는 탑재된 카메라의 움직임을 조건으로 사용해서 카메라 제어 가능성과 다중 시점 일관성을 향상시키는 데 도움을 줘.

MyGo는 추가 플러그인 모듈을 사용해서 카메라 파라미터를 사전 훈련된 비디오 확산 모델에 주입해. 이렇게 하면 사전 훈련된 모델의 방대한 지식을 최대한 유지할 수 있어. 그리고 각 뷰의 생성 과정에서 에피폴라 제약조건과 이웃 뷰 정보를 사용해서 공간-시간 일관성을 높여.

실험 결과, MyGo는 일반적인 카메라 제어 비디오 생성과 다중 시점 운전 비디오 생성 작업 모두에서 최첨단 결과를 달성했어. 이로 인해 자율 주행의 더 정확한 환경 시뮬레이션을 위한 기초가 마련되었지.

================================================================================

URL:
https://arxiv.org/pdf/2409.06385.pdf

Title: AMNS: Attention-Weighted Selective Mask and Noise Label Suppression for Text-to-Image Person Retrieval

Original Abstract:
Text-to-image person retrieval aims to retrieve images of person given textual descriptions, and most methods implicitly assume that the training image-text pairs are correctly aligned, but in practice, under-correlated and false-correlated problems arise for image-text pairs due to poor image quality and mislabeling. Meanwhile, the random masking augmentation strategy may incorrectly discard semantic content resulting in the problem of generating noisy pairings between image lexical elements and text descriptions. To solve these two problems, we propose a new noise label suppression method and alleviate the problem generated by random mask through an attention-weighted selective mask strategy. In the proposed noise label suppression method, the effect of noise labels is suppressed by preventing the model from being overconfident by considering the inverse KL scatter loss, which is combined with the weight adjustment focus loss to further improve the model's recognition ability on difficult samples. On the other hand, Attention-Weighted Selective Mask processes the raw image through the EMA version of the image encoder, retaining some of the tokens with strong semantic associations with the corresponding text descriptions in order to extract better features. Numerous experiments validate the effectiveness of our approach in terms of dealing with noisy problems. The code will be available soon at this https URL.

Translated Abstract:
텍스트-이미지 인물 검색은 주어진 텍스트 설명을 바탕으로 인물의 이미지를 찾는 것을 목표로 해. 대부분의 방법들은 훈련 이미지-텍스트 쌍이 제대로 정렬되어 있다고 가정하는데, 실제로는 이미지 품질이 낮거나 잘못 레이블링된 경우 때문에 이미지-텍스트 쌍에서 상관관계가 떨어지거나 잘못된 상관관계 문제가 생기곤 해.

이런 문제를 해결하기 위해 우리는 새로운 노이즈 레이블 억제 방법을 제안하고, 랜덤 마스크로 인해 발생하는 문제를 주의 가중 선택 마스크 전략을 통해 완화할 거야. 제안하는 노이즈 레이블 억제 방법에서는 노이즈 레이블의 영향을 줄이기 위해 모델이 과도하게 자신감을 가지지 않도록 역 KL 산포 손실을 고려하고, 이를 가중치 조정 집중 손실과 결합해 어려운 샘플에 대한 모델의 인식 능력을 키워.

한편, 주의 가중 선택 마스크는 원본 이미지를 이미지 인코더의 EMA 버전을 통해 처리하고, 해당 텍스트 설명과 강한 의미적 연관이 있는 일부 토큰을 유지해 더 나은 특징을 추출해. 여러 실험을 통해 우리의 방법이 노이즈 문제를 처리하는 데 효과적이라는 걸 확인했어. 코드도 곧 이 URL에서 제공될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06644.pdf

Title: EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis

Original Abstract:
Early detection of eye diseases like glaucoma, macular degeneration, and diabetic retinopathy is crucial for preventing vision loss. While artificial intelligence (AI) foundation models hold significant promise for addressing these challenges, existing ophthalmic foundation models primarily focus on a single modality, whereas diagnosing eye diseases requires multiple modalities. A critical yet often overlooked aspect is harnessing the multi-view information across various modalities for the same patient. Additionally, due to the long-tail nature of ophthalmic diseases, standard fully supervised or unsupervised learning approaches often struggle. Therefore, it is essential to integrate clinical text to capture a broader spectrum of diseases. We propose EyeCLIP, a visual-language foundation model developed using over 2.77 million multi-modal ophthalmology images with partial text data. To fully leverage the large multi-modal unlabeled and labeled data, we introduced a pretraining strategy that combines self-supervised reconstructions, multi-modal image contrastive learning, and image-text contrastive learning to learn a shared representation of multiple modalities. Through evaluation using 14 benchmark datasets, EyeCLIP can be transferred to a wide range of downstream tasks involving ocular and systemic diseases, achieving state-of-the-art performance in disease classification, visual question answering, and cross-modal retrieval. EyeCLIP represents a significant advancement over previous methods, especially showcasing few-shot, even zero-shot capabilities in real-world long-tail scenarios.

Translated Abstract:
안과 질병인 녹내장, 황반변성, 당뇨병성 망막병증 같은 것들을 빨리 발견하는 게 시력을 잃지 않기 위해 정말 중요해. 인공지능(AI) 기반 모델들이 이런 문제를 해결할 가능성이 큰데, 기존의 안과 모델들은 주로 한 가지 방식에만 집중하고 있어. 하지만 눈 질병 진단은 여러 가지 정보를 필요로 해. 환자에 대한 여러 관점의 정보를 활용하는 게 중요한데, 이 부분은 종종 간과돼.

또한, 안과 질병은 종류가 많아서 일반적인 감독 학습이나 비감독 학습 방식으로는 잘 안 돼. 그래서 다양한 질병을 포괄하기 위해 임상 데이터를 통합하는 게 필수적이야. 우리는 EyeCLIP이라는 시각-언어 모델을 제안하는데, 이 모델은 277만 개 이상의 다중 모달 안과 이미지를 부분적인 텍스트 데이터와 함께 사용해 개발했어.

EyeCLIP은 방대한 다중 모달 데이터(레이블이 있는 것과 없는 것)를 최대한 활용하기 위해 자기 감독식 재구성, 다중 모달 이미지 대조 학습, 이미지-텍스트 대조 학습을 결합한 사전 학습 전략을 도입했어. 14개의 벤치마크 데이터셋을 사용한 평가를 통해 EyeCLIP은 안과 및 전신 질병과 관련된 다양한 작업에 잘 적용될 수 있으며, 질병 분류, 시각 질문 응답, 교차 모달 검색에서 최첨단 성능을 달성해.

EyeCLIP은 이전 방법들보다 큰 발전을 보여주고, 특히 실제 세계의 다양한 상황에서도 몇 번의 학습만으로, 심지어 학습 없이도 잘 작동하는 능력을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.06683.pdf

Title: Alignist: CAD-Informed Orientation Distribution Estimation by Fusing Shape and Correspondences

Original Abstract:
Object pose distribution estimation is crucial in robotics for better path planning and handling of symmetric objects. Recent distribution estimation approaches employ contrastive learning-based approaches by maximizing the likelihood of a single pose estimate in the absence of a CAD model. We propose a pose distribution estimation method leveraging symmetry respecting correspondence distributions and shape information obtained using a CAD model. Contrastive learning-based approaches require an exhaustive amount of training images from different viewpoints to learn the distribution properly, which is not possible in realistic scenarios. Instead, we propose a pipeline that can leverage correspondence distributions and shape information from the CAD model, which are later used to learn pose distributions. Besides, having access to pose distribution based on correspondences before learning pose distributions conditioned on images, can help formulate the loss between distributions. The prior knowledge of distribution also helps the network to focus on getting sharper modes instead. With the CAD prior, our approach converges much faster and learns distribution better by focusing on learning sharper distribution near all the valid modes, unlike contrastive approaches, which focus on a single mode at a time. We achieve benchmark results on SYMSOL-I and T-Less datasets.

Translated Abstract:
물체 자세 분포 추정은 로봇 공학에서 경로 계획과 대칭 물체 처리에 중요해. 최근의 분포 추정 방법들은 CAD 모델 없이 단일 자세 추정의 가능성을 최대화하는 대조 학습 기반 접근 방식을 사용하고 있어. 우리는 CAD 모델을 활용해 대칭성을 고려한 대응 분포와 형태 정보를 이용한 자세 분포 추정 방법을 제안해.

대조 학습 기반 방법들은 분포를 제대로 배우기 위해 다양한 시점에서 많은 훈련 이미지를 요구하는데, 현실적인 상황에서는 이게 쉽지 않아. 대신, 우리는 CAD 모델에서 얻은 대응 분포와 형태 정보를 활용하는 파이프라인을 제안해, 이 정보들을 나중에 자세 분포 학습에 사용해.

또한, 이미지에 대한 자세 분포를 학습하기 전에 대응에 기반한 자세 분포에 접근할 수 있으면, 분포 간의 손실을 설정하는 데 도움이 돼. 분포에 대한 사전 지식은 네트워크가 더 선명한 모드에 집중하는 데도 도움이 돼. CAD 모델을 이용한 우리의 접근 방식은 훨씬 더 빠르게 수렴하고, 유효한 모든 모드 근처에서 더 선명한 분포를 배우는 데 집중해, 대조 접근 방식이 한 번에 하나의 모드에만 집중하는 것과는 달라. 우리는 SYMSOL-I와 T-Less 데이터셋에서 기준 결과를 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2305.09868.pdf

Title: The Principle of Uncertain Maximum Entropy

Original Abstract:
The principle of maximum entropy is a well-established technique for choosing a distribution that matches available information while minimizing bias. It finds broad use across scientific disciplines and in machine learning. However, the principle as defined by is susceptible to noise and error in observations. This forces real-world practitioners to use relaxed versions of the principle in an ad hoc way, negatively impacting interpretation. To address this situation, we present a new principle we call uncertain maximum entropy that generalizes the classic principle and provides interpretable solutions irrespective of the observational methods in use. We introduce a convex approximation and expectation-maximization based algorithm for finding solutions to our new principle. Finally, we contrast this new technique with two simpler generally applicable solutions theoretically and experimentally show our technique provides superior accuracy.

Translated Abstract:
최대 엔트로피 원리는 가용 정보를 기반으로 배포를 선택하면서 편향을 최소화하는 잘 알려진 기술이야. 이 원리는 과학 분야와 머신러닝에서 널리 사용돼. 

하지만 이 원리는 관측에서의 잡음과 오류에 영향을 받을 수 있어. 그래서 실제로 이 원리를 사용하는 사람들은 원리를 완화한 버전을 임시방편적으로 사용하게 되고, 이건 해석에 부정적인 영향을 미쳐.

이를 해결하기 위해 우리는 '불확실한 최대 엔트로피'라는 새로운 원리를 제안해. 이 원리는 고전 원리를 일반화하고, 사용하는 관측 방법에 관계없이 해석 가능한 솔루션을 제공해. 

우리는 새로운 원리에 대한 솔루션을 찾기 위해 볼록 근사와 기대-최대화 기반 알고리즘을 소개해. 마지막으로, 이 새로운 기술을 두 가지 간단한 일반 적용 가능한 솔루션과 비교했는데, 이론적으로나 실험적으로나 우리 기술이 더 높은 정확성을 제공한다는 것을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2308.05731.pdf

Title: The Integration of Prediction and Planning in Deep Learning Automated Driving Systems: A Review

Original Abstract:
Automated driving has the potential to revolutionize personal, public, and freight mobility. Beside accurately perceiving the environment, automated vehicles must plan a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential, separate tasks. While this accounts for the influence of surrounding traffic on the ego vehicle, it fails to anticipate the reactions of traffic participants to the ego vehicle's behavior. Recent methods increasingly integrate prediction and planning in a joint or interdependent step to model bidirectional interactions. To date, a comprehensive overview of different integration principles is lacking. We systematically review state-of-the-art deep learning-based planning systems, and focus on how they integrate prediction. Different facets of the integration ranging from system architecture to high-level behavioral aspects are considered and related to each other. Moreover, we discuss the implications, strengths, and limitations of different integration principles. By pointing out research gaps, describing relevant future challenges, and highlighting trends in the research field, we identify promising directions for future research.

Translated Abstract:
자동차 자동 운전은 개인, 대중, 화물 이동 방식을 혁신할 수 있는 잠재력을 가지고 있어. 환경을 정확하게 인식하는 것 외에도, 자동 운전 차량은 안전하고 편안하며 효율적인 주행 경로를 계획해야 해. 안전과 발전을 위해 많은 연구들이 주변 교통의 미래 움직임을 예측하는 모듈에 의존하고 있어. 

모듈식 자동 운전 시스템은 예측과 계획을 순차적이고 별개의 작업으로 처리하는 경우가 많아. 이렇게 하면 주변 교통이 자차에 미치는 영향은 고려할 수 있지만, 자차의 행동에 대한 교통 참여자들의 반응을 예측하지는 못해. 최근에는 예측과 계획을 함께 통합해 양방향 상호작용을 모델링하는 방법이 점점 더 많아지고 있어. 

하지만 지금까지 다양한 통합 원칙에 대한 종합적인 개요는 부족해. 우리는 최첨단 딥러닝 기반 계획 시스템을 체계적으로 검토하고, 이들이 예측을 어떻게 통합하는지에 초점을 맞췄어. 통합의 여러 측면, 즉 시스템 구조부터 고급 행동적 측면까지 다양한 요소를 고려하고 서로 연결했어. 

게다가, 다양한 통합 원칙의 의미, 강점, 그리고 한계에 대해서도 논의했어. 연구의 공백을 지적하고, 관련된 미래 도전에 대해 설명하며, 연구 분야의 트렌드를 강조함으로써 향후 연구를 위한 유망한 방향을 제시했어.

================================================================================

URL:
https://arxiv.org/pdf/2309.08434.pdf

Title: Segment Anything Model for Brain Tumor Segmentation

Original Abstract:
Glioma is a prevalent brain tumor that poses a significant health risk to individuals. Accurate segmentation of brain tumor is essential for clinical diagnosis and treatment. The Segment Anything Model(SAM), released by Meta AI, is a fundamental model in image segmentation and has excellent zero-sample generalization capabilities. Thus, it is interesting to apply SAM to the task of brain tumor segmentation. In this study, we evaluated the performance of SAM on brain tumor segmentation and found that without any model fine-tuning, there is still a gap between SAM and the current state-of-the-art(SOTA) model.

Translated Abstract:
신경교종은 흔한 뇌 종양으로 개인에게 큰 건강 위험을 초래해. 뇌 종양을 정확하게 분할하는 건 임상 진단과 치료에 매우 중요해. 메타 AI에서 출시한 세그먼트 애니씽 모델(SAM)은 이미지 분할에서 기본 모델로, 제로 샘플 일반화 능력이 뛰어나. 그래서 SAM을 뇌 종양 분할 작업에 적용해보는 게 흥미로워.

이번 연구에서는 뇌 종양 분할에서 SAM의 성능을 평가했어. 그런데 모델을 별도로 조정하지 않고도, SAM과 현재 최첨단(SOTA) 모델 사이에는 여전히 차이가 있다는 걸 발견했어.

================================================================================

URL:
https://arxiv.org/pdf/2311.05477.pdf

Title: Using ResNet to Utilize 4-class T2-FLAIR Slice Classification Based on the Cholinergic Pathways Hyperintensities Scale for Pathological Aging

Original Abstract:
The Cholinergic Pathways Hyperintensities Scale (CHIPS) is a visual rating scale used to assess the extent of cholinergic white matter hyperintensities in T2-FLAIR images, serving as an indicator of dementia severity. However, the manual selection of four specific slices for rating throughout the entire brain is a time-consuming process. Our goal was to develop a deep learning-based model capable of automatically identifying the four slices relevant to CHIPS. To achieve this, we trained a 4-class slice classification model (BSCA) using the ADNI T2-FLAIR dataset (N=150) with the assistance of ResNet. Subsequently, we tested the model's performance on a local dataset (N=30). The results demonstrated the efficacy of our model, with an accuracy of 99.82% and an F1-score of 99.83%. This achievement highlights the potential impact of BSCA as an automatic screening tool, streamlining the selection of four specific T2-FLAIR slices that encompass white matter landmarks along the cholinergic pathways. Clinicians can leverage this tool to assess the risk of clinical dementia development efficiently.

Translated Abstract:
콜린성 경로 고강도 척도(CHIPS)는 T2-FLAIR 이미지에서 콜린성 백질 고강도의 정도를 평가하는 시각적 평가 척도야. 이 척도는 치매의 심각도를 나타내는 지표로 사용돼. 하지만, 전체 뇌에서 네 개의 특정 슬라이스를 수동으로 선택하는 건 시간이 많이 걸리는 일이야. 

우리는 CHIPS와 관련된 네 개의 슬라이스를 자동으로 식별할 수 있는 딥러닝 기반 모델을 개발하는 걸 목표로 했어. 이를 위해 ADNI T2-FLAIR 데이터셋(N=150)을 사용해서 4 클래스 슬라이스 분류 모델(BSCA)을 ResNet의 도움으로 훈련했어. 그리고 나서, 지역 데이터셋(N=30)에서 모델의 성능을 테스트했지. 

결과는 우리 모델의 효율성을 보여줬어. 정확도는 99.82%였고, F1-score는 99.83%였어. 이 성과는 BSCA가 자동 스크리닝 도구로서의 잠재력을 강조해, 콜린성 경로를 따라 백질 랜드마크를 포함하는 네 개의 특정 T2-FLAIR 슬라이스 선택을 간소화할 수 있다는 걸 보여줘. 임상의들은 이 도구를 활용해서 임상적인 치매 발생 위험을 효과적으로 평가할 수 있을 거야.

================================================================================

URL:
https://arxiv.org/pdf/2402.05930.pdf

Title: WebLINX: Real-World Website Navigation with Multi-Turn Dialogue

Original Abstract:
We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots. However, all finetuned models struggle to generalize to unseen websites. Our findings highlight the need for large multimodal models that can generalize to novel settings. Our code, data and models are available for research: this https URL

Translated Abstract:
우리는 대화형 웹 탐색 문제를 제안해. 여기서 디지털 에이전트가 웹 브라우저를 제어하고 사용자의 지시에 따라 실제 작업을 멀티 턴 대화 방식으로 해결하는 거야. 이 문제를 지원하기 위해서 우리는 WEBLINX라는 것을 소개해. 이건 2300개의 전문가 데모를 포함한 10만 개의 상호작용으로 구성된 대규모 벤치마크야. 

우리 벤치마크는 150개 이상의 실제 웹사이트에서 다양한 패턴을 다루고 있어서, 다양한 시나리오에서 에이전트를 훈련하고 평가하는 데 사용할 수 있어. 하지만 방대한 정보 때문에 대형 언어 모델(LLM)은 전체 웹 페이지를 실시간으로 처리하기 어려워. 이 문제를 해결하기 위해, 우리는 관련 요소를 순위별로 정리해 HTML 페이지를 효율적으로 줄이는 검색 기반 모델을 설계했어. 

선택된 요소들과 스크린샷, 행동 기록을 사용해서 다양한 모델이 웹 탐색 시 인간 행동을 얼마나 잘 재현하는지 평가했어. 우리의 실험은 작은 텍스트 전용 모델부터 독점적인 멀티모달 LLM까지 다양해. 우리는 작고 세밀하게 조정된 디코더가 최고의 제로샷 LLM(예: GPT-4V)보다 더 나은 성능을 보인다는 것을 발견했어. 하지만 모든 조정된 모델은 보지 못한 웹사이트에 일반화하는 데 어려움을 겪고 있어. 

이 연구 결과는 새로운 환경에 일반화할 수 있는 대형 멀티모달 모델의 필요성을 강조해. 우리 코드, 데이터, 모델은 연구를 위해 사용할 수 있어: 이 https URL

================================================================================

URL:
https://arxiv.org/pdf/2403.10568.pdf

Title: MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts

Original Abstract:
Despite the demonstrated parameter efficiency of prompt-based multimodal fusion methods, their limited adaptivity and expressiveness often result in suboptimal performance compared to other tuning approaches. In this paper, we address these limitations by decomposing the vanilla prompts to adaptively capture instance-level features. Building upon this decomposition, we introduce the mixture of prompt experts (MoPE) technique to enhance the expressiveness of prompt tuning. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based fusion method exhibits greater expressiveness, scaling more effectively with the training data and the overall number of trainable parameters. We also investigate regularization terms for expert routing, which lead to emergent expert specialization during training, paving the way for interpretable soft prompting. Extensive experiments across six multimodal datasets spanning four modalities demonstrate that our method achieves state-of-the-art results for prompt fusion, matching or even surpassing the performance of fine-tuning while requiring only 0.8% of the trainable parameters. Code will be released: this https URL.

Translated Abstract:
프롬프트 기반 멀티모달 융합 방법의 파라미터 효율성은 이미 입증되었지만, 적응력과 표현력이 제한되어서 다른 조정 방법에 비해 성능이 떨어지는 경우가 많아. 이 논문에서는 이러한 한계를 해결하기 위해 기본 프롬프트를 분해해서 인스턴스 레벨의 특징을 적응적으로 포착할 수 있게 했어. 

이 분해를 바탕으로, 우리는 프롬프트 조정의 표현력을 높이기 위해 혼합 프롬프트 전문가(MoPE) 기법을 도입했어. MoPE는 멀티모달 페어링 선행 지식을 이용해서 각 인스턴스에 가장 효과적인 프롬프트를 선택하도록 해. 기본 프롬프트와 비교했을 때, 우리 MoPE 기반 융합 방법은 더 나은 표현력을 보여주고, 훈련 데이터와 전체 훈련 가능한 파라미터 수에 더 잘 확장돼. 

또한 전문가 라우팅을 위한 정규화 항을 조사했는데, 이게 훈련 중에 전문가의 전문화를 유도해서 해석 가능성이 있는 소프트 프롬프트를 가능하게 해. 여섯 개의 멀티모달 데이터셋에서 네 가지 모달리티를 아우르는 광범위한 실험을 통해, 우리의 방법이 프롬프트 융합에서 최첨단 결과를 달성했음을 보여줬어. 심지어 파인튜닝의 성능과 맞먹거나 그 이상을 기록하면서도 훈련 가능한 파라미터는 겨우 0.8%밖에 필요하지 않아. 코드도 공개할 예정이야: 이 https URL.

================================================================================

URL:
https://arxiv.org/pdf/2404.05102.pdf

Title: LHU-Net: A Light Hybrid U-Net for Cost-Efficient, High-Performance Volumetric Medical Image Segmentation

Original Abstract:
The rise of Transformer architectures has revolutionized medical image segmentation, leading to hybrid models that combine Convolutional Neural Networks (CNNs) and Transformers for enhanced accuracy. However, these models often suffer from increased complexity and overlook the interplay between spatial and channel features, which is vital for segmentation precision. We introduce LHU-Net, a streamlined Hybrid U-Net for volumetric medical image segmentation, designed to first analyze spatial and then channel features for effective feature extraction. Tested on five benchmark datasets (Synapse, LA, Pancreas, ACDC, BRaTS 2018), LHU-Net demonstrated superior efficiency and accuracy, notably achieving a 92.66 Dice score on ACDC with 85\% fewer parameters and a quarter of the computational demand compared to leading models. This performance, achieved without pre-training, extra data, or model ensembles, sets new benchmarks for computational efficiency and accuracy in segmentation, using under 11 million parameters. This achievement highlights that balancing computational efficiency with high accuracy in medical image segmentation is feasible. Our implementation of LHU-Net is freely accessible to the research community on GitHub (this https URL).

Translated Abstract:
Transformer 아키텍처의 발전은 의료 이미지 분할에 큰 변화를 가져왔고, CNN(합성곱 신경망)과 Transformer를 결합한 하이브리드 모델들이 더 높은 정확도를 보여주고 있어. 하지만 이런 모델들은 복잡성이 증가하고, 공간적 특성과 채널 특성 간의 상호작용을 간과하는 문제가 있어. 이 두 가지는 분할의 정밀도를 높이는 데 매우 중요해.

우리는 LHU-Net이라는 간소화된 하이브리드 U-Net을 소개해. 이 모델은 부피가 큰 의료 이미지를 분할하기 위해 먼저 공간적 특성을 분석하고, 그 다음에 채널 특성을 분석해서 효과적으로 특성을 추출할 수 있도록 설계됐어.

다섯 개의 벤치마크 데이터셋(Synapse, LA, Pancreas, ACDC, BRaTS 2018)에서 테스트한 결과, LHU-Net은 효율성과 정확성에서 뛰어난 성능을 보였어. 특히 ACDC에서 92.66의 Dice 점수를 기록했는데, 이는 기존 모델들보다 85% 적은 파라미터와 4배 적은 계산량으로 달성한 거야. 이 성능은 사전 훈련이나 추가 데이터, 모델 앙상블 없이도 가능했어. 

이로 인해 의료 이미지 분할에서 계산 효율성과 높은 정확성을 동시에 달성하는 것이 가능하다는 새로운 기준을 제시했어. LHU-Net의 구현은 GitHub에서 연구 커뮤니티에 무료로 제공되고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2404.09226.pdf

Title: Breast Cancer Image Classification Method Based on Deep Transfer Learning

Original Abstract:
To address the issues of limited samples, time-consuming feature design, and low accuracy in detection and classification of breast cancer pathological images, a breast cancer image classification model algorithm combining deep learning and transfer learning is proposed. This algorithm is based on the DenseNet structure of deep neural networks, and constructs a network model by introducing attention mechanisms, and trains the enhanced dataset using multi-level transfer learning. Experimental results demonstrate that the algorithm achieves an efficiency of over 84.0\% in the test set, with a significantly improved classification accuracy compared to previous models, making it applicable to medical breast cancer detection tasks.

Translated Abstract:
유방암 병리 이미지의 샘플이 적고, 특징 설계가 시간이 많이 걸리며, 탐지와 분류의 정확도가 낮은 문제를 해결하기 위해, 딥러닝과 전이학습을 결합한 유방암 이미지 분류 모델 알고리즘을 제안해. 

이 알고리즘은 딥 뉴럴 네트워크의 DenseNet 구조를 기반으로 하고, 주의 메커니즘을 도입해 네트워크 모델을 구성해. 그리고 다중 수준의 전이학습을 사용해 강화된 데이터셋을 훈련시켜. 

실험 결과, 이 알고리즘은 테스트 세트에서 84.0% 이상의 효율성을 달성했고, 이전 모델들에 비해 분류 정확도가 크게 향상된 걸 보여줘. 그래서 의료 유방암 탐지 작업에 적용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2405.01205.pdf

Title: Error-Driven Uncertainty Aware Training

Original Abstract:
Neural networks are often overconfident about their predictions, which undermines their reliability and trustworthiness. In this work, we present a novel technique, named Error-Driven Uncertainty Aware Training (EUAT), which aims to enhance the ability of neural classifiers to estimate their uncertainty correctly, namely to be highly uncertain when they output inaccurate predictions and low uncertain when their output is accurate. The EUAT approach operates during the model's training phase by selectively employing two loss functions depending on whether the training examples are correctly or incorrectly predicted by the model. This allows for pursuing the twofold goal of i) minimizing model uncertainty for correctly predicted inputs and ii) maximizing uncertainty for mispredicted inputs, while preserving the model's misprediction rate. We evaluate EUAT using diverse neural models and datasets in the image recognition domains considering both non-adversarial and adversarial settings. The results show that EUAT outperforms existing approaches for uncertainty estimation (including other uncertainty-aware training techniques, calibration, ensembles, and DEUP) by providing uncertainty estimates that not only have higher quality when evaluated via statistical metrics (e.g., correlation with residuals) but also when employed to build binary classifiers that decide whether the model's output can be trusted or not and under distributional data shifts.

Translated Abstract:
신경망은 예측에 대해 과신하는 경우가 많아서 신뢰성과 믿음성이 떨어지는 문제가 있어. 이 연구에서는 '오류 기반 불확실성 인식 훈련' (EUAT)이라는 새로운 기법을 소개해. 이 기법은 신경 분류기가 자신의 불확실성을 제대로 추정할 수 있도록 도와주는 게 목표야. 즉, 잘못된 예측을 할 때는 불확실성이 높고, 정확한 예측을 할 때는 불확실성이 낮아야 해.

EUAT는 모델 훈련 단계에서 작동해. 훈련 예제가 모델에 의해 정확하게 예측되는지 아니면 잘못 예측되는지에 따라 두 가지 손실 함수를 선택적으로 사용해. 이 방법은 i) 정확하게 예측된 입력에 대해 모델의 불확실성을 최소화하고, ii) 잘못 예측된 입력에 대해 불확실성을 최대화하면서, 모델의 잘못된 예측 비율을 유지하는 두 가지 목표를 추구할 수 있어.

우리는 다양한 신경망 모델과 데이터셋을 사용해서 EUAT를 평가했어. 이미지 인식 분야에서 비적대적 및 적대적 환경 모두를 고려했지. 결과적으로 EUAT는 기존의 불확실성 추정 방법들보다 더 나은 성능을 보여줬어. 통계적 지표(예: 잔차와의 상관관계)를 통해 평가했을 때 더 높은 품질의 불확실성 추정치를 제공하고, 모델 출력의 신뢰성을 판단하는 이진 분류기를 만드는 데에도 효과적이었어. 데이터 분포가 변화할 때에도 잘 작동했어.

================================================================================

URL:
https://arxiv.org/pdf/2406.03087.pdf

Title: Lossless Image Compression Using Multi-level Dictionaries: Binary Images

Original Abstract:
Lossless image compression is required in various applications to reduce storage or transmission costs of images, while requiring the reconstructed images to have zero information loss compared to the original. Existing lossless image compression methods either have simple design but poor compression performance, or complex design, better performance, but with no performance guarantees. In our endeavor to develop a lossless image compression method with low complexity and guaranteed performance, we argue that compressibility of a color image is essentially derived from the patterns in its spatial structure, intensity variations, and color variations. Thus, we divide the overall design of a lossless image compression scheme into three parts that exploit corresponding redundancies. We further argue that the binarized version of an image captures its fundamental spatial structure. In this first part of our work, we propose a scheme for lossless compression of binary images.
The proposed scheme first learns dictionaries of $16\times16$, $8\times8$, $4\times4$, and $2\times 2$ square pixel patterns from various datasets of binary images. It then uses these dictionaries to encode binary images. These dictionaries have various interesting properties that are further exploited to construct an efficient and scalable scheme. Our preliminary results show that the proposed scheme consistently outperforms existing conventional and learning based lossless compression approaches, and provides, on average, as much as $1.5\times$ better performance than a common general purpose lossless compression scheme (WebP), more than $3\times$ better performance than a state of the art learning based scheme, and better performance than a specialized scheme for binary image compression (JBIG2).

Translated Abstract:
무손실 이미지 압축은 이미지를 저장하거나 전송하는 비용을 줄이기 위해 필요해. 이때 재구성된 이미지는 원본 이미지와 비교해 정보 손실이 없어야 해. 기존의 무손실 이미지 압축 방법들은 디자인이 간단하지만 압축 성능이 좋지 않거나, 복잡한 디자인으로 성능은 좋지만 성능 보장이 없는 경우가 많아.

우리는 낮은 복잡성과 보장된 성능을 가진 무손실 이미지 압축 방법을 개발하기 위해 노력하고 있어. 우리는 색상 이미지의 압축 가능성이 주로 공간 구조의 패턴, 강도 변화, 색상 변화에서 비롯된다고 주장해. 그래서 전체 무손실 이미지 압축 설계를 세 가지 부분으로 나누어 각각의 중복성을 활용해.

또한, 이미지를 이진화한 버전이 기본적인 공간 구조를 잘 나타낸다고 생각해. 이 작업의 첫 번째 부분에서는 이진 이미지의 무손실 압축을 위한 방식을 제안해.

제안한 방식은 먼저 다양한 이진 이미지 데이터셋에서 $16\times16$, $8\times8$, $4\times4$, $2\times2$ 크기의 픽셀 패턴 사전을 배워. 그런 다음 이 사전을 사용해 이진 이미지를 인코딩해. 이 사전들은 여러 흥미로운 특성을 가지고 있어서 효율적이고 확장 가능한 방식을 만드는 데 활용돼. 우리의 초기 결과는 제안한 방식이 기존의 전통적인 압축 방법이나 학습 기반 무손실 압축 방법보다 일관되게 성능이 좋다는 것을 보여줘. 평균적으로 일반적인 무손실 압축 방식(WebP)보다 $1.5\times$ 더 좋은 성능을, 최신 학습 기반 방식보다 $3\times$ 더 좋은 성능을, 이진 이미지 압축을 위한 전문 방식(JBIG2)보다도 더 나은 성능을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2407.10926.pdf

Title: In-Loop Filtering via Trained Look-Up Tables

Original Abstract:
In-loop filtering (ILF) is a key technology for removing the artifacts in image/video coding standards. Recently, neural network-based in-loop filtering methods achieve remarkable coding gains beyond the capability of advanced video coding standards, which becomes a powerful coding tool candidate for future video coding standards. However, the utilization of deep neural networks brings heavy time and computational complexity, and high demands of high-performance hardware, which is challenging to apply to the general uses of coding scene. To address this limitation, inspired by explorations in image restoration, we propose an efficient and practical in-loop filtering scheme by adopting the Look-up Table (LUT). We train the DNN of in-loop filtering within a fixed filtering reference range, and cache the output values of the DNN into a LUT via traversing all possible inputs. At testing time in the coding process, the filtered pixel is generated by locating input pixels (to-be-filtered pixel with reference pixels) and interpolating cached filtered pixel values. To further enable the large filtering reference range with the limited storage cost of LUT, we introduce the enhanced indexing mechanism in the filtering process, and clipping/finetuning mechanism in the training. The proposed method is implemented into the Versatile Video Coding (VVC) reference software, VTM-11.0. Experimental results show that the ultrafast, very fast, and fast mode of the proposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39% BD-rate reduction, under the all intra (AI) and random access (RA) configurations. Especially, our method has friendly time and computational complexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel, and only 164-1148 KB storage cost for a single model. Our solution may shed light on the journey of practical neural network-based coding tool evolution.

Translated Abstract:
인-루프 필터링(ILF)은 이미지/비디오 코딩 표준에서 아티팩트를 제거하는 데 중요한 기술이야. 최근에 신경망 기반의 인-루프 필터링 방법이 고급 비디오 코딩 표준의 능력을 넘어서는 놀라운 코딩 성능을 보여주면서, 미래 비디오 코딩 표준을 위한 강력한 코딩 도구 후보로 떠오르고 있어. 하지만, 딥 뉴럴 네트워크를 사용하는 건 시간과 계산 복잡도가 크고, 고성능 하드웨어에 대한 요구도 높아져서 일반적인 코딩 장면에 적용하기가 어려워.

이런 제한을 해결하기 위해, 이미지 복원에서 영감을 받아 Look-up Table (LUT)을 이용한 효율적이고 실용적인 인-루프 필터링 방안을 제안해. 우리는 고정된 필터링 참조 범위 내에서 인-루프 필터링의 DNN을 학습하고, 가능한 모든 입력을 탐색하면서 DNN의 출력 값을 LUT에 캐시해. 코딩 과정에서 테스트할 때는, 필터링할 픽셀(참조 픽셀과 함께 필터링될 픽셀)의 위치를 찾고 캐시된 필터링된 픽셀 값을 보간해서 필터링된 픽셀을 생성해.

LUT의 제한된 저장 비용으로 더 큰 필터링 참조 범위를 가능하게 하기 위해, 우리는 필터링 과정에서 향상된 인덱싱 메커니즘과 훈련 과정에서 클리핑/미세 조정 메커니즘을 도입했어. 제안된 방법은 다목적 비디오 코딩(VVC) 기준 소프트웨어인 VTM-11.0에 구현되었고, 실험 결과에 따르면 제안된 방법의 초고속, 매우 빠름, 빠름 모드가 각각 평균 0.13%/0.34%/0.51%와 0.10%/0.27%/0.39% BD-rate 감소를 달성했어, 모든 인트라(AI) 및 랜덤 접근(RA) 설정에서 말이지. 특히, 우리 방법은 시간과 계산 복잡도가 친화적이고, 각각 101%/102%-104%/108%의 시간 증가와 0.13-0.93 kMACs/픽셀로, 단일 모델에 대해 저장 비용이 164-1148 KB밖에 안 들어. 우리 솔루션은 실용적인 신경망 기반 코딩 도구의 발전에 도움을 줄 수 있을 거야.

================================================================================

URL:
https://arxiv.org/pdf/2408.07947.pdf

Title: Conditional Brownian Bridge Diffusion Model for VHR SAR to Optical Image Translation

Original Abstract:
Synthetic Aperture Radar (SAR) imaging technology provides the unique advantage of being able to collect data regardless of weather conditions and time. However, SAR images exhibit complex backscatter patterns and speckle noise, which necessitate expertise for interpretation. Research on translating SAR images into optical-like representations has been conducted to aid the interpretation of SAR data. Nevertheless, existing studies have predominantly utilized low-resolution satellite imagery datasets and have largely been based on Generative Adversarial Network (GAN) which are known for their training instability and low fidelity. To overcome these limitations of low-resolution data usage and GAN-based approaches, this paper introduces a conditional image-to-image translation approach based on Brownian Bridge Diffusion Model (BBDM). We conducted comprehensive experiments on the MSAW dataset, a paired SAR and optical images collection of 0.5m Very-High-Resolution (VHR). The experimental results indicate that our method surpasses both the Conditional Diffusion Models (CDMs) and the GAN-based models in diverse perceptual quality metrics.

Translated Abstract:
합성 개구 레이더(SAR) 이미징 기술은 날씨나 시간에 관계없이 데이터를 수집할 수 있는 특별한 장점이 있어. 하지만 SAR 이미지는 복잡한 후방산란 패턴과 스펙클 노이즈를 보여서 해석하는 데 전문 지식이 필요해. SAR 데이터를 이해하는 데 도움을 주기 위해 SAR 이미지를 광학처럼 표현하는 연구가 진행됐어. 

하지만 기존 연구들은 주로 저해상도 위성 이미지 데이터셋을 사용했고, 훈련의 불안정성과 낮은 신뢰도로 알려진 생성적 적대 신경망(GAN)에 기반한 경우가 많았어. 이런 저해상도 데이터 사용과 GAN 기반 접근 방식의 한계를 극복하기 위해, 이 논문에서는 브라운 운동 다리 확산 모델(BBDM)을 기반으로 한 조건부 이미지-이미지 변환 접근 방식을 소개해. 

우리는 0.5m 매우 높은 해상도(VHR)의 쌍 SAR 및 광학 이미지 컬렉션인 MSAW 데이터셋에서 포괄적인 실험을 진행했어. 실험 결과, 우리의 방법이 조건부 확산 모델(CDM)과 GAN 기반 모델 모두를 다양한 지각 품질 지표에서 능가한다는 것을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.00143.pdf

Title: Semantic-Guided Multimodal Sentiment Decoding with Adversarial Temporal-Invariant Learning

Original Abstract:
Multimodal sentiment analysis aims to learn representations from different modalities to identify human emotions. However, existing works often neglect the frame-level redundancy inherent in continuous time series, resulting in incomplete modality representations with noise. To address this issue, we propose temporal-invariant learning for the first time, which constrains the distributional variations over time steps to effectively capture long-term temporal dynamics, thus enhancing the quality of the representations and the robustness of the model. To fully exploit the rich semantic information in textual knowledge, we propose a semantic-guided fusion module. By evaluating the correlations between different modalities, this module facilitates cross-modal interactions gated by modality-invariant representations. Furthermore, we introduce a modality discriminator to disentangle modality-invariant and modality-specific subspaces. Experimental results on two public datasets demonstrate the superiority of our model. Our code is available at this https URL.

Translated Abstract:
멀티모달 감정 분석은 여러 다른 방식에서 사람의 감정을 알아내기 위해 표현을 배우는 거야. 그런데 기존 연구들은 연속적인 시간 시퀀스에서 발생하는 프레임 수준의 중복성을 종종 무시하는데, 이 때문에 불완전한 표현과 노이즈가 생기는 문제가 있어.

이 문제를 해결하기 위해 우리는 처음으로 시간 불변 학습(temporal-invariant learning)을 제안해. 이 방법은 시간 단계에 따른 분포 변화를 제한해서 장기적인 시간 동력을 효과적으로 포착할 수 있도록 해. 이렇게 하면 표현의 질과 모델의 강건성이 향상돼.

또한, 텍스트 지식의 풍부한 의미 정보를 완전히 활용하기 위해 의미 기반 융합 모듈(semantic-guided fusion module)을 제안해. 이 모듈은 서로 다른 방식 간의 상관관계를 평가해서 방식에 무관한 표현을 통해 교차 모달 상호작용을 촉진해.

그리고 우리는 모달리티 판별기(modality discriminator)를 도입해서 모달리티에 무관한 부분과 모달리티 특정 부분을 분리해. 두 개의 공공 데이터셋에서 실험한 결과, 우리의 모델이 뛰어난 성능을 보여줬어. 우리의 코드는 이 링크에서 확인할 수 있어.

================================================================================

