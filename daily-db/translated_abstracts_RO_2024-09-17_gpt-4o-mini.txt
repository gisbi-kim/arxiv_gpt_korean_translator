URL:
https://arxiv.org/pdf/2409.09140.pdf

Title: ResPilot: Teleoperated Finger Gaiting via Gaussian Process Residual Learning

Original Abstract:
Dexterous robot hand teleoperation allows for long-range transfer of human manipulation expertise, and could simultaneously provide a way for humans to teach these skills to robots. However, current methods struggle to reproduce the functional workspace of the human hand, often limiting them to simple grasping tasks. We present a novel method for finger-gaited manipulation with multi-fingered robot hands. Our method provides the operator enhanced flexibility in making contacts by expanding the reachable workspace of the robot hand through residual Gaussian Process learning. We also assist the operator in maintaining stable contacts with the object by allowing them to constrain fingertips of the hand to move in concert. Extensive quantitative evaluations show that our method significantly increases the reachable workspace of the robot hand and enables the completion of novel dexterous finger gaiting tasks. Project website: this http URL

Translated Abstract:
정교한 로봇 손의 원격 조작은 인간의 조작 기술을 멀리서 전달할 수 있게 해주고, 동시에 사람들이 이 기술을 로봇에게 가르칠 수 있는 방법을 제공할 수 있어. 하지만 현재 방법들은 인간 손의 기능적 작업 공간을 재현하는 데 어려움을 겪고 있어서, 주로 간단한 물체 잡기 작업에만 제한되고 있어. 

우리는 다중 손가락 로봇 손을 이용한 새로운 손가락 보행 조작 방법을 제시해. 이 방법은 로봇 손의 도달 가능한 작업 공간을 넓혀줘서 조작자가 더 유연하게 접촉할 수 있게 해줘. 또한, 조작자가 손가락을 함께 움직일 수 있도록 제약을 두게 해서 물체와의 안정적인 접촉을 유지할 수 있도록 도와줘. 

광범위한 정량적 평가 결과, 우리의 방법은 로봇 손의 도달 가능한 작업 공간을 크게 증가시키고, 새로운 정교한 손가락 보행 작업을 완료할 수 있게 해줘. 프로젝트 웹사이트: 이 URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.09164.pdf

Title: Measure Preserving Flows for Ergodic Search in Convoluted Environments

Original Abstract:
Autonomous robotic search has important applications in robotics, such as the search for signs of life after a disaster. When \emph{a priori} information is available, for example in the form of a distribution, a planner can use that distribution to guide the search. Ergodic search is one method that uses the information distribution to generate a trajectory that minimizes the ergodic metric, in that it encourages the robot to spend more time in regions with high information and proportionally less time in the remaining regions. Unfortunately, prior works in ergodic search do not perform well in complex environments with obstacles such as a building's interior or a maze. To address this, our work presents a modified ergodic metric using the Laplace-Beltrami eigenfunctions to capture map geometry and obstacle locations within the ergodic metric. Further, we introduce an approach to generate trajectories that minimize the ergodic metric while guaranteeing obstacle avoidance using measure-preserving vector fields. Finally, we leverage the divergence-free nature of these vector fields to generate collision-free trajectories for multiple agents. We demonstrate our approach via simulations with single and multi-agent systems on maps representing interior hallways and long corridors with non-uniform information distribution. In particular, we illustrate the generation of feasible trajectories in complex environments where prior methods fail.

Translated Abstract:
자율 로봇 검색은 재난 후 생명체의 흔적을 찾는 것처럼 로봇 공학에서 중요한 응용 분야야. 만약 \emph{a priori} 정보, 예를 들어 분포 형태의 정보가 있다면, 계획자는 그 분포를 이용해서 검색을 안내할 수 있어. 에르고딕 검색은 정보 분포를 이용해서 에르고딕 메트릭을 최소화하는 경로를 만드는 방법 중 하나야. 이 방법은 로봇이 정보가 많은 지역에서 더 많은 시간을 보내고, 나머지 지역에서는 비례적으로 적은 시간을 보내도록 유도해.

하지만 안타깝게도 기존의 에르고딕 검색은 건물 내부나 미로 같은 장애물이 있는 복잡한 환경에서는 잘 작동하지 않아. 그래서 이 연구에서는 라플라스-벨트라미 고유함수를 사용해서 맵의 기하학과 장애물 위치를 에르고딕 메트릭에 반영한 수정된 에르고딕 메트릭을 제안해. 또한, 측정 보존 벡터 필드를 사용해서 장애물을 피하면서 에르고딕 메트릭을 최소화하는 경로를 생성하는 방법도 소개해.

마지막으로, 이 벡터 필드의 발산이 없는 특성을 이용해서 여러 대의 로봇이 충돌 없이 경로를 생성할 수 있도록 했어. 우리는 단일 및 다중 에이전트 시스템을 사용한 시뮬레이션을 통해 내부 복도와 비균일 정보 분포를 가진 긴 복도를 나타내는 맵에서 우리의 접근 방식을 보여줬어. 특히, 이전 방법들이 실패하는 복잡한 환경에서 실행 가능한 경로를 생성하는 과정을 설명해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09203.pdf

Title: Pinto: A latched spring actuated robot for jumping and perching

Original Abstract:
Arboreal environments challenge current robots but are deftly traversed by many familiar animal locomotors such as squirrels. We present a small, 450 g robot "Pinto" developed for tree-jumping, a behavior seen in squirrels but rarely in legged robots: jumping from the ground onto a vertical tree trunk. We develop a powerful and lightweight latched series-elastic actuator using a twisted string and carbon fiber springs. We consider the effects of scaling down conventional quadrupeds and experimentally show how storing energy in a parallel-elastic fashion using a latch increases jump energy compared to series-elastic or springless strategies. By switching between series and parallel-elastic modes with our latched 5-bar leg mechanism, Pinto executes energetic jumps as well as maintains continuous control during shorter bounding motions. We also develop sprung 2-DoF arms equipped with spined grippers to grasp tree bark for high-speed perching following a jump.

Translated Abstract:
현재 로봇들은 나무 환경에서 어려움을 겪고 있지만, 다람쥐 같은 많은 동물들은 쉽게 그곳을 지나갑니다. 우리는 "Pinto"라는 작은 로봇을 개발했는데, 이 로봇은 나무 줄기로 점프하는 행동을 할 수 있어요. 이 행동은 다람쥐에서는 자주 볼 수 있지만, 다리로 움직이는 로봇에서는 드물어요. 

Pinto는 450g의 무게를 가지고 있고, 비틀린 끈과 탄소 섬유 스프링을 사용해 강력하고 가벼운 잠금식 시리즈 탄성 액추에이터를 만들었어요. 전통적인 4족 보행 로봇을 작게 줄이는 것의 영향을 고려하면서, 우리는 에너지를 병렬 탄성 방식으로 저장하는 것이 시리즈 탄성이나 스프링이 없는 방법보다 점프 에너지를 증가시킨다는 것을 실험적으로 보여줬어요. 

Pinto는 잠금식 5바 킬레그 메커니즘을 사용해 시리즈와 병렬 탄성 모드를 전환하며 에너지가 넘치는 점프를 실행할 수 있고, 짧은 바운딩 동작 동안 지속적인 제어도 가능해요. 또, 점프 후 나무 껍질을 잡기 위해 스파인 그리퍼가 장착된 스프링 2-자유도 팔도 개발했어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.09224.pdf

Title: Optimal Control Approach for Gait Transition with Riemannian Splines

Original Abstract:
Robotic locomotion often relies on sequenced gaits to efficiently convert control input into desired motion. Despite extensive studies on gait optimization, achieving smooth and efficient gait transitions remains challenging. In this paper, we propose a general solver based on geometric optimal control methods, leveraging insights from previous works on gait efficiency. Building upon our previous work, we express the effort to execute the trajectory as distinct geometric objects, transforming the optimization problems into boundary value problems. To validate our approach, we generate gait transition trajectories for three-link swimmers across various fluid environments. This work provides insights into optimal trajectory geometries and mechanical considerations for robotic locomotion.

Translated Abstract:
로봇의 이동은 보통 순서가 있는 걸음걸이를 사용해서 제어 입력을 원하는 움직임으로 효율적으로 변환해. 걸음걸이 최적화에 대해 많은 연구가 있었지만, 부드럽고 효율적인 걸음걸이 전환을 이루는 건 여전히 어려운 일이지. 

이 논문에서는 기하학적 최적 제어 방법을 기반으로 한 일반적인 해결책을 제안해. 이전 연구에서 얻은 걸음걸이 효율에 대한 통찰을 활용하고 있어. 우리의 이전 연구를 바탕으로, 경로를 실행하는 데 필요한 노력을 서로 다른 기하학적 객체로 표현해서 최적화 문제를 경계값 문제로 변환했어. 

우리의 방법을 검증하기 위해, 다양한 유체 환경에서 세 개의 링크로 이루어진 수영 로봇의 걸음걸이 전환 경로를 생성했어. 이 연구는 로봇 이동을 위한 최적 경로 기하학과 기계적 고려 사항에 대한 통찰을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09266.pdf

Title: TransformerMPC: Accelerating Model Predictive Control via Transformers

Original Abstract:
In this paper, we address the problem of reducing the computational burden of Model Predictive Control (MPC) for real-time robotic applications. We propose TransformerMPC, a method that enhances the computational efficiency of MPC algorithms by leveraging the attention mechanism in transformers for both online constraint removal and better warm start initialization. Specifically, TransformerMPC accelerates the computation of optimal control inputs by selecting only the active constraints to be included in the MPC problem, while simultaneously providing a warm start to the optimization process. This approach ensures that the original constraints are satisfied at optimality. TransformerMPC is designed to be seamlessly integrated with any MPC solver, irrespective of its implementation. To guarantee constraint satisfaction after removing inactive constraints, we perform an offline verification to ensure that the optimal control inputs generated by the MPC solver meet all constraints. The effectiveness of TransformerMPC is demonstrated through extensive numerical simulations on complex robotic systems, achieving up to 35x improvement in runtime without any loss in performance.

Translated Abstract:
이 논문에서는 실시간 로봇 애플리케이션을 위한 모델 예측 제어(MPC)의 계산 부담을 줄이는 문제를 다뤄요. 우리는 TransformerMPC라는 방법을 제안하는데, 이 방법은 트랜스포머의 주의(attention) 메커니즘을 활용해서 MPC 알고리즘의 계산 효율성을 높여요. 이 방식은 온라인 제약 조건 제거와 더 나은 초기화(warm start)를 동시에 개선해줘요.

구체적으로, TransformerMPC는 MPC 문제에 포함할 활성 제약 조건만 선택해서 최적 제어 입력의 계산을 가속화해요. 동시에 최적화 과정에 초기화를 제공하죠. 이 접근 방식은 원래의 제약 조건이 최적 상태에서 만족되도록 보장해요. TransformerMPC는 어떤 MPC 솔버와도 쉽게 통합될 수 있도록 설계되었어요.

비활성 제약 조건을 제거한 후에도 제약 조건 만족을 보장하기 위해, 우리는 오프라인 검증을 수행해요. 이 검증을 통해 MPC 솔버가 생성한 최적 제어 입력이 모든 제약 조건을 충족하는지 확인해요. TransformerMPC의 효과는 복잡한 로봇 시스템에 대한 광범위한 수치 시뮬레이션을 통해 입증되었고, 성능 저하 없이 최대 35배의 런타임 향상을 달성했어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.09276.pdf

Title: Visuo-Tactile Zero-Shot Object Recognition with Vision-Language Model

Original Abstract:
Tactile perception is vital, especially when distinguishing visually similar objects. We propose an approach to incorporate tactile data into a Vision-Language Model (VLM) for visuo-tactile zero-shot object recognition. Our approach leverages the zero-shot capability of VLMs to infer tactile properties from the names of tactilely similar objects. The proposed method translates tactile data into a textual description solely by annotating object names for each tactile sequence during training, making it adaptable to various contexts with low training costs. The proposed method was evaluated on the FoodReplica and Cube datasets, demonstrating its effectiveness in recognizing objects that are difficult to distinguish by vision alone.

Translated Abstract:
촉각 인식은 정말 중요해, 특히 시각적으로 비슷한 물체를 구분할 때 말이야. 우리는 촉각 데이터를 비전-언어 모델(VLM)에 통합하는 방법을 제안해. 이 방법은 비전-촉각 제로샷 객체 인식을 위해서야.

우리의 접근법은 VLM의 제로샷 기능을 활용해서 촉각적으로 비슷한 물체의 이름으로부터 촉각 속성을 추론해. 이 방법은 훈련 중에 각 촉각 시퀀스에 대해 물체 이름을 주석 처리하는 것만으로 촉각 데이터를 텍스트 설명으로 변환해, 다양한 상황에 적응할 수 있고 훈련 비용도 낮아.

우리가 제안한 방법은 FoodReplica와 Cube 데이터셋에서 평가되었고, 시각적으로 구분하기 어려운 물체를 인식하는 데 효과적임을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09287.pdf

Title: Panoramic Direct LiDAR-assisted Visual Odometry

Original Abstract:
Enhancing visual odometry by exploiting sparse depth measurements from LiDAR is a promising solution for improving tracking accuracy of an odometry. Most existing works utilize a monocular pinhole camera, yet could suffer from poor robustness due to less available information from limited field-of-view (FOV). This paper proposes a panoramic direct LiDAR-assisted visual odometry, which fully associates the 360-degree FOV LiDAR points with the 360-degree FOV panoramic image datas. 360-degree FOV panoramic images can provide more available information, which can compensate inaccurate pose estimation caused by insufficient texture or motion blur from a single view. In addition to constraints between a specific view at different times, constraints can also be built between different views at the same moment. Experimental results on public datasets demonstrate the benefit of large FOV of our panoramic direct LiDAR-assisted visual odometry to state-of-the-art approaches.

Translated Abstract:
LiDAR의 희소 깊이 측정을 활용해서 시각 항법을 향상시키는 건, 항법의 추적 정확도를 높이는 데 좋은 방법이야. 대부분의 기존 연구는 단안 카메라를 사용하지만, 제한된 시야각(FOV) 때문에 정보가 적어서 강인성이 떨어질 수 있어. 

이 논문은 360도 FOV를 가진 LiDAR 포인트와 360도 FOV 파노라마 이미지 데이터를 완전히 연결하는 파노라마 직접 LiDAR 보조 시각 항법을 제안해. 360도 FOV 파노라마 이미지는 더 많은 정보를 제공할 수 있어서, 단일 시점에서의 부족한 질감이나 모션 블러로 인한 부정확한 자세 추정을 보완해줘. 

또한, 다른 시간의 특정 뷰 간의 제약 외에도, 같은 순간의 서로 다른 뷰 간에도 제약을 만들 수 있어. 실험 결과는 공공 데이터셋에서 우리 파노라마 직접 LiDAR 보조 시각 항법의 넓은 시야각이 최신 기술보다 더 이점이 있음을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09295.pdf

Title: GEVO: Memory-Efficient Monocular Visual Odometry Using Gaussians

Original Abstract:
Constructing a high-fidelity representation of the 3D scene using a monocular camera can enable a wide range of applications on mobile devices, such as micro-robots, smartphones, and AR/VR headsets. On these devices, memory is often limited in capacity and its access often dominates the consumption of compute energy. Although Gaussian Splatting (GS) allows for high-fidelity reconstruction of 3D scenes, current GS-based SLAM is not memory efficient as a large number of past images is stored to retrain Gaussians for reducing catastrophic forgetting. These images often require two-orders-of-magnitude higher memory than the map itself and thus dominate the total memory usage. In this work, we present GEVO, a GS-based monocular SLAM framework that achieves comparable fidelity as prior methods by rendering (instead of storing) them from the existing map. Novel Gaussian initialization and optimization techniques are proposed to remove artifacts from the map and delay the degradation of the rendered images over time. Across a variety of environments, GEVO achieves comparable map fidelity while reducing the memory overhead to around 58 MBs, which is up to 94x lower than prior works.

Translated Abstract:
모노큘러 카메라를 이용해 3D 장면의 고충실도 표현을 만드는 건 마이크로 로봇, 스마트폰, AR/VR 헤드셋 같은 모바일 기기에서 다양한 응용 프로그램에 활용될 수 있어. 하지만 이런 기기들은 메모리 용량이 제한적이고, 메모리 접근이 컴퓨팅 에너지를 많이 소모하는 문제가 있어. 

가우시안 스플래팅(GS)은 3D 장면을 고충실도로 재구성할 수 있게 해주지만, 현재의 GS 기반 SLAM은 많은 과거 이미지를 저장해야 해서 메모리 효율성이 떨어져. 이 이미지들은 맵 자체보다 메모리를 100배 이상 더 소모할 수 있어서 전체 메모리 사용량에 큰 영향을 미쳐. 

이번 연구에서는 GEVO라는 GS 기반 모노큘러 SLAM 프레임워크를 제안해. GEVO는 기존 맵에서 이미지를 저장하는 대신 렌더링하여 이전 방법들과 비슷한 충실도를 달성해. 새로운 가우시안 초기화 및 최적화 기술을 통해 맵에서 아티팩트를 제거하고 렌더링된 이미지의 품질 저하를 늦출 수 있어. 다양한 환경에서 GEVO는 맵의 충실도를 유지하면서 메모리 사용량을 약 58MB로 줄였고, 이는 이전 연구들보다 최대 94배 낮은 수치야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09354.pdf

Title: PeriGuru: A Peripheral Robotic Mobile App Operation Assistant based on GUI Image Understanding and Prompting with LLM

Original Abstract:
Smartphones have significantly enhanced our daily learning, communication, and entertainment, becoming an essential component of modern life. However, certain populations, including the elderly and individuals with disabilities, encounter challenges in utilizing smartphones, thus necessitating mobile app operation assistants, a.k.a. mobile app agent. With considerations for privacy, permissions, and cross-platform compatibility issues, we endeavor to devise and develop PeriGuru in this work, a peripheral robotic mobile app operation assistant based on GUI image understanding and prompting with Large Language Model (LLM). PeriGuru leverages a suite of computer vision techniques to analyze GUI screenshot images and employs LLM to inform action decisions, which are then executed by robotic arms. PeriGuru achieves a success rate of 81.94% on the test task set, which surpasses by more than double the method without PeriGuru's GUI image interpreting and prompting design. Our code is available on this https URL.

Translated Abstract:
스마트폰은 우리의 일상적인 학습, 소통, 그리고 오락을 크게 향상시켜 주면서 현대 생활의 필수 요소가 되었어. 하지만 노인이나 장애인 같은 특정 집단은 스마트폰 사용에 어려움을 겪고 있어서, 모바일 앱을 도와주는 보조 도구가 필요해. 그래서 우리는 이 연구에서 PeriGuru라는, GUI 이미지 이해와 대형 언어 모델(LLM)을 기반으로 한 주변 로봇 모바일 앱 운영 보조 도구를 개발하기 위해 노력했어.

PeriGuru는 컴퓨터 비전 기술을 활용해 GUI 스크린샷 이미지를 분석하고, LLM을 사용해 행동 결정을 내리는 방식이야. 그런 다음 로봇 팔이 그 행동을 수행해. PeriGuru는 테스트 과제에서 81.94%의 성공률을 기록했는데, 이는 PeriGuru의 GUI 이미지 해석과 프롬프트 디자인이 없는 방법보다 두 배 이상 높은 수치야. 우리의 코드는 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09410.pdf

Title: Distributed Invariant Kalman Filter for Object-level Multi-robot Pose SLAM

Original Abstract:
Cooperative localization and target tracking are essential for multi-robot systems to implement high-level tasks. To this end, we propose a distributed invariant Kalman filter based on covariance intersection for effective multi-robot pose estimation. The paper utilizes the object-level measurement models, which have condensed information further reducing the communication burden. Besides, by modeling states on special Lie groups, the better linearity and consistency of the invariant Kalman filter structure can be stressed. We also use a combination of CI and KF to avoid overly confident or conservative estimates in multi-robot systems with intricate and unknown correlations, and some level of robot degradation is acceptable through multi-robot collaboration. The simulation and real data experiment validate the practicability and superiority of the proposed algorithm.

Translated Abstract:
협력적 위치 추정과 목표 추적은 다중 로봇 시스템이 고급 작업을 수행하는 데 꼭 필요해. 그래서 우리는 효과적인 다중 로봇 자세 추정을 위해 공분산 교차 기반의 분산 불변 칼만 필터를 제안해.

이 논문에서는 객체 수준의 측정 모델을 활용하는데, 이 모델은 정보를 압축해서 통신 부담을 더 줄여줘. 그리고 특별한 리 군에 상태를 모델링함으로써 불변 칼만 필터 구조의 선형성과 일관성을 더욱 강조할 수 있어.

우리는 복잡하고 미지의 상관관계가 있는 다중 로봇 시스템에서 과도하게 자신감 넘치거나 보수적인 추정을 피하기 위해 CI와 KF의 조합을 사용해. 또한, 다중 로봇 협력을 통해 어느 정도의 로봇 성능 저하는 수용 가능해.

시뮬레이션과 실제 데이터 실험을 통해 제안한 알고리즘의 실용성과 우수성을 검증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09429.pdf

Title: Real-Time Adaptive Industrial Robots: Improving Safety And Comfort In Human-Robot Collaboration

Original Abstract:
Industrial robots become increasingly prevalent, resulting in a growing need for intuitive, comforting human-robot collaboration. We present a user-aware robotic system that adapts to operator behavior in real time while non-intrusively monitoring physiological signals to create a more responsive and empathetic environment. Our prototype dynamically adjusts robot speed and movement patterns while measuring operator pupil dilation and proximity. Our user study compares this adaptive system to a non-adaptive counterpart, and demonstrates that the adaptive system significantly reduces both perceived and physiologically measured cognitive load while enhancing usability. Participants reported increased feelings of comfort, safety, trust, and a stronger sense of collaboration when working with the adaptive robot. This highlights the potential of integrating real-time physiological data into human-robot interaction paradigms. This novel approach creates more intuitive and collaborative industrial environments where robots effectively 'read' and respond to human cognitive states, and we feature all data and code for future use.

Translated Abstract:
산업 로봇이 점점 더 많이 사용되면서, 사람과 로봇이 잘 협력할 수 있는 직관적이고 편안한 환경에 대한 필요성이 커지고 있어. 우리는 사용자 행동에 실시간으로 적응하는 로봇 시스템을 소개해. 이 시스템은 생리 신호를 비침해적으로 모니터링하면서 더 반응적이고 공감할 수 있는 환경을 만들어.

우리 프로토타입은 로봇의 속도와 움직임 패턴을 동적으로 조정하고, 조작자의 동공 확장과 거리도 측정해. 사용자 연구에서는 이 적응형 시스템과 비적응형 시스템을 비교했는데, 적응형 시스템이 인식된 인지 부하와 생리적으로 측정된 인지 부하를 크게 줄이고 사용성을 높인다는 걸 보여줬어. 

참여자들은 적응형 로봇과 작업할 때 편안함, 안전함, 신뢰감, 그리고 협력의 느낌이 증가했다고 보고했어. 이 결과는 실시간 생리 데이터를 인간-로봇 상호작용 방식에 통합할 가능성을 강조해. 이 새로운 접근 방식은 로봇이 사람의 인지 상태를 효과적으로 '읽고' 반응할 수 있는 더 직관적이고 협력적인 산업 환경을 만들어. 모든 데이터와 코드는 향후 사용을 위해 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09435.pdf

Title: Behavior Tree Generation using Large Language Models for Sequential Manipulation Planning with Human Instructions and Feedback

Original Abstract:
In this work, we propose an LLM-based BT generation framework to leverage the strengths of both for sequential manipulation planning. To enable human-robot collaborative task planning and enhance intuitive robot programming by nonexperts, the framework takes human instructions to initiate the generation of action sequences and human feedback to refine BT generation in runtime. All presented methods within the framework are tested on a real robotic assembly example, which uses a gear set model from the Siemens Robot Assembly Challenge. We use a single manipulator with a tool-changing mechanism, a common practice in flexible manufacturing, to facilitate robust grasping of a large variety of objects. Experimental results are evaluated regarding success rate, logical coherence, executability, time consumption, and token consumption. To our knowledge, this is the first human-guided LLM-based BT generation framework that unifies various plausible ways of using LLMs to fully generate BTs that are executable on the real testbed and take into account granular knowledge of tool use.

Translated Abstract:
이 연구에서는 LLM(대형 언어 모델)을 기반으로 한 BT(행동 트리) 생성 프레임워크를 제안해. 이 프레임워크는 순차적인 조작 계획을 위해 LLM과 BT의 장점을 모두 활용할 수 있도록 해.

이 프레임워크는 사람의 지시를 받아서 행동 시퀀스를 생성하고, 사람의 피드백을 통해 BT 생성을 실시간으로 개선해. 그래서 인간-로봇 협업 작업 계획을 가능하게 하고 비전문가도 직관적으로 로봇 프로그래밍을 할 수 있게 해줘.

모든 방법은 실제 로봇 조립 예제에서 테스트했어. 이 예제는 Siemens 로봇 조립 챌린지의 기어 세트 모델을 사용했어. 우리는 도구 교환 메커니즘이 있는 단일 조작기를 사용했는데, 이는 유연한 제조에서 흔히 쓰이는 방식이야. 이렇게 하면 다양한 물체를 강하게 잡을 수 있어.

실험 결과는 성공률, 논리적 일관성, 실행 가능성, 소요 시간, 그리고 토큰 소비 등을 기준으로 평가했어. 우리가 아는 바로는, 이 연구는 인간이 안내하는 LLM 기반의 BT 생성 프레임워크로, LLM을 활용해 실제 테스트베드에서 실행 가능한 BT를 완전히 생성하고 도구 사용에 대한 세부 지식을 고려하는 첫 번째 사례야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09441.pdf

Title: PIP-Loco: A Proprioceptive Infinite Horizon Planning Framework for Quadrupedal Robot Locomotion

Original Abstract:
A core strength of Model Predictive Control (MPC) for quadrupedal locomotion has been its ability to enforce constraints and provide interpretability of the sequence of commands over the horizon. However, despite being able to plan, MPC struggles to scale with task complexity, often failing to achieve robust behavior on rapidly changing surfaces. On the other hand, model-free Reinforcement Learning (RL) methods have outperformed MPC on multiple terrains, showing emergent motions but inherently lack any ability to handle constraints or perform planning. To address these limitations, we propose a framework that integrates proprioceptive planning with RL, allowing for agile and safe locomotion behaviors through the horizon. Inspired by MPC, we incorporate an internal model that includes a velocity estimator and a Dreamer module. During training, the framework learns an expert policy and an internal model that are co-dependent, facilitating exploration for improved locomotion behaviors. During deployment, the Dreamer module solves an infinite-horizon MPC problem, adapting actions and velocity commands to respect the constraints. We validate the robustness of our training framework through ablation studies on internal model components and demonstrate improved robustness to training noise. Finally, we evaluate our approach across multi-terrain scenarios in both simulation and hardware.

Translated Abstract:
모델 예측 제어(MPC)는 네 발 보행 로봇에서 제약 조건을 적용하고 명령 시퀀스를 해석하는 데 강점을 가지고 있어. 하지만 MPC는 복잡한 작업을 처리하는 데 한계가 있어서, 빠르게 변하는 표면에서는 안정적인 행동을 보이지 못하는 경우가 많아. 반면에, 모델 없는 강화 학습(RL) 방법은 다양한 지형에서 MPC보다 성능이 뛰어나고, 자연스러운 움직임을 보여주지만 제약 조건을 처리하거나 계획을 세우는 능력이 부족해.

이런 한계를 극복하기 위해, 우리는 RL과 고유(신체 감각) 계획을 통합한 프레임워크를 제안해. 이 방식은 지평선에 걸쳐 민첩하고 안전한 보행 행동을 가능하게 해. MPC에서 영감을 받아, 우리는 속도 추정기와 드리머 모듈을 포함한 내부 모델을 도입했어. 훈련 중에 이 프레임워크는 전문가 정책과 상호 의존적인 내부 모델을 학습해서 보행 행동을 개선하기 위한 탐색을 촉진해. 배포할 때는 드리머 모듈이 무한 지평선 MPC 문제를 해결해서 제약 조건을 준수하는 행동과 속도 명령을 조정해.

우리는 내부 모델 구성 요소에 대한 제거 연구를 통해 훈련 프레임워크의 강인성을 검증하고, 훈련 노이즈에 대한 강인성이 개선된 것을 보여줬어. 마지막으로, 우리는 시뮬레이션과 하드웨어 모두에서 다양한 지형 시나리오를 통해 우리의 접근 방식을 평가했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09473.pdf

Title: Learning to enhance multi-legged robot on rugged landscapes

Original Abstract:
Navigating rugged landscapes poses significant challenges for legged locomotion. Multi-legged robots (those with 6 and greater) offer a promising solution for such terrains, largely due to their inherent high static stability, resulting from a low center of mass and wide base of support. Such systems require minimal effort to maintain balance. Recent studies have shown that a linear controller, which modulates the vertical body undulation of a multi-legged robot in response to shifts in terrain roughness, can ensure reliable mobility on challenging terrains. However, the potential of a learning-based control framework that adjusts multiple parameters to address terrain heterogeneity remains underexplored. We posit that the development of an experimentally validated physics-based simulator for this robot can rapidly advance capabilities by allowing wide parameter space exploration. Here we develop a MuJoCo-based simulator tailored to this robotic platform and use the simulation to develop a reinforcement learning-based control framework that dynamically adjusts horizontal and vertical body undulation, and limb stepping in real-time. Our approach improves robot performance in simulation, laboratory experiments, and outdoor tests. Notably, our real-world experiments reveal that the learning-based controller achieves a 30\% to 50\% increase in speed compared to a linear controller, which only modulates vertical body waves. We hypothesize that the superior performance of the learning-based controller arises from its ability to adjust multiple parameters simultaneously, including limb stepping, horizontal body wave, and vertical body wave.

Translated Abstract:
험난한 풍경을 탐색하는 건 다리로 걷는 로봇에게 큰 도전 과제가 있어. 다리가 6개 이상인 로봇, 즉 다중 다리 로봇은 이런 지형에서 좋은 해결책이 될 수 있어. 왜냐하면 이 로봇들은 중심이 낮고 지지대가 넓어서 정적 안정성이 높기 때문이야. 그래서 균형을 유지하는 데 힘이 별로 안 들어.

최근 연구에 따르면, 지형이 울퉁불퉁해질 때 로봇의 수직 움직임을 조절하는 선형 제어기가 어려운 지형에서도 신뢰성 있게 움직일 수 있도록 도와준대. 하지만 여러 매개변수를 조정해서 지형의 다양성에 대응하는 학습 기반 제어 방식의 가능성은 아직 많이 연구되지 않았어.

우리는 이 로봇을 위한 실험적으로 검증된 물리 기반 시뮬레이터를 개발하면 다양한 매개변수를 탐색할 수 있어서 성능을 빠르게 개선할 수 있다고 생각해. 그래서 우리는 MuJoCo를 기반으로 한 시뮬레이터를 만들고, 이 시뮬레이션을 활용해 수평과 수직 움직임, 그리고 다리의 보폭을 실시간으로 조절하는 강화 학습 기반 제어 방식을 개발했어.

우리의 접근 방식은 로봇의 성능을 시뮬레이션, 실험실 실험, 그리고 야외 테스트에서 모두 개선했어. 특히 실제 실험에서는 학습 기반 제어기가 수직 움직임만 조절하는 선형 제어기보다 속도가 30%에서 50% 빨라진다는 결과를 보여줬어. 우리는 학습 기반 제어기가 다리 보폭, 수평 움직임, 수직 움직임 같은 여러 매개변수를 동시에 조정할 수 있어서 성능이 더 뛰어나다고 가정하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09479.pdf

Title: MAC-VO: Metrics-aware Covariance for Learning-based Stereo Visual Odometry

Original Abstract:
We propose the MAC-VO, a novel learning-based stereo VO that leverages the learned metrics-aware matching uncertainty for dual purposes: selecting keypoint and weighing the residual in pose graph optimization. Compared to traditional geometric methods prioritizing texture-affluent features like edges, our keypoint selector employs the learned uncertainty to filter out the low-quality features based on global inconsistency. In contrast to the learning-based algorithms that model the scale-agnostic diagonal weight matrix for covariance, we design a metrics-aware covariance model to capture the spatial error during keypoint registration and the correlations between different axes. Integrating this covariance model into pose graph optimization enhances the robustness and reliability of pose estimation, particularly in challenging environments with varying illumination, feature density, and motion patterns. On public benchmark datasets, MAC-VO outperforms existing VO algorithms and even some SLAM algorithms in challenging environments. The covariance map also provides valuable information about the reliability of the estimated poses, which can benefit decision-making for autonomous systems.

Translated Abstract:
우리는 MAC-VO라는 새로운 학습 기반의 스테레오 시각항법(Vision Odometry, VO) 방법을 제안해. 이 방법은 학습된 메트릭스 인식 매칭 불확실성을 활용해 두 가지 목적을 달성해: 키포인트 선택과 포즈 그래프 최적화에서 잔여값의 가중치 조정.

전통적인 기하학적 방법들이 엣지 같은 질감이 풍부한 특징을 우선시하는 것과 달리, 우리의 키포인트 선택기는 학습된 불확실성을 이용해 전반적인 일관성에 기반해 저품질 특징을 걸러내. 그리고 기존의 학습 기반 알고리즘들은 공분산을 위해 스케일 무관 대각선 가중치 행렬을 모델링하는 반면, 우리는 키포인트 등록 시 공간적 오류와 서로 다른 축 간의 상관관계를 포착하기 위해 메트릭스 인식 공분산 모델을 설계했어. 

이 공분산 모델을 포즈 그래프 최적화에 통합하면, 다양한 조명, 특징 밀도, 움직임 패턴이 있는 어려운 환경에서도 포즈 추정의 강인성과 신뢰성이 향상돼. 공공 벤치마크 데이터셋에서 MAC-VO는 기존 VO 알고리즘과 몇몇 SLAM 알고리즘까지 초월하는 성능을 보여줬어. 공분산 맵은 추정된 포즈의 신뢰성에 대한 유용한 정보를 제공해, 자율 시스템의 의사결정에 도움이 될 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09491.pdf

Title: Robot Learning as an Empirical Science: Best Practices for Policy Evaluation

Original Abstract:
The robot learning community has made great strides in recent years, proposing new architectures and showcasing impressive new capabilities; however, the dominant metric used in the literature, especially for physical experiments, is "success rate", i.e. the percentage of runs that were successful. Furthermore, it is common for papers to report this number with little to no information regarding the number of runs, the initial conditions, and the success criteria, little to no narrative description of the behaviors and failures observed, and little to no statistical analysis of the findings. In this paper we argue that to move the field forward, researchers should provide a nuanced evaluation of their methods, especially when evaluating and comparing learned policies on physical robots. To do so, we propose best practices for future evaluations: explicitly reporting the experimental conditions, evaluating several metrics designed to complement success rate, conducting statistical analysis, and adding a qualitative description of failures modes. We illustrate these through an evaluation on physical robots of several learned policies for manipulation tasks.

Translated Abstract:
로봇 학습 분야는 최근 몇 년 동안 큰 발전을 이루었고, 새로운 구조와 인상적인 능력을 보여주고 있어. 그런데, 이 분야에서 주로 사용되는 평가 기준은 "성공률"이야. 즉, 성공적으로 수행된 실험의 비율을 말하지. 

그리고 많은 논문이 이 성공률만 보고하면서, 실험 횟수, 초기 조건, 성공 기준 같은 정보는 거의 제공하지 않거나 아예 안 해. 행동이나 실패에 대한 설명도 부족하고, 통계 분석도 거의 없지. 

이 논문에서는 이 분야를 발전시키기 위해서 연구자들이 더 정교한 평가를 해야 한다고 주장해. 특히 물리적인 로봇에서 학습 정책을 평가하고 비교할 때 이게 중요해. 그래서 우리는 미래의 평가를 위한 최선의 방법을 제안해: 실험 조건을 명확히 보고하고, 성공률을 보완할 수 있는 여러 지표를 평가하고, 통계 분석을 하고, 실패 모드에 대한 질적 설명을 추가하는 거야. 

이런 방법들을 물리적인 로봇에서 여러 가지 학습 정책을 조작하는 과제로 평가하면서 보여줄 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09523.pdf

Title: Lab2Car: A Versatile Wrapper for Deploying Experimental Planners in Complex Real-world Environments

Original Abstract:
Human-level autonomous driving is an ever-elusive goal, with planning and decision making -- the cognitive functions that determine driving behavior -- posing the greatest challenge. Despite a proliferation of promising approaches, progress is stifled by the difficulty of deploying experimental planners in naturalistic settings. In this work, we propose Lab2Car, an optimization-based wrapper that can take a trajectory sketch from an arbitrary motion planner and convert it to a safe, comfortable, dynamically feasible trajectory that the car can follow. This allows motion planners that do not provide such guarantees to be safely tested and optimized in real-world environments. We demonstrate the versatility of Lab2Car by using it to deploy a machine learning (ML) planner and a search-based planner on self-driving cars in Las Vegas. The resulting systems handle challenging scenarios, such as cut-ins, overtaking, and yielding, in complex urban environments like casino pick-up/drop-off areas. Our work paves the way for quickly deploying and evaluating candidate motion planners in realistic settings, ensuring rapid iteration and accelerating progress towards human-level autonomy.

Translated Abstract:
인간 수준의 자율주행은 언제나 어려운 목표야. 주행 행동을 결정하는 계획과 의사결정, 즉 인지 기능이 가장 큰 도전 과제가 돼. 많은 유망한 접근 방식이 있지만, 실험적인 계획기를 실제 환경에 적용하는 게 힘들어서 발전이 더디고 있어.

이 연구에서는 Lab2Car라는 최적화 기반의 래퍼를 제안해. 이건 어떤 모션 플래너의 궤적 스케치를 받아서 차가 따를 수 있는 안전하고 편안하며 동적으로 가능한 궤적으로 변환해줘. 이렇게 하면 그런 보장이 없는 모션 플래너도 현실 세계에서 안전하게 테스트하고 최적화할 수 있어.

우리는 Lab2Car의 다재다능함을 보여주기 위해, 라스베가스의 자율주행차에 머신러닝(ML) 플래너와 탐색 기반 플래너를 적용했어. 이 시스템들은 카지노의 픽업/드롭오프 구역 같은 복잡한 도시 환경에서 끼어들기, 추월, 양보 같은 어려운 상황을 잘 처리해. 우리의 연구는 현실적인 환경에서 후보 모션 플래너를 빠르게 배포하고 평가할 수 있는 길을 열어줘서, 빠른 반복을 보장하고 인간 수준의 자율성으로 나아가는 진전을 가속화하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09536.pdf

Title: VernaCopter: Disambiguated Natural-Language-Driven Robot via Formal Specifications

Original Abstract:
It has been an ambition of many to control a robot for a complex task using natural language (NL). The rise of large language models (LLMs) makes it closer to coming true. However, an LLM-powered system still suffers from the ambiguity inherent in an NL and the uncertainty brought up by LLMs. This paper proposes a novel LLM-based robot motion planner, named \textit{VernaCopter}, with signal temporal logic (STL) specifications serving as a bridge between NL commands and specific task objectives. The rigorous and abstract nature of formal specifications allows the planner to generate high-quality and highly consistent paths to guide the motion control of a robot. Compared to a conventional NL-prompting-based planner, the proposed VernaCopter planner is more stable and reliable due to less ambiguous uncertainty. Its efficacy and advantage have been validated by two small but challenging experimental scenarios, implying its potential in designing NL-driven robots.

Translated Abstract:
많은 사람들이 자연어(NL)를 사용해서 로봇을 복잡한 작업을 수행하도록 제어하는 것을 목표로 해왔어. 최근 대규모 언어 모델(LLM)의 발전 덕분에 이 목표가 더 가까워진 것 같아. 하지만 LLM을 활용한 시스템은 여전히 자연어의 모호성과 LLM에 의해 발생하는 불확실성 때문에 어려움을 겪고 있어.

이 논문에서는 \textit{VernaCopter}라는 새로운 LLM 기반 로봇 동작 계획기를 제안해. 이 계획기는 신호 시계열 논리(STL) 명세를 사용해서 자연어 명령과 특정 작업 목표를 연결해줘. 공식적인 명세의 엄격하고 추상적인 특성 덕분에 이 계획기는 로봇의 동작 제어를 위한 고품질의 일관된 경로를 생성할 수 있어.

기존의 자연어 프롬프트 기반 계획기와 비교했을 때, VernaCopter 계획기는 덜 모호한 불확실성 덕분에 더 안정적이고 신뢰성이 높아. 이 시스템의 효과와 장점은 두 개의 작지만 도전적인 실험 시나리오를 통해 검증되었고, 자연어 기반 로봇 설계에 대한 잠재력을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09557.pdf

Title: Adaptable, shape-conforming robotic endoscope

Original Abstract:
This paper introduces a size-adaptable robotic endoscope design, which aims to improve the efficiency and comfort of colonoscopy. The robotic endoscope proposed in this paper combines the expansion mechanism and the external drive system, which can adjust the shape according to the different pipe diameters, thus improving the stability and propulsion force during propulsion. As an actuator in the expansion mechanism, flexible bellows can provide a normal force of 3.89 N and an axial deformation of nearly 10mm at the maximum pressure, with a 53% expansion rate in the size of expandable tip. In the test of the locomotion performance of the prototype, we obtained the relationship with the propelling of the prototype by changing the friction coefficient of the pipe and the motor angular velocity. In the experiment with artificial bowel tissues, the prototype can generate a propelling force of 2.83 N, and the maximum linear speed is 29.29 m/s in average, and could produce effective propulsion when it passes through different pipe sizes. The results show that the prototype can realize the ability of shape adaptation in order to obtain more propulsion. The relationship between propelling force and traction force, structural optimization and miniaturization still need further exploration.

Translated Abstract:
이 논문은 크기 조정이 가능한 로봇 내시경 디자인을 소개해. 이 디자인은 대장내시경의 효율성과 편안함을 개선하는 게 목표야. 

제안된 로봇 내시경은 확장 메커니즘과 외부 구동 시스템을 결합했어. 이 시스템은 파이프의 직경에 따라 모양을 조정할 수 있어서, 추진할 때 안정성과 추진력을 향상시켜. 확장 메커니즘에 사용되는 액추에이터인 유연한 벨로우즈는 최대 압력에서 3.89 N의 정상력을 제공하고, 거의 10mm의 축 방향 변형을 할 수 있어. 확장 가능한 팁의 크기는 53%까지 늘어날 수 있어. 

프로토타입의 이동 성능 테스트에서는 파이프의 마찰 계수와 모터의 각속도를 바꿔가며 프로토타입의 추진력과의 관계를 알아봤어. 인공 장기 조직을 이용한 실험에서는 프로토타입이 2.83 N의 추진력을 생성할 수 있었고, 평균 최대 선속도는 29.29 m/s였어. 다양한 파이프 크기를 통과할 때 효과적인 추진력을 발휘할 수 있었어. 

결과적으로, 프로토타입은 더 많은 추진력을 얻기 위해 모양을 조정할 수 있는 능력을 보여줬어. 하지만 추진력과 견인력의 관계, 구조 최적화, 소형화는 아직 더 탐구해야 할 부분이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09572.pdf

Title: A Novel Aerial-Aquatic Locomotion Robot with Variable Stiffness Propulsion Module

Original Abstract:
In recent years, the development of robots capable of operating in both aerial and aquatic environments has gained significant attention. This study presents the design and fabrication of a novel aerial-aquatic locomotion robot (AALR). Inspired by the diving beetle, the AALR incorporates a biomimetic propulsion mechanism with power and recovery strokes. The variable stiffness propulsion module (VSPM) uses low melting point alloy (LMPA) and variable stiffness joints (VSJ) to achieve efficient aquatic locomotion while reduce harm to marine life. The AALR's innovative design integrates the VSPM into the arms of a traditional quadrotor, allowing for effective aerial-aquatic locomotion. The VSPM adjusts joint stiffness through temperature control, meeting locomotion requirements in both aerial and aquatic modes. A dynamic model for the VSPM was developed, with optimized dimensional parameters to increase propulsion force. Experiments focused on aquatic mode analysis and demonstrated the AALR's swimming capability, achieving a maximum swimming speed of 77 mm/s underwater. The results confirm the AALR's effective performance in water environment, highlighting its potential for versatile, eco-friendly operations.

Translated Abstract:
최근 몇 년 동안 공중과 수중에서 모두 작동할 수 있는 로봇 개발이 많은 주목을 받고 있어. 이 연구에서는 새로운 공중-수중 이동 로봇(AALR)의 설계와 제작을 소개해. 수중에서 수영하는 딱정벌레에서 영감을 받아서 AALR는 생체 모방 추진 메커니즘을 가지고 있어, 힘을 주는 동작과 회복 동작이 결합되어 있지.

변형 강성 추진 모듈(VSPM)은 저융점 합금(LMPA)과 변형 강성 관절(VSJ)을 사용해 수중에서 효율적으로 이동하면서 해양 생물에 대한 피해를 줄여. AALR의 혁신적인 디자인은 VSPM을 전통적인 쿼드로터의 팔에 통합해 공중과 수중 모두에서 효과적으로 이동할 수 있게 해.

VSPM은 온도 조절을 통해 관절의 강성을 조절하며, 공중과 수중 모드에서의 이동 요구를 충족시켜. VSPM에 대한 동적 모델을 개발했고, 추진력을 증가시키기 위해 치수를 최적화했어. 실험은 수중 모드 분석에 집중했으며 AALR의 수영 능력을 보여주었어. 수중에서 최대 수영 속도는 77 mm/s에 달했어. 

결과는 AALR가 수중 환경에서 효과적으로 작동한다는 걸 확인시켜 주었고, 다양한 친환경 작업을 수행할 수 있는 가능성을 강조해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09573.pdf

Title: Decentralized Safe and Scalable Multi-Agent Control under Limited Actuation

Original Abstract:
To deploy safe and agile robots in cluttered environments, there is a need to develop fully decentralized controllers that guarantee safety, respect actuation limits, prevent deadlocks, and scale to thousands of agents. Current approaches fall short of meeting all these goals: optimization-based methods ensure safety but lack scalability, while learning-based methods scale but do not guarantee safety. We propose a novel algorithm to achieve safe and scalable control for multiple agents under limited actuation. Specifically, our approach includes: $(i)$ learning a decentralized neural Integral Control Barrier function (neural ICBF) for scalable, input-constrained control, $(ii)$ embedding a lightweight decentralized Model Predictive Control-based Integral Control Barrier Function (MPC-ICBF) into the neural network policy to ensure safety while maintaining scalability, and $(iii)$ introducing a novel method to minimize deadlocks based on gradient-based optimization techniques from machine learning to address local minima in deadlocks. Our numerical simulations show that this approach outperforms state-of-the-art multi-agent control algorithms in terms of safety, input constraint satisfaction, and minimizing deadlocks. Additionally, we demonstrate strong generalization across scenarios with varying agent counts, scaling up to 1000 agents.

Translated Abstract:
혼잡한 환경에서 안전하고 민첩한 로봇을 배치하기 위해서는 안전을 보장하고, 작동 한계를 존중하며, 교착 상태를 방지하고, 수천 개의 에이전트에 맞게 확장할 수 있는 완전 분산형 제어기를 개발해야 해. 현재의 접근 방식은 이 모든 목표를 충족하지 못하고 있어. 최적화 기반 방법은 안전성을 보장하지만 확장성이 부족하고, 학습 기반 방법은 확장 가능하지만 안전성을 보장하지 못해.

우리는 제한된 작동 조건에서 여러 에이전트에 대한 안전하고 확장 가능한 제어를 달성하기 위한 새로운 알고리즘을 제안해. 구체적으로, 우리의 접근 방식은 다음과 같아: 

$(i)$ 확장 가능하고 입력 제약이 있는 제어를 위한 분산형 신경 적분 제어 장벽 함수(neural ICBF)를 학습하고,  
$(ii)$ 안전성을 보장하면서 확장성을 유지하기 위해 신경망 정책에 경량의 분산형 모델 예측 제어 기반 적분 제어 장벽 함수(MPC-ICBF)를 포함하고,  
$(iii)$ 교착 상태의 지역 최솟값 문제를 해결하기 위해 기계 학습의 경량 최적화 기법을 적용해 교착 상태를 최소화하는 새로운 방법을 도입해. 

우리의 수치 시뮬레이션 결과, 이 접근 방식이 안전성, 입력 제약 조건 만족, 교착 상태 최소화 측면에서 최신 다중 에이전트 제어 알고리즘보다 뛰어나다는 것을 보여줘. 또한, 다양한 에이전트 수에 따라 강력한 일반화를 보여주며, 최대 1000개의 에이전트까지 확장할 수 있음을 입증해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09575.pdf

Title: Traffic Scene Generation from Natural Language Description for Autonomous Vehicles with Large Language Model

Original Abstract:
Text-to-scene generation, transforming textual descriptions into detailed scenes, typically relies on generating key scenarios along predetermined paths, constraining environmental diversity and limiting customization flexibility. To address these limitations, we propose a novel text-to-traffic scene framework that leverages a large language model to generate diverse traffic scenarios within the Carla simulator based on natural language descriptions. Users can define specific parameters such as weather conditions, vehicle types, and road signals, while our pipeline can autonomously select the starting point and scenario details, generating scenes from scratch without relying on predetermined locations or trajectories. Furthermore, our framework supports both critical and routine traffic scenarios, enhancing its applicability. Experimental results indicate that our approach promotes diverse agent planning and road selection, enhancing the training of autonomous agents in traffic environments. Notably, our methodology has achieved a 16% reduction in average collision rates. Our work is made publicly available at this https URL.

Translated Abstract:
텍스트를 기반으로 장면을 생성하는 기술은 보통 미리 정해진 경로를 따라 주요 시나리오를 생성하는 방식이에요. 이러다 보니 환경의 다양성이 제한되고, 사용자 맞춤 설정이 어려운 문제가 있었죠. 

그래서 우리는 새로운 텍스트-교통 장면 생성 프레임워크를 제안해요. 이건 큰 언어 모델을 활용해서 자연어 설명에 따라 카를라 시뮬레이터 내에서 다양한 교통 시나리오를 생성할 수 있게 해줘요. 사용자는 날씨 조건이나 차량 종류, 도로 신호 같은 특정 매개변수를 설정할 수 있고, 우리의 시스템은 자동으로 시작 지점과 시나리오 세부사항을 선택해서 미리 정해진 위치나 경로에 의존하지 않고 장면을 처음부터 생성해요.

또한, 이 프레임워크는 중요한 교통 시나리오와 일상적인 교통 시나리오 모두를 지원해서 활용 가능성을 높였어요. 실험 결과, 우리의 접근 방식이 다양한 에이전트 계획과 도로 선택을 촉진시켜 자율 에이전트의 훈련을 개선하는 데 도움이 된다는 걸 보여줬어요. 특히, 우리의 방법론은 평균 충돌률을 16% 줄이는 성과를 올렸어요. 

이 연구 결과는 이 URL에서 공개되고 있어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.09633.pdf

Title: A Scalable Tabletop Satellite Automation Testbed:Design And Experiments

Original Abstract:
This paper presents a detailed system design and component selection for the Transforming Proximity Operations and Docking Service (TPODS) module, designed to gain custody of uncontrolled resident space objects (RSOs) via rendezvous and proximity operation (RPO). In addition to serving as a free-flying robotic manipulator to work with cooperative and uncooperative RSOs, the TPODS modules are engineered to have the ability to cooperate with one another to build scaffolding for more complex satellite servicing activities. The structural design of the prototype module is inspired by Tensegrity principles, minimizing the structural mass of the modules frame. The prototype TPODS module is fabricated using lightweight polycarbonate with an aluminum or carbon fiber frame. The inner shell that houses various electronic and pneumatic components is 3-D printed using ABS material. Four OpenMV H7 R1 cameras are used for the pose estimation of resident space objects (RSOs), including other TPODS modules. Compressed air supplied by an external source is used for the initial testing and can be replaced by module-mounted nitrogen pressure vessels for full on-board propulsion later. A Teensy 4.1 single-board computer is used as a central command unit that receives data from the four OpenMV cameras, and commands its thrusters based on the control logic.

Translated Abstract:
이 논문에서는 '변형 근접 작전 및 도킹 서비스'(TPODS) 모듈의 시스템 설계와 구성 요소 선택에 대해 자세히 설명하고 있어. 이 모듈은 비협력적인 거주 우주 물체(RSO)를 확보하기 위해 만남과 근접 작전(RPO)을 통해 설계됐어.

TPODS 모듈은 협력적인 RSO뿐만 아니라 비협력적인 RSO와도 작업할 수 있는 자유 비행 로봇 조작기로 기능해. 이 모듈들은 서로 협력해서 더 복잡한 위성 서비스 활동을 위한 비계를 만들 수 있도록 설계됐어. 프로토타입 모듈의 구조 디자인은 텐세그리티 원칙에서 영감을 받아서 모듈 프레임의 구조적 질량을 최소화했어.

프로토타입 TPODS 모듈은 경량 폴리카보네이트와 알루미늄 또는 탄소 섬유 프레임으로 제작됐어. 다양한 전자 및 공압 부품을 수용하는 내부 쉘은 ABS 재료로 3D 프린팅됐어. 거주 우주 물체(RSO)와 다른 TPODS 모듈의 자세 추정을 위해 네 개의 OpenMV H7 R1 카메라를 사용해. 외부에서 공급된 압축 공기는 초기 테스트에 사용되며, 나중에는 모듈 장착된 질소 압력 용기로 교체될 수 있어. 

Teensy 4.1 단일 보드 컴퓨터가 중앙 명령 장치로 사용되며, 네 개의 OpenMV 카메라에서 데이터를 받고, 그에 따라 추진기를 제어하는 명령을 내리지.

================================================================================

URL:
https://arxiv.org/pdf/2409.09682.pdf

Title: A Robust Probability-based Joint Registration Method of Multiple Point Clouds Considering Local Consistency

Original Abstract:
In robotic inspection, joint registration of multiple point clouds is an essential technique for estimating the transformation relationships between measured parts, such as multiple blades in a propeller. However, the presence of noise and outliers in the data can significantly impair the registration performance by affecting the correctness of correspondences. To address this issue, we incorporate local consistency property into the probability-based joint registration method. Specifically, each measured point set is treated as a sample from an unknown Gaussian Mixture Model (GMM), and the registration problem is framed as estimating the probability model. By incorporating local consistency into the optimization process, we enhance the robustness and accuracy of the posterior distributions, which represent the one-to-all correspondences that directly determine the registration results. Effective closed-form solution for transformation and probability parameters are derived with Expectation-Maximization (EM) algorithm. Extensive experiments demonstrate that our method outperforms the existing methods, achieving high accuracy and robustness with the existence of noise and outliers. The code will be available at this https URL.

Translated Abstract:
로봇 점검에서 여러 포인트 클라우드의 조인트 등록은 측정된 부품들, 예를 들어 프로펠러의 여러 날개 사이의 변환 관계를 추정하는 데 중요한 기술이야. 하지만 데이터에 노이즈나 아웃라이어가 있으면 등록 성능이 크게 저하될 수 있어. 이 문제를 해결하기 위해 우리는 확률 기반의 조인트 등록 방법에 지역 일관성 속성을 추가했어.

구체적으로, 각 측정된 포인트 세트를 알려지지 않은 가우시안 혼합 모델(GMM)의 샘플로 보고, 등록 문제를 확률 모델을 추정하는 것으로 설정했어. 지역 일관성을 최적화 과정에 포함시킴으로써, 등록 결과를 직접 결정하는 일대다 대응 관계를 나타내는 후행 분포의 강인성과 정확성을 높였어. 변환 및 확률 매개변수에 대한 효과적인 닫힌 형식의 해법은 기대 최대화(EM) 알고리즘을 사용해 도출했어.

광범위한 실험 결과, 우리의 방법이 기존 방법들보다 성능이 뛰어나고, 노이즈와 아웃라이어가 있어도 높은 정확성과 강인성을 달성한다는 것을 보여줬어. 코드도 이 링크에서 사용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09725.pdf

Title: Precise Pick-and-Place using Score-Based Diffusion Networks

Original Abstract:
In this paper, we propose a novel coarse-to-fine continuous pose diffusion method to enhance the precision of pick-and-place operations within robotic manipulation tasks. Leveraging the capabilities of diffusion networks, we facilitate the accurate perception of object poses. This accurate perception enhances both pick-and-place success rates and overall manipulation precision. Our methodology utilizes a top-down RGB image projected from an RGB-D camera and adopts a coarse-to-fine architecture. This architecture enables efficient learning of coarse and fine models. A distinguishing feature of our approach is its focus on continuous pose estimation, which enables more precise object manipulation, particularly concerning rotational angles. In addition, we employ pose and color augmentation techniques to enable effective training with limited data. Through extensive experiments in simulated and real-world scenarios, as well as an ablation study, we comprehensively evaluate our proposed methodology. Taken together, the findings validate its effectiveness in achieving high-precision pick-and-place tasks.

Translated Abstract:
이 논문에서는 로봇 조작 작업에서 집기 및 놓기 작업의 정확성을 높이기 위한 새로운 연속 포즈 확산 방법을 제안해. 확산 네트워크의 능력을 활용해서 물체의 포즈를 정확하게 인식할 수 있도록 해. 이렇게 정확한 인식은 집기 및 놓기 성공률과 전체 조작 정밀도를 높이는 데 도움을 줘.

우리 방법론은 RGB-D 카메라에서 투영된 상향식 RGB 이미지를 사용하고, 거칠고 세밀한 모델을 효율적으로 학습할 수 있는 거시적-세밀적 아키텍처를 채택해. 이 방법의 특징은 연속 포즈 추정에 초점을 맞춘 점인데, 덕분에 물체 조작을 더 정밀하게 할 수 있어, 특히 회전 각도에 관해서 말이지. 

또한, 제한된 데이터로 효과적인 학습을 할 수 있도록 포즈와 색상 증강 기법도 사용해. 시뮬레이션과 실제 상황에서의 광범위한 실험과 제거 연구를 통해 제안한 방법론을 종합적으로 평가했어. 결과적으로, 이 방법이 고정밀 집기 및 놓기 작업을 달성하는 데 효과적임을 검증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09726.pdf

Title: High Definition Map Mapping and Update: A General Overview and Future Directions

Original Abstract:
Along with the rapid growth of autonomous vehicles (AVs), more and more demands are required for environment perception technology. Among others, HD mapping has become one of the more prominent roles in helping the vehicle realize essential tasks such as localization and path planning. While increasing research efforts have been directed toward HD Map development. However, a comprehensive overview of the overall HD map mapping and update framework is still lacking. This article introduces the development and current state of the algorithm involved in creating HD map mapping and its maintenance. As part of this study, the primary data preprocessing approach of processing raw data to information ready to feed for mapping and update purposes, semantic segmentation, and localization are also briefly reviewed. Moreover, the map taxonomy, ontology, and quality assessment are extensively discussed, the map data's general representation method is presented, and the mapping algorithm ranging from SLAM to transformers learning-based approaches are also discussed. The development of the HD map update algorithm, from change detection to the update methods, is also presented. Finally, the authors discuss possible future developments and the remaining challenges in HD map mapping and update technology. This paper simultaneously serves as a position paper and tutorial to those new to HD map mapping and update domains.

Translated Abstract:
자율주행차(AV)의 빠른 성장과 함께 환경 인식 기술에 대한 수요도 급증하고 있어. 그 중에서도 HD 맵핑은 차량이 위치 확인과 경로 계획 같은 중요한 작업을 수행하는 데 큰 역할을 하고 있어. HD 맵 개발에 대한 연구가 많이 진행되고 있지만, 전체 HD 맵 맵핑과 업데이트 프레임워크에 대한 포괄적인 개요는 아직 부족한 상황이야.

이 논문은 HD 맵을 만들고 유지하는 데 사용되는 알고리즘의 개발과 현재 상태를 소개해. 연구의 일환으로, 원시 데이터를 맵핑과 업데이트에 사용할 수 있는 정보로 처리하는 주요 데이터 전처리 방법과 의미 분할, 위치 확인 등에 대해서도 간단히 살펴봤어. 게다가, 맵의 분류, 온톨로지, 품질 평가에 대해서도 자세히 논의하고, 맵 데이터의 일반적인 표현 방법을 소개했어. SLAM부터 변환기 학습 기반 접근 방식에 이르는 맵핑 알고리즘도 다뤘어.

HD 맵 업데이트 알고리즘의 개발, 즉 변화 감지부터 업데이트 방법까지도 설명하고 있어. 마지막으로, 저자들은 HD 맵 맵핑과 업데이트 기술의 미래 발전 가능성과 남아 있는 과제에 대해서도 이야기해. 이 논문은 HD 맵 맵핑과 업데이트 분야에 처음 접하는 사람들에게는 입문서 역할도 해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09763.pdf

Title: Range-SLAM: Ultra-Wideband-Based Smoke-Resistant Real-Time Localization and Mapping

Original Abstract:
This paper presents Range-SLAM, a real-time, lightweight SLAM system designed to address the challenges of localization and mapping in environments with smoke and other harsh conditions using Ultra-Wideband (UWB) signals. While optical sensors like LiDAR and cameras struggle in low-visibility environments, UWB signals provide a robust alternative for real-time positioning. The proposed system uses general UWB devices to achieve accurate mapping and localization without relying on expensive LiDAR or other dedicated hardware. By utilizing only the distance and Received Signal Strength Indicator (RSSI) provided by UWB sensors in relation to anchors, we combine the motion of the tag-carrying agent with raycasting algorithm to construct a 2D occupancy grid map in real time. To enhance localization in challenging conditions, a Weighted Least Squares (WLS) method is employed. Extensive real-world experiments, including smoke-filled environments and simulated

Translated Abstract:
이 논문에서는 Range-SLAM이라는 실시간 경량 SLAM 시스템을 제안해. 이 시스템은 연기 같은 험난한 환경에서 위치 측정과 맵핑 문제를 해결하기 위해 Ultra-Wideband (UWB) 신호를 사용해.

LiDAR나 카메라 같은 광학 센서는 시야가 좋지 않은 환경에서 잘 작동하지 않지만, UWB 신호는 실시간 위치 측정에 강력한 대안을 제공해. 제안된 시스템은 비싼 LiDAR나 다른 전용 하드웨어에 의존하지 않고 일반 UWB 장치를 사용해 정확한 맵핑과 위치 측정을 가능하게 해.

UWB 센서가 제공하는 거리와 수신 신호 세기(RSSI)만 사용해서, 태그를 가진 에이전트의 움직임과 레이캐스팅 알고리즘을 결합해 실시간으로 2D 점유 그리드 맵을 만들어. 어려운 조건에서 위치 측정을 개선하기 위해 가중 최소 제곱법(WLS)도 사용해. 

연기가 가득한 환경과 시뮬레이션을 포함한 다양한 실제 실험을 통해 이 시스템의 성능을 검증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09816.pdf

Title: Fast Shortest Path Polyline Smoothing With G1 Continuity and Bounded Curvature

Original Abstract:
In this work, we propose a novel and efficient method for smoothing polylines in motion planning tasks. The algorithm applies to motion planning of vehicles with bounded curvature. In the paper, we show that the generated path: 1) has minimal length, 2) is $G^1$ continuous, and 3) is collision-free by construction, if the hypotheses are respected. We compare our solution with the state-of.the-art and show its convenience both in terms of computation time and of length of the compute path.

Translated Abstract:
이 연구에서는 모션 계획 작업에서 폴리라인을 부드럽게 만드는 새로운 방법을 제안해. 이 알고리즘은 제한된 곡률을 가진 차량의 모션 계획에 적용돼.

논문에서는 생성된 경로가 다음과 같은 특성을 가진다고 보여줘: 1) 최소 길이를 가짐, 2) $G^1$ 연속성을 가짐, 3) 가정이 지켜진다면 본질적으로 충돌이 없음. 

우리는 우리의 방법을 최신 기술과 비교하고 계산 시간과 경로 길이 측면에서 얼마나 편리한지를 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09827.pdf

Title: On the Effect of Robot Errors on Human Teaching Dynamics

Original Abstract:
Human-in-the-loop learning is gaining popularity, particularly in the field of robotics, because it leverages human knowledge about real-world tasks to facilitate agent learning. When people instruct robots, they naturally adapt their teaching behavior in response to changes in robot performance. While current research predominantly focuses on integrating human teaching dynamics from an algorithmic perspective, understanding these dynamics from a human-centered standpoint is an under-explored, yet fundamental problem. Addressing this issue will enhance both robot learning and user experience. Therefore, this paper explores one potential factor contributing to the dynamic nature of human teaching: robot errors. We conducted a user study to investigate how the presence and severity of robot errors affect three dimensions of human teaching dynamics: feedback granularity, feedback richness, and teaching time, in both forced-choice and open-ended teaching contexts. The results show that people tend to spend more time teaching robots with errors, provide more detailed feedback over specific segments of a robot's trajectory, and that robot error can influence a teacher's choice of feedback modality. Our findings offer valuable insights for designing effective interfaces for interactive learning and optimizing algorithms to better understand human intentions.

Translated Abstract:
인간-루프 학습은 특히 로봇 분야에서 인기를 끌고 있어. 이 방법은 사람들이 실제 작업에 대한 지식을 활용해서 로봇이 더 잘 배우게 도와주거든. 사람들이 로봇에게 지시할 때, 로봇 성능이 바뀌면 자연스럽게 가르치는 방식도 조정해. 하지만 현재 연구는 주로 알고리즘적 관점에서 인간의 가르침 동력을 통합하는 데 집중하고 있어. 인간 중심의 시각에서 이 동력을 이해하는 건 잘 다뤄지지 않은 중요한 문제야. 이 문제를 해결하면 로봇의 학습과 사용자 경험이 모두 향상될 수 있어.

그래서 이 논문에서는 인간의 가르침 동력의 역동성에 기여하는 하나의 요소인 로봇 오류를 살펴봤어. 우리는 사용자 연구를 통해 로봇 오류의 존재와 심각성이 인간 가르침 동력의 세 가지 차원, 즉 피드백의 세부 정도, 피드백의 풍부함, 그리고 가르침 시간에 어떻게 영향을 미치는지 조사했어. 이 연구는 강제 선택과 개방형 가르침 맥락에서 진행됐어. 결과적으로, 사람들은 오류가 있는 로봇을 가르치는 데 더 많은 시간을 쓰고, 로봇의 경로 특정 구간에 대해 더 자세한 피드백을 제공하며, 로봇 오류가 교사의 피드백 방식 선택에도 영향을 미칠 수 있다는 걸 알게 됐어. 우리의 연구 결과는 상호작용 학습을 위한 효과적인 인터페이스를 설계하고, 인간의 의도를 더 잘 이해할 수 있도록 알고리즘을 최적화하는 데 유용한 통찰을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09829.pdf

Title: NARF24: Estimating Articulated Object Structure for Implicit Rendering

Original Abstract:
Articulated objects and their representations pose a difficult problem for robots. These objects require not only representations of geometry and texture, but also of the various connections and joint parameters that make up each articulation. We propose a method that learns a common Neural Radiance Field (NeRF) representation across a small number of collected scenes. This representation is combined with a parts-based image segmentation to produce an implicit space part localization, from which the connectivity and joint parameters of the articulated object can be estimated, thus enabling configuration-conditioned rendering.

Translated Abstract:
관절이 있는 물체와 그 표현은 로봇에게 어려운 문제야. 이런 물체는 형태와 질감뿐만 아니라, 각 관절을 구성하는 다양한 연결과 조인트 파라미터에 대한 표현도 필요해.

우리는 몇 개의 수집된 장면에서 공통된 신경 방사장(NeRF) 표현을 학습하는 방법을 제안해. 이 표현은 부품 기반 이미지 분할과 결합돼서 암묵적인 공간 내 부품 위치를 추정할 수 있어. 이렇게 하면 관절이 있는 물체의 연결성과 조인트 파라미터를 추정할 수 있어서, 설정에 맞춘 렌더링이 가능해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09845.pdf

Title: FSL-LVLM: Friction-Aware Safety Locomotion using Large Vision Language Model in Wheeled Robots

Original Abstract:
Wheeled-legged robots offer significant mobility and versatility but face substantial challenges when operating on slippery terrains. Traditional model-based controllers for these robots assume no slipping. While reinforcement learning (RL) helps quadruped robots adapt to different surfaces, recovering from slips remains challenging, especially for systems with few contact points. Estimating the ground friction coefficient is another open challenge. In this paper, we propose a novel friction-aware safety locomotion framework that integrates Large Vision Language Models (LVLMs) with a RL policy. Our approach explicitly incorporates the estimated friction coefficient into the RL policy, enabling the robot to adapt its behavior in advance based on the surface type before reaching it. We introduce a Friction-From-Vision (FFV) module, which leverages LVLMs to estimate ground friction coefficients, eliminating the need for large datasets and extensive training. The framework was validated on a customized wheeled inverted pendulum, and experimental results demonstrate that our framework increases the success rate in completing driving tasks by adjusting speed according to terrain type, while achieving better tracking performance compared to baseline methods. Our framework can be simply integrated with any other RL policies.

Translated Abstract:
바퀴와 다리를 가진 로봇은 이동성과 다재다능함이 뛰어나지만, 미끄러운 지형에서 작동할 때 큰 어려움이 있어. 전통적인 모델 기반 제어기는 미끄러짐이 없다고 가정해. 강화 학습(RL)은 4족 보행 로봇이 다양한 표면에 적응하도록 도와주지만, 미끄러짐에서 회복하는 건 여전히 어렵고, 접촉점이 적은 시스템에서는 더 힘들어. 바닥 마찰 계수를 추정하는 것도 해결해야 할 문제야.

이 논문에서는 대규모 비전 언어 모델(LVLM)과 RL 정책을 통합한 새로운 마찰 인식 안전 보행 프레임워크를 제안해. 우리의 접근법은 추정된 마찰 계수를 RL 정책에 명시적으로 포함시켜서 로봇이 표면 유형에 따라 미리 행동을 조정할 수 있게 해. 우리는 LVLM을 활용해 바닥 마찰 계수를 추정하는 'Friction-From-Vision(FFV)' 모듈을 도입했어. 이 방법은 큰 데이터셋이나 많은 훈련이 필요 없어.

이 프레임워크는 맞춤형 바퀴가 달린 인버티드 진자에서 검증되었고, 실험 결과는 우리의 프레임워크가 지형 유형에 따라 속도를 조정함으로써 주행 작업의 성공률을 높일 수 있음을 보여줘. 또한, 기본 방법과 비교해 더 나은 추적 성능을 얻었어. 우리의 프레임워크는 다른 RL 정책과 쉽게 통합할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09848.pdf

Title: A Comprehensive Survey of PID and Pure Pursuit Control Algorithms for Autonomous Vehicle Navigation

Original Abstract:
The autonomous driving industry is experiencing unprecedented growth, driven by rapid advancements in technology and increasing demand for safer, more efficient transportation. At the heart of this revolution are two critical factors: lateral and longitudinal controls, which together enable vehicles to track complex environments with high accuracy and minimal errors. This paper provides a detailed overview of two of the field's most commonly used and stable control algorithms: proportional-integral-derivative (PID) and pure pursuit. These algorithms have proved useful in solving the issues of lateral (steering) and longitudinal (speed and distance) control in autonomous vehicles. This survey aims to provide researchers, engineers, and industry professionals with an in depth understanding of these fundamental control algorithms, their current applications, and their potential to shape the future of autonomous driving technology.

Translated Abstract:
자율주행 산업은 기술의 빠른 발전과 더 안전하고 효율적인 교통 수단에 대한 수요 증가 덕분에 엄청난 성장을 하고 있어. 이 혁명의 중심에는 두 가지 중요한 요소가 있어. 바로 측면 제어와 종방향 제어인데, 이 두 가지가 함께 차량이 복잡한 환경을 높은 정확도로, 최소한의 오류로 추적할 수 있게 해줘.

이 논문은 이 분야에서 가장 많이 사용되고 안정적인 두 가지 제어 알고리즘인 비례-적분-미분(PID) 제어와 순수 추적 알고리즘에 대해 자세히 설명하고 있어. 이 알고리즘들은 자율주행 차량의 측면(조향) 및 종방향(속도와 거리) 제어 문제를 해결하는 데 유용하다는 걸 증명했어.

이 조사는 연구자들, 엔지니어들, 그리고 산업 전문가들에게 이 기본적인 제어 알고리즘에 대한 깊이 있는 이해와 현재의 적용 사례, 그리고 자율주행 기술의 미래를 형성할 수 있는 가능성에 대해 제공하는 것이 목표야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09849.pdf

Title: Dynamic Layer Detection of a Thin Silk Cloth using DenseTact Optical Tactile Sensors

Original Abstract:
Cloth manipulation is an important aspect of many everyday tasks and remains a significant challenge for robots. While existing research has made strides in tasks like cloth smoothing and folding, many studies struggle with common failure modes (crumpled corners/edges, incorrect grasp configurations) that a preliminary step of cloth layer detection can solve. We present a novel method for classifying the number of grasped cloth layers using a custom gripper equipped with DenseTact 2.0 optical tactile sensors. After grasping a cloth, the gripper performs an anthropomorphic rubbing motion while collecting optical flow, 6-axis wrench, and joint state data. Using this data in a transformer-based network achieves a test accuracy of 98.21% in correctly classifying the number of grasped layers, showing the effectiveness of our dynamic rubbing method. Evaluating different inputs and model architectures highlights the usefulness of using tactile sensor information and a transformer model for this task. A comprehensive dataset of 368 labeled trials was collected and made open-source along with this paper. Our project page is available at this https URL.

Translated Abstract:
천 조작은 일상적인 작업에서 중요한 부분이며 로봇에게는 여전히 큰 도전 과제야. 기존 연구에서는 천을 부드럽게 하거나 접는 작업에서 발전이 있었지만, 많은 연구들이 일반적인 실패 모드(구겨진 모서리/가장자리, 잘못된 잡는 방식)로 어려움을 겪고 있어. 이런 문제는 천 층을 감지하는 초기 단계로 해결할 수 있어.

우리는 DenseTact 2.0 광학 촉각 센서가 장착된 맞춤형 그리퍼를 사용해 잡은 천 층의 개수를 분류하는 새로운 방법을 제안해. 천을 잡고 나면, 그리퍼가 인체와 유사한 문지르는 동작을 하면서 광학 흐름, 6축 렌치, 그리고 관절 상태 데이터를 수집해. 이 데이터를 변환기 기반 네트워크에서 사용하니, 잡은 층의 개수를 98.21%의 정확도로 올바르게 분류할 수 있었어. 이 동적인 문지르는 방법의 효과를 보여주는 결과야.

다양한 입력과 모델 구조를 평가하면서 촉각 센서 정보와 변환기 모델이 이 작업에 유용하다는 걸 강조했어. 368개의 레이블이 붙은 실험 데이터셋을 수집해서 이 논문과 함께 오픈 소스로 제공했어. 우리 프로젝트 페이지는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09850.pdf

Title: Physically-Consistent Parameter Identification of Robots in Contact

Original Abstract:
Accurate inertial parameter identification is crucial for the simulation and control of robots encountering intermittent contact with the environment. Classically, robots' inertial parameters are obtained from CAD models that are not precise (and sometimes not available, e.g., Spot from Boston Dynamics), hence requiring identification. To do that, existing methods require access to contact force measurement, a modality not present in modern quadruped and humanoid robots. This paper presents an alternative technique that utilizes joint current/torque measurements -- a standard sensing modality in modern robots -- to identify inertial parameters without requiring direct contact force measurements. By projecting the whole-body dynamics into the null space of contact constraints, we eliminate the dependency on contact forces and reformulate the identification problem as a linear matrix inequality that can handle physical and geometrical constraints. We compare our proposed method against a common black-box identification mrethod using a deep neural network and show that incorporating physical consistency significantly improves the sample efficiency and generalizability of the model. Finally, we validate our method on the Spot quadruped robot across various locomotion tasks, showcasing its accuracy and generalizability in real-world scenarios over different gaits.

Translated Abstract:
정확한 관성 파라미터 식별은 환경과 간헐적으로 접촉하는 로봇의 시뮬레이션과 제어에 매우 중요해. 보통 로봇의 관성 파라미터는 CAD 모델에서 얻는데, 이 모델이 정확하지 않거나(가끔은 아예 없기도 해, 예를 들어 Boston Dynamics의 Spot 같은 경우) 그래서 식별이 필요해. 

기존 방법들은 접촉력 측정에 접근해야 하는데, 이게 현대의 4족 보행 로봇이나 인간형 로봇에서는 잘 제공되지 않아. 이 논문에서는 관절 전류/토크 측정을 이용하는 대안 기술을 제안해. 이건 현대 로봇에서 일반적으로 사용하는 센서 방식으로, 직접적인 접촉력 측정 없이도 관성 파라미터를 식별할 수 있게 해. 

우리는 전체 몸의 동역학을 접촉 제약의 널 공간에 투영해서 접촉력에 대한 의존성을 없애고, 식별 문제를 물리적 및 기하학적 제약을 처리할 수 있는 선형 행렬 부등식으로 재구성해. 제안한 방법을 딥 뉴럴 네트워크를 사용하는 일반적인 블랙박스 식별 방법과 비교해봤고, 물리적 일관성을 포함하는 것이 샘플 효율성과 모델의 일반화 가능성을 크게 향상시킨다는 걸 보여줬어. 

마지막으로, Spot 4족 로봇을 다양한 보행 과제를 통해 우리의 방법을 검증했어. 현실적인 상황에서 다양한 보행 방식에 대해 정확성과 일반화 가능성을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09852.pdf

Title: A Complete Algorithm for a Moving Target Traveling Salesman Problem with Obstacles

Original Abstract:
The moving target traveling salesman problem with obstacles (MT-TSP-O) is a generalization of the traveling salesman problem (TSP) where, as its name suggests, the targets are moving. A solution to the MT-TSP-O is a trajectory that visits each moving target during a certain time window(s), and this trajectory avoids stationary obstacles. We assume each target moves at a constant velocity during each of its time windows. The agent has a speed limit, and this speed limit is no smaller than any target's speed. This paper presents the first complete algorithm for finding feasible solutions to the MT-TSP-O. Our algorithm builds a tree where the nodes are agent trajectories intercepting a unique sequence of targets within a unique sequence of time windows. We generate each of a parent node's children by extending the parent's trajectory to intercept one additional target, each child corresponding to a different choice of target and time window. This extension consists of planning a trajectory from the parent trajectory's final point in space-time to a moving target. To solve this point-to-moving-target subproblem, we define a novel generalization of a visibility graph called a moving target visibility graph (MTVG). Our overall algorithm is called MTVG-TSP. To validate MTVG-TSP, we test it on 570 instances with up to 30 targets. We implement a baseline method that samples trajectories of targets into points, based on prior work on special cases of the MT-TSP-O. MTVG-TSP finds feasible solutions in all cases where the baseline does, and when the sum of the targets' time window lengths enters a critical range, MTVG-TSP finds a feasible solution with up to 38 times less computation time.

Translated Abstract:
움직이는 목표 여행 판매원 문제와 장애물(MT-TSP-O)은 여행 판매원 문제(TSP)의 확장된 형태야. 이름에서 알 수 있듯이, 목표들이 움직이고 있어. MT-TSP-O의 해결책은 특정 시간 안에 각 이동 목표를 방문하면서, 정적인 장애물을 피하는 경로야. 각 목표는 각 시간 동안 일정한 속도로 움직인다고 가정해. 에이전트는 속도 제한이 있고, 이 속도 제한은 어떤 목표의 속도보다 작지 않아.

이 논문에서는 MT-TSP-O에 대한 첫 번째 완전한 알고리즘을 제시해. 우리의 알고리즘은 노드가 특정 시간 안에 고유한 목표 시퀀스를 가로채는 에이전트 경로로 구성된 트리를 만들어. 부모 노드의 자식 노드는 부모의 경로를 확장해 하나의 추가 목표를 가로채도록 만들어져. 각 자식은 목표와 시간 선택이 다르지. 이 확장은 부모 경로의 마지막 지점에서 이동 목표까지의 경로를 계획하는 것으로 이루어져.

이 포인트-투-이동 목표 하위 문제를 해결하기 위해, 우리는 이동 목표 가시성 그래프(Moving Target Visibility Graph, MTVG)라는 새로운 형태의 가시성 그래프를 정의해. 우리의 전체 알고리즘은 MTVG-TSP라고 불려. MTVG-TSP를 검증하기 위해, 우리는 최대 30개의 목표를 가진 570개의 사례에서 테스트했어. 우리는 MT-TSP-O의 특별한 경우에 대한 이전 연구를 바탕으로 목표의 경로를 점으로 샘플링하는 기준 방법도 구현했어. MTVG-TSP는 기준 방법이 해결할 수 있는 모든 경우에 대해 실행 가능 솔루션을 찾고, 목표들의 시간 창 길이의 합이 특정 범위에 들어가면 MTVG-TSP는 최대 38배 빠른 계산 시간으로 실행 가능한 솔루션을 찾아.

================================================================================

URL:
https://arxiv.org/pdf/2409.09868.pdf

Title: SAFER-Splat: A Control Barrier Function for Safe Navigation with Online Gaussian Splatting Maps

Original Abstract:
SAFER-Splat (Simultaneous Action Filtering and Environment Reconstruction) is a real-time, scalable, and minimally invasive action filter, based on control barrier functions, for safe robotic navigation in a detailed map constructed at runtime using Gaussian Splatting (GSplat). We propose a novel Control Barrier Function (CBF) that not only induces safety with respect to all Gaussian primitives in the scene, but when synthesized into a controller, is capable of processing hundreds of thousands of Gaussians while maintaining a minimal memory footprint and operating at 15 Hz during online Splat training. Of the total compute time, a small fraction of it consumes GPU resources, enabling uninterrupted training. The safety layer is minimally invasive, correcting robot actions only when they are unsafe. To showcase the safety filter, we also introduce SplatBridge, an open-source software package built with ROS for real-time GSplat mapping for robots. We demonstrate the safety and robustness of our pipeline first in simulation, where our method is 20-50x faster, safer, and less conservative than competing methods based on neural radiance fields. Further, we demonstrate simultaneous GSplat mapping and safety filtering on a drone hardware platform using only on-board perception. We verify that under teleoperation a human pilot cannot invoke a collision. Our videos and codebase can be found at this https URL.

Translated Abstract:
SAFER-Splat(동시 행동 필터링 및 환경 재구성)는 제어 장벽 함수에 기반한 실시간, 확장 가능, 최소 침습적인 행동 필터로, Gaussian Splatting(GSplat)을 사용해 런타임에 세밀한 지도를 만들며 안전하게 로봇이 탐색할 수 있도록 돕는 시스템이야.

우리는 새로운 제어 장벽 함수(CBF)를 제안하는데, 이 함수는 장면에 있는 모든 Gaussian 요소에 대해 안전성을 제공해. 그리고 이 CBF를 컨트롤러와 합치면, 수십만 개의 Gaussian을 처리할 수 있으면서도 메모리 사용량이 적고, 온라인 Splat 훈련 중에는 15 Hz로 작동할 수 있어. 전체 계산 시간 중에서 GPU 리소스를 사용하는 부분은 아주 적어서, 훈련이 끊기지 않고 진행될 수 있어. 안전성 레이어는 최소한으로 간섭하며, 로봇의 행동이 안전하지 않을 때만 수정해.

안전 필터를 보여주기 위해 SplatBridge라는 오픈소스 소프트웨어 패키지도 소개해. 이건 ROS로 만들어졌고, 로봇을 위한 실시간 GSplat 맵핑을 지원해. 우리는 먼저 시뮬레이션에서 우리의 방법이 신경 방사 필드를 기반으로 한 경쟁 방법보다 20-50배 더 빠르고, 안전하며, 덜 보수적이라는 걸 보여줬어.

더 나아가, 우리는 드론 하드웨어 플랫폼에서 오직 탑재된 감지기만을 사용해 GSplat 맵핑과 안전 필터링을 동시에 시연했어. 원격 조종 하에서도 인간 조종사가 충돌을 일으킬 수 없다는 걸 확인했어. 우리의 영상과 코드베이스는 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09869.pdf

Title: Critic as Lyapunov function (CALF): a model-free, stability-ensuring agent

Original Abstract:
This work presents and showcases a novel reinforcement learning agent called Critic As Lyapunov Function (CALF) which is model-free and ensures online environment, in other words, dynamical system stabilization. Online means that in each learning episode, the said environment is stabilized. This, as demonstrated in a case study with a mobile robot simulator, greatly improves the overall learning performance. The base actor-critic scheme of CALF is analogous to SARSA. The latter did not show any success in reaching the target in our studies. However, a modified version thereof, called SARSA-m here, did succeed in some learning scenarios. Still, CALF greatly outperformed the said approach. CALF was also demonstrated to improve a nominal stabilizer provided to it. In summary, the presented agent may be considered a viable approach to fusing classical control with reinforcement learning. Its concurrent approaches are mostly either offline or model-based, like, for instance, those that fuse model-predictive control into the agent.

Translated Abstract:
이 연구는 CALF라는 새로운 강화 학습 에이전트를 소개해. CALF는 모델이 필요 없는 방식으로 온라인 환경에서 작동해, 즉 동적 시스템 안정화를 보장해. 온라인이라는 것은 각 학습 에피소드마다 환경이 안정화된다는 의미야. 모바일 로봇 시뮬레이터를 이용한 사례 연구에서 이 점이 크게 도움이 됐어.

CALF의 기본 구조는 SARSA와 비슷해. 하지만 우리 연구에서는 SARSA가 목표에 도달하는 데 성공하지 못했어. 대신에 SARSA를 수정한 SARSA-m은 몇 가지 학습 상황에서 성공을 거두긴 했어. 그래도 CALF가 이 방법보다 훨씬 더 좋은 성과를 냈어. CALF는 주어진 기준 안정기를 개선하는 데도 효과적이었어.

결론적으로, CALF는 고전 제어와 강화 학습을 결합하는 유망한 접근법으로 볼 수 있어. 기존의 방법들은 대부분 오프라인이거나 모델 기반이야, 예를 들어 모델 예측 제어를 에이전트에 결합하는 방식처럼.

================================================================================

URL:
https://arxiv.org/pdf/2409.09870.pdf

Title: TransForce: Transferable Force Prediction for Vision-based Tactile Sensors with Sequential Image Translation

Original Abstract:
Vision-based tactile sensors (VBTSs) provide high-resolution tactile images crucial for robot in-hand manipulation. However, force sensing in VBTSs is underutilized due to the costly and time-intensive process of acquiring paired tactile images and force labels. In this study, we introduce a transferable force prediction model, TransForce, designed to leverage collected image-force paired data for new sensors under varying illumination colors and marker patterns while improving the accuracy of predicted forces, especially in the shear direction. Our model effectively achieves translation of tactile images from the source domain to the target domain, ensuring that the generated tactile images reflect the illumination colors and marker patterns of the new sensors while accurately aligning the elastomer deformation observed in existing sensors, which is beneficial to force prediction of new sensors. As such, a recurrent force prediction model trained with generated sequential tactile images and existing force labels is employed to estimate higher-accuracy forces for new sensors with lowest average errors of 0.69N (5.8\% in full work range) in $x$-axis, 0.70N (5.8\%) in $y$-axis, and 1.11N (6.9\%) in $z$-axis compared with models trained with single images. The experimental results also reveal that pure marker modality is more helpful than the RGB modality in improving the accuracy of force in the shear direction, while the RGB modality show better performance in the normal direction.

Translated Abstract:
비전 기반 촉각 센서(VBTS)는 로봇의 손 안 조작에 중요한 고해상도 촉각 이미지를 제공해. 하지만, VBTS에서 힘 감지는 쌍으로 된 촉각 이미지와 힘 레이블을 얻는 과정이 비싸고 시간이 많이 걸려서 제대로 활용되지 않고 있어. 

이 연구에서는 TransForce라는 전이 가능한 힘 예측 모델을 소개해. 이 모델은 수집한 이미지-힘 쌍 데이터를 활용해서 새로운 센서의 다양한 조명 색상과 마커 패턴에서 힘을 예측할 수 있도록 설계됐어. 특히, 전단 방향에서 힘 예측의 정확도를 높이는 데 초점을 맞추고 있어. 

우리 모델은 원본 도메인의 촉각 이미지를 목표 도메인으로 잘 변환할 수 있어. 그래서 생성된 촉각 이미지는 새로운 센서의 조명 색상과 마커 패턴을 반영하고, 기존 센서에서 관찰된 엘라스토머 변형과 정확하게 일치해. 이게 새로운 센서의 힘 예측에 유리해. 

결과적으로, 생성된 순차적 촉각 이미지와 기존의 힘 레이블로 훈련된 순환 힘 예측 모델을 사용해서 새로운 센서에 대해 더 높은 정확도의 힘을 추정할 수 있었어. 평균 오차는 $x$축에서 0.69N (5.8\%), $y$축에서 0.70N (5.8\%), $z$축에서 1.11N (6.9\%)로, 단일 이미지로 훈련된 모델보다 낮았어. 실험 결과도 순수 마커 모달리티가 RGB 모달리티보다 전단 방향에서 힘의 정확도를 높이는 데 더 도움이 된다는 걸 보여줬어. 반면, RGB 모달리티는 수직 방향에서 더 좋은 성능을 보였어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09871.pdf

Title: Marginalizing and Conditioning Gaussians onto Linear Approximations of Smooth Manifolds with Applications in Robotics

Original Abstract:
We present closed-form expressions for marginalizing and conditioning Gaussians onto linear manifolds, and demonstrate how to apply these expressions to smooth nonlinear manifolds through linearization. Although marginalization and conditioning onto axis-aligned manifolds are well-established procedures, doing so onto non-axis-aligned manifolds is not as well understood. We demonstrate the utility of our expressions through three applications: 1) approximation of the projected normal distribution, where the quality of our linearized approximation increases as problem nonlinearity decreases; 2) covariance extraction in Koopman SLAM, where our covariances are shown to be consistent on a real-world dataset; and 3) covariance extraction in constrained GTSAM, where our covariances are shown to be consistent in simulation.

Translated Abstract:
우리는 가우시안을 선형 다양체에 대해 마진화하고 조건화하는 방법을 간단한 형태로 표현했어. 그리고 이 표현을 부드러운 비선형 다양체에 선형화해서 적용하는 방법도 보여줄 거야. 축 정렬된 다양체에 대한 마진화와 조건화는 잘 알려져 있지만, 비축 정렬된 다양체에 대한 건 아직 잘 이해되지 않았어.

우리가 제안한 표현의 유용성을 세 가지 응용을 통해 보여줄게: 

1) 투영된 정규 분포의 근사화. 여기서는 문제의 비선형성이 줄어들수록 우리의 선형화된 근사 품질이 좋아져.
  
2) 쿠프만 SLAM에서의 공분산 추출. 여기서는 우리의 공분산이 실제 데이터셋에서 일관성이 있음을 보여줘.

3) 제약된 GTSAM에서의 공분산 추출. 이 경우에도 우리의 공분산이 시뮬레이션에서 일관성을 보인다고 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09883.pdf

Title: Robots that Suggest Safe Alternatives

Original Abstract:
Goal-conditioned policies, such as those learned via imitation learning, provide an easy way for humans to influence what tasks robots accomplish. However, these robot policies are not guaranteed to execute safely or to succeed when faced with out-of-distribution requests. In this work, we enable robots to know when they can confidently execute a user's desired goal, and automatically suggest safe alternatives when they cannot. Our approach is inspired by control-theoretic safety filtering, wherein a safety filter minimally adjusts a robot's candidate action to be safe. Our key idea is to pose alternative suggestion as a safe control problem in goal space, rather than in action space. Offline, we use reachability analysis to compute a goal-parameterized reach-avoid value network which quantifies the safety and liveness of the robot's pre-trained policy. Online, our robot uses the reach-avoid value network as a safety filter, monitoring the human's given goal and actively suggesting alternatives that are similar but meet the safety specification. We demonstrate our Safe ALTernatives (SALT) framework in simulation experiments with indoor navigation and Franka Panda tabletop manipulation, and with both discrete and continuous goal representations. We find that SALT is able to learn to predict successful and failed closed-loop executions, is a less pessimistic monitor than open-loop uncertainty quantification, and proposes alternatives that consistently align with those people find acceptable.

Translated Abstract:
목표 조건 정책은 모방 학습을 통해 배운 것처럼, 인간이 로봇이 어떤 작업을 수행할지를 쉽게 조정할 수 있게 해줘. 하지만 이런 로봇 정책은 안전하게 실행되거나, 요청이 일반적이지 않을 때 성공할 거란 보장이 없어. 

이번 연구에서는 로봇이 사용자가 원하는 목표를 자신 있게 실행할 수 있는지 알 수 있도록 하고, 실행할 수 없을 때는 안전한 대안을 자동으로 제안할 수 있게 했어. 우리의 접근 방식은 안전 필터링이라는 제어 이론에서 영감을 받았는데, 여기서는 안전하게 조정하기 위해 로봇의 후보 행동을 최소한으로 수정해. 

우리의 핵심 아이디어는 대안 제안을 행동 공간이 아니라 목표 공간에서 안전 제어 문제로 설정하는 거야. 오프라인에서는 도달 가능성 분석을 사용해서 목표 기반의 도달-회피 가치 네트워크를 계산해. 이 네트워크는 로봇의 사전 훈련된 정책의 안전성과 실행 가능성을 정량화해. 

온라인에서는 로봇이 이 도달-회피 가치 네트워크를 안전 필터로 사용해, 인간이 제시한 목표를 모니터링하고 안전 규격을 충족하면서 비슷한 대안을 적극적으로 제안해. 우리는 이 Safe ALTernatives (SALT) 프레임워크를 실내 내비게이션과 Franka Panda 테이블 조작에 대한 시뮬레이션 실험에서 보여줬고, 이산 및 연속 목표 표현 모두에서 테스트했어. 

결과적으로 SALT는 성공적이고 실패한 폐쇄 루프 실행을 예측하는 데 학습할 수 있었고, 오픈 루프 불확실성 정량화보다 덜 비관적인 모니터가 되며, 사람들이 수용할 수 있는 대안을 일관되게 제안하는 걸 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09895.pdf

Title: Materials Matter: Investigating Functional Advantages of Bio-Inspired Materials via Simulated Robotic Hopping

Original Abstract:
In contrast with the diversity of materials found in nature, most robots are designed with some combination of aluminum, stainless steel, and 3D-printed filament. Additionally, robotic systems are typically assumed to follow basic rigid-body dynamics. However, several examples in nature illustrate how changes in physical material properties yield functional advantages. In this paper, we explore how physical materials (non-rigid bodies) affect the functional performance of a hopping robot. In doing so, we address the practical question of how to model and simulate material properties. Through these simulations we demonstrate that material gradients in the limb system of a single-limb hopper provide functional advantages compared to homogeneous designs. For example, when considering incline ramp hopping, a material gradient with increasing density provides a 35\% reduction in tracking error and a 23\% reduction in power consumption compared to isotropic stainless steel.
By providing bio-inspiration to the rigid limbs in a robotic system, we seek to show that future fabrication of robots should look to leverage the material anisotropies of moduli and density found in nature. This would allow for reduced vibrations in the system and would provide offsets of joint torques and vibrations while protecting their structural integrity against reduced fatigue and wear. This simulation system could inspire future intelligent material gradients of custom-fabricated robotic locomotive devices.

Translated Abstract:
자연에서 발견되는 다양한 물질들과는 달리, 대부분의 로봇은 알루미늄, 스테인리스 스틸, 그리고 3D 프린팅 필라멘트 조합으로 만들어져. 그리고 로봇 시스템은 보통 기본적인 강체 역학을 따른다고 가정해. 하지만 자연의 몇 가지 예를 보면, 물질의 물리적 특성이 변하면 기능적인 이점을 얻을 수 있어. 

이 논문에서는 물리적 물질(비강체)이 점프 로봇의 기능적 성능에 어떻게 영향을 미치는지 탐구해. 이 과정에서 물질의 특성을 모델링하고 시뮬레이션하는 방법에 대해 다뤄. 시뮬레이션을 통해 단일 다리 점프 로봇의 다리 시스템에서 물질의 밀도 변화가 균일한 디자인과 비교해 기능적인 이점을 제공함을 보여줬어. 예를 들어, 경사진 경사면에서 점프할 때, 밀도가 증가하는 물질의 그라디언트를 사용하면 추적 오류가 35% 줄어들고, 전력 소비가 23% 줄어들어.

로봇 시스템의 강직한 다리들에 생물학적 영감을 주면서, 앞으로 로봇 제작 시 자연에서 발견되는 물질의 비등방성(모듈과 밀도)을 활용해야 한다는 점을 보여주고 싶어. 이렇게 하면 시스템의 진동을 줄이고, 관절 토크와 진동을 보완하면서 구조적 무결성을 유지할 수 있어. 이 시뮬레이션 시스템은 맞춤 제작된 로봇 이동 장치의 미래 지능형 물질 그라디언트에 영감을 줄 수 있을 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09899.pdf

Title: Semantic2D: A Semantic Dataset for 2D Lidar Semantic Segmentation

Original Abstract:
This paper presents a 2D lidar semantic segmentation dataset to enhance the semantic scene understanding for mobile robots in different indoor robotics applications. While most existing lidar semantic datasets focus on 3D lidar sensors and autonomous driving scenarios, the proposed 2D lidar semantic dataset is the first public dataset for 2D lidar sensors and mobile robots. It contains data collected in six different indoor environments and has nine categories of typical objects in indoor environments. A novel semi-automatic semantic labeling framework is proposed to provide point-wise annotation for the dataset with minimal human effort. Based on this 2D lidar dataset, a hardware-friendly stochastic semantic segmentation benchmark is proposed to enable 2D lidar sensors to have semantic scene understanding capabilities. A series of segmentation tests are performed to demonstrate that the proposed learning-based segmentation benchmark can achieve more accurate and richer segmentation for each lidar point compared to traditional geometry-based extraction algorithms.

Translated Abstract:
이 논문은 모바일 로봇이 다양한 실내 로봇 응용 프로그램에서 장면을 더 잘 이해할 수 있도록 돕는 2D 라이다 의미 분할 데이터셋을 소개해. 기존의 라이다 의미 데이터셋들은 대부분 3D 라이다 센서와 자율주행 상황에 집중했는데, 이 데이터셋은 2D 라이다 센서와 모바일 로봇을 위한 첫 번째 공개 데이터셋이야. 

이 데이터셋은 여섯 가지 다른 실내 환경에서 수집된 데이터로, 실내에서 자주 보이는 아홉 가지 객체 카테고리를 포함하고 있어. 그리고 최소한의 사람의 노력이 필요하도록 점별 주석을 제공하는 새로운 반자동 의미 라벨링 프레임워크도 제안했어.

이 2D 라이다 데이터셋을 바탕으로 하드웨어 친화적인 확률적 의미 분할 벤치마크를 제안해서 2D 라이다 센서가 장면을 이해할 수 있는 능력을 갖추게 했어. 여러 분할 테스트를 진행해서 이 학습 기반 분할 벤치마크가 전통적인 기하학 기반 추출 알고리즘보다 각 라이다 포인트에 대해 더 정확하고 풍부한 분할을 이룰 수 있다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09904.pdf

Title: Enhancing Visual Inertial SLAM with Magnetic Measurements

Original Abstract:
This paper presents an extension to visual inertial odometry (VIO) by introducing tightly-coupled fusion of magnetometer measurements. A sliding window of keyframes is optimized by minimizing re-projection errors, relative inertial errors, and relative magnetometer orientation errors. The results of IMU orientation propagation are used to efficiently transform magnetometer measurements between frames producing relative orientation constraints between consecutive frames. The soft and hard iron effects are calibrated using an ellipsoid fitting algorithm. The introduction of magnetometer data results in significant reductions in the orientation error and also in recovery of the true yaw orientation with respect to the magnetic north. The proposed framework operates in all environments with slow-varying magnetic fields, mainly outdoors and underwater. We have focused our work on the underwater domain, especially in underwater caves, as the narrow passage and turbulent flow make it difficult to perform loop closures and reset the localization drift. The underwater caves present challenges to VIO due to the absence of ambient light and the confined nature of the environment, while also being a crucial source of fresh water and providing valuable historical records. Experimental results from underwater caves demonstrate the improvements in accuracy and robustness introduced by the proposed VIO extension.

Translated Abstract:
이 논문은 비주얼 관성 오도메트리(VIO)에 자력계 측정을 잘 결합한 확장을 소개해. 키프레임의 슬라이딩 윈도우를 최적화하면서 재투영 오차, 상대 관성 오차, 상대 자력계 방향 오차를 최소화해. IMU 방향 전파 결과를 활용해 자력계 측정을 효율적으로 변환하고, 이로 인해 연속된 프레임 간의 상대 방향 제약을 만들어.

부드러운 철과 단단한 철 효과는 타원체 적합 알고리즘을 이용해 보정해. 자력계 데이터를 도입하면 방향 오차가 크게 줄어들고, 자기 북쪽에 대한 실제 요 방향을 회복할 수 있어. 이 제안된 프레임워크는 주로 야외와 수중에서 느리게 변하는 자기장이 있는 모든 환경에서 작동해.

우리는 특히 수중 동굴에 초점을 맞췄어. 좁은 통로와 격렬한 흐름 때문에 루프 클로저를 수행하고 위치 드리프트를 리셋하는 게 쉽지 않아. 수중 동굴은 주변 조명이 없고 환경이 제한적이라서 VIO에 도전 과제가 되지만, 신선한 물의 중요한 원천이기도 하고 귀중한 역사적 기록을 제공해.

수중 동굴에서의 실험 결과는 제안한 VIO 확장이 정확성과 강인성을 어떻게 개선했는지를 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09918.pdf

Title: Hardware-Accelerated Ray Tracing for Discrete and Continuous Collision Detection on GPUs

Original Abstract:
This paper presents a set of simple and intuitive robot collision detection algorithms that show substantial scaling improvements for high geometric complexity and large numbers of collision queries by leveraging hardware-accelerated ray tracing on GPUs. It is the first leveraging hardware-accelerated ray-tracing for direct volume mesh-to-mesh discrete collision detection and applying it to continuous collision detection. We introduce two methods: Ray-Traced Discrete-Pose Collision Detection for exact robot mesh to obstacle mesh collision detection, and Ray-Traced Continuous Collision Detection for robot sphere representation to obstacle mesh swept collision detection, using piecewise-linear or quadratic B-splines. For robot link meshes totaling 24k triangles and obstacle meshes of over 190k triangles, our methods were up to 3 times faster in batched discrete-pose queries than a state-of-the-art GPU-based method using a sphere robot representation. For the same obstacle mesh scene, our sphere-robot continuous collision detection was up to 9 times faster depending on trajectory batch size. We also performed a detailed measurement of the volume coverage accuracy of various sphere/mesh pose/path representations to provide insight into the tradeoffs between speed and accuracy of different robot collision detection methods.

Translated Abstract:
이 논문은 로봇 충돌 감지를 위한 간단하고 직관적인 알고리즘 세트를 소개해. 이 알고리즘은 하드웨어 가속 레이 트레이싱을 활용해서 기하학적 복잡성이 크고 충돌 쿼리가 많은 경우에도 성능이 크게 향상돼.

우리는 두 가지 방법을 소개해: 
첫 번째는 '레이 트레이스 이산 자세 충돌 감지'로, 로봇 메쉬와 장애물 메쉬 사이의 정확한 충돌 감지를 할 수 있어. 
두 번째는 '레이 트레이스 연속 충돌 감지'로, 로봇을 구형으로 표현해서 장애물 메쉬와의 스윕 충돌 감지를 할 수 있어. 이때 조각별 선형 또는 이차 B-스플라인을 사용해.

로봇 링크 메쉬가 총 24,000개의 삼각형을 가지고 있고, 장애물 메쉬는 190,000개 이상의 삼각형을 가지고 있는데, 우리의 방법은 기존의 최첨단 GPU 기반 방법보다 최대 3배 빠른 성능을 보여줬어. 같은 장애물 메쉬 장면에서, 우리의 구형 로봇 연속 충돌 감지는 경로 배치 크기에 따라 최대 9배 더 빨랐어.

또한, 다양한 구형/메쉬 자세/경로 표현의 부피 커버리지 정확도를 자세히 측정해서 속도와 정확도 사이의 트레이드오프에 대한 통찰력을 제공했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09921.pdf

Title: Towards Real-Time Generation of Delay-Compensated Video Feeds for Outdoor Mobile Robot Teleoperation

Original Abstract:
Teleoperation is an important technology to enable supervisors to control agricultural robots remotely. However, environmental factors in dense crop rows and limitations in network infrastructure hinder the reliability of data streamed to teleoperators. These issues result in delayed and variable frame rate video feeds that often deviate significantly from the robot's actual viewpoint. We propose a modular learning-based vision pipeline to generate delay-compensated images in real-time for supervisors. Our extensive offline evaluations demonstrate that our method generates more accurate images compared to state-of-the-art approaches in our setting. Additionally, we are one of the few works to evaluate a delay-compensation method in outdoor field environments with complex terrain on data from a real robot in real-time. Additional videos are provided at this https URL.

Translated Abstract:
원격 조작은 농업 로봇을 감독자가 멀리서 제어할 수 있게 해주는 중요한 기술이야. 하지만 밀집한 작물 사이의 환경 요인과 네트워크 인프라의 한계 때문에 원격 조작자에게 스트리밍되는 데이터의 신뢰성이 떨어져. 이런 문제로 인해 비디오 피드가 지연되거나 프레임 속도가 변동이 심해서 로봇의 실제 시점과 크게 다르게 나타나곤 해. 

우리는 감독자를 위해 실시간으로 지연 보정된 이미지를 생성하는 모듈형 학습 기반 비전 파이프라인을 제안해. 우리의 광범위한 오프라인 평가 결과, 이 방법이 기존의 최첨단 접근 방식보다 더 정확한 이미지를 생성한다는 걸 보여줬어. 

게다가 우리는 복잡한 지형의 야외 환경에서 실제 로봇의 데이터를 사용해 지연 보정 방법을 평가한 몇 안 되는 연구 중 하나야. 추가 비디오는 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09939.pdf

Title: Real-time Coupled Centroidal Motion and Footstep Planning for Biped Robots

Original Abstract:
This paper presents an algorithm that finds a centroidal motion and footstep plan for a Spring-Loaded Inverted Pendulum (SLIP)-like bipedal robot model substantially faster than real-time. This is achieved with a novel representation of the dynamic footstep planning problem, where each point in the environment is considered a potential foothold that can apply a force to the center of mass to keep it on a desired trajectory. For a biped, up to two such footholds per time step must be selected, and we approximate this cardinality constraint with an iteratively reweighted $l_1$-norm minimization. Along with a linearizing approximation of an angular momentum constraint, this results in a quadratic program can be solved for a contact schedule and center of mass trajectory with automatic gait discovery. A 2 s planning horizon with 13 time steps and 20 surfaces available at each time is solved in 142 ms, roughly ten times faster than comparable existing methods in the literature. We demonstrate the versatility of this program in a variety of simulated environments.

Translated Abstract:
이 논문은 스프링-로드 인버티드 펜듈럼(SLIP)처럼 생긴 이족 로봇 모델을 위해 중심 운동과 발걸음 계획을 실시간보다 훨씬 빠르게 찾는 알고리즘을 제안해. 

이 알고리즘은 동적인 발걸음 계획 문제를 새롭게 표현하는 방식으로 작동해. 여기서 환경의 각 지점은 무게 중심에 힘을 가해주어 원하는 경로를 유지할 수 있는 잠재적인 발판으로 간주돼. 이족 보행 로봇의 경우, 매 시간 단계마다 최대 두 개의 발판을 선택해야 하고, 우리는 이 제약 조건을 반복적으로 가중치를 조정하는 $l_1$-노름 최소화를 통해 근사해.

각속도 제약 조건의 선형 근사와 함께, 이 과정은 접촉 일정과 무게 중심 경로를 자동으로 발견하는 2차 최적화 문제로 이어져. 2초의 계획 시간 동안 13개의 시간 단계와 매 시간마다 20개의 표면을 사용해 142ms에 문제를 해결해. 이는 기존 방법들보다 대략 10배 빠른 속도야. 

우리는 다양한 시뮬레이션 환경에서 이 프로그램의 다재다능함을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09940.pdf

Title: Robots with Attitude: Singularity-Free Quaternion-Based Model-Predictive Control for Agile Legged Robots

Original Abstract:
We present a model-predictive control (MPC) framework for legged robots that avoids the singularities associated with common three-parameter attitude representations like Euler angles during large-angle rotations. Our method parameterizes the robot's attitude with singularity-free unit quaternions and makes modifications to the iterative linear-quadratic regulator (iLQR) algorithm to deal with the resulting geometry. The derivation of our algorithm requires only elementary calculus and linear algebra, deliberately avoiding the abstraction and notation of Lie groups. We demonstrate the performance and computational efficiency of quaternion MPC in several experiments on quadruped and humanoid robots.

Translated Abstract:
우리는 다리 로봇을 위한 모델 예측 제어(MPC) 프레임워크를 제안해. 이 방법은 큰 각도로 회전할 때 유러 각도 같은 일반적인 세 가지 매개변수 자세 표현과 관련된 특이점을 피할 수 있어.

우리 방법은 로봇의 자세를 특이점 없는 단위 쿼터니언으로 매개변수화하고, 그에 따라 발생하는 기하학적 문제를 해결하기 위해 반복 선형-이차 조절기(iLQR) 알고리즘을 수정해. 알고리즘의 유도는 기초적인 미적분학과 선형대수만 필요하고, 고의로 리 군의 추상화나 기호는 피했어.

우리는 쿼터니언 MPC의 성능과 계산 효율성을 여러 실험에서 사족 로봇과 인간형 로봇을 통해 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09941.pdf

Title: ROS2WASM: Bringing the Robot Operating System to the Web

Original Abstract:
The Robot Operating System (ROS) has become the de facto standard middleware in robotics, widely adopted across domains ranging from education to industrial applications. The RoboStack distribution has extended ROS's accessibility by facilitating installation across all major operating systems and architectures, integrating seamlessly with scientific tools such as PyTorch and Open3D. This paper presents ROS2WASM, a novel integration of RoboStack with WebAssembly, enabling the execution of ROS 2 and its associated software directly within web browsers, without requiring local installations. This approach significantly enhances reproducibility and shareability of research, lowers barriers to robotics education, and leverages WebAssembly's robust security framework to protect against malicious code. We detail our methodology for cross-compiling ROS 2 packages into WebAssembly, the development of a specialized middleware for ROS 2 communication within browsers, and the implementation of a web platform available at www.ros2wasm.dev that allows users to interact with ROS 2 environments. Additionally, we extend support to the Robotics Toolbox for Python and adapt its Swift simulator for browser compatibility. Our work paves the way for unprecedented accessibility in robotics, offering scalable, secure, and reproducible environments that have the potential to transform educational and research paradigms.

Translated Abstract:
로봇 운영 체제(ROS)는 로봇 분야에서 사실상 표준 미들웨어가 되었고, 교육부터 산업 응용까지 다양한 분야에서 널리 사용되고 있어. RoboStack 배포판은 모든 주요 운영 체제와 아키텍처에서 설치를 쉽게 할 수 있도록 해서 ROS의 접근성을 높였고, PyTorch나 Open3D 같은 과학적 도구와도 잘 통합돼.

이 논문에서는 ROS2WASM이라는 새로운 통합 방법을 소개할게. 이건 RoboStack과 WebAssembly를 결합한 것으로, 웹 브라우저에서 직접 ROS 2와 관련 소프트웨어를 실행할 수 있게 해줘. 로컬 설치가 필요 없어서 연구의 재현성과 공유 가능성을 크게 높이고, 로봇 교육의 진입 장벽을 낮춰. 또한, WebAssembly의 강력한 보안 프레임워크 덕분에 악성 코드로부터 보호할 수 있어.

우리는 ROS 2 패키지를 WebAssembly로 크로스 컴파일하는 방법, 브라우저 내에서 ROS 2 통신을 위한 특수 미들웨어 개발, 그리고 사용자가 ROS 2 환경과 상호작용할 수 있는 웹 플랫폼(www.ros2wasm.dev) 구현 방법에 대해 자세히 설명할 거야. 게다가 Robotics Toolbox for Python에 대한 지원을 확장하고, Swift 시뮬레이터를 브라우저 호환성에 맞게 조정했어.

이 연구는 로봇 분야의 접근성을 획기적으로 높이는 길을 열어주고, 확장 가능하고 안전하며 재현 가능한 환경을 제공해서 교육과 연구의 패러다임을 변화시킬 잠재력을 가지고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09959.pdf

Title: Mission Planning on Autonomous Avoidance for Spacecraft Confronting Orbital Debris

Original Abstract:
This paper investigates the mission planning problem for spacecraft confronting orbital debris to achieve autonomous avoidance. Firstly, combined with the avoidance requirements, a closed-loop framework of autonomous avoidance for orbital debris is proposed. Under the established model of mission planning, a two-stage planning is proposed to coordinate the conflict between routine tasks and debris avoidance. During the planning for expansion, the temporal constraints for duration actions are handled by the ordering choices. Meanwhile, dynamic resource variables satisfying instantaneous numerical change and continuous linear change are reasoned in the execution of actions. Linear Programming (LP) can solve the bounds of variables in each state, which is used to check the consistency of the interactive constraints on duration and resource. Then, the temporal relaxed planning graph (TRPG) heuristics is rationally developed to guide the plan towards the goal. Finally, the simulation demonstrates that the proposed mission planning strategy can effectively achieve the autonomous debris avoidance of the spacecraft.

Translated Abstract:
이 논문은 우주선이 궤도 잔해를 피하는 자율 회피를 위한 임무 계획 문제를 다루고 있어. 먼저, 회피 요구사항과 함께 궤도 잔해를 위한 자율 회피의 폐쇄 루프 프레임워크를 제안해. 설정된 임무 계획 모델 아래에서, 일상적인 작업과 잔해 회피 간의 충돌을 조정하기 위해 두 단계의 계획을 제안해.

계획을 확장하는 동안, 행동의 지속 시간에 대한 시간 제약은 순서 선택으로 처리돼. 동시에, 순간적인 수치 변화와 지속적인 선형 변화에 맞는 동적 자원 변수가 행동 실행에 대해 고려돼. 선형 프로그래밍(LP)을 사용해서 각 상태의 변수 경계를 해결할 수 있고, 이걸로 지속 시간과 자원에 대한 상호 제약의 일관성을 확인해.

그 다음, 목표를 향해 계획을 안내하기 위해 시간 완화 계획 그래프(TRPG) 휴리스틱이 합리적으로 개발돼. 마지막으로, 시뮬레이션 결과가 제안된 임무 계획 전략이 우주선의 자율 잔해 회피를 효과적으로 달성할 수 있음을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.09967.pdf

Title: Hybrid Aerial-Ground Vehicle Autonomy in GPS-denied Environments

Original Abstract:
The DARPA Subterranean Challenge is leading the development of robots capable of mapping underground mines and tunnels up to 8km in length and identify objects and people. Developing these autonomous abilities paves the way for future planetary cave and surface exploration missions. The Co-STAR team, competing in this challenge, is developing a hybrid aerial-ground vehicle, known as the Rollocopter. The current design of this vehicle is a drone with wheels attached. This allows for the vehicle to roll, actuated by the propellers, and fly only when necessary, hence benefiting from the reduced power consumption of the ground mode and the enhanced mobility of the aerial mode. This thesis focuses on the development and increased robustness of the local planning architecture for the Rollocopter. The first development of thesis is a local planner capable of collision avoidance. The local planning node provides the basic functionality required for the vehicle to navigate autonomously. The next stage was augmenting this with the ability to plan more reliably without localisation. This was then integrated with a hybrid mobility mode capable of rolling and flying to exploit power and mobility benefits of the respective configurations. A traversability analysis algorithm as well as determining the terrain that the vehicle is able to traverse is in the late stages of development for informing the decisions of the hybrid planner. A simulator was developed to test the planning algorithms and improve the robustness of the vehicle to different environments. The results presented in this thesis are related to the mobility of the rollocopter and the range of environments that the vehicle is capable of traversing. Videos are included in which the vehicle successfully navigates through dust-ridden tunnels, horizontal mazes, and areas with rough terrain.

Translated Abstract:
DARPA 지하 도전 과제는 최대 8km 길이의 지하 광산과 터널을 맵핑하고 물체와 사람을 식별할 수 있는 로봇 개발을 이끌고 있어. 이런 자율 능력을 개발하는 것은 미래의 행성 탐사와 동굴 탐사 미션에 큰 도움이 될 거야.

이 도전 과제에 참가하는 Co-STAR 팀은 Rollocopter라는 하이브리드 공중-지상 차량을 개발하고 있어. 이 차량은 바퀴가 달린 드론 형태로 디자인되어 있어. 그래서 차량이 프로펠러로 구동되어 굴러다닐 수 있고, 필요할 때만 날 수 있어. 이렇게 하면 지상 모드에서 전력 소모를 줄이고 공중 모드에서는 이동성을 높일 수 있어.

이 논문은 Rollocopter의 로컬 계획 아키텍처 개발과 강인성을 높이는 데 초점을 맞추고 있어. 첫 번째 개발은 충돌 회피가 가능한 로컬 플래너야. 이 로컬 계획 노드는 차량이 자율적으로 탐색하는 데 필요한 기본 기능을 제공해. 그 다음 단계는 위치 확인 없이 더 신뢰성 있게 계획할 수 있는 능력을 추가하는 거였어.

이후에는 굴러다니고 날 수 있는 하이브리드 이동성 모드와 통합해서 각각의 구성의 전력과 이동성 이점을 활용했어. 차량이 통과할 수 있는 지형을 결정하는 탐색 가능성 분석 알고리즘도 개발 중에 있어. 이 알고리즘은 하이브리드 플래너의 결정을 도와줄 거야.

계획 알고리즘을 테스트하고 다양한 환경에 대한 차량의 강인성을 높이기 위해 시뮬레이터도 개발했어. 이 논문에서 발표된 결과는 Rollocopter의 이동성과 차량이 통과할 수 있는 환경의 범위와 관련이 있어. 차량이 먼지 가득한 터널, 수평 미로, 거친 지형 등을 성공적으로 탐색하는 영상도 포함되어 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09970.pdf

Title: A Non-Linear Model Predictive Task-Space Controller Satisfying Shape Constraints for Tendon-Driven Continuum Robots

Original Abstract:
Tendon-Driven Continuum Robots (TDCRs) have the potential to be used in minimally invasive surgery and industrial inspection, where the robot must enter narrow and confined spaces. We propose a Model Predictive Control (MPC) approach to leverage the non-linear kinematics and redundancy of TDCRs for whole-body collision avoidance, with real-time capabilities for handling inputs at 30Hz. Key to our method's effectiveness is the integration of a nominal Piecewise Constant Curvature (PCC) model for efficient computation of feasible trajectories, with a local feedback controller to handle modeling uncertainty and disturbances. Our experiments in simulation show that our MPC outperforms conventional Jacobian-based controller in position tracking, particularly under disturbances and user-defined shape constraints, while also allowing the incorporation of control limits. We further validate our method on a hardware prototype, showcasing its potential for enhancing the safety of teleoperation tasks.

Translated Abstract:
힘줄 구동 연속 로봇(TDCR)은 최소 침습 수술과 산업 검사에서 좁고 제한된 공간에 들어가야 할 때 사용할 수 있는 가능성이 있어. 우리는 TDCR의 비선형 운동학과 중복성을 활용해서 전체 몸체 충돌 회피를 위한 모델 예측 제어(MPC) 접근 방식을 제안해. 이 방법은 30Hz로 입력을 처리할 수 있는 실시간 능력을 가지고 있어.

우리 방법의 핵심은 효율적인 경로 계산을 위한 기준 조각별 일정 곡률(PCC) 모델을 통합하는 거야. 그리고 모델링 불확실성과 방해를 처리하기 위해 로컬 피드백 컨트롤러도 사용해. 시뮬레이션 실험 결과, 우리의 MPC가 기존의 야코비안 기반 컨트롤러보다 위치 추적에서 성능이 우수하다는 걸 보여줬어. 특히 방해가 있을 때나 사용자가 정의한 형태 제약 조건 아래에서 더욱 두드러졌고, 제어 한계를 포함할 수 있는 점도 장점이야.

또한, 하드웨어 프로토타입에서 우리의 방법을 검증해봤고, 원격 조작 작업의 안전성을 높일 수 있는 가능성을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09971.pdf

Title: A Preliminary Add-on Differential Drive System for MRI-Compatible Prostate Robotic System

Original Abstract:
MRI-targeted biopsy has shown significant advantages over conventional random sextant biopsy, detecting more clinically significant cancers and improving risk stratification. However, needle targeting accuracy, especially in transperineal MRI-guided biopsies, presents a challenge due to needle deflection. This can negatively impact patient outcomes, leading to repeated sampling and inaccurate diagnoses if cancerous tissue isn't properly collected. To address this, we developed a novel differential drive prototype designed to improve needle control and targeting precision. This system, featuring a 2-degree-of-freedom (2-DOF) MRI-compatible cooperative needle driver, distances the robot from the MRI imaging area, minimizing image artifacts and distortions. By using two motors for simultaneous needle insertion and rotation without relative movement, the design reduces MRI interference. In this work, we introduced two mechanical differential drive designs: the ball screw/spline and lead screw/bushing types, and explored both hollow-type and side-pulley differentials. Validation through low-resolution rapid-prototyping demonstrated the feasibility of differential drives in prostate biopsies, with the custom hollow-type hybrid ultrasonic motor (USM) achieving a rotary speed of 75 rpm. The side-pulley differential further increased the speed to 168 rpm, ideal for needle rotation applications. Accuracy assessments showed minimal errors in both insertion and rotation motions, indicating that this proof-of-concept design holds great promise for further development. Ultimately, the differential drive offers a promising solution to the critical issue of needle targeting accuracy in MRI-guided prostate biopsies.

Translated Abstract:
MRI를 사용한 생검은 전통적인 무작위 생검보다 훨씬 더 많은 중요한 암을 발견하고, 위험도 분류를 개선하는 데 큰 장점이 있어. 하지만 바늘을 목표로 하는 정확성, 특히 경직장 MRI 유도 생검에서 바늘이 휘어지는 문제 때문에 어려움이 있어. 이렇게 되면 환자 결과에 부정적인 영향을 줄 수 있고, 암 조직을 제대로 수집하지 못하면 재검사가 필요하고 잘못된 진단이 나올 수도 있어. 

그래서 우리는 바늘 조정과 타겟 정확성을 높이기 위해 새로운 차동 구동 프로토타입을 개발했어. 이 시스템은 2자유도(2-DOF) MRI 호환 협동 바늘 드라이버를 사용하고, 로봇을 MRI 이미징 영역에서 멀리 떨어뜨려서 이미지 왜곡과 아티팩트를 최소화해. 두 개의 모터를 사용해서 바늘을 동시에 삽입하고 회전시킬 수 있어서, 상대적인 움직임 없이 MRI 간섭을 줄일 수 있어.

이번 연구에서는 두 가지 기계적 차동 구동 설계를 소개했어: 볼 스크류/스플라인 타입과 리드 스크류/부싱 타입이야. 그리고 홀 타입과 측면 풀리 차동 장치도 탐색했어. 저해상도 신속 프로토타입을 통해 전립선 생검에서 차동 구동의 가능성을 검증했는데, 맞춤형 홀 타입 하이브리드 초음파 모터(USM)는 75 rpm의 회전 속도를 달성했어. 측면 풀리 차동 장치는 속도를 168 rpm으로 더 높여서 바늘 회전 응용에 이상적이야. 

정확성 평가 결과, 삽입과 회전 동작 모두에서 오류가 최소화된 것으로 나타났어. 이 개념 증명 디자인이 앞으로 더 발전할 가능성이 크다는 걸 보여줘. 궁극적으로 이 차동 구동 시스템은 MRI 유도 전립선 생검에서 바늘 목표 정확성 문제를 해결할 수 있는 유망한 솔루션이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09972.pdf

Title: Securing the Future: Exploring Privacy Risks and Security Questions in Robotic Systems

Original Abstract:
The integration of artificial intelligence, especially large language models in robotics, has led to rapid advancements in the field. We are now observing an unprecedented surge in the use of robots in our daily lives. The development and continual improvements of robots are moving at an astonishing pace. Although these remarkable improvements facilitate and enhance our lives, several security and privacy concerns have not been resolved yet. Therefore, it has become crucial to address the privacy and security threats of robotic systems while improving our experiences. In this paper, we aim to present existing applications and threats of robotics, anticipated future evolution, and the security and privacy issues they may imply. We present a series of open questions for researchers and practitioners to explore further.

Translated Abstract:
인공지능, 특히 대형 언어 모델이 로봇에 통합되면서 이 분야가 빠르게 발전하고 있어. 이제 로봇이 우리 일상에서 전례 없는 속도로 사용되고 있어. 로봇 기술의 발전과 지속적인 개선이 정말 놀라운 속도로 진행되고 있어. 

하지만 이런 멋진 발전들이 우리의 삶을 편리하게 만들어 주는 반면, 몇 가지 보안과 개인정보 보호 문제는 아직 해결되지 않았어. 그래서 로봇 시스템의 경험을 개선하는 동시에 프라이버시와 보안 위협을 해결하는 게 정말 중요해. 

이 논문에서는 로봇의 기존 응용 프로그램과 위협, 앞으로의 발전 방향, 그리고 이와 관련된 보안 및 개인정보 문제를 다룰 거야. 연구자들과 실무자들이 더 깊이 탐구할 수 있도록 여러 가지 열린 질문도 제시할 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09975.pdf

Title: Constrained Bandwidth Observation Sharing for Multi-Robot Navigation in Dynamic Environments via Intelligent Knapsack

Original Abstract:
Multi-robot navigation is increasingly crucial in various domains, including disaster response, autonomous vehicles, and warehouse and manufacturing automation. Robot teams often must operate in highly dynamic environments and under strict bandwidth constraints imposed by communication infrastructure, rendering effective observation sharing within the system a challenging problem. This paper presents a novel optimal communication scheme, Intelligent Knapsack (iKnap), for multi-robot navigation in dynamic environments under bandwidth constraints. We model multi-robot communication as belief propagation in a graph of inferential agents. We then formulate the combinatorial optimization for observation sharing as a 0/1 knapsack problem, where each potential pairwise communication between robots is assigned a decision-making utility to be weighed against its bandwidth cost, and the system has some cumulative bandwidth limit. Compared to state-of-the-art broadcast-based optimal communication schemes, iKnap yields significant improvements in navigation performance with respect to scenario complexity while maintaining a similar runtime. Furthermore, iKnap utilizes allocated bandwidth and observational resources more efficiently than existing approaches, especially in very low-resource and high-uncertainty settings. Based on these results, we claim that the proposed method enables more robust collaboration for multi-robot teams in real-world navigation problems.

Translated Abstract:
다중 로봇 내비게이션은 재난 대응, 자율주행 차량, 창고 및 제조 자동화 등 여러 분야에서 점점 더 중요해지고 있어. 로봇 팀은 종종 매우 동적인 환경에서 작업해야 하고, 통신 인프라로 인해 엄격한 대역폭 제약이 있어. 그래서 시스템 내에서 효과적으로 관찰 정보를 공유하는 게 어려운 문제야.

이 논문에서는 동적인 환경에서 대역폭 제약을 고려한 다중 로봇 내비게이션을 위한 새로운 최적 통신 방식인 '지능형 배낭(iKnap)'을 제안해. 우리는 다중 로봇 통신을 추론 에이전트의 그래프에서 믿음 전파로 모델링해. 그리고 관찰 공유를 위한 조합 최적화를 0/1 배낭 문제로 설정해. 여기서 각 로봇 간의 잠재적인 쌍방 통신은 결정 유틸리티를 부여받아 대역폭 비용과 비교되며, 시스템에는 누적 대역폭 한도가 있어.

최신 방송 기반 최적 통신 방식과 비교했을 때, iKnap은 시나리오 복잡성에 대한 내비게이션 성능을 크게 개선하면서 비슷한 실행 시간을 유지해. 게다가 iKnap은 기존 방식보다 할당된 대역폭과 관찰 자원을 더 효율적으로 활용해, 특히 자원이 아주 적고 불확실성이 높은 상황에서 효과적이야. 이런 결과를 바탕으로 우리는 제안된 방법이 실제 내비게이션 문제에서 다중 로봇 팀의 더 강력한 협력을 가능하게 한다고 주장해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09997.pdf

Title: ViewActive: Active viewpoint optimization from a single image

Original Abstract:
When observing objects, humans benefit from their spatial visualization and mental rotation ability to envision potential optimal viewpoints based on the current observation. This capability is crucial for enabling robots to achieve efficient and robust scene perception during operation, as optimal viewpoints provide essential and informative features for accurately representing scenes in 2D images, thereby enhancing downstream tasks.
To endow robots with this human-like active viewpoint optimization capability, we propose ViewActive, a modernized machine learning approach drawing inspiration from aspect graph, which provides viewpoint optimization guidance based solely on the current 2D image input. Specifically, we introduce the 3D Viewpoint Quality Field (VQF), a compact and consistent representation for viewpoint quality distribution similar to an aspect graph, composed of three general-purpose viewpoint quality metrics: self-occlusion ratio, occupancy-aware surface normal entropy, and visual entropy. We utilize pre-trained image encoders to extract robust visual and semantic features, which are then decoded into the 3D VQF, allowing our model to generalize effectively across diverse objects, including unseen categories.The lightweight ViewActive network (72 FPS on a single GPU) significantly enhances the performance of state-of-the-art object recognition pipelines and can be integrated into real-time motion planning for robotic applications. Our code and dataset are available here: this https URL

Translated Abstract:
물체를 관찰할 때, 사람들은 공간 시각화와 정신 회전 능력을 활용해 현재 관찰을 바탕으로 최적의 시점을 상상할 수 있어. 이 능력은 로봇이 작동 중에 효율적이고 견고한 장면 인식을 할 수 있도록 도와줘. 최적의 시점은 2D 이미지에서 장면을 정확하게 표현하는 데 필요한 중요한 정보와 특징을 제공해, 이후 작업을 더 잘 수행할 수 있게 해줘.

우리는 로봇에게 사람처럼 능동적으로 시점을 최적화하는 능력을 부여하기 위해, ViewActive라는 새로운 머신 러닝 방법을 제안해. 이 방법은 현재 2D 이미지 입력만을 바탕으로 시점 최적화 지침을 제공하는 aspect graph에서 영감을 받았어. 구체적으로는, 3D Viewpoint Quality Field (VQF)를 도입했는데, 이건 aspect graph와 비슷하게 시점 품질 분포를 간결하고 일관되게 나타내는 거야. 이 VQF는 자기 가림 비율, 점유 인식 표면 법선 엔트로피, 시각 엔트로피라는 세 가지 일반적인 시점 품질 메트릭으로 구성돼.

우리는 미리 학습된 이미지 인코더를 사용해 강력한 시각적이고 의미 있는 특징을 추출하고, 이를 3D VQF로 디코딩해. 이렇게 하면 우리 모델이 다양한 물체, 심지어 보지 못한 카테고리에서도 효과적으로 일반화할 수 있어. 이 경량 ViewActive 네트워크는 (단일 GPU에서 72 FPS) 최첨단 물체 인식 파이프라인의 성능을 크게 향상시키고, 로봇 응용을 위한 실시간 모션 계획에 통합될 수 있어. 우리의 코드와 데이터셋은 여기에 있어: 이 URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.10000.pdf

Title: Development and Testing of a Vine Robot for Urban Search and Rescue in Confined Rubble Environments

Original Abstract:
The request for fast response and safe operation after natural and man-made disasters in urban environments has spurred the development of robotic systems designed to assist in search and rescue operations within complex rubble sites. Traditional Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) face significant limitations in such confined and obstructed environments. This paper introduces a novel vine robot designed to navigate dense rubble, drawing inspiration from natural growth mechanisms found in plants. Unlike conventional robots, vine robots are soft robots that can grow by everting their material, allowing them to navigate through narrow spaces and obstacles. The prototype presented in this study incorporates pneumatic muscles for steering and oscillation, an equation-based robot length control plus feedback pressure regulating system for extending and retracting the robot body. We conducted a series of controlled experiments in an artificial rubble testbed to assess the robot performance under varying environmental conditions and robot parameters, including volume ratio, environmental weight, oscillation, and steering. The results show that the vine robot can achieve significant penetration depths in cluttered environments with mixed obstacle sizes and weights, and can maintain repeated trajectories, demonstrating potential for mapping and navigating complex underground paths. Our findings highlight the suitability of the vine robot for urban search and rescue missions, with further research planned to enhance its robustness and deployability in real-world scenarios.

Translated Abstract:
자연 재해와 인위적 재해 후에 도시 환경에서 빠르게 반응하고 안전하게 작동할 수 있는 로봇 시스템의 필요성이 커지고 있어. 이 로봇들은 복잡한 잔해 속에서 수색 및 구조 작업을 도와주기 위해 개발됐어. 전통적인 드론(UAV)이나 무인 지상 차량(UGV)은 이런 좁고 장애물이 많은 환경에서 한계가 많아.

이 논문에서는 밀집된 잔해를 탐색할 수 있도록 설계된 새로운 덩굴 로봇을 소개해. 이 로봇은 식물의 자연 성장 방식을 모델로 삼았어. 일반적인 로봇과 달리, 덩굴 로봇은 부드러운 로봇으로, 재료를 뒤집어가면서 성장할 수 있어서 좁은 공간과 장애물을 잘 통과할 수 있어. 이번 연구에서 제시된 프로토타입은 방향 조정과 진동을 위한 공압 근육, 로봇 길이 제어를 위한 방정식 기반 시스템, 그리고 로봇 몸체를 늘리거나 줄이는 피드백 압력 조절 시스템을 포함하고 있어.

우리는 인위적으로 만든 잔해 테스트베드에서 다양한 환경 조건과 로봇 매개변수(부피 비율, 환경 무게, 진동, 방향 조정 등)에 따라 로봇 성능을 평가하는 일련의 통제된 실험을 진행했어. 결과적으로 덩굴 로봇은 혼합된 장애물 크기와 무게가 있는 복잡한 환경에서 깊숙이 침투할 수 있고, 반복적인 경로를 유지할 수 있다는 것을 보여줬어. 이건 복잡한 지하 경로를 매핑하고 탐색하는 데 큰 가능성을 보여줘.

우리의 연구 결과는 덩굴 로봇이 도시 수색 및 구조 임무에 적합하다는 것을 강조하고 있어. 앞으로 이 로봇의 내구성과 실제 상황에서의 배치를 개선하기 위한 추가 연구가 계획되고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10009.pdf

Title: GA-TEB: Goal-Adaptive Framework for Efficient Navigation Based on Goal Lines

Original Abstract:
In crowd navigation, the local goal plays a crucial role in trajectory initialization, optimization, and evaluation. Recognizing that when the global goal is distant, the robot's primary objective is avoiding collisions, making it less critical to pass through the exact local goal point, this work introduces the concept of goal lines, which extend the traditional local goal from a single point to multiple candidate lines. Coupled with a topological map construction strategy that groups obstacles to be as convex as possible, a goal-adaptive navigation framework is proposed to efficiently plan multiple candidate trajectories. Simulations and experiments demonstrate that the proposed GA-TEB framework effectively prevents deadlock situations, where the robot becomes frozen due to a lack of feasible trajectories in crowded environments. Additionally, the framework greatly increases planning frequency in scenarios with numerous non-convex obstacles, enhancing both robustness and safety.

Translated Abstract:
군중 속에서의 내비게이션에서, 로컬 목표는 경로 초기화, 최적화, 평가에 매우 중요한 역할을 해. 글로벌 목표가 멀리 있을 때, 로봇의 주요 목표는 충돌을 피하는 거라서 정확한 로컬 목표 지점을 지나가는 게 그렇게 중요하지 않아. 그래서 이 연구에서는 목표 지점을 단일 지점에서 여러 후보 선으로 확장하는 '목표 선' 개념을 도입했어.

또한, 장애물을 가능한 한 볼록하게 그룹화하는 위상 맵 구축 전략과 결합해서, 여러 후보 경로를 효율적으로 계획할 수 있는 목표 적응형 내비게이션 프레임워크를 제안했어. 시뮬레이션과 실험 결과, 제안된 GA-TEB 프레임워크가 군중이 많은 환경에서 로봇이 실행 가능한 경로가 없어 정지해버리는 '교착 상태'를 효과적으로 방지한다는 걸 보여줬어. 그리고 이 프레임워크는 비볼록 장애물이 많은 상황에서도 계획 빈도를 크게 높여서, 강건성과 안전성을 모두 향상시켜줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.10015.pdf

Title: RPC: A Modular Framework for Robot Planning, Control, and Deployment

Original Abstract:
This paper presents an open-source, lightweight, yet comprehensive software framework, named RPC, which integrates physics-based simulators, planning and control libraries, debugging tools, and a user-friendly operator interface. RPC enables users to thoroughly evaluate and develop control algorithms for robotic systems. While existing software frameworks provide some of these capabilities, integrating them into a cohesive system can be challenging and cumbersome. To overcome this challenge, we have modularized each component in RPC to ensure easy and seamless integration or replacement with new modules. Additionally, our framework currently supports a variety of model-based planning and control algorithms for robotic manipulators and legged robots, alongside essential debugging tools, making it easier for users to design and execute complex robotics tasks. The code and usage instructions of RPC are available at this https URL.

Translated Abstract:
이 논문에서는 RPC라는 이름의 오픈소스 소프트웨어 프레임워크를 소개해. 이 프레임워크는 물리 기반 시뮬레이터, 계획 및 제어 라이브러리, 디버깅 도구, 그리고 사용자 친화적인 조작 인터페이스를 통합하고 있어. RPC는 사용자들이 로봇 시스템의 제어 알고리즘을 철저하게 평가하고 개발할 수 있게 해줘.

기존의 소프트웨어 프레임워크들은 이런 기능들을 일부 제공하지만, 이걸 하나의 통합된 시스템으로 만드는 건 어려운 경우가 많아. 그래서 우리는 RPC의 각 구성 요소를 모듈화해서 새로운 모듈과 쉽게 통합하거나 교체할 수 있도록 했어.

또한, 우리 프레임워크는 로봇 조작기와 다리 로봇을 위한 다양한 모델 기반 계획 및 제어 알고리즘을 지원하고, 필수적인 디버깅 도구도 포함되어 있어서 사용자가 복잡한 로봇 작업을 설계하고 실행하는 데 더 수월해. RPC의 코드와 사용 설명서는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10019.pdf

Title: Learning Agile Swimming: An End-to-End Approach without CPGs

Original Abstract:
The pursuit of agile and efficient underwater robots, especially bio-mimetic robotic fish, has been impeded by challenges in creating motion controllers that are able to fully exploit their hydrodynamic capabilities. This paper addresses these challenges by introducing a novel, model-free, end-to-end control framework that leverages Deep Reinforcement Learning (DRL) to enable agile and energy-efficient swimming of robotic fish. Unlike existing methods that rely on predefined trigonometric swimming patterns like Central Pattern Generators (CPG), our approach directly outputs low-level actuator commands without strong constraint, enabling the robotic fish to learn agile swimming behaviors. In addition, by integrating a high-performance Computational Fluid Dynamics (CFD) simulator with innovative sim-to-real strategies, such as normalized density matching and servo response matching, the proposed framework significantly mitigates the sim-to-real gap, facilitating direct transfer of control policies to real-world environments without fine-tuning. Comparative experiments demonstrate that our method achieves faster swimming speeds, smaller turning radii, and reduced energy consumption compared to the conventional CPG-PID-based controllers. Furthermore, the proposed framework shows promise in addressing complex tasks in diverse scenario, paving the way for more effective deployment of robotic fish in real aquatic environments.

Translated Abstract:
수중 로봇, 특히 생체 모방 로봇 물고기를 빠르고 효율적으로 만드는 데 어려움이 많아. 이 연구에서는 물리적 특성을 최대한 활용할 수 있는 모션 컨트롤러를 만드는 게 도전 과제라고 말해. 

우리는 새로운 모델 없는 엔드 투 엔드 제어 프레임워크를 소개하는데, 이건 딥 강화 학습(Deep Reinforcement Learning, DRL)을 활용해서 로봇 물고기가 민첩하고 에너지 효율적으로 수영할 수 있게 해. 기존의 방법들은 중앙 패턴 생성기(Central Pattern Generators, CPG) 같은 미리 정해진 삼각함수 수영 패턴에 의존하는데, 우리 방법은 강한 제약 없이 저수준 액추에이터 명령을 바로 출력해 로봇 물고기가 민첩한 수영 행동을 배울 수 있게 해.

또한, 고성능의 전산 유체 역학(Computational Fluid Dynamics, CFD) 시뮬레이터와 노멀라이즈드 밀도 매칭, 서보 반응 매칭 같은 혁신적인 시뮬레이션-실제 전이 전략을 통합해서, 제안한 프레임워크가 시뮬레이션과 실제 간의 격차를 크게 줄여줘. 그래서 제어 정책을 실제 환경에 바로 적용할 수 있게 돼, 세밀한 조정 없이 말이지.

비교 실험에서는 우리 방법이 기존의 CPG-PID 기반 컨트롤러보다 더 빠른 수영 속도, 작은 회전 반경, 그리고 낮은 에너지 소비를 달성한다는 걸 보여줬어. 게다가, 제안한 프레임워크는 다양한 상황에서 복잡한 과제를 해결하는 데도 가능성을 보여줘. 이건 로봇 물고기를 실제 수중 환경에 더 효과적으로 배치할 수 있는 길을 열어줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.10024.pdf

Title: Highly dynamic physical interaction for robotics: design and control of an active remote center of compliance

Original Abstract:
Robot interaction control is often limited to low dynamics or low flexibility, depending on whether an active or passive approach is chosen. In this work, we introduce a hybrid control scheme that combines the advantages of active and passive interaction control. To accomplish this, we propose the design of a novel Active Remote Center of Compliance (ARCC), which is based on a passive and active element which can be used to directly control the interaction forces. We introduce surrogate models for a dynamic comparison against purely robot-based interaction schemes. In a comparative validation, ARCC drastically improves the interaction dynamics, leading to an increase in the motion bandwidth of up to 31 times. We introduce further our control approach as well as the integration in the robot controller. Finally, we analyze ARCC on different industrial benchmarks like peg-in-hole, top-hat rail assembly and contour following problems and compare it against the state of the art, to highlight the dynamic and flexibility. The proposed system is especially suited if the application requires a low cycle time combined with a sensitive manipulation.

Translated Abstract:
로봇 상호작용 제어는 주로 능동적 접근 방식이나 수동적 접근 방식에 따라 저역학적 또는 저유연성으로 제한돼 있어. 이 연구에서는 능동적 상호작용 제어와 수동적 상호작용 제어의 장점을 결합한 하이브리드 제어 방식을 소개해. 

이걸 위해, 우리는 새로운 능동 원격 유연성 중심(ARCC)을 디자인했어. 이건 직접 상호작용 힘을 제어할 수 있는 수동적이고 능동적인 요소를 기반으로 하고 있어. 그리고 순수 로봇 기반 상호작용 방식과 비교하기 위한 대체 모델도 소개해. 

비교 검증 결과, ARCC는 상호작용 역학을 크게 개선해서 최대 31배까지 모션 대역폭을 증가시켰어. 우리의 제어 접근 방식과 로봇 컨트롤러에 통합하는 방법도 설명했어. 

마지막으로, ARCC를 다양한 산업 벤치마크(예: 핀에 구멍 넣기, 모자 레일 조립, 윤곽 따라가기 문제)에서 분석하고 최신 기술과 비교해서 동적 성능과 유연성을 강조했어. 제안한 시스템은 낮은 사이클 타임과 민감한 조작이 필요한 경우에 특히 적합해.

================================================================================

URL:
https://arxiv.org/pdf/2409.10027.pdf

Title: E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation with Language Models

Original Abstract:
Large language models (LLMs) have shown significant potential in guiding embodied agents to execute language instructions across a range of tasks, including robotic manipulation and navigation. However, existing methods are primarily designed for static environments and do not leverage the agent's own experiences to refine its initial plans. Given that real-world environments are inherently stochastic, initial plans based solely on LLMs' general knowledge may fail to achieve their objectives, unlike in static scenarios. To address this limitation, this study introduces the Experience-and-Emotion Map (E2Map), which integrates not only LLM knowledge but also the agent's real-world experiences, drawing inspiration from human emotional responses. The proposed methodology enables one-shot behavior adjustments by updating the E2Map based on the agent's experiences. Our evaluation in stochastic navigation environments, including both simulations and real-world scenarios, demonstrates that the proposed method significantly enhances performance in stochastic environments compared to existing LLM-based approaches. Code and supplementary materials are available at this https URL.

Translated Abstract:
대형 언어 모델(LLM)은 로봇 조작이나 내비게이션 같은 여러 작업에서 언어 지시를 수행하도록 몸체를 가진 에이전트를 안내하는 데 큰 잠재력을 보여줬어. 하지만 기존 방법들은 주로 정적인 환경에 맞춰져 있어서, 에이전트의 경험을 활용해 초기 계획을 다듬지 못해.

실제 환경은 본질적으로 불확실성이 있기 때문에, LLM의 일반적인 지식만으로 세운 초기 계획은 정적인 상황과 달리 목표를 달성하지 못할 수도 있어. 이 문제를 해결하기 위해, 이 연구에서는 경험과 감정 지도를 뜻하는 E2Map을 소개해. E2Map은 LLM의 지식뿐만 아니라 에이전트의 실제 경험도 통합해, 인간의 감정 반응에서 영감을 받았어.

제안된 방법론은 에이전트의 경험에 기반해 E2Map을 업데이트함으로써 단 한 번의 행동 조정이 가능하게 해. 우리가 불확실한 내비게이션 환경에서 시뮬레이션과 실제 상황을 포함한 평가를 했는데, 제안한 방법이 기존 LLM 기반 접근법에 비해 불확실한 환경에서 성능을 크게 향상시킨다는 것을 보여줬어. 코드와 보조 자료는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10032.pdf

Title: Embodiment-Agnostic Action Planning via Object-Part Scene Flow

Original Abstract:
Observing that the key for robotic action planning is to understand the target-object motion when its associated part is manipulated by the end effector, we propose to generate the 3D object-part scene flow and extract its transformations to solve the action trajectories for diverse embodiments. The advantage of our approach is that it derives the robot action explicitly from object motion prediction, yielding a more robust policy by understanding the object motions. Also, beyond policies trained on embodiment-centric data, our method is embodiment-agnostic, generalizable across diverse embodiments, and being able to learn from human demonstrations. Our method comprises three components: an object-part predictor to locate the part for the end effector to manipulate, an RGBD video generator to predict future RGBD videos, and a trajectory planner to extract embodiment-agnostic transformation sequences and solve the trajectory for diverse embodiments. Trained on videos even without trajectory data, our method still outperforms existing works significantly by 27.7% and 26.2% on the prevailing virtual environments MetaWorld and Franka-Kitchen, respectively. Furthermore, we conducted real-world experiments, showing that our policy, trained only with human demonstration, can be deployed to various embodiments.

Translated Abstract:
로봇의 행동 계획에서 중요한 건, 로봇의 끝 부분이 조작하는 대상 물체의 움직임을 이해하는 거야. 그래서 우리는 3D 물체-부품 장면 흐름을 생성하고, 그 변환을 추출해서 다양한 형태의 행동 경로를 해결하는 방법을 제안해. 

우리 방법의 장점은 물체의 움직임 예측에서 로봇 행동을 명확하게 이끌어 낸다는 거야. 그래서 물체의 동작을 이해함으로써 더 강력한 정책을 만들어낼 수 있어. 그리고 우리의 방법은 특정 형태에 국한되지 않고, 다양한 형태에 일반화할 수 있으며, 인간의 시연에서 배울 수 있어. 

우리 방법은 세 가지 구성 요소로 이루어져 있어: 엔드 이펙터가 조작할 부품을 찾는 물체-부품 예측기, 미래의 RGBD 비디오를 예측하는 RGBD 비디오 생성기, 그리고 다양한 형태를 위한 경로를 해결하는 경로 계획기. 경로 데이터 없이 비디오로만 훈련했는데도, 우리의 방법은 기존 연구보다 각각 27.7%와 26.2% 더 나은 성능을 보였어, 메타월드와 프랑카-키친 같은 주류 가상 환경에서 말이야. 

더 나아가, 실제 실험도 진행했는데, 인간의 시연만으로 훈련한 정책이 다양한 형태에 배포될 수 있다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10049.pdf

Title: A Social Force Model for Multi-Agent Systems With Application to Robots Traversal in Cluttered Environments

Original Abstract:
This letter presents a model to address the collaborative effects in multi-agent systems from the perspective of microscopic mechanism. The model utilizes distributed control for robot swarms in traversal applications. Inspired by pedestrian planning dynamics, the model employs three types of forces to regulate the behavior of agents: intrinsic propulsion, interaction among agents, and repulsion from obstacles. These forces are able to balance the convergence, divergence and avoidance effects among agents. Additionally, we present a planning and decision method based on resultant forces to enable real-world deployment of the model. Experimental results demonstrate the effectiveness on system path optimization in unknown cluttered environments. The sensor data is swiftly digital filtered and the data transmitted is significantly compressed. Consequently, the model has low computation costs and minimal communication loads, thereby promoting environmental adaptability and system scalability.

Translated Abstract:
이 연구에서는 미세한 메커니즘 관점에서 다중 에이전트 시스템의 협력 효과를 다룬 모델을 제안해. 이 모델은 로봇 군집이 이동할 때 사용할 수 있는 분산 제어 방식을 활용해.

보행자 계획 동역학에서 영감을 받아서, 이 모델은 에이전트의 행동을 조절하기 위해 세 가지 힘을 사용해: 내적 추진력, 에이전트 간 상호작용, 그리고 장애물에서의 반발력. 이 힘들은 에이전트 간의 수렴, 발산, 회피 효과를 균형 있게 조절할 수 있어.

또한, 결과 힘을 기반으로 한 계획 및 결정 방법도 제시해서 실제 환경에 모델을 적용할 수 있게 해. 실험 결과는 복잡한 환경에서 시스템 경로 최적화에 효과적임을 보여줬어. 센서 데이터는 빠르게 디지털 필터링되고, 전송되는 데이터는 상당히 압축돼. 그래서 이 모델은 계산 비용이 낮고 통신 부담이 적어서 환경에 잘 적응하고 시스템 확장성도 좋아.

================================================================================

URL:
https://arxiv.org/pdf/2409.10078.pdf

Title: IRIS: Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis

Original Abstract:
Recent advancements in large language and vision-language models have significantly enhanced multimodal understanding, yet translating high-level linguistic instructions into precise robotic actions in 3D space remains challenging. This paper introduces IRIS (Interactive Responsive Intelligent Segmentation), a novel training-free multimodal system for 3D affordance segmentation, alongside a benchmark for evaluating interactive language-guided affordance in everyday environments. IRIS integrates a large multimodal model with a specialized 3D vision network, enabling seamless fusion of 2D and 3D visual understanding with language comprehension. To facilitate evaluation, we present a dataset of 10 typical indoor environments, each with 50 images annotated for object actions and 3D affordance segmentation. Extensive experiments demonstrate IRIS's capability in handling interactive 3D affordance segmentation tasks across diverse settings, showcasing competitive performance across various metrics. Our results highlight IRIS's potential for enhancing human-robot interaction based on affordance understanding in complex indoor environments, advancing the development of more intuitive and efficient robotic systems for real-world applications.

Translated Abstract:
최근 대형 언어 모델과 비전-언어 모델의 발전 덕분에 다중 모달 이해가 많이 향상됐어. 하지만 고급 언어 지시를 3D 공간에서 정확한 로봇 동작으로 변환하는 건 여전히 어려운 문제야. 

이 논문에서는 IRIS(Interactive Responsive Intelligent Segmentation)라는 새로운 훈련이 필요 없는 다중 모달 시스템을 소개해. 이 시스템은 3D 가능성 세분화를 위해 만들어졌고, 일상 환경에서의 상호작용 언어 안내 가능성을 평가할 수 있는 벤치마크도 함께 제공해. 

IRIS는 대형 다중 모달 모델과 특화된 3D 비전 네트워크를 통합해서 2D와 3D 시각 이해를 언어 이해와 매끄럽게 결합할 수 있어. 평가를 쉽게 하기 위해, 10개의 대표적인 실내 환경에 대한 데이터셋을 제공해. 각 환경마다 50장의 이미지가 객체 동작과 3D 가능성 세분화에 대해 주석이 달려 있어. 

광범위한 실험을 통해 IRIS가 다양한 설정에서 상호작용 3D 가능성 세분화 작업을 잘 처리할 수 있다는 걸 보여줬어. 여러 지표에서도 경쟁력 있는 성능을 보였고. 우리의 결과는 복잡한 실내 환경에서 가능성 이해를 기반으로 한 인간-로봇 상호작용을 향상시킬 수 있는 IRIS의 잠재력을 강조해. 이로 인해 현실 세계의 응용을 위한 더 직관적이고 효율적인 로봇 시스템 개발이 진전을 이룰 수 있을 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10106.pdf

Title: Industry 6.0: New Generation of Industry driven by Generative AI and Swarm of Heterogeneous Robots

Original Abstract:
This paper presents the concept of Industry 6.0, introducing the world's first fully automated production system that autonomously handles the entire product design and manufacturing process based on user-provided natural language descriptions. By leveraging generative AI, the system automates critical aspects of production, including product blueprint design, component manufacturing, logistics, and assembly. A heterogeneous swarm of robots, each equipped with individual AI through integration with Large Language Models (LLMs), orchestrates the production process. The robotic system includes manipulator arms, delivery drones, and 3D printers capable of generating assembly blueprints. The system was evaluated using commercial and open-source LLMs, functioning through APIs and local deployment. A user study demonstrated that the system reduces the average production time to 119.10 minutes, significantly outperforming a team of expert human developers, who averaged 528.64 minutes (an improvement factor of 4.4). Furthermore, in the product blueprinting stage, the system surpassed human CAD operators by an unprecedented factor of 47, completing the task in 0.5 minutes compared to 23.5 minutes. This breakthrough represents a major leap towards fully autonomous manufacturing.

Translated Abstract:
이 논문은 산업 6.0의 개념을 소개해. 이건 사용자 제공 자연어 설명에 기반해 전체 제품 설계와 제조 과정을 자동으로 처리하는 세계 최초의 완전 자동화 생산 시스템이야. 생성 AI를 활용해서 제품 청사진 설계, 부품 제조, 물류, 조립 같은 중요한 생산 과정을 자동화해.

이 시스템은 각기 다른 AI가 탑재된 로봇 무리로 구성되어 있어. 이 로봇들은 대형 언어 모델(LLM)과 통합돼서 생산 과정을 조율해. 로봇 시스템에는 조작 팔, 배달 드론, 조립 청사진을 생성할 수 있는 3D 프린터가 포함돼 있어.

시스템은 상용 LLM과 오픈 소스 LLM을 사용해서 평가됐고, API와 로컬 배포 방식으로 운영돼. 사용자 연구 결과, 이 시스템은 평균 생산 시간을 119.10분으로 줄였고, 전문가 개발팀의 평균 528.64분보다 훨씬 뛰어난 성능을 보였어(개선 비율 4.4배). 게다가 제품 청사진 단계에서는 이 시스템이 인간 CAD 운영자보다 47배 빠르게 작업을 완료했어. 0.5분 걸린 반면, 인간은 23.5분이 걸렸거든. 이 breakthrough는 완전 자율 제조를 향한 큰 도약을 의미해.

================================================================================

URL:
https://arxiv.org/pdf/2409.10117.pdf

Title: Multi-Agent Obstacle Avoidance using Velocity Obstacles and Control Barrier Functions

Original Abstract:
Velocity Obstacles (VO) methods form a paradigm for collision avoidance strategies among moving obstacles and agents. While VO methods perform well in simple multi-agent environments, they don't guarantee safety and can show overly conservative behavior in common situations. In this paper, we propose to combine a VO-strategy for guidance with a CBF-approach for safety, which overcomes the overly conservative behavior of VOs and formally guarantees safety. We validate our method in a baseline comparison study, using 2nd order integrator and car-like dynamics. Results support that our method outperforms the baselines w.r.t. path smoothness, collision avoidance, and success rates.

Translated Abstract:
속도 장애물(Velocity Obstacles, VO) 방법은 움직이는 장애물과 에이전트 사이에서 충돌을 피하는 전략의 한 형태입니다. VO 방법은 간단한 다중 에이전트 환경에서는 잘 작동하지만, 안전을 보장하지 않거나 일반적인 상황에서 지나치게 보수적인 행동을 보일 수 있습니다.

이 논문에서는 VO 전략을 안내에 사용하고, 안전을 보장하기 위해 CBF(차원 최적화 기반 안전) 접근 방식을 결합할 것을 제안합니다. 이렇게 하면 VO의 지나치게 보수적인 행동을 극복하고 안전을 공식적으로 보장할 수 있습니다.

우리는 2차 적분기와 자동차와 유사한 동역학을 사용한 기준 비교 연구에서 우리의 방법을 검증했습니다. 결과는 우리의 방법이 경로 부드러움, 충돌 회피, 성공률 측면에서 기준 방법들보다 뛰어나다는 것을 보여줍니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.10135.pdf

Title: A hierarchical framework for collision avoidance in robot-assisted minimally invasive surgery

Original Abstract:
Minimally invasive surgery (MIS) procedures benefit significantly from robotic systems due to their improved precision and dexterity. However, ensuring safety in these dynamic and cluttered environments is an ongoing challenge. This paper proposes a novel hierarchical framework for collision avoidance in MIS. This framework integrates multiple tasks, including maintaining the Remote Center of Motion (RCM) constraint, tracking desired tool poses, avoiding collisions, optimizing manipulability, and adhering to joint limits. The proposed approach utilizes Hierarchical Quadratic Programming (HQP) to seamlessly manage these constraints while enabling smooth transitions between task priorities for collision avoidance. Experimental validation through simulated scenarios demonstrates the framework's robustness and effectiveness in handling diverse scenarios involving static and dynamic obstacles, as well as inter-tool collisions.

Translated Abstract:
최소 침습 수술(MIS)은 로봇 시스템 덕분에 더 정확하고 섬세하게 진행할 수 있어 큰 이점이 있어. 하지만 이런 동적이고 복잡한 환경에서 안전을 보장하는 건 여전히 어려운 과제야. 

이 논문에서는 MIS에서 충돌을 피하기 위한 새로운 계층적 프레임워크를 제안해. 이 프레임워크는 원격 운동 중심(Remote Center of Motion, RCM) 제약 유지, 원하는 도구 자세 추적, 충돌 회피, 조작성 최적화, 관절 제한 준수 같은 여러 작업을 통합해. 

제안된 방법은 계층적 이차 프로그래밍(Hierarchical Quadratic Programming, HQP)을 사용해서 이런 제약을 부드럽게 관리하고, 충돌 회피를 위한 작업 우선순위 사이에서 원활한 전환을 가능하게 해. 

실험 시뮬레이션을 통해 이 프레임워크가 정적 및 동적 장애물, 그리고 도구 간 충돌을 포함한 다양한 상황을 잘 처리할 수 있다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10143.pdf

Title: P2U-SLAM: A Monocular Wide-FoV SLAM System Based on Point Uncertainty and Pose Uncertainty

Original Abstract:
This paper presents P2U-SLAM, a visual Simultaneous Localization And Mapping (SLAM) system with a wide Field of View (FoV) camera, which utilizes pose uncertainty and point uncertainty. While the wide FoV enables considerable repetitive observations of historical map points for matching cross-view features, the data properties of the historical map points and the poses of historical keyframes have changed during the optimization process. The neglect of data property changes triggers the absence of a partial information matrix in optimization and leads to the risk of long-term positioning performance degradation. The purpose of our research is to reduce the risk of the wide field of view visual input to the SLAM system. Based on the conditional probability model, this work reveals the definite impact of the above data properties changes on the optimization process, concretizes it as point uncertainty and pose uncertainty, and gives a specific mathematical form. P2U-SLAM respectively embeds point uncertainty and pose uncertainty into the tracking module and local mapping, and updates these uncertainties after each optimization operation including local mapping, map merging, and loop closing. We present an exhaustive evaluation in 27 sequences from two popular public datasets with wide-FoV visual input. P2U-SLAM shows excellent performance compared with other state-of-the-art methods. The source code will be made publicly available at this https URL.

Translated Abstract:
이 논문은 P2U-SLAM이라는 시각적 SLAM 시스템을 소개해. 이 시스템은 넓은 시야각(FoV) 카메라를 사용하고, 포즈 불확실성과 포인트 불확실성을 활용해. 넓은 시야각 덕분에 과거 지도 점들을 많이 관찰할 수 있어서 서로 다른 뷰에서 특징들을 매칭하는 데 유리해. 

하지만 최적화 과정 중에 과거 지도 점들의 데이터 속성과 과거 주요 프레임의 포즈가 변하게 돼. 이런 데이터 속성의 변화를 무시하면 최적화에서 일부 정보 행렬이 없어지게 되고, 이로 인해 장기적인 위치 성능이 저하될 위험이 생겨. 그래서 우리의 연구 목적은 SLAM 시스템에 대한 넓은 시야각의 시각 입력으로 인한 위험을 줄이는 거야.

조건부 확률 모델을 바탕으로, 이 연구는 위에서 언급한 데이터 속성 변화가 최적화 과정에 미치는 명확한 영향을 밝혀냈고, 이를 포인트 불확실성과 포즈 불확실성으로 구체화했어. P2U-SLAM은 각각 포인트 불확실성과 포즈 불확실성을 추적 모듈과 지역 매핑에 포함시키고, 지역 매핑, 지도 병합, 루프 닫기 같은 각 최적화 작업 후에 이러한 불확실성을 업데이트해.

우리는 넓은 시야각의 시각 입력을 가진 두 개의 인기 있는 공개 데이터셋에서 27개의 시퀀스를 통해 철저한 평가를 진행했어. P2U-SLAM은 다른 최신 방법들과 비교했을 때 뛰어난 성능을 보여줬어. 소스 코드는 이 URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10161.pdf

Title: SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using Gaussian Splatting

Original Abstract:
Sim2Real transfer, particularly for manipulation policies relying on RGB images, remains a critical challenge in robotics due to the significant domain shift between synthetic and real-world visual data. In this paper, we propose SplatSim, a novel framework that leverages Gaussian Splatting as the primary rendering primitive to reduce the Sim2Real gap for RGB-based manipulation policies. By replacing traditional mesh representations with Gaussian Splats in simulators, SplatSim produces highly photorealistic synthetic data while maintaining the scalability and cost-efficiency of simulation. We demonstrate the effectiveness of our framework by training manipulation policies within SplatSim}and deploying them in the real world in a zero-shot manner, achieving an average success rate of 86.25%, compared to 97.5% for policies trained on real-world data.

Translated Abstract:
Sim2Real 전이, 특히 RGB 이미지를 기반으로 한 조작 정책에서는 합성 데이터와 실제 데이터 간의 큰 차이 때문에 여전히 큰 도전 과제야. 이 논문에서는 Gaussian Splatting을 기본 렌더링 방법으로 활용하는 새로운 프레임워크인 SplatSim을 제안해. 이렇게 해서 RGB 기반 조작 정책을 위한 Sim2Real 격차를 줄이려는 거지.

SplatSim은 전통적인 메시 표현을 시뮬레이터에서 Gaussian Splat으로 바꿔서, 매우 사실적인 합성 데이터를 만들어내면서도 시뮬레이션의 확장성과 비용 효율성을 유지해. 우리는 SplatSim 안에서 조작 정책을 훈련하고, 이를 실제 환경에서 제로샷으로 배포해 그 효과를 보여주었어. 그 결과, 평균 성공률이 86.25%였고, 이는 실제 데이터로 훈련된 정책의 97.5%와 비교할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10165.pdf

Title: Maneuver Decision-Making with Trajectory Streams Prediction for Autonomous Vehicles

Original Abstract:
Decision-making, motion planning, and trajectory prediction are crucial in autonomous driving systems. By accurately forecasting the movements of other road users, the decision-making capabilities of the autonomous system can be enhanced, making it more effective in responding to dynamic and unpredictable environments and more adaptive to diverse road scenarios. This paper presents the FFStreams++ approach for decision-making and motion planning of different maneuvers, including unprotected left turn, overtaking, and keep-lane. FFStreams++ is a combination of sampling-based and search-based approaches, where iteratively new sampled trajectories for different maneuvers are generated and optimized, and afterward, a heuristic search planner is called, searching for an optimal plan. We model the autonomous diving system in the Planning Domain Definition Language (PDDL) and search for the optimal plan using a heuristic Fast-Forward planner. In this approach, the initial state of the problem is modified iteratively through streams, which will generate maneuver-specific trajectory candidates, increasing the iterating level until an optimal plan is found. FFStreams++ integrates a query-connected network model for predicting possible future trajectories for each surrounding obstacle along with their probabilities. The proposed approach was tested on the CommonRoad simulation framework. We use a collection of randomly generated driving scenarios for overtaking and unprotected left turns at intersections to evaluate the FFStreams++ planner. The test results confirmed that the proposed approach can effectively execute various maneuvers to ensure safety and reduce the risk of collisions with nearby traffic agents.

Translated Abstract:
자율주행 시스템에서 의사결정, 동작 계획, 그리고 경로 예측은 정말 중요해. 다른 도로 사용자들의 움직임을 정확히 예측하면, 자율 시스템의 의사결정 능력이 향상돼서 변화무쌍한 환경에 더 잘 대응하고 다양한 도로 상황에 적응할 수 있게 돼. 

이 논문에서는 보호되지 않은 좌회전, 추월, 차선 유지와 같은 다양한 동작을 위한 의사결정과 동작 계획을 위한 FFStreams++ 접근법을 소개해. FFStreams++는 샘플링 기반과 탐색 기반 접근법의 조합으로, 여러 동작을 위해 새로운 샘플 경로를 반복적으로 생성하고 최적화해. 그 다음에는 휴리스틱 탐색 계획기를 호출해서 최적 계획을 찾아. 

우리는 Planning Domain Definition Language (PDDL)를 사용해 자율주행 시스템을 모델링하고, 휴리스틱 Fast-Forward 계획기를 사용해 최적 계획을 찾아. 이 접근법에서는 문제의 초기 상태를 스트림을 통해 반복적으로 수정해서 동작에 맞는 경로 후보를 생성하고, 최적 계획이 발견될 때까지 반복 수준을 높여. FFStreams++는 주변 장애물의 가능한 미래 경로와 그 확률을 예측하기 위해 쿼리 연결 네트워크 모델을 통합해. 

제안된 접근법은 CommonRoad 시뮬레이션 프레임워크에서 테스트했어. 우리는 교차로에서 추월과 보호되지 않은 좌회전을 위해 무작위로 생성된 주행 시나리오 모음을 사용해서 FFStreams++ 계획기를 평가했어. 테스트 결과는 제안된 접근법이 다양한 동작을 효과적으로 수행해 안전을 보장하고 인근 교통 참여자와의 충돌 위험을 줄일 수 있음을 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10172.pdf

Title: LiLoc: Lifelong Localization using Adaptive Submap Joining and Egocentric Factor Graph

Original Abstract:
This paper proposes a versatile graph-based lifelong localization framework, LiLoc, which enhances its timeliness by maintaining a single central session while improves the accuracy through multi-modal factors between the central and subsidiary sessions. First, an adaptive submap joining strategy is employed to generate prior submaps (keyframes and poses) for the central session, and to provide priors for subsidiaries when constraints are needed for robust localization. Next, a coarse-to-fine pose initialization for subsidiary sessions is performed using vertical recognition and ICP refinement in the global coordinate frame. To elevate the accuracy of subsequent localization, we propose an egocentric factor graph (EFG) module that integrates the IMU preintegration, LiDAR odometry and scan match factors in a joint optimization manner. Specifically, the scan match factors are constructed by a novel propagation model that efficiently distributes the prior constrains as edges to the relevant prior pose nodes, weighted by noises based on keyframe registration errors. Additionally, the framework supports flexible switching between two modes: relocalization (RLM) and incremental localization (ILM) based on the proposed overlap-based mechanism to select or update the prior submaps from central session. The proposed LiLoc is tested on public and custom datasets, demonstrating accurate localization performance against state-of-the-art methods. Our codes will be publicly available on this https URL.

Translated Abstract:
이 논문에서는 LiLoc이라는 다재다능한 그래프 기반 평생 로컬라이제이션 프레임워크를 제안해. 이 시스템은 단일 중앙 세션을 유지하면서 시간 효율성을 높이고, 중앙 세션과 자회사 세션 간의 다중 모달 요인을 통해 정확성을 향상시켜.

먼저, 적응형 서브맵 결합 전략을 사용해서 중앙 세션을 위한 이전 서브맵(키프레임과 포즈)을 생성하고, 강력한 로컬라이제이션을 위해 자회사 세션에서 제약이 필요할 때 사용할 수 있도록 해. 다음으로, 수직 인식을 이용한 대략적인 포즈 초기화와 ICP 정제를 통해 자회사 세션의 포즈를 미세 조정해.

이후의 로컬라이제이션 정확성을 높이기 위해, IMU 프리인티그레이션, LiDAR 오도메트리, 스캔 매치 요인을 통합하는 자가 중심 요인 그래프(EFG) 모듈을 제안해. 특히, 스캔 매치 요인은 새롭게 설계된 전파 모델을 통해 효율적으로 이전 제약을 관련된 이전 포즈 노드에 엣지로 배포하고, 키프레임 등록 오류에 기반한 노이즈로 가중치를 부여해.

또한, 이 프레임워크는 중앙 세션에서 이전 서브맵을 선택하거나 업데이트하기 위한 제안된 겹침 기반 메커니즘에 따라 재로컬라이제이션(RLM)과 점진적 로컬라이제이션(ILM) 모드 간의 유연한 전환을 지원해. 제안된 LiLoc은 공개 데이터셋과 사용자 정의 데이터셋에서 테스트되었고, 최신 기법들과 비교해 정확한 로컬라이제이션 성능을 보여줬어. 우리의 코드는 이 URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10193.pdf

Title: Relative Positioning for Aerial Robot Path Planning in GPS Denied Environment

Original Abstract:
One of the most useful applications of intelligent aerial robots sometimes called Unmanned Aerial Vehicles (UAV) in Australia is known to be in bushfire monitoring and prediction operations. A swarm of autonomous drones/UAVs programmed to work in real-time observing the fire parameters using their onboard sensors would be valuable in reducing the life-threatening impact of that fire. However autonomous UAVs face serious challenges in their positioning and navigation in critical bushfire conditions such as remoteness and severe weather conditions where GPS signals could also be unreliable. This paper tackles one of the most important factors in autonomous UAV navigation, namely Initial Positioning sometimes called Localisation. The solution provided by this paper will enable a team of autonomous UAVs to establish a relative position to their base of operation to be able to commence a team search and reconnaissance in a bushfire-affected area and find their way back to their base without the help of GPS signals.

Translated Abstract:
호주에서 지능형 공중 로봇, 즉 무인 항공기(UAV)의 가장 유용한 응용 중 하나는 산불 모니터링과 예측 작업으로 알려져 있어. 실시간으로 화재 관련 정보를 관찰하는 자율 드론/UAV 무리가 onboard 센서를 사용해서 운영된다면, 그 화재가 가져오는 생명 위협을 줄이는 데 큰 도움이 될 거야. 

하지만 자율 UAV는 원거리와 악천후 같은 극한의 산불 조건에서 위치 파악과 내비게이션에 심각한 도전 과제를 마주해. 이럴 때 GPS 신호가 불안정할 수 있어서 더욱 어려워지지. 이 논문에서는 자율 UAV 내비게이션에서 가장 중요한 요소 중 하나인 초기 위치 파악, 즉 로컬리제이션을 다루고 있어. 

이 논문에서 제시하는 해결책은 자율 UAV 팀이 작전 기지에 상대적인 위치를 설정할 수 있게 해줘. 이렇게 하면 산불 영향을 받은 지역에서 팀 탐색과 정찰을 시작하고 GPS 신호 없이도 다시 기지로 돌아갈 수 있게 되는 거지.

================================================================================

URL:
https://arxiv.org/pdf/2409.10194.pdf

Title: Underwater robot guidance, navigation and control in fish net pens

Original Abstract:
Aquaculture robotics is receiving increased attention and is subject to unique challenges and opportunities for research and development. Guidance, navigation and control are all important aspects for realizing aquaculture robotics solutions that can greatly benefit the industry in the future. Sensor technologies, navigation methods, motion planners and state control all have a role to play, and this paper introduces some technologies and methods that are currently being applied in research and industry before providing some examples of challenges that can be targeted in the future.

Translated Abstract:
수산양식 로봇에 대한 관심이 커지고 있어. 이 분야는 연구와 개발에서 독특한 도전과 기회가 있어. 

수산양식 로봇 솔루션을 실현하려면 안내, 탐색, 제어가 모두 중요해. 이런 기술들은 산업에 큰 도움이 될 수 있어. 센서 기술, 탐색 방법, 동작 계획 및 상태 제어 등 여러 가지 역할이 있어. 

이 논문에서는 현재 연구와 산업에서 적용되고 있는 기술과 방법을 소개하고, 앞으로 해결할 수 있는 도전 과제의 예시도 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.10196.pdf

Title: NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous Perception, Reasoning, and Planning in Complex UAV Search Missions

Original Abstract:
This paper addresses the problem of autonomous UAV search missions, where a UAV must locate specific Entities of Interest (EOIs) within a time limit, based on brief descriptions in large, hazard-prone environments with keep-out zones. The UAV must perceive, reason, and make decisions with limited and uncertain information. We propose NEUSIS, a compositional neuro-symbolic system designed for interpretable UAV search and navigation in realistic scenarios. NEUSIS integrates neuro-symbolic visual perception, reasoning, and grounding (GRiD) to process raw sensory inputs, maintains a probabilistic world model for environment representation, and uses a hierarchical planning component (SNaC) for efficient path planning. Experimental results from simulated urban search missions using AirSim and Unreal Engine show that NEUSIS outperforms a state-of-the-art (SOTA) vision-language model and a SOTA search planning model in success rate, search efficiency, and 3D localization. These results demonstrate the effectiveness of our compositional neuro-symbolic approach in handling complex, real-world scenarios, making it a promising solution for autonomous UAV systems in search missions.

Translated Abstract:
이 논문은 자율 UAV 검색 임무에 대한 문제를 다루고 있어. UAV가 특정 관심 객체(EOI)를 시간 제한 안에 찾는 건데, 이때 간단한 설명을 바탕으로 위험이 많은 환경에서 작업해야 해. 게다가 접근 금지 구역도 있어. UAV는 제한적이고 불확실한 정보를 바탕으로 인식하고, 추론하며, 결정을 내려야 해.

우리는 NEUSIS라는 시스템을 제안해. 이건 해석 가능한 UAV 검색과 내비게이션을 위한 조합적 신경-기호 시스템이야. NEUSIS는 원시 감각 입력을 처리하기 위해 신경-기호 시각 인식, 추론, 그리고 그라운딩(GRiD)을 통합해. 환경을 표현하기 위해 확률적 세계 모델을 유지하고, 효율적인 경로 계획을 위해 계층적 계획 구성 요소(SNaC)를 사용해.

AirSim과 Unreal Engine을 이용한 도시 검색 임무의 실험 결과에 따르면, NEUSIS는 최신 비전-언어 모델과 최신 검색 계획 모델보다 성공률, 검색 효율, 3D 위치 추정에서 더 나은 성과를 보여줬어. 이 결과는 복잡한 현실 세계 시나리오를 처리하는 데 있어 우리의 조합적 신경-기호 접근법이 효과적임을 보여주고, 자율 UAV 시스템의 검색 임무에 대한 유망한 솔루션이 될 수 있음을 나타내.

================================================================================

URL:
https://arxiv.org/pdf/2409.10202.pdf

Title: SteeredMarigold: Steering Diffusion Towards Depth Completion of Largely Incomplete Depth Maps

Original Abstract:
Even if the depth maps captured by RGB-D sensors deployed in real environments are often characterized by large areas missing valid depth measurements, the vast majority of depth completion methods still assumes depth values covering all areas of the scene. To address this limitation, we introduce SteeredMarigold, a training-free, zero-shot depth completion method capable of producing metric dense depth, even for largely incomplete depth maps. SteeredMarigold achieves this by using the available sparse depth points as conditions to steer a denoising diffusion probabilistic model. Our method outperforms relevant top-performing methods on the NYUv2 dataset, in tests where no depth was provided for a large area, achieving state-of-art performance and exhibiting remarkable robustness against depth map incompleteness. Our code will be publicly available.

Translated Abstract:
RGB-D 센서로 찍은 깊이 맵은 실제 환경에서 유효한 깊이 측정값이 빠진 큰 영역이 많아. 그런데 대부분의 깊이 보완 방법들은 여전히 모든 영역에 깊이 값이 있다고 가정해. 이 문제를 해결하기 위해 우리는 SteeredMarigold라는 방법을 소개해. 이건 훈련이 필요 없는 제로샷 깊이 보완 방법으로, 깊이 맵이 많이 불완전해도 밀도 있는 깊이를 만들어낼 수 있어.

SteeredMarigold는 사용할 수 있는 희소 깊이 포인트를 조건으로 삼아, 노이즈 제거 확산 확률 모델을 조정하는 방식으로 작동해. 이 방법은 NYUv2 데이터셋에서 관련된 최상위 방법들보다 더 나은 성능을 보여줬고, 깊이가 제공되지 않은 큰 영역에서도 최첨단 성능을 달성했어. 또한 깊이 맵이 불완전해도 놀라운 강인성을 보여줘. 우리의 코드는 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10204.pdf

Title: Embedded Image-to-Image Translation for Efficient Sim-to-Real Transfer in Learning-based Robot-Assisted Soft Manipulation

Original Abstract:
Recent advances in robotic learning in simulation have shown impressive results in accelerating learning complex manipulation skills. However, the sim-to-real gap, caused by discrepancies between simulation and reality, poses significant challenges for the effective deployment of autonomous surgical systems. We propose a novel approach utilizing image translation models to mitigate domain mismatches and facilitate efficient robot skill learning in a simulated environment. Our method involves the use of contrastive unpaired Image-to-image translation, allowing for the acquisition of embedded representations from these transformed images. Subsequently, these embeddings are used to improve the efficiency of training surgical manipulation models. We conducted experiments to evaluate the performance of our approach, demonstrating that it significantly enhances task success rates and reduces the steps required for task completion compared to traditional methods. The results indicate that our proposed system effectively bridges the sim-to-real gap, providing a robust framework for advancing the autonomy of surgical robots in minimally invasive procedures.

Translated Abstract:
최근 로봇 학습 분야에서는 시뮬레이션에서 복잡한 조작 기술 학습을 빠르게 할 수 있는 놀라운 결과들이 나왔어. 하지만, 시뮬레이션과 실제 사이의 차이 때문에 생기는 '시뮬레이션-현실 간극'이 자율 수술 시스템의 효과적인 배치를 어렵게 만들고 있어. 

우리는 이미지 변환 모델을 활용해서 이런 도메인 불일치를 줄이고, 시뮬레이션 환경에서 로봇 기술 학습을 효율적으로 할 수 있는 새로운 방법을 제안해. 이 방법은 대조적 비유사 이미지-이미지 변환을 사용해서 변환된 이미지에서 내재된 표현을 얻는 거야. 그 다음에 이 표현을 활용해서 수술 조작 모델의 학습 효율을 높여. 

우리는 이 방법의 성능을 평가하기 위한 실험을 했고, 전통적인 방법에 비해 작업 성공률을 크게 높이고 작업 완료에 필요한 단계를 줄인다는 걸 보여줬어. 결과적으로, 우리가 제안한 시스템이 시뮬레이션-현실 간극을 효과적으로 줄여주고, 최소 침습 수술에서 수술 로봇의 자율성을 높일 수 있는 강력한 프레임워크를 제공한다는 걸 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10216.pdf

Title: BEINGS: Bayesian Embodied Image-goal Navigation with Gaussian Splatting

Original Abstract:
Image-goal navigation enables a robot to reach the location where a target image was captured, using visual cues for guidance. However, current methods either rely heavily on data and computationally expensive learning-based approaches or lack efficiency in complex environments due to insufficient exploration strategies. To address these limitations, we propose Bayesian Embodied Image-goal Navigation Using Gaussian Splatting, a novel method that formulates ImageNav as an optimal control problem within a model predictive control framework. BEINGS leverages 3D Gaussian Splatting as a scene prior to predict future observations, enabling efficient, real-time navigation decisions grounded in the robot's sensory experiences. By integrating Bayesian updates, our method dynamically refines the robot's strategy without requiring extensive prior experience or data. Our algorithm is validated through extensive simulations and physical experiments, showcasing its potential for embodied robot systems in visually complex scenarios.

Translated Abstract:
이미지 목표 탐색은 로봇이 특정 이미지가 촬영된 장소에 도달할 수 있도록 시각적 단서를 활용하는 방법이야. 하지만 현재 방법들은 데이터에 많이 의존하거나 복잡한 환경에서 효율성이 떨어지는 경우가 많아. 

이런 문제를 해결하기 위해 우리는 "가우시안 스플래팅을 이용한 베이지안 체화 이미지 목표 탐색"이라는 새로운 방법을 제안해. 이 방법은 이미지 탐색을 최적 제어 문제로 설정하고, 모델 예측 제어 프레임워크 안에서 작동해. BEINGS는 3D 가우시안 스플래팅을 장면의 사전 정보로 사용해서 미래 관측을 예측해. 이렇게 하면 로봇의 감각 경험에 기반한 효율적이고 실시간 탐색 결정을 할 수 있어. 

베이지안 업데이트를 통합함으로써, 이 방법은 로봇의 전략을 동적으로 개선해. 그래서 많은 사전 경험이나 데이터가 없어도 잘 작동해. 우리의 알고리즘은 다양한 시뮬레이션과 실제 실험을 통해 검증되었고, 복잡한 시각적 상황에서 체화 로봇 시스템의 가능성을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.10225.pdf

Title: Voice control interface for surgical robot assistants

Original Abstract:
Traditional control interfaces for robotic-assisted minimally invasive surgery impose a significant cognitive load on surgeons. To improve surgical efficiency, surgeon-robot collaboration capabilities, and reduce surgeon burden, we present a novel voice control interface for surgical robotic assistants. Our system integrates Whisper, state-of-the-art speech recognition, within the ROS framework to enable real-time interpretation and execution of voice commands for surgical manipulator control. The proposed system consists of a speech recognition module, an action mapping module, and a robot control module. Experimental results demonstrate the system's high accuracy and inference speed, and demonstrates its feasibility for surgical applications in a tissue triangulation task. Future work will focus on further improving its robustness and clinical applicability.

Translated Abstract:
기존의 로봇 보조 최소 침습 수술을 위한 제어 인터페이스는 외과의사에게 큰 인지 부담을 줘. 수술 효율을 높이고, 외과의사와 로봇 간의 협력 능력을 향상시키며, 외과의사의 부담을 줄이기 위해 우리는 새로운 음성 제어 인터페이스를 제안해. 

우리 시스템은 최신 음성 인식 기술인 Whisper를 ROS 프레임워크에 통합해서, 수술 조작기를 제어하는 음성 명령을 실시간으로 해석하고 실행할 수 있게 해. 제안된 시스템은 음성 인식 모듈, 동작 매핑 모듈, 로봇 제어 모듈로 구성돼. 

실험 결과, 이 시스템의 정확도와 추론 속도가 매우 높다는 것을 보여줬고, 조직 삼각 측량 작업에서 수술 응용 가능성을 입증했어. 앞으로는 시스템의 견고성과 임상 적용 가능성을 더 개선하는 데 초점을 맞출 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10274.pdf

Title: Safety-critical Locomotion of Biped Robots in Infeasible Paths: Overcoming Obstacles during Navigation toward Destination

Original Abstract:
This paper proposes a safety-critical locomotion control framework employed for legged robots exploring through infeasible path in obstacle-rich environments. Our research focus is on achieving safe and robust locomotion where robots confront unavoidable obstacles en route to their designated destination. Through the utilization of outcomes from physical interactions with unknown objects, we establish a hierarchy among the safety-critical conditions avoiding the obstacles. This hierarchy enables the generation of a safe reference trajectory that adeptly mitigates conflicts among safety conditions and reduce the risk while controlling the robot toward its destination without additional motion planning methods. In addition, robust bipedal locomotion is achieved by utilizing the Hybrid Linear Inverted Pendulum model, coupled with a disturbance observer addressing a disturbance from the physical interaction.

Translated Abstract:
이 논문에서는 장애물이 많은 환경에서 길을 탐색하는 다리 로봇을 위한 안전 중심의 이동 제어 프레임워크를 제안해. 우리의 연구 목표는 로봇이 목적지로 가는 길에 피할 수 없는 장애물에 마주쳤을 때도 안전하고 튼튼하게 이동하는 거야.

우리는 알 수 없는 물체와의 물리적 상호작용 결과를 활용해서 장애물을 피하는 안전 중심 조건들 사이에 계층을 만들어. 이 계층 덕분에 안전 조건 간의 갈등을 잘 해결하면서 로봇이 목적지로 가는 동안 위험을 줄일 수 있는 안전한 참조 경로를 생성할 수 있어. 그러면서 추가적인 동작 계획 방법 없이도 로봇을 잘 제어할 수 있게 돼.

그리고 하이브리드 선형 역진자 모델을 이용해서 강력한 이족 보행을 달성했어. 여기에는 물리적 상호작용으로 인한 방해를 처리하는 방해 관찰기도 포함되어 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10283.pdf

Title: ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone Navigation via Scene-Aware Control Barrier Functions

Original Abstract:
In the rapidly evolving field of vision-language navigation (VLN), ensuring robust safety mechanisms remains an open challenge. Control barrier functions (CBFs) are efficient tools which guarantee safety by solving an optimal control problem. In this work, we consider the case of a teleoperated drone in a VLN setting, and add safety features by formulating a novel scene-aware CBF using ego-centric observations obtained through an RGB-D sensor. As a baseline, we implement a vision-language understanding module which uses the contrastive language image pretraining (CLIP) model to query about a user-specified (in natural language) landmark. Using the YOLO (You Only Look Once) object detector, the CLIP model is queried for verifying the cropped landmark, triggering downstream navigation. To improve navigation safety of the baseline, we propose ASMA -- an Adaptive Safety Margin Algorithm -- that crops the drone's depth map for tracking moving object(s) to perform scene-aware CBF evaluation on-the-fly. By identifying potential risky observations from the scene, ASMA enables real-time adaptation to unpredictable environmental conditions, ensuring optimal safety bounds on a VLN-powered drone actions. Using the robot operating system (ROS) middleware on a parrot bebop2 quadrotor in the gazebo environment, ASMA offers 59.4% - 61.8% increase in success rates with insignificant 5.4% - 8.2% increases in trajectory lengths compared to the baseline CBF-less VLN while recovering from unsafe situations.

Translated Abstract:
시각-언어 내비게이션(VLN) 분야는 빠르게 발전하고 있지만, 안전 메커니즘을 보장하는 것은 여전히 해결해야 할 문제야. 여기서 제어 장벽 기능(Control Barrier Functions, CBFs)은 최적 제어 문제를 해결해서 안전을 보장하는 유용한 도구야. 

이번 연구에서는 VLN 환경에서 원격 조종 드론의 사례를 다뤄봤어. 새로운 장면 인식 CBF를 만들면서 RGB-D 센서를 통해 얻은 자기 중심 관찰을 이용해 안전 기능을 추가했어. 기본적으로는 사용자가 자연어로 지정한 랜드마크에 대해 질문할 수 있는 시각-언어 이해 모듈을 구현했어. 이 모듈은 CLIP 모델을 사용해. 

YOLO(You Only Look Once) 객체 탐지기를 사용해서, CLIP 모델에 잘라낸 랜드마크를 확인하도록 요청하고, 이를 통해 내비게이션을 시작해. 기본 내비게이션의 안전성을 높이기 위해, ASMA(Adaptive Safety Margin Algorithm)를 제안했어. 이 알고리즘은 드론의 깊이 맵을 잘라내서 움직이는 객체를 추적하고, 장면 인식 CBF 평가를 실시간으로 수행해.

ASMA는 장면에서 위험할 수 있는 관찰을 식별함으로써, 예측할 수 없는 환경 조건에 실시간으로 적응할 수 있어, VLN 기반 드론의 행동에 최적의 안전 경계를 유지해. 로봇 운영 체제(ROS) 미들웨어를 사용해서 파롯 베바프2 쿼드로터를 가제보 환경에서 테스트해봤고, ASMA는 기본 CBF 없는 VLN에 비해 성공률을 59.4%에서 61.8%까지 높여주면서, 궤적 길이도 5.4%에서 8.2% 정도만 증가했어. 안전하지 않은 상황에서도 잘 회복할 수 있었고.

================================================================================

URL:
https://arxiv.org/pdf/2409.10308.pdf

Title: Know your limits! Optimize the robot's behavior through self-awareness

Original Abstract:
As humanoid robots transition from labs to real-world environments, it is essential to democratize robot control for non-expert users. Recent human-robot imitation algorithms focus on following a reference human motion with high precision, but they are susceptible to the quality of the reference motion and require the human operator to simplify its movements to match the robot's capabilities. Instead, we consider that the robot should understand and adapt the reference motion to its own abilities, facilitating the operator's task. For that, we introduce a deep-learning model that anticipates the robot's performance when imitating a given reference. Then, our system can generate multiple references given a high-level task command, assign a score to each of them, and select the best reference to achieve the desired robot behavior. Our Self-AWare model (SAW) ranks potential robot behaviors based on various criteria, such as fall likelihood, adherence to the reference motion, and smoothness. We integrate advanced motion generation, robot control, and SAW in one unique system, ensuring optimal robot behavior for any task command. For instance, SAW can anticipate falls with 99.29% accuracy. For more information check our project page: this https URL

Translated Abstract:
휴머노이드 로봇이 연구실에서 실제 환경으로 이동하면서 비전문가도 로봇을 쉽게 제어할 수 있도록 하는 것이 중요해. 최근의 인간-로봇 모방 알고리즘은 인간의 동작을 정확하게 따르는 데 집중하고 있지만, 참조 동작의 질에 영향을 많이 받고, 인간 조작자가 로봇의 능력에 맞게 동작을 단순화해야 해. 

대신에, 우리는 로봇이 참조 동작을 이해하고 자신의 능력에 맞게 조정해야 한다고 생각해. 이렇게 하면 조작자가 더 쉽게 작업할 수 있어. 그래서 우리는 주어진 참조를 모방할 때 로봇의 성능을 예측하는 딥러닝 모델을 도입했어. 

우리 시스템은 높은 수준의 작업 명령을 바탕으로 여러 개의 참조를 생성하고, 각 참조에 점수를 매겨서 원하는 로봇 행동을 달성하는 데 가장 좋은 참조를 선택할 수 있어. 우리의 Self-Aware 모델(SAW)은 낙하 가능성, 참조 동작 준수, 부드러움 등의 다양한 기준에 따라 잠재적인 로봇 행동을 순위 매겨. 

우리는 고급 모션 생성, 로봇 제어, SAW를 하나의 독특한 시스템에 통합해서 어떤 작업 명령에 대해서도 최적의 로봇 행동을 보장해. 예를 들어, SAW는 99.29%의 정확도로 낙하를 예측할 수 있어. 더 많은 정보는 우리 프로젝트 페이지에서 확인해: 이 URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.10310.pdf

Title: Safe and Real-Time Consistent Planning for Autonomous Vehicles in Partially Observed Environments via Parallel Consensus Optimization

Original Abstract:
Ensuring safety and driving consistency is a significant challenge for autonomous vehicles operating in partially observed environments. This work introduces a consistent parallel trajectory optimization (CPTO) approach to enable safe and consistent driving in dense obstacle environments with perception uncertainties. Utilizing discrete-time barrier function theory, we develop a consensus safety barrier module that ensures reliable safety coverage within the spatiotemporal trajectory space across potential obstacle configurations. Following this, a bi-convex parallel trajectory optimization problem is derived that facilitates decomposition into a series of low-dimensional quadratic programming problems to accelerate computation. By leveraging the consensus alternating direction method of multipliers (ADMM) for parallel optimization, each generated candidate trajectory corresponds to a possible environment configuration while sharing a common consensus trajectory segment. This ensures driving safety and consistency when executing the consensus trajectory segment for the ego vehicle in real time. We validate our CPTO framework through extensive comparisons with state-of-the-art baselines across multiple driving tasks in partially observable environments. Our results demonstrate improved safety and consistency using both synthetic and real-world traffic datasets.

Translated Abstract:
자율주행차가 부분적으로 관찰된 환경에서 안전성과 주행 일관성을 유지하는 건 큰 도전이에요. 이 연구에서는 밀집 장애물 환경에서 인식 불확실성을 고려해 안전하고 일관된 주행을 가능하게 하는 일관된 병렬 경로 최적화(CPTO) 방법을 소개해요.

먼저, 이산 시간 장벽 함수 이론을 활용해 잠재적인 장애물 배치에 따라 신뢰할 수 있는 안전 범위를 보장하는 합의 안전 장벽 모듈을 개발했어요. 그 다음, 이 문제를 해결하기 위해 이중 볼록 병렬 경로 최적화 문제를 도출하고, 이걸 저차원의 퀴드라틱 프로그래밍 문제로 나눌 수 있게 해요. 이렇게 하면 계산 속도가 빨라지죠.

또한, 합의 대체 방향 방법(ADMM)을 사용해 병렬 최적화를 수행하면, 생성된 각 후보 경로는 가능한 환경 구성에 대응하고, 공통 합의 경로 구간을 공유해요. 이 덕분에 자율주행차가 실시간으로 합의 경로 구간을 실행할 때 안전성과 일관성을 보장할 수 있어요.

우리는 CPTO 프레임워크를 여러 부분적으로 관찰된 환경에서 최첨단 기준들과 비교하면서 검증했어요. 그 결과, 합성 및 실제 교통 데이터셋을 사용해 안전성과 일관성이 개선된 걸 보여줬어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.10319.pdf

Title: Catch It! Learning to Catch in Flight with Mobile Dexterous Hands

Original Abstract:
Catching objects in flight (i.e., thrown objects) is a common daily skill for humans, yet it presents a significant challenge for robots. This task requires a robot with agile and accurate motion, a large spatial workspace, and the ability to interact with diverse objects. In this paper, we build a mobile manipulator composed of a mobile base, a 6-DoF arm, and a 12-DoF dexterous hand to tackle such a challenging task. We propose a two-stage reinforcement learning framework to efficiently train a whole-body-control catching policy for this high-DoF system in simulation. The objects' throwing configurations, shapes, and sizes are randomized during training to enhance policy adaptivity to various trajectories and object characteristics in flight. The results show that our trained policy catches diverse objects with randomly thrown trajectories, at a high success rate of about 80\% in simulation, with a significant improvement over the baselines. The policy trained in simulation can be directly deployed in the real world with onboard sensing and computation, which achieves catching sandbags in various shapes, randomly thrown by humans. Our project page is available at this https URL.

Translated Abstract:
날아오는 물체를 잡는 건 인간에게는 흔한 기술이지만, 로봇에게는 큰 도전이에요. 이 작업은 민첩하고 정확한 움직임, 넓은 작업 공간, 그리고 다양한 물체와 상호작용할 수 있는 능력이 필요한데요. 

이 논문에서는 이런 어려운 작업을 해결하기 위해 모바일 베이스, 6자유도 팔, 12자유도 손으로 구성된 모바일 조작기를 만들었어요. 우리는 이 고자유도 시스템을 위한 전체 몸 제어 잡기 정책을 효율적으로 훈련시키기 위해 두 단계 강화 학습 프레임워크를 제안해요. 훈련 동안 물체의 던지는 방식, 형태, 크기를 무작위로 바꿔서 다양한 궤적과 비행 중 물체 특성에 적응할 수 있도록 했어요. 

실험 결과, 훈련된 정책이 무작위로 던진 다양한 물체를 약 80%의 높은 성공률로 잡았고, 기본 방법들보다 성능이 크게 개선되었어요. 시뮬레이션에서 훈련된 정책은 실제 세계에서도 직접 사용할 수 있고, onboard 센싱과 계산을 통해 인간이 무작위로 던진 다양한 형태의 모래주머니를 잡는 데 성공했어요. 우리 프로젝트 페이지는 이 URL에 있어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.10320.pdf

Title: SEAL: Towards Safe Autonomous Driving via Skill-Enabled Adversary Learning for Closed-Loop Scenario Generation

Original Abstract:
Verification and validation of autonomous driving (AD) systems and components is of increasing importance, as such technology increases in real-world prevalence. Safety-critical scenario generation is a key approach to robustify AD policies through closed-loop training. However, existing approaches for scenario generation rely on simplistic objectives, resulting in overly-aggressive or non-reactive adversarial behaviors. To generate diverse adversarial yet realistic scenarios, we propose SEAL, a scenario perturbation approach which leverages learned scoring functions and adversarial, human-like skills. SEAL-perturbed scenarios are more realistic than SOTA baselines, leading to improved ego task success across real-world, in-distribution, and out-of-distribution scenarios, of more than 20%. To facilitate future research, we release our code and tools: this https URL

Translated Abstract:
자율주행(AD) 시스템과 구성 요소의 검증 및 유효성 검사는 실제로 이 기술이 더 많이 사용됨에 따라 점점 더 중요해지고 있어. 안전-critical 시나리오 생성은 AD 정책을 강화하기 위한 중요한 방법인데, 이는 닫힌 루프 훈련을 통해 이루어져. 

하지만 기존의 시나리오 생성 방법들은 너무 단순한 목표에 의존해서, 지나치게 공격적이거나 반응이 없는 적대적인 행동을 만들어내고 있어. 그래서 우리는 SEAL이라는 방법을 제안해. 이건 학습된 점수 함수와 인간과 비슷한 적대적인 기술을 활용해서 다양한 적대적이면서도 현실적인 시나리오를 생성하는 방법이야.

SEAL로 변형된 시나리오는 최신 기술(SOTA)보다 더 현실적이어서, 실제 환경, 분포 내, 분포 외 시나리오에서 에고 작업 성공률이 20% 이상 향상됐어. 앞으로의 연구를 도와주기 위해 우리의 코드와 도구도 공개할 예정이야: 이 링크 URL

================================================================================

URL:
https://arxiv.org/pdf/2409.10330.pdf

Title: DRIVE: Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving

Original Abstract:
Recent advancements in autonomous driving have seen a paradigm shift towards end-to-end learning paradigms, which map sensory inputs directly to driving actions, thereby enhancing the robustness and adaptability of autonomous vehicles. However, these models often sacrifice interpretability, posing significant challenges to trust, safety, and regulatory compliance. To address these issues, we introduce DRIVE -- Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving, a comprehensive framework designed to improve the dependability and stability of explanations in end-to-end unsupervised autonomous driving models. Our work specifically targets the inherent instability problems observed in the Driving through the Concept Gridlock (DCG) model, which undermine the trustworthiness of its explanations and decision-making processes. We define four key attributes of DRIVE: consistent interpretability, stable interpretability, consistent output, and stable output. These attributes collectively ensure that explanations remain reliable and robust across different scenarios and perturbations. Through extensive empirical evaluations, we demonstrate the effectiveness of our framework in enhancing the stability and dependability of explanations, thereby addressing the limitations of current models. Our contributions include an in-depth analysis of the dependability issues within the DCG model, a rigorous definition of DRIVE with its fundamental properties, a framework to implement DRIVE, and novel metrics for evaluating the dependability of concept-based explainable autonomous driving models. These advancements lay the groundwork for the development of more reliable and trusted autonomous driving systems, paving the way for their broader acceptance and deployment in real-world applications.

Translated Abstract:
최근 자율 주행 기술이 발전하면서, 감지 입력을 주행 행동에 직접 연결하는 '엔드 투 엔드' 학습 방식으로 변화하고 있어. 이렇게 하면 자율 차량의 신뢰성과 적응력이 향상돼. 하지만 이런 모델들은 해석 가능성을 희생하는 경우가 많아서, 신뢰성, 안전성, 규제 준수에 큰 도전 과제가 될 수 있어.

이런 문제를 해결하기 위해 우리는 DRIVE라는 프레임워크를 소개해. DRIVE는 '신뢰할 수 있고 강력하며 해석 가능한 비전 기반 앙상블 프레임워크'로, 엔드 투 엔드 비지도 자율 주행 모델에서 설명의 신뢰성과 안정성을 높이도록 설계된 거야. 우리의 연구는 특히 '개념 정체(gridlock)' 문제에서 나타나는 불안정성을 해결하는 데 초점을 맞추고 있어. 이 문제는 DCG 모델의 설명과 의사 결정 과정의 신뢰성을 떨어뜨려.

DRIVE의 네 가지 주요 특성을 정의했어: 일관된 해석 가능성, 안정적인 해석 가능성, 일관된 출력, 안정적인 출력. 이 특성들은 다양한 상황과 변동에서도 설명이 신뢰할 수 있고 강력하게 유지되도록 보장해. 광범위한 실험 평가를 통해, 우리는 이 프레임워크가 설명의 안정성과 신뢰성을 높이는 데 효과적이라는 걸 보여줬어. 

우리가 기여한 부분으로는 DCG 모델 내 신뢰성 문제에 대한 심층 분석, DRIVE의 기본 특성에 대한 엄격한 정의, DRIVE를 구현할 수 있는 프레임워크, 그리고 개념 기반 설명 가능한 자율 주행 모델의 신뢰성을 평가하기 위한 새로운 지표들이 있어. 이런 발전은 더 신뢰할 수 있는 자율 주행 시스템 개발의 기초를 마련하고, 실제 응용에서 더 널리 받아들여지고 사용될 수 있는 길을 열어줄 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10332.pdf

Title: Escaping Local Minima: Hybrid Artificial Potential Field with Wall-Follower for Decentralized Multi-Robot Navigation

Original Abstract:
We tackle the challenges of decentralized multi-robot navigation in environments with nonconvex obstacles, where complete environmental knowledge is unavailable. While reactive methods like Artificial Potential Field (APF) offer simplicity and efficiency, they suffer from local minima, causing robots to become trapped due to their lack of global environmental awareness. Other existing solutions either rely on inter-robot communication, are limited to single-robot scenarios, or struggle to overcome nonconvex obstacles effectively.
Our proposed methods enable collision-free navigation using only local sensor and state information without a map. By incorporating a wall-following (WF) behavior into the APF approach, our method allows robots to escape local minima, even in the presence of nonconvex and dynamic obstacles including other robots. We introduce two algorithms for switching between APF and WF: a rule-based system and an encoder network trained on expert demonstrations. Experimental results show that our approach achieves substantially higher success rates compared to state-of-the-art methods, highlighting its ability to overcome the limitations of local minima in complex environments

Translated Abstract:
우리는 비볼록 장애물이 있는 환경에서 여러 로봇이 협력하여 내비게이션하는 문제를 다루고 있어. 이 환경에서는 모든 정보를 알 수 없어서 어려움이 있지. 인공지능 잠재장(APF) 같은 반응형 방법은 간단하고 효율적이지만, 로컬 미니마에 갇히는 문제점이 있어. 그래서 로봇들이 전역 환경을 잘 인식하지 못하면 움직이지 못하게 되지. 

기존의 다른 방법들은 로봇 간 통신에 의존하거나, 단일 로봇 상황에만 적합하거나, 비볼록 장애물을 효과적으로 넘지 못하는 경우가 많아. 

우리가 제안하는 방법은 지도 없이도 로컬 센서와 상태 정보만으로 충돌 없는 내비게이션을 가능하게 해. APF 접근 방식에 벽 따르기(WF) 행동을 추가함으로써, 로봇들이 비볼록 장애물이나 다른 로봇이 있는 상황에서도 로컬 미니마에서 벗어날 수 있어. 

우리는 APF와 WF 사이를 전환하는 두 가지 알고리즘을 소개하는데, 하나는 규칙 기반 시스템이고 다른 하나는 전문가 시연으로 훈련된 인코더 네트워크야. 실험 결과, 우리의 접근 방식이 최신 방법들보다 훨씬 높은 성공률을 보였고, 복잡한 환경에서 로컬 미니마의 한계를 극복하는 능력을 강조했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10333.pdf

Title: Stretchable Arduinos embedded in soft robots

Original Abstract:
To achieve real-world functionality, robots must have the ability to carry out decision-making computations. However, soft robots stretch and therefore need a solution other than rigid computers. Examples of embedding computing capacity into soft robots currently include appending rigid printed circuit boards (PCBs) to the robot, integrating soft logic gates, and exploiting material responses for material-embedded computation. Although promising, these approaches introduce limitations such as rigidity, tethers, or low logic gate density. The field of stretchable electronics has sought to solve these challenges, but a complete pipeline for direct integration of single-board computers, microcontrollers, and other complex circuitry into soft robots has remained elusive. We present a generalized method to translate any complex two-layer circuit into a soft, stretchable form. This enabled the creation of stretchable single-board microcontrollers (including Arduinos) and other commercial circuits (including Sparkfun circuits), without design simplifications. As demonstrations of the method's utility, we embed highly stretchable (>300% strain) Arduino Pro Minis into the bodies of multiple soft robots. This makes use of otherwise inert structural material, fulfilling the promise of the stretchable electronics field to integrate state-of-the-art computational power into robust, stretchable systems during active use.

Translated Abstract:
로봇이 실제 세상에서 제대로 기능하려면 의사결정 계산을 할 수 있어야 해. 하지만 소프트 로봇은 늘어나기 때문에 딱딱한 컴퓨터로는 해결이 안 돼. 현재 소프트 로봇에 컴퓨팅 능력을 넣는 방법으로는 로봇에 딱딱한 인쇄 회로 기판(PCB)을 붙이거나, 소프트 논리 게이트를 통합하거나, 재료의 반응을 이용하는 방법이 있어. 이런 방법들이 가능성은 있지만, 딱딱함, 연결선, 낮은 논리 게이트 밀도 같은 한계도 있어.

신축성 전자공학 분야는 이런 문제를 해결하려고 했지만, 단일 보드 컴퓨터나 마이크로컨트롤러, 복잡한 회로를 소프트 로봇에 직접 통합하는 완벽한 방법은 아직 없었어. 우리는 어떤 복잡한 2층 회로도 소프트하고 신축 가능한 형태로 변환하는 일반화된 방법을 제시해. 이 덕분에 설계 단순화 없이 신축 가능한 단일 보드 마이크로컨트롤러(아두이노 포함)와 다른 상용 회로(스파크펀 회로 포함)를 만들 수 있게 됐어.

이 방법의 유용성을 보여주기 위해서, 우리는 여러 소프트 로봇의 몸체에 신축성이 매우 높은 아두이노 프로 미니를 내장했어. 이렇게 해서 일반적으로는 무해한 구조적 재료를 활용하면서, 신축성 전자공학 분야의 약속을 이뤄서 최첨단 계산 능력을 강력하고 신축 가능한 시스템에 통합할 수 있게 됐어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10347.pdf

Title: Digital Twins Meet the Koopman Operator: Data-Driven Learning for Robust Autonomy

Original Abstract:
Contrary to on-road autonomous navigation, off-road autonomy is complicated by various factors ranging from sensing challenges to terrain variability. In such a milieu, data-driven approaches have been commonly employed to capture intricate vehicle-environment interactions effectively. However, the success of data-driven methods depends crucially on the quality and quantity of data, which can be compromised by large variability in off-road environments. To address these concerns, we present a novel workflow to recreate the exact vehicle and its target operating conditions digitally for domain-specific data generation. This enables us to effectively model off-road vehicle dynamics from simulation data using the Koopman operator theory, and employ the obtained models for local motion planning and optimal vehicle control. The capabilities of the proposed methodology are demonstrated through an autonomous navigation problem of a 1:5 scale vehicle, where a terrain-informed planner is employed for global mission planning. Results indicate a substantial improvement in off-road navigation performance with the proposed algorithm (5.84x) and underscore the efficacy of digital twinning in terms of improving the sample efficiency (3.2x) and reducing the sim2real gap (5.2%).

Translated Abstract:
도로에서의 자율 주행과는 달리, 오프로드 자율 주행은 센서 문제부터 지형의 다양성까지 여러 가지 요인 때문에 복잡해. 이런 상황에서 데이터 기반 접근 방식이 차량과 환경 간의 복잡한 상호작용을 잘 포착하기 위해 흔히 사용돼. 하지만 데이터 기반 방법의 성공은 데이터의 질과 양에 크게 의존하는데, 오프로드 환경에서는 이게 잘 안 될 수 있어.

이 문제를 해결하기 위해, 우리는 도메인 특화 데이터 생성을 위해 차량과 그 작동 조건을 정확하게 디지털로 재현하는 새로운 워크플로우를 제안해. 이 방법으로 우리는 Koopman 운영자 이론을 사용해서 시뮬레이션 데이터를 기반으로 오프로드 차량의 동역학을 효과적으로 모델링할 수 있어. 그리고 이렇게 얻은 모델을 사용해서 지역적인 동작 계획과 최적 차량 제어를 할 수 있어.

제안된 방법론의 능력은 1:5 스케일 차량의 자율 주행 문제를 통해 보여졌어. 여기서 지형 정보를 고려한 계획기가 글로벌 미션 계획에 사용됐어. 결과적으로, 제안된 알고리즘으로 오프로드 내비게이션 성능이 크게 개선되었고(5.84배), 디지털 트윈이 샘플 효율성을 높이고(3.2배) 실제와 시뮬레이션 간의 격차를 줄이는 데 효과적임을 강조했어(5.2%).

================================================================================

URL:
https://arxiv.org/pdf/2409.10350.pdf

Title: Point2Graph: An End-to-end Point Cloud-based 3D Open-Vocabulary Scene Graph for Robot Navigation

Original Abstract:
Current open-vocabulary scene graph generation algorithms highly rely on both 3D scene point cloud data and posed RGB-D images and thus have limited applications in scenarios where RGB-D images or camera poses are not readily available. To solve this problem, we propose Point2Graph, a novel end-to-end point cloud-based 3D open-vocabulary scene graph generation framework in which the requirement of posed RGB-D image series is eliminated. This hierarchical framework contains room and object detection/segmentation and open-vocabulary classification. For the room layer, we leverage the advantage of merging the geometry-based border detection algorithm with the learning-based region detection to segment rooms and create a "Snap-Lookup" framework for open-vocabulary room classification. In addition, we create an end-to-end pipeline for the object layer to detect and classify 3D objects based solely on 3D point cloud data. Our evaluation results show that our framework can outperform the current state-of-the-art (SOTA) open-vocabulary object and room segmentation and classification algorithm on widely used real-scene datasets.

Translated Abstract:
현재의 개방형 어휘 장면 그래프 생성 알고리즘은 3D 장면 포인트 클라우드 데이터와 포즈가 있는 RGB-D 이미지에 크게 의존하고 있어서, RGB-D 이미지나 카메라 포즈가 쉽게 구할 수 없는 상황에서는 제한적으로만 사용될 수 있어. 이 문제를 해결하기 위해 우리는 Point2Graph라는 새로운 엔드 투 엔드 포인트 클라우드 기반 3D 개방형 어휘 장면 그래프 생성 프레임워크를 제안해. 여기서는 포즈가 있는 RGB-D 이미지 시리즈를 필요로 하지 않아.

이 계층적 프레임워크는 방과 물체 감지/분할 및 개방형 어휘 분류를 포함하고 있어. 방 레이어에서는 기하학 기반의 경계 감지 알고리즘과 학습 기반의 영역 감지를 결합해 방을 분할하고, 개방형 어휘 방 분류를 위한 "Snap-Lookup" 프레임워크를 만들어. 그리고 물체 레이어를 위해 3D 포인트 클라우드 데이터만을 기반으로 3D 물체를 감지하고 분류하는 엔드 투 엔드 파이프라인도 만들었어.

우리의 평가 결과에 따르면, 이 프레임워크는 널리 사용되는 실제 장면 데이터셋에서 현재의 최첨단(open-vocabulary) 물체와 방 분할 및 분류 알고리즘보다 더 뛰어난 성능을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.10366.pdf

Title: Global Uncertainty-Aware Planning for Magnetic Anomaly-Based Navigation

Original Abstract:
Navigating and localizing in partially observable, stochastic environments with magnetic anomalies presents significant challenges, especially when balancing the accuracy of state estimation and the stability of localization. Traditional approaches often struggle to maintain performance due to limited localization updates and dynamic conditions. This paper introduces a multi-objective global path planner for magnetic anomaly navigation (MagNav), which leverages entropy maps to assess spatial frequency variations in magnetic fields and identify high-information areas. The system generates paths toward these regions by employing a potential field planner, enhancing active localization. Hardware experiments demonstrate that the proposed method significantly improves localization stability and accuracy compared to existing active localization techniques. The results underscore the effectiveness of this method in reducing localization uncertainty and highlight its adaptability to various gradient-based navigation maps, including topographical and underwater depth-based environments.

Translated Abstract:
부분적으로 관찰할 수 있는 확률적 환경에서 자기 이상을 탐색하고 위치를 파악하는 것은 큰 도전 과제를 안고 있어. 특히 상태 추정의 정확성과 위치 파악의 안정성을 균형 있게 맞추는 게 어려워. 전통적인 방법들은 제한된 위치 업데이트와 동적인 조건 때문에 성능을 유지하기 힘든 경우가 많아.

이 논문에서는 자기 이상 탐색을 위한 다목적 글로벌 경로 계획기인 MagNav를 소개해. 이 시스템은 엔트로피 맵을 활용해서 자기장 내 공간 주파수 변화를 분석하고 높은 정보가 있는 지역을 찾아내. 그런 다음, 잠재적 필드 계획기를 사용하여 이 지역을 향한 경로를 생성해, 능동적인 위치 파악을 강화하는 거야.

하드웨어 실험 결과, 이 방법이 기존의 능동적 위치 파악 기술과 비교했을 때 위치 파악의 안정성과 정확성을 크게 향상시킨다는 걸 보여줬어. 이 결과는 위치 파악의 불확실성을 줄이는 데 이 방법이 효과적이라는 걸 강조하고, 지형 기반이나 수중 깊이 기반 환경 같은 다양한 기울기 기반 탐색 맵에 적응할 수 있는 능력도 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.10371.pdf

Title: Learning Gentle Grasping from Human-Free Force Control Demonstration

Original Abstract:
Humans can steadily and gently grasp unfamiliar objects based on tactile perception. Robots still face challenges in achieving similar performance due to the difficulty of learning accurate grasp-force predictions and force control strategies that can be generalized from limited data. In this article, we propose an approach for learning grasping from ideal force control demonstrations, to achieve similar performance of human hands with limited data size. Our approach utilizes objects with known contact characteristics to automatically generate reference force curves without human demonstrations. In addition, we design the dual convolutional neural networks (Dual-CNN) architecture which incorporating a physics-based mechanics module for learning target grasping force predictions from demonstrations. The described method can be effectively applied in vision-based tactile sensors and enables gentle and stable grasping of objects from the ground. The described prediction model and grasping strategy were validated in offline evaluations and online experiments, and the accuracy and generalizability were demonstrated.

Translated Abstract:
사람들은 촉각을 바탕으로 낯선 물체를 부드럽고 안정적으로 잡을 수 있어. 그런데 로봇은 비슷한 성능을 내기 어려운 상황이야. 왜냐하면 제한된 데이터로부터 정확한 힘 예측과 힘 제어 전략을 배우는 게 힘들기 때문이지. 

이 논문에서는 이상적인 힘 제어 시연을 통해 물체를 잡는 방법을 배우는 접근법을 제안해. 이 방법은 제한된 데이터로도 사람의 손처럼 비슷한 성능을 내는 걸 목표로 하고 있어. 우리가 사용하는 물체는 접촉 특성이 알려진 것들이고, 이걸로 자동으로 기준 힘 곡선을 만들 수 있어. 그래서 사람의 시연 없이도 가능한 거지.

게다가 우리는 물체를 잡기 위한 목표 힘 예측을 배우기 위해 물리 기반 메커닉스 모듈을 포함한 이중 컨볼루션 신경망(Dual-CNN) 구조를 설계했어. 이 방법은 비전 기반 촉각 센서에 효과적으로 적용될 수 있고, 바닥에 있는 물체를 부드럽고 안정적으로 잡을 수 있게 해. 

이 예측 모델과 잡는 전략은 오프라인 평가와 온라인 실험에서 검증되었고, 정확성과 일반화 가능성을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10375.pdf

Title: Decentralized and Asymmetric Multi-Agent Learning in Construction Sites

Original Abstract:
Multi-agent collaboration involves multiple participants working together in a shared environment to achieve a common goal. These agents share information, divide tasks, and synchronize their actions. Key aspects of multi agent collaboration include coordination, communication, task allocation, cooperation, adaptation, and decentralization. On construction sites, surface grading is the process of leveling sand piles to increase a specific area's height. In this scenario, a bulldozer grades while a dumper allocates sand piles. Our work aims to utilize a multi-agent approach to enable these vehicles to collaborate effectively. To this end, we propose a decentralized and asymmetric multi-agent learning approach for construction sites (DAMALCS). We formulate DAMALCS to reduce expected collisions for operating vehicles. Therefore, we develop two heuristic experts capable of achieving their joint goal optimally by applying an innovative prioritization method. In this approach, the bulldozer's movements take precedence over the dumper's operations, enabling the bulldozer to clear the path for the dumper and ensure continuous operation of both vehicles. Since heuristics alone are insufficient in real-world scenarios, we utilize them to train AI agents, which proves to be highly effective. We simultaneously train the bulldozer and dumper agents to operate within the same environment, aiming to avoid collisions and optimize performance in terms of time efficiency and sand volume handling. Our trained agents and heuristics are evaluated in both simulation and real-world lab experiments, testing them under various conditions, such as visual noise and localization errors. The results demonstrate that our approach significantly reduces collision rates for these vehicles.

Translated Abstract:
멀티 에이전트 협업은 여러 참가자가 함께 작업하여 공동 목표를 달성하는 걸 말해. 이 에이전트들은 정보를 공유하고, 작업을 나누고, 행동을 조정해. 멀티 에이전트 협업의 핵심 요소로는 조정, 커뮤니케이션, 작업 배분, 협력, 적응, 분산화가 있어.

건설 현장에서 표면 평탄화는 특정 지역의 높이를 증가시키기 위해 모래 더미를 평탄하게 만드는 과정이야. 이 상황에서 불도저가 평탄화를 하고, 덤퍼는 모래 더미를 배분해. 우리 연구는 이런 차량들이 효과적으로 협력할 수 있도록 멀티 에이전트 접근 방식을 활용하는 걸 목표로 해. 그래서 우리는 건설 현장을 위한 분산 비대칭 멀티 에이전트 학습 접근법인 DAMALCS를 제안해.

DAMALCS는 운전하는 차량 간의 예상 충돌을 줄이도록 설계했어. 이를 위해 혁신적인 우선순위 방법을 적용해 최적의 공동 목표를 달성할 수 있는 두 가지 휴리스틱 전문가를 개발했어. 이 접근에서는 불도저의 움직임이 덤퍼의 작업보다 우선시돼, 불도저가 덤퍼가 작업할 수 있도록 길을 확보해주고 두 차량이 계속 작동할 수 있도록 해.

하지만 현실 세계에서는 휴리스틱만으로는 부족해. 그래서 우리는 이들을 사용해 AI 에이전트를 훈련시키는데, 이게 정말 효과적이야. 불도저와 덤퍼 에이전트를 같은 환경에서 동시에 훈련시켜서 충돌을 피하고 시간 효율성과 모래 처리량을 최적화하는 걸 목표로 해. 훈련된 에이전트와 휴리스틱은 시뮬레이션과 실제 실험실 실험에서 평가돼. 다양한 조건, 예를 들어 시각적 잡음이나 위치 오류 같은 상황에서 테스트했어. 결과적으로 우리 접근법이 이 차량들의 충돌률을 크게 줄인다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10419.pdf

Title: HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models

Original Abstract:
Robots interacting with humans through natural language can unlock numerous applications such as Referring Grasp Synthesis (RGS). Given a text query, RGS determines a stable grasp pose to manipulate the referred object in the robot's workspace. RGS comprises two steps: visual grounding and grasp pose estimation. Recent studies leverage powerful Vision-Language Models (VLMs) for visually grounding free-flowing natural language in real-world robotic execution. However, comparisons in complex, cluttered environments with multiple instances of the same object are lacking. This paper introduces HiFi-CS, featuring hierarchical application of Featurewise Linear Modulation (FiLM) to fuse image and text embeddings, enhancing visual grounding for complex attribute rich text queries encountered in robotic grasping. Visual grounding associates an object in 2D/3D space with natural language input and is studied in two scenarios: Closed and Open Vocabulary. HiFi-CS features a lightweight decoder combined with a frozen VLM and outperforms competitive baselines in closed vocabulary settings while being 100x smaller in size. Our model can effectively guide open-set object detectors like GroundedSAM to enhance open-vocabulary performance. We validate our approach through real-world RGS experiments using a 7-DOF robotic arm, achieving 90.33\% visual grounding accuracy in 15 tabletop scenes. We include our codebase in the supplementary material.

Translated Abstract:
로봇이 자연어를 통해 사람과 소통하면 여러 가지 응용이 가능해져. 그 중 하나가 '참조 그립 합성(Referring Grasp Synthesis, RGS)'이야. 텍스트 쿼리를 주면 RGS는 로봇 작업 공간에서 언급된 객체를 조작하기 위한 안정적인 그립 자세를 결정해. RGS는 두 단계로 나뉘는데, 시각적 위치 확인과 그립 자세 추정이야.

최근 연구에서는 강력한 비전-언어 모델(VLM)을 활용해 실제 로봇 실행에서 자연어를 시각적으로 확인하는 방법이 사용되고 있어. 하지만 같은 객체가 여러 개 있는 복잡한 환경에서의 비교는 부족해. 이 논문에서는 HiFi-CS를 소개하는데, 이건 이미지와 텍스트 임베딩을 결합하기 위해 피처별 선형 변조(Featurewise Linear Modulation, FiLM)를 계층적으로 적용하는 방법이야. 이 덕분에 로봇 그립에 필요한 복잡한 속성을 가진 텍스트 쿼리의 시각적 위치 확인이 강화돼.

시각적 위치 확인은 2D/3D 공간에서 객체를 자연어 입력과 연결하는 과정인데, 두 가지 시나리오에서 연구했어: 닫힌 어휘와 열린 어휘. HiFi-CS는 경량 디코더와 고정된 VLM을 결합해서 닫힌 어휘 환경에서 경쟁 모델들보다 더 나은 성능을 보여주고, 크기는 100배 작아. 우리의 모델은 GroundedSAM 같은 열린 세트 객체 탐지기를 효과적으로 안내해 열려 있는 어휘 성능을 높일 수 있어.

우리는 7-DOF 로봇 팔을 사용한 실제 RGS 실험을 통해 우리의 접근 방식을 검증했고, 15개의 테이블 장면에서 90.33%의 시각적 위치 확인 정확도를 달성했어. 코드베이스는 보충 자료에 포함되어 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10441.pdf

Title: CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera

Original Abstract:
Camera-to-robot calibration is crucial for vision-based robot control and requires effort to make it accurate. Recent advancements in markerless pose estimation methods have eliminated the need for time-consuming physical setups for camera-to-robot calibration. While the existing markerless pose estimation methods have demonstrated impressive accuracy without the need for cumbersome setups, they rely on the assumption that all the robot joints are visible within the camera's field of view. However, in practice, robots usually move in and out of view, and some portion of the robot may stay out-of-frame during the whole manipulation task due to real-world constraints, leading to a lack of sufficient visual features and subsequent failure of these approaches. To address this challenge and enhance the applicability to vision-based robot control, we propose a novel framework capable of estimating the robot pose with partially visible robot manipulators. Our approach leverages the Vision-Language Models for fine-grained robot components detection, and integrates it into a keypoint-based pose estimation network, which enables more robust performance in varied operational conditions. The framework is evaluated on both public robot datasets and self-collected partial-view datasets to demonstrate our robustness and generalizability. As a result, this method is effective for robot pose estimation in a wider range of real-world manipulation scenarios.

Translated Abstract:
카메라와 로봇의 보정은 비전 기반 로봇 제어에 정말 중요해. 정확하게 보정하려면 많은 노력이 필요해. 최근에 마커가 필요 없는 포즈 추정 방법이 발전하면서, 카메라와 로봇 보정을 위한 번거로운 물리적 세팅이 필요 없어졌어.

하지만 기존의 마커 없는 포즈 추정 방법들은 로봇의 모든 관절이 카메라의 시야에 있어야 한다는 가정에 의존해. 실제로는 로봇이 시야 안팎으로 움직이고, 조작 작업 동안 로봇의 일부가 프레임 밖에 있을 수 있어. 이런 상황 때문에 충분한 시각적 특징이 부족해지고, 결과적으로 방법들이 실패할 수 있어.

이 문제를 해결하고 비전 기반 로봇 제어에 더 잘 적용할 수 있도록, 우리는 부분적으로 보이는 로봇 조작기를 가지고 로봇의 포즈를 추정할 수 있는 새로운 프레임워크를 제안해. 우리 접근법은 비전-언어 모델을 활용해서 로봇 부품을 세밀하게 감지하고, 이를 키포인트 기반 포즈 추정 네트워크에 통합해. 이렇게 하면 다양한 작업 환경에서 더 강력한 성능을 낼 수 있어.

이 프레임워크는 공개된 로봇 데이터셋과 우리가 직접 수집한 부분 시야 데이터셋에서 평가되어, 우리의 강력함과 일반화 가능성을 보여줬어. 결과적으로, 이 방법은 다양한 실제 조작 시나리오에서 로봇 포즈 추정에 효과적이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.10444.pdf

Title: LLM as BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot Task Planning

Original Abstract:
Robotic assembly tasks are open challenges due to the long task horizon and complex part relations. Behavior trees (BTs) are increasingly used in robot task planning for their modularity and flexibility, but manually designing them can be effort-intensive. Large language models (LLMs) have recently been applied in robotic task planning for generating action sequences, but their ability to generate BTs has not been fully investigated. To this end, We propose LLM as BT-planner, a novel framework to leverage LLMs for BT generation in robotic assembly task planning and execution. Four in-context learning methods are introduced to utilize the natural language processing and inference capabilities of LLMs to produce task plans in BT format, reducing manual effort and ensuring robustness and comprehensibility. We also evaluate the performance of fine-tuned, fewer-parameter LLMs on the same tasks. Experiments in simulated and real-world settings show that our framework enhances LLMs' performance in BT generation, improving success rates in BT generation through in-context learning and supervised fine-tuning.

Translated Abstract:
로봇 조립 작업은 작업이 길고 부품 간의 관계가 복잡해서 해결하기 어려운 문제야. 행동 트리(Behavior Trees, BTs)는 모듈화와 유연성 덕분에 로봇 작업 계획에 점점 더 많이 사용되고 있지만, 직접 설계하기는 꽤 힘들어. 

최근에 대규모 언어 모델(LLMs)이 로봇 작업 계획에서 행동 시퀀스를 생성하는 데 사용되고 있는데, BT를 생성하는 능력은 아직 제대로 연구되지 않았어. 그래서 우리는 LLM을 BT 플래너로 활용하는 새로운 프레임워크를 제안해. 이 프레임워크는 로봇 조립 작업 계획과 실행에서 BT 생성을 위해 LLM을 활용하는 거야. 

여기서 네 가지 인컨텍스트 학습 방법을 소개하여 LLM의 자연어 처리와 추론 능력을 이용해 BT 형식으로 작업 계획을 만들어내고, 수작업을 줄이며 강력함과 이해하기 쉬운 결과를 보장해. 

또한, 동일한 작업에 대해 미세 조정된 적은 수의 매개변수를 가진 LLM의 성능도 평가해봤어. 시뮬레이션과 실제 환경에서의 실험 결과, 우리의 프레임워크가 LLM의 BT 생성 성능을 향상시켜 성공률을 높인다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10469.pdf

Title: Real-Time Whole-Body Control of Legged Robots with Model-Predictive Path Integral Control

Original Abstract:
This paper presents a system for enabling real-time synthesis of whole-body locomotion and manipulation policies for real-world legged robots. Motivated by recent advancements in robot simulation, we leverage the efficient parallelization capabilities of the MuJoCo simulator to achieve fast sampling over the robot state and action trajectories. Our results show surprisingly effective real-world locomotion and manipulation capabilities with a very simple control strategy. We demonstrate our approach on several hardware and simulation experiments: robust locomotion over flat and uneven terrains, climbing over a box whose height is comparable to the robot, and pushing a box to a goal position. To our knowledge, this is the first successful deployment of whole-body sampling-based MPC on real-world legged robot hardware. Experiment videos and code can be found at: this https URL

Translated Abstract:
이 논문에서는 실제 다리 로봇을 위한 전신 이동 및 조작 정책을 실시간으로 합성하는 시스템을 소개해. 최근 로봇 시뮬레이션에서의 발전 덕분에, 우리는 MuJoCo 시뮬레이터의 효율적인 병렬 처리 기능을 활용해서 로봇의 상태와 동작 경로를 빠르게 샘플링할 수 있었어.

결과적으로, 아주 간단한 제어 전략으로도 현실 세계에서 놀라운 이동 및 조작 능력을 보여줬어. 우리는 여러 하드웨어와 시뮬레이션 실험을 통해 이 방법을 입증했어: 평평한 지면과 고르지 않은 지면에서의 강력한 이동, 로봇과 비슷한 높이의 상자를 넘는 것, 그리고 상자를 목표 위치로 밀어내는 실험이 포함돼.

우리가 아는 한, 이것은 실제 다리 로봇 하드웨어에서 전신 샘플링 기반 MPC를 성공적으로 배포한 첫 번째 사례야. 실험 비디오와 코드는 이 링크에서 확인할 수 있어: this https URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.10491.pdf

Title: Radar Teach and Repeat: Architecture and Initial Field Testing

Original Abstract:
Frequency-modulated continuous-wave (FMCW) scanning radar has emerged as an alternative to spinning LiDAR for state estimation on mobile robots. Radar's longer wavelength is less affected by small particulates, providing operational advantages in challenging environments such as dust, smoke, and fog. This paper presents Radar Teach and Repeat (RT&R): a full-stack radar system for long-term off-road robot autonomy. RT&R can drive routes reliably in off-road cluttered areas without any GPS. We benchmark the radar system's closed-loop path-tracking performance and compare it to its 3D LiDAR counterpart. 11.8 km of autonomous driving was completed without interventions using only radar and gyro for navigation. RT&R was evaluated on different routes with progressively less structured scene geometry. RT&R achieved lateral path-tracking root mean squared errors (RMSE) of 5.6 cm, 7.5 cm, and 12.1 cm as the routes became more challenging. On the robot we used for testing, these RMSE values are less than half of the width of one tire (24 cm). These same routes have worst-case errors of 21.7 cm, 24.0 cm, and 43.8 cm. We conclude that radar is a viable alternative to LiDAR for long-term autonomy in challenging off-road scenarios. The implementation of RT&R is open-source and available at: this https URL.

Translated Abstract:
주파수 변조 연속파(FMCW) 스캐닝 레이더는 모바일 로봇의 상태 추정을 위해 회전식 LiDAR의 대안으로 떠올랐어. 레이더는 긴 파장을 가지고 있어서 작은 입자에 덜 영향을 받아서, 먼지, 연기, 안개 같은 어려운 환경에서도 잘 작동해.

이 논문에서는 레이더 교육 및 반복(RT&R)이라는 시스템을 소개해. RT&R은 오프로드 로봇의 장기 자율 주행을 위한 완전한 레이더 시스템이야. 이 시스템은 GPS 없이도 복잡한 오프로드 지역에서 신뢰성 있게 경로를 주행할 수 있어. 우리는 레이더 시스템의 폐쇄 루프 경로 추적 성능을 테스트하고, 3D LiDAR와 비교했어. 레이더와 자이로만 사용해서 11.8km의 자율 주행을 중단 없이 완료했어.

RT&R은 점점 더 구조가 덜한 장면에서 다양한 경로로 평가했어. 경로가 더 어려워질수록 RT&R의 측면 경로 추적 평균 제곱근 오차(RMSE)는 각각 5.6 cm, 7.5 cm, 12.1 cm로 나타났어. 우리가 테스트에 사용한 로봇에서 이 RMSE 값들은 한 타이어의 폭(24 cm)보다도 절반도 안 돼. 같은 경로에서 최악의 경우 오차는 21.7 cm, 24.0 cm, 43.8 cm였어.

결론적으로, 레이더는 어려운 오프로드 상황에서 장기 자율 주행을 위한 LiDAR의 유효한 대안이야. RT&R의 구현은 오픈 소스로 제공되며, 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09059.pdf

Title: SDP Synthesis of Distributionally Robust Backward Reachable Trees for Probabilistic Planning

Original Abstract:
The paper presents Maximal Ellipsoid Backward Reachable Trees MAXELLIPSOID BRT, which is a multi-query algorithm for planning of dynamic systems under stochastic motion uncertainty and constraints on the control input. In contrast to existing probabilistic planning methods that grow a roadmap of distributions, our proposed method introduces a framework to construct a roadmap of ambiguity sets of distributions such that each edge in our proposed roadmap provides a feasible control sequence for a family of distributions at once leading to efficient multi-query planning. Specifically, we construct a backward reachable tree of maximal size ambiguity sets and the corresponding distributionally robust edge controllers. Experiments show that the computation of these sets of distributions, in a backwards fashion from the goal, leads to efficient planning at a fraction of the size of the roadmap required for state-of-the-art methods. The computation of these maximal ambiguity sets and edges is carried out via a convex semidefinite relaxation to a novel nonlinear program. We also formally prove a theorem on maximum coverage for a technique proposed in our prior work.

Translated Abstract:
이 논문에서는 MAXELLIPSOID BRT라는 최대 타원체 역도달 트리를 제안해. 이건 동적 시스템을 계획하는 멀티 쿼리 알고리즘으로, 불확실한 움직임과 제어 입력의 제약 조건을 고려해. 기존의 확률적 계획 방법들은 분포의 로드맵을 만드는 반면, 우리가 제안하는 방법은 모호성 집합의 로드맵을 구성하는 프레임워크를 도입해. 그래서 우리의 로드맵의 각 엣지는 여러 분포에 대해 동시에 가능한 제어 순서를 제공해서 효율적인 멀티 쿼리 계획이 가능해.

구체적으로, 우리는 최대 크기의 모호성 집합의 역도달 트리와 이에 해당하는 분포적으로 강건한 엣지 제어기를 구축해. 실험 결과에 따르면, 목표 지점에서 역으로 이들 분포 집합을 계산하는 것이 최신 방법에 필요한 로드맵 크기의 일부만으로 효율적인 계획을 이끌어낸다는 걸 보여줘. 이 최대 모호성 집합과 엣지를 계산하는 과정은 새로운 비선형 프로그램에 대해 볼록 반정형 완화 방법을 통해 진행돼. 마지막으로, 우리는 이전 연구에서 제안한 기술에 대한 최대 범위에 관한 정리를 공식적으로 증명했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09312.pdf

Title: Registration between Point Cloud Streams and Sequential Bounding Boxes via Gradient Descent

Original Abstract:
In this paper, we propose an algorithm for registering sequential bounding boxes with point cloud streams. Unlike popular point cloud registration techniques, the alignment of the point cloud and the bounding box can rely on the properties of the bounding box, such as size, shape, and temporal information, which provides substantial support and performance gains. Motivated by this, we propose a new approach to tackle this problem. Specifically, we model the registration process through an overall objective function that includes the final goal and all constraints. We then optimize the function using gradient descent. Our experiments show that the proposed method performs remarkably well with a 40\% improvement in IoU and demonstrates more robust registration between point cloud streams and sequential bounding boxes

Translated Abstract:
이 논문에서는 점 구름 스트림과 순차적인 바운딩 박스를 등록하는 알고리즘을 제안해. 기존의 점 구름 등록 기술과는 달리, 점 구름과 바운딩 박스의 정렬은 바운딩 박스의 크기, 형태, 시간 정보 같은 속성에 의존할 수 있어. 이게 성능 향상에 크게 도움을 줘.

그래서 우리는 이 문제를 해결하기 위한 새로운 접근 방식을 제안했어. 구체적으로는, 최종 목표와 모든 제약 조건을 포함한 전체 목적 함수를 통해 등록 과정을 모델링했어. 그 다음에 경량 하강법을 사용해서 이 함수를 최적화했지.

실험 결과, 제안한 방법이 IoU에서 40% 개선된 성과를 보였고, 점 구름 스트림과 순차적인 바운딩 박스 간의 등록이 더 견고하게 이루어졌어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09500.pdf

Title: A Data-Informed Analysis of Scalable Supervision for Safety in Autonomous Vehicle Fleets

Original Abstract:
Autonomous driving is a highly anticipated approach toward eliminating roadway fatalities. At the same time, the bar for safety is both high and costly to verify. This work considers the role of remotely-located human operators supervising a fleet of autonomous vehicles (AVs) for safety. Such a 'scalable supervision' concept was previously proposed to bridge the gap between still-maturing autonomy technology and the pressure to begin commercial offerings of autonomous driving. The present article proposes DISCES, a framework for Data-Informed Safety-Critical Event Simulation, to investigate the practicality of this concept from a dynamic network loading standpoint. With a focus on the safety-critical context of AVs merging into mixed-autonomy traffic, vehicular arrival processes at 1,097 highway merge points are modeled using microscopic traffic reconstruction with historical data from interstates across three California counties. Combined with a queuing theoretic model, these results characterize the dynamic supervision requirements and thereby scalability of the teleoperation approach. Across all scenarios we find reductions in operator requirements greater than 99% as compared to in-vehicle supervisors for the time period analyzed. The work also demonstrates two methods for reducing these empirical supervision requirements: (i) the use of cooperative connected AVs -- which are shown to produce an average 3.67 orders-of-magnitude system reliability improvement across the scenarios studied -- and (ii) aggregation across larger regions.

Translated Abstract:
자율주행은 도로에서의 사고를 없애기 위한 기대되는 방법이야. 하지만 안전 기준이 높고, 이를 검증하는 데 비용이 많이 들어. 이 연구는 원격에 있는 인간 운영자가 자율주행차(AV)를 감독하는 역할을 고려하고 있어. 이런 '확장 가능한 감독' 개념은 아직 발전 중인 자율주행 기술과 상용화를 시작해야 하는 압박 사이의 간극을 메우기 위해 이전에 제안된 거야.

이번 논문에서는 DISCES라는 프레임워크를 제안해. 이건 데이터 기반 안전 중요 사건 시뮬레이션을 통해 이 개념의 실용성을 동적인 네트워크 부하 관점에서 조사하는 거야. AV가 혼합 자율 교통에 합류하는 안전 중요 상황에 초점을 맞추고, 1,097개의 고속도로 합류 지점에서 차량 도착 과정을 마이크로 교통 재구성을 통해 모델링했어. 캘리포니아의 세 개 카운티에서의 역사적 데이터를 사용했지. 

이 결과는 대기 이론 모델과 결합돼서 동적 감독 요구 사항과 원격 조작 방식의 확장성을 나타내. 모든 시나리오에서 분석된 기간 동안 차량 내 감독자와 비교했을 때 운영자 요구가 99% 이상 줄어드는 걸 발견했어. 이 연구는 또한 두 가지 방법으로 이러한 감독 요구를 줄이는 방법을 보여줘: (i) 협력적인 연결 AV를 사용하는 것, 이건 연구된 시나리오에서 평균적으로 시스템 신뢰성을 3.67 배 향상시킨다는 걸 보여줬고, (ii) 더 큰 지역에서의 집합이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09550.pdf

Title: Swarm Algorithms for Dynamic Task Allocation in Unknown Environments

Original Abstract:
Robot swarms, systems of many robots that operate in a distributed fashion, have many applications in areas such as search-and-rescue, natural disaster response, and self-assembly. Several of these applications can be abstracted to the general problem of task allocation in an environment, in which robots must assign themselves to and complete tasks. While several algorithms for task allocation have been proposed, most of them assume either prior knowledge of task locations or a static set of tasks. Operating under a discrete general model where tasks dynamically appear in unknown locations, we present three new swarm algorithms for task allocation. We demonstrate that when tasks appear slowly, our variant of a distributed algorithm based on propagating task information completes tasks more efficiently than a Levy random walk algorithm, which is a strategy used by many organisms in nature to efficiently search an environment. We also propose a division of labor algorithm where some agents are using our algorithm based on propagating task information while the remaining agents are using the Levy random walk algorithm. Finally, we introduce a hybrid algorithm where each agent dynamically switches between using propagated task information and following a Levy random walk. We show that our division of labor and hybrid algorithms can perform better than both our algorithm based on propagated task information and the Levy walk algorithm, especially at low and medium task rates. When tasks appear fast, we observe the Levy random walk strategy performs as well or better when compared to these novel approaches. Our work demonstrates the relative performance of these algorithms on a variety of task rates and also provide insight into optimizing our algorithms based on environment parameters.

Translated Abstract:
로봇 군집은 여러 로봇이 분산 방식으로 작동하는 시스템으로, 구조 작업, 자연 재해 대응, 자가 조립 등 다양한 분야에서 활용되고 있어. 이런 응용 중 몇 가지는 로봇이 환경에서 스스로 작업을 할당받고 수행해야 하는 일반적인 작업 할당 문제로 추상화할 수 있어. 여러 작업 할당 알고리즘이 제안되었지만, 대부분은 작업 위치에 대한 사전 지식이나 고정된 작업 세트를 가정해.

우리는 작업이 동적으로 나타나는 미지의 장소에서 작동하는 이산 일반 모델을 기반으로 세 가지 새로운 군집 알고리즘을 제시해. 작업이 느리게 나타날 때, 작업 정보를 전파하는 분산 알고리즘의 변형이 자연에서 많은 생물들이 환경을 효율적으로 탐색할 때 사용하는 레비 랜덤 워크 알고리즘보다 작업을 더 효율적으로 완료하는 것을 보여줘. 

또한, 일부 에이전트는 작업 정보를 전파하는 알고리즘을 사용하고 나머지 에이전트는 레비 랜덤 워크 알고리즘을 사용하는 노동 분담 알고리즘을 제안해. 마지막으로, 각 에이전트가 전파된 작업 정보와 레비 랜덤 워크를 동적으로 전환하는 하이브리드 알고리즘을 소개해. 우리의 노동 분담 알고리즘과 하이브리드 알고리즘은 특히 낮은 작업 속도와 중간 작업 속도에서 전파된 작업 정보 기반 알고리즘과 레비 워크 알고리즘보다 더 나은 성능을 보여줘. 

작업이 빠르게 나타날 때는 레비 랜덤 워크 전략이 이러한 새로운 접근 방식들과 비교했을 때 비슷한 성능을 보이거나 더 나은 성능을 발휘하는 것을 관찰했어. 우리의 연구는 다양한 작업 속도에서 이 알고리즘들의 상대적 성능을 보여주고, 환경 매개변수에 따라 알고리즘을 최적화하는 통찰도 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09754.pdf

Title: Towards Single-Lens Controllable Depth-of-Field Imaging via All-in-Focus Aberration Correction and Monocular Depth Estimation

Original Abstract:
Controllable Depth-of-Field (DoF) imaging commonly produces amazing visual effects based on heavy and expensive high-end lenses. However, confronted with the increasing demand for mobile scenarios, it is desirable to achieve a lightweight solution with Minimalist Optical Systems (MOS). This work centers around two major limitations of MOS, i.e., the severe optical aberrations and uncontrollable DoF, for achieving single-lens controllable DoF imaging via computational methods. A Depth-aware Controllable DoF Imaging (DCDI) framework is proposed equipped with All-in-Focus (AiF) aberration correction and monocular depth estimation, where the recovered image and corresponding depth map are utilized to produce imaging results under diverse DoFs of any high-end lens via patch-wise convolution. To address the depth-varying optical degradation, we introduce a Depth-aware Degradation-adaptive Training (DA2T) scheme. At the dataset level, a Depth-aware Aberration MOS (DAMOS) dataset is established based on the simulation of Point Spread Functions (PSFs) under different object distances. Additionally, we design two plug-and-play depth-aware mechanisms to embed depth information into the aberration image recovery for better tackling depth-aware degradation. Furthermore, we propose a storage-efficient Omni-Lens-Field model to represent the 4D PSF library of various lenses. With the predicted depth map, recovered image, and depth-aware PSF map inferred by Omni-Lens-Field, single-lens controllable DoF imaging is achieved. Comprehensive experimental results demonstrate that the proposed framework enhances the recovery performance, and attains impressive single-lens controllable DoF imaging results, providing a seminal baseline for this field. The source code and the established dataset will be publicly available at this https URL.

Translated Abstract:
컨트롤 가능한 심도(DoF) 이미징은 보통 비싸고 고급 렌즈를 사용해서 멋진 시각 효과를 만들어내. 하지만 모바일 환경에서의 수요가 늘어나면서, 가벼운 솔루션이 필요해. 그래서 이 연구는 최소한의 광학 시스템(MOS)에서 두 가지 큰 한계, 즉 심각한 광학 왜곡과 통제할 수 없는 DoF를 극복하고자 해. 

우리는 Depth-aware Controllable DoF Imaging (DCDI) 프레임워크를 제안해. 이 프레임워크는 모든 초점(AiF) 왜곡 보정과 단안 깊이 추정을 결합해서 작동해. 복원된 이미지와 해당 깊이 맵을 사용하여 다양한 고급 렌즈의 여러 DoF에서 이미지를 생성할 수 있어. 깊이에 따라 달라지는 광학 저하를 해결하기 위해 Depth-aware Degradation-adaptive Training (DA2T) 방식을 도입했어.

데이터셋 차원에서는 서로 다른 물체 거리에서의 점 확산 함수(PSF)를 시뮬레이션하여 Depth-aware Aberration MOS (DAMOS) 데이터셋을 만들었어. 또한, 우리는 깊이 정보를 왜곡된 이미지 복구 과정에 통합하기 위한 두 가지 플러그 앤 플레이 깊이 인식 메커니즘을 설계했어. 

더 나아가, 다양한 렌즈의 4D PSF 라이브러리를 표현하기 위해 저장 효율적인 Omni-Lens-Field 모델을 제안해. 예측된 깊이 맵과 복원된 이미지, 그리고 Omni-Lens-Field로 추론한 깊이 인식 PSF 맵을 통해 단일 렌즈에서의 컨트롤 가능한 DoF 이미징을 달성했어. 

종합적인 실험 결과는 제안된 프레임워크가 복원 성능을 향상시키고, 인상적인 단일 렌즈의 컨트롤 가능한 DoF 이미징 결과를 얻었다는 것을 보여줘. 이 연구는 이 분야의 기초가 될 거야. 소스 코드와 데이터셋은 이 URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09769.pdf

Title: Risk-Aware Autonomous Driving for Linear Temporal Logic Specifications

Original Abstract:
Decision-making for autonomous driving incorporating different types of risks is a challenging topic. This paper proposes a novel risk metric to facilitate the driving task specified by linear temporal logic (LTL) by balancing the risk brought up by different uncertain events. Such a balance is achieved by discounting the costs of these uncertain events according to their timing and severity, thereby reflecting a human-like awareness of risk. We have established a connection between this risk metric and the occupation measure, a fundamental concept in stochastic reachability problems, such that a risk-aware control synthesis problem under LTL specifications is formulated for autonomous vehicles using occupation measures. As a result, the synthesized policy achieves balanced decisions across different types of risks with associated costs, showcasing advantageous versatility and generalizability. The effectiveness and scalability of the proposed approach are validated by three typical traffic scenarios in Carla simulator.

Translated Abstract:
자율 주행에서 다양한 위험을 고려한 의사 결정은 어려운 주제야. 이 논문에서는 선형 시간 논리(LTL)로 설정된 주행 작업을 쉽게 하기 위해 새로운 위험 지표를 제안해. 이 지표는 여러 불확실한 사건들이 가져오는 위험을 균형 있게 조절하는 방식이야.

이 균형은 불확실한 사건들의 시기와 심각도에 따라 그 비용을 할인함으로써 이루어져. 이렇게 하면 사람처럼 위험을 인지하는 느낌이 반영되는 거지. 우리는 이 위험 지표와 점유 측정(occupation measure)이라는 개념을 연결했어. 점유 측정은 확률적 도달성 문제에서 중요한 개념인데, 이를 바탕으로 LTL 규격 아래에서 위험을 고려한 제어 합성 문제를 자율 차량에 맞게 정리한 거야.

결과적으로, 만들어진 정책은 다양한 위험 유형에 대한 결정을 균형 있게 내릴 수 있도록 해. 그러면서 관련된 비용도 고려하고, 유연성과 일반화 가능성을 보여줘. 제안한 접근 방식의 효과와 확장성은 Carla 시뮬레이터에서 세 가지 전형적인 교통 시나리오로 검증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09777.pdf

Title: DiFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and Iterative Refinement for Efficient End-to-End Autonomous Driving

Original Abstract:
Current end-to-end autonomous driving methods resort to unifying modular designs for various tasks (e.g. perception, prediction and planning). Although optimized in a planning-oriented spirit with a fully differentiable framework, existing end-to-end driving systems without ego-centric designs still suffer from unsatisfactory performance and inferior efficiency, owing to the rasterized scene representation learning and redundant information transmission. In this paper, we revisit the human driving behavior and propose an ego-centric fully sparse paradigm, named DiFSD, for end-to-end self-driving. Specifically, DiFSD mainly consists of sparse perception, hierarchical interaction and iterative motion planner. The sparse perception module performs detection, tracking and online mapping based on sparse representation of the driving scene. The hierarchical interaction module aims to select the Closest In-Path Vehicle / Stationary (CIPV / CIPS) from coarse to fine, benefiting from an additional geometric prior. As for the iterative motion planner, both selected interactive agents and ego-vehicle are considered for joint motion prediction, where the output multi-modal ego-trajectories are optimized in an iterative fashion. Besides, both position-level motion diffusion and trajectory-level planning denoising are introduced for uncertainty modeling, thus facilitating the training stability and convergence of the whole framework. Extensive experiments conducted on nuScenes dataset demonstrate the superior planning performance and great efficiency of DiFSD, which significantly reduces the average L2 error by \textbf{66\%} and collision rate by \textbf{77\%} than UniAD while achieves \textbf{8.2$\times$} faster running efficiency.

Translated Abstract:
현재의 완전 자동 운전 방법은 여러 작업(예: 인식, 예측 및 계획)을 위해 모듈형 디자인을 통합하는 방식으로 진행돼. 계획 중심의 최적화가 이루어졌지만, 기존의 자아 중심 디자인이 없는 완전 자동 운전 시스템은 여전히 성능이 만족스럽지 않고 효율이 떨어져. 이는 장면 표현 학습이 세분화되어 있고, 불필요한 정보 전송 때문이야.

이 논문에서는 인간의 운전 행동을 다시 살펴보고, 자아 중심의 완전 희소 패러다임인 DiFSD를 제안해. DiFSD는 주로 희소 인식, 계층적 상호작용, 반복적 동작 계획기로 구성돼. 희소 인식 모듈은 운전 장면의 희소 표현을 기반으로 탐지, 추적 및 온라인 매핑을 수행해. 

계층적 상호작용 모듈은 추가적인 기하학적 정보 덕분에 가장 가까운 경로 차량/정지물(CIPV / CIPS)을 거칠게부터 세밀하게 선택하는 걸 목표로 해. 반복적 동작 계획기는 선택된 상호작용 에이전트와 자아 차량을 함께 고려해 움직임을 예측하며, 출력된 다중 모드 자아 궤적은 반복적으로 최적화돼. 

또한, 위치 수준의 동작 확산과 궤적 수준의 계획 잡음 제거가 불확실성 모델링을 위해 도입되어, 전체 프레임워크의 훈련 안정성과 수렴을 촉진해. nuScenes 데이터셋에서 수행된 광범위한 실험은 DiFSD의 뛰어난 계획 성능과 높은 효율성을 보여주고, 평균 L2 오차를 66% 줄이고 충돌률을 77% 감소시켰으며, 실행 효율은 8.2배 더 빨라.

================================================================================

URL:
https://arxiv.org/pdf/2409.09790.pdf

Title: Multiple Rotation Averaging with Constrained Reweighting Deep Matrix Factorization

Original Abstract:
Multiple rotation averaging plays a crucial role in computer vision and robotics domains. The conventional optimization-based methods optimize a nonlinear cost function based on certain noise assumptions, while most previous learning-based methods require ground truth labels in the supervised training process. Recognizing the handcrafted noise assumption may not be reasonable in all real-world scenarios, this paper proposes an effective rotation averaging method for mining data patterns in a learning manner while avoiding the requirement of labels. Specifically, we apply deep matrix factorization to directly solve the multiple rotation averaging problem in unconstrained linear space. For deep matrix factorization, we design a neural network model, which is explicitly low-rank and symmetric to better suit the background of multiple rotation averaging. Meanwhile, we utilize a spanning tree-based edge filtering to suppress the influence of rotation outliers. What's more, we also adopt a reweighting scheme and dynamic depth selection strategy to further improve the robustness. Our method synthesizes the merit of both optimization-based and learning-based methods. Experimental results on various datasets validate the effectiveness of our proposed method.

Translated Abstract:
다중 회전 평균화는 컴퓨터 비전과 로봇 분야에서 중요한 역할을 해. 전통적인 최적화 방법들은 특정한 노이즈 가정을 바탕으로 비선형 비용 함수를 최적화하고, 대부분의 이전 학습 기반 방법들은 감독 학습 과정에서 정답 레이블이 필요해.

하지만 손으로 만든 노이즈 가정이 모든 실제 상황에서 합리적이지 않을 수 있다는 점을 인식하고, 이 논문에서는 레이블 없이 학습 방식으로 데이터 패턴을 발견할 수 있는 효과적인 회전 평균화 방법을 제안해. 구체적으로, 우리는 깊은 행렬 분해(deep matrix factorization)를 적용해서 제약 없는 선형 공간에서 다중 회전 평균화 문제를 직접 해결해.

깊은 행렬 분해를 위해, 우리는 다중 회전 평균화에 더 잘 맞도록 명시적으로 저랭크(low-rank)이고 대칭적인 신경망 모델을 설계했어. 동시에 회전 아웃라이어의 영향을 줄이기 위해 스패닝 트리 기반의 엣지 필터링을 사용해. 게다가, 우리는 재가중치 기법과 동적 깊이 선택 전략을 도입해서 견고성을 더 개선했어.

우리 방법은 최적화 기반 방법과 학습 기반 방법의 장점을 통합해. 다양한 데이터셋에 대한 실험 결과는 우리가 제안한 방법의 효과성을 입증해.

================================================================================

URL:
https://arxiv.org/pdf/2409.09820.pdf

Title: Introducing DAIMYO: a first-time-right dynamic design architecture and its application to tail-sitter UAS development

Original Abstract:
In recent years, there has been a notable evolution in various multidisciplinary design methodologies for dynamic systems. Among these approaches, a noteworthy concept is that of concurrent conceptual and control design or co-design. This approach involves the tuning of feedforward and/or feedback control strategies in conjunction with the conceptual design of the dynamic system. The primary aim is to discover integrated solutions that surpass those attainable through a disjointed or decoupled approach. This concurrent design paradigm exhibits particular promise in the context of hybrid unmanned aerial systems (UASs), such as tail-sitters, where the objectives of versatility (driven by control considerations) and efficiency (influenced by conceptual design) often present conflicting demands. Nevertheless, a persistent challenge lies in the potential disparity between the theoretical models that underpin the design process and the real-world operational environment, the so-called reality gap. Such disparities can lead to suboptimal performance when the designed system is deployed in reality. To address this issue, this paper introduces DAIMYO, a novel design architecture that incorporates a high-fidelity environment, which emulates real-world conditions, into the procedure in pursuit of a `first-time-right' design. The outcome of this innovative approach is a design procedure that yields versatile and efficient UAS designs capable of withstanding the challenges posed by the reality gap.

Translated Abstract:
최근 몇 년 동안 동적 시스템을 위한 다양한 다학제 설계 방법론이 눈에 띄게 발전했어. 그 중에서 '동시 개념 및 제어 설계' 또는 '코디자인'이라는 개념이 특히 주목받고 있어. 이 방법은 피드포워드와 피드백 제어 전략을 동적 시스템의 개념 설계와 함께 조정하는 걸 포함해. 주된 목표는 따로따로 접근했을 때보다 더 나은 통합 솔루션을 찾는 거야.

이 동시 설계 방식은 하이브리드 무인 항공 시스템(UAS), 예를 들어 테일 시터와 같은 경우에 특히 가능성이 커. 왜냐하면 제어와 관련된 다재다능함과 개념 설계에 의해 영향을 받는 효율성이라는 목표가 종종 서로 충돌하는 요구를 만들어내거든. 하지만 여기서 지속적인 도전 과제가 있어. 바로 설계 과정의 이론적 모델과 실제 운영 환경 간의 차이, 즉 '현실 갭'이야. 이런 차이로 인해 실제로 설계된 시스템이 배치될 때 최적의 성능을 내지 못할 수 있어.

이 문제를 해결하기 위해 이 논문에서는 DAIMYO라는 새로운 설계 아키텍처를 소개해. 이 아키텍처는 실제 조건을 모방한 고충실도의 환경을 설계 과정에 통합해서 '한 번에 제대로' 설계를 목표로 해. 이 혁신적인 접근 방식의 결과는 현실 갭이 주는 도전에 견딜 수 있는 다재다능하고 효율적인 UAS 설계 절차야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09882.pdf

Title: Safe Control of Quadruped in Varying Dynamics via Safety Index Adaptation

Original Abstract:
Varying dynamics pose a fundamental difficulty when deploying safe control laws in the real world. Safety Index Synthesis (SIS) deeply relies on the system dynamics and once the dynamics change, the previously synthesized safety index becomes invalid. In this work, we show the real-time efficacy of Safety Index Adaptation (SIA) in varying dynamics. SIA enables real-time adaptation to the changing dynamics so that the adapted safe control law can still guarantee 1) forward invariance within a safe region and 2) finite time convergence to that safe region. This work employs SIA on a package-carrying quadruped robot, where the payload weight changes in real-time. SIA updates the safety index when the dynamics change, e.g., a change in payload weight, so that the quadruped can avoid obstacles while achieving its performance objectives. Numerical study provides theoretical guarantees for SIA and a series of hardware experiments demonstrate the effectiveness of SIA in real-world deployment in avoiding obstacles under varying dynamics.

Translated Abstract:
변화하는 동역학은 실제 세계에서 안전한 제어 법칙을 적용하는 데 큰 어려움을 줘. 안전 지수 합성(SIS)은 시스템 동역학에 많이 의존하는데, 동역학이 바뀌면 이전에 만든 안전 지수가 쓸모가 없어져. 

이번 연구에서는 변화하는 동역학 속에서 안전 지수 적응(SIA)의 실시간 효율성을 보여줘. SIA는 동역학이 바뀔 때 이를 실시간으로 적응시켜서, 적응된 안전 제어 법칙이 1) 안전한 지역 내에서의 전진 불변성과 2) 그 안전한 지역으로의 유한 시간 수렴을 보장할 수 있게 해. 

이 연구는 짐을 운반하는 네 발 달린 로봇에 SIA를 적용했어. 이 로봇은 실시간으로 짐의 무게가 바뀌거든. SIA는 동역학이 바뀔 때, 예를 들어 짐의 무게가 변화할 때 안전 지수를 업데이트해서 로봇이 장애물을 피하면서 성능 목표를 달성할 수 있게 해. 

수치 연구는 SIA에 대한 이론적 보장을 제공하고, 여러 하드웨어 실험을 통해 변화하는 동역학 속에서도 장애물을 피하는 데 SIA의 효과성을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.09915.pdf

Title: Forearm Ultrasound based Gesture Recognition on Edge

Original Abstract:
Ultrasound imaging of the forearm has demonstrated significant potential for accurate hand gesture classification. Despite this progress, there has been limited focus on developing a stand-alone end- to-end gesture recognition system which makes it mobile, real-time and more user friendly. To bridge this gap, this paper explores the deployment of deep neural networks for forearm ultrasound-based hand gesture recognition on edge devices. Utilizing quantization techniques, we achieve substantial reductions in model size while maintaining high accuracy and low latency. Our best model, with Float16 quantization, achieves a test accuracy of 92% and an inference time of 0.31 seconds on a Raspberry Pi. These results demonstrate the feasibility of efficient, real-time gesture recognition on resource-limited edge devices, paving the way for wearable ultrasound-based systems.

Translated Abstract:
팔뚝 초음파 이미지는 손 제스처 분류에서 큰 가능성을 보여줬어. 그런데 지금까지 모바일하고 실시간으로 사용할 수 있는 독립적인 제스처 인식 시스템 개발에는 많이 신경을 쓰지 않았어. 

이 논문은 그런 문제를 해결하기 위해, 팔뚝 초음파를 이용한 손 제스처 인식을 엣지 디바이스에서 딥 뉴럴 네트워크를 활용해 구현하는 방법을 다뤄. 양자화 기법을 사용해서 모델 크기를 크게 줄이면서도 높은 정확도와 낮은 지연 시간을 유지할 수 있었어. 

가장 좋은 모델은 Float16 양자화를 사용했을 때 테스트 정확도가 92%에, Raspberry Pi에서 추론 시간은 0.31초를 기록했어. 이 결과는 자원이 제한된 엣지 디바이스에서도 효율적이고 실시간으로 제스처 인식이 가능하다는 걸 보여줘. 그래서 앞으로 착용 가능한 초음파 기반 시스템을 개발하는 데 큰 도움이 될 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.09990.pdf

Title: SHIRE: Enhancing Sample Efficiency using Human Intuition in REinforcement Learning

Original Abstract:
The ability of neural networks to perform robotic perception and control tasks such as depth and optical flow estimation, simultaneous localization and mapping (SLAM), and automatic control has led to their widespread adoption in recent years. Deep Reinforcement Learning has been used extensively in these settings, as it does not have the unsustainable training costs associated with supervised learning. However, DeepRL suffers from poor sample efficiency, i.e., it requires a large number of environmental interactions to converge to an acceptable solution. Modern RL algorithms such as Deep Q Learning and Soft Actor-Critic attempt to remedy this shortcoming but can not provide the explainability required in applications such as autonomous robotics. Humans intuitively understand the long-time-horizon sequential tasks common in robotics. Properly using such intuition can make RL policies more explainable while enhancing their sample efficiency. In this work, we propose SHIRE, a novel framework for encoding human intuition using Probabilistic Graphical Models (PGMs) and using it in the Deep RL training pipeline to enhance sample efficiency. Our framework achieves 25-78% sample efficiency gains across the environments we evaluate at negligible overhead cost. Additionally, by teaching RL agents the encoded elementary behavior, SHIRE enhances policy explainability. A real-world demonstration further highlights the efficacy of policies trained using our framework.

Translated Abstract:
신경망이 깊이 추정, 광학 흐름 추정, SLAM(동시 위치 추정 및 지도 작성), 자동 제어 같은 로봇 인식 및 제어 작업을 수행하는 능력 덕분에 최근 몇 년간 많이 사용되고 있어. 딥 강화 학습은 이러한 작업에 많이 활용되는데, 감독 학습과 달리 지속 가능한 훈련 비용이 없어서 그래.

하지만 딥RL은 샘플 효율성이 낮아서, 괜찮은 해결책을 찾기 위해서는 많은 환경 상호작용이 필요해. 최신 RL 알고리즘인 딥 Q 학습이나 소프트 액터-비평자 같은 것들이 이 문제를 해결하려고 하지만, 자율 로봇 같은 응용 프로그램에서 필요한 설명 가능성을 제공하지 못해.

사람들은 로봇에서 흔히 발생하는 긴 시간 지연의 순차 작업을 직관적으로 이해해. 이런 직관을 잘 활용하면 RL 정책을 더 설명 가능하게 만들 수 있고, 샘플 효율성도 높일 수 있어. 이번 연구에서는 사람의 직관을 확률 그래픽 모델(PGM)을 사용해 인코딩하고, 이를 딥 RL 훈련 파이프라인에 활용해 샘플 효율성을 높이는 SHIRE라는 새로운 프레임워크를 제안해.

우리의 프레임워크는 평가한 환경에서 25-78%의 샘플 효율성 향상을 이루었고, 비용은 거의 들지 않아. 게다가 RL 에이전트에게 인코딩된 기본 행동을 가르쳐줌으로써 SHIRE는 정책의 설명 가능성도 높여. 실제 사례를 통해 우리 프레임워크로 훈련된 정책의 효과를 더 강조했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10063.pdf

Title: GlobalMapNet: An Online Framework for Vectorized Global HD Map Construction

Original Abstract:
High-definition (HD) maps are essential for autonomous driving systems. Traditionally, an expensive and labor-intensive pipeline is implemented to construct HD maps, which is limited in scalability. In recent years, crowdsourcing and online mapping have emerged as two alternative methods, but they have limitations respectively. In this paper, we provide a novel methodology, namely global map construction, to perform direct generation of vectorized global maps, combining the benefits of crowdsourcing and online mapping. We introduce GlobalMapNet, the first online framework for vectorized global HD map construction, which updates and utilizes a global map on the ego vehicle. To generate the global map from scratch, we propose GlobalMapBuilder to match and merge local maps continuously. We design a new algorithm, Map NMS, to remove duplicate map elements and produce a clean map. We also propose GlobalMapFusion to aggregate historical map information, improving consistency of prediction. We examine GlobalMapNet on two widely recognized datasets, Argoverse2 and nuScenes, showing that our framework is capable of generating globally consistent results.

Translated Abstract:
고해상도(HD) 지도는 자율주행 시스템에 필수적이야. 전통적으로 HD 지도를 만드는 과정은 비싸고 노동력이 많이 드는데, 이 방식은 확장성에 한계가 있어. 최근에는 크라우드소싱과 온라인 맵핑이 두 가지 대안으로 떠올랐지만, 각각의 방법에도 한계가 있어.

이번 논문에서는 크라우드소싱과 온라인 맵핑의 장점을 결합한 새로운 방법론인 글로벌 맵 구축(Global Map Construction)을 제안해. 우리는 글로벌 HD 맵을 벡터화하여 직접 생성할 수 있는 첫 번째 온라인 프레임워크인 GlobalMapNet을 소개해. 이 시스템은 자율주행 차량에 있는 글로벌 맵을 업데이트하고 활용해.

글로벌 맵을 처음부터 만들기 위해, 우리는 로컬 맵을 지속적으로 맞추고 합치는 GlobalMapBuilder를 제안해. 그리고 중복된 맵 요소를 제거하고 깔끔한 맵을 만들기 위한 새로운 알고리즘인 Map NMS를 설계했어. 마지막으로, 역사적인 맵 정보를 집계하여 예측의 일관성을 향상시키는 GlobalMapFusion을 제안해.

우리는 GlobalMapNet을 두 가지 잘 알려진 데이터셋인 Argoverse2와 nuScenes에서 시험해봤고, 우리의 프레임워크가 전 세계적으로 일관된 결과를 생성할 수 있다는 것을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10071.pdf

Title: Towards Physically-Realizable Adversarial Attacks in Embodied Vision Navigation

Original Abstract:
The deployment of embodied navigation agents in safety-critical environments raises concerns about their vulnerability to adversarial attacks on deep neural networks. However, current attack methods often lack practicality due to challenges in transitioning from the digital to the physical world, while existing physical attacks for object detection fail to achieve both multi-view effectiveness and naturalness. To address this, we propose a practical attack method for embodied navigation by attaching adversarial patches with learnable textures and opacity to objects. Specifically, to ensure effectiveness across varying viewpoints, we employ a multi-view optimization strategy based on object-aware sampling, which uses feedback from the navigation model to optimize the patch's texture. To make the patch inconspicuous to human observers, we introduce a two-stage opacity optimization mechanism, where opacity is refined after texture optimization. Experimental results show our adversarial patches reduce navigation success rates by about 40%, outperforming previous methods in practicality, effectiveness, and naturalness. Code is available at: [this https URL].

Translated Abstract:
안전이 중요한 환경에서 로봇 내비게이션 에이전트를 사용할 때, 딥 뉴럴 네트워크에 대한 공격에 취약할 수 있다는 걱정이 있어. 하지만 현재의 공격 방법들은 디지털 세계에서 물리적 세계로 넘어가는 데 어려움이 있어서 실용성이 떨어져. 또, 기존의 물체 탐지를 위한 물리적 공격도 여러 관점에서 효과적이거나 자연스럽지 못해.

그래서 우리는 물체에 배치할 수 있는 적대적 패치를 제안해. 이 패치는 학습 가능한 질감과 불투명도를 가지고 있어. 다양한 시점에서도 효과적이게 하려고, 물체 인식 샘플링을 기반으로 한 다중 관점 최적화 전략을 사용해. 내비게이션 모델의 피드백을 이용해서 패치의 질감을 최적화하는 거야. 그리고 인간이 보기에는 잘 보이지 않게 하기 위해 두 단계의 불투명도 최적화 기법을 도입했어. 이 단계에서는 질감 최적화 후에 불투명도를 조정해.

실험 결과, 우리의 적대적 패치가 내비게이션 성공률을 약 40% 낮추는 걸 보여줬어. 이전 방법들보다 실용성, 효과성, 자연스러움에서 더 나은 성과를 냈어. 코드는 [이 링크]에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.10215.pdf

Title: Synchronization-Based Cooperative Distributed Model Predictive Control

Original Abstract:
Distributed control algorithms are known to reduce overall computation time compared to centralized control algorithms. However, they can result in inconsistent solutions leading to the violation of safety-critical constraints. Inconsistent solutions can arise when two or more agents compute concurrently while making predictions on each others control actions. To address this issue, we propose an iterative algorithm called Synchronization-Based Cooperative Distributed Model Predictive Control, which we presented in [1]. The algorithm consists of two steps: 1. computing the optimal control inputs for each agent and 2. synchronizing the predicted states across all agents. We demonstrate the efficacy of our algorithm in the control of multiple small-scale vehicles in our Cyber-Physical Mobility Lab.

Translated Abstract:
분산 제어 알고리즘은 중앙 집중식 제어 알고리즘에 비해 전체 계산 시간을 줄이는 데 유리해. 하지만 이게 안전에 중요한 제약 조건을 위반하는 불일치한 해결책을 초래할 수 있어. 불일치한 해결책은 두 개 이상의 에이전트가 서로의 제어 행동을 예측하면서 동시에 계산할 때 발생할 수 있어.

이 문제를 해결하기 위해, 우리는 '동기화 기반 협력 분산 모델 예측 제어'라는 반복 알고리즘을 제안해. 이 알고리즘은 두 가지 단계로 구성돼: 1. 각 에이전트에 대한 최적의 제어 입력을 계산하는 것과 2. 모든 에이전트 간에 예측된 상태를 동기화하는 거야.

우리는 이 알고리즘의 효과를 사이버 물리적 이동 연구실에서 여러 소형 차량의 제어에 적용해 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2206.03809.pdf

Title: Control with Patterns: A D-learning Method

Original Abstract:
Learning-based control policies are widely used in various tasks in the field of robotics and control. However, formal (Lyapunov) stability guarantees for learning-based controllers with nonlinear dynamical systems are difficult to obtain. We propose a novel control approach, namely Control with Patterns (CWP), to address the stability issue over data sets corresponding to nonlinear dynamical systems. For such data sets, we introduce a new definition, namely exponential attraction on data sets, to describe the nonlinear dynamical systems under consideration. The problem of exponential attraction on data sets is transformed into a problem of pattern classification one based on the data sets and parameterized Lyapunov functions. Furthermore, D-learning is proposed as a method to perform CWP without knowledge of the system dynamics. Finally, the effectiveness of CWP based on D-learning is demonstrated through simulations and real flight experiments. In these experiments, the position of the multicopter is stabilized using real-time images as feedback, which can be considered as an Image-Based Visual Servoing (IBVS) problem.

Translated Abstract:
학습 기반 제어 정책은 로봇 공학과 제어 분야의 다양한 작업에서 많이 사용돼. 하지만 비선형 동적 시스템에 대한 학습 기반 컨트롤러의 공식적인 안정성 보장은 얻기 어려워. 그래서 우리는 데이터 세트를 기반으로 안정성 문제를 해결하기 위해 새로운 제어 접근법인 패턴 기반 제어(CWP)를 제안해.

이런 데이터 세트에 대해 우리는 비선형 동적 시스템을 설명하기 위해 데이터 세트에서의 지수적 끌림이라는 새로운 정의를 도입해. 데이터 세트에서의 지수적 끌림 문제는 데이터 세트와 매개변수화된 리아푸노프 함수에 기반한 패턴 분류 문제로 변환돼. 게다가 D-학습이라는 방법을 제안해서 시스템 동적에 대한 지식 없이도 CWP를 수행할 수 있게 해.

마지막으로, D-학습 기반의 CWP의 효과를 시뮬레이션과 실제 비행 실험을 통해 보여줘. 이 실험에서는 실시간 이미지를 피드백으로 사용해서 다중 로터의 위치를 안정화하는데, 이건 이미지 기반 비주얼 서보잉(IBVS) 문제로 볼 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2210.01672.pdf

Title: Bringing motion taxonomies to continuous domains via GPLVM on hyperbolic manifolds

Original Abstract:
Human motion taxonomies serve as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite substantial efforts devoted to design their hierarchy and underlying categories, their use remains limited. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. We achieve this by formulating a novel Gaussian process hyperbolic latent variable model that incorporates the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We validate our model on three different human motion taxonomies to learn hyperbolic embeddings that faithfully preserve the original graph structure. We show that our model properly encodes unseen data from existing or new taxonomy categories, and outperforms its Euclidean and VAE-based counterparts. Finally, through proof-of-concept experiments, we show that our model may be used to generate realistic trajectories between the learned embeddings.

Translated Abstract:
인간의 움직임 분류는 사람들이 어떻게 움직이고 환경과 상호작용하는지를 고수준으로 분류하는 방법이야. 이 분류는 물체 잡기, 조작 기술, 전체 몸 지지 자세를 분석하는 데 유용해. 하지만 분류 체계를 설계하는 데 많은 노력이 들어갔음에도 불구하고, 그 사용은 여전히 제한적이야. 그 이유는 분류의 계층적 구조와 그에 관련된 고차원 데이터 사이의 간극을 메울 수 있는 계산 모델이 부족하기 때문이야.

이 문제를 해결하기 위해 우리는 하이퍼볼릭 임베딩을 통해 분류 데이터를 모델링할 것을 제안해. 이 방법은 관련된 계층적 구조를 잘 포착해. 우리는 그래프 기반의 사전 정보를 활용하고 거리 보존 제약을 적용한 새로운 가우시안 프로세스 하이퍼볼릭 잠재 변수 모델을 만들었어. 이 모델을 세 가지 다른 인간 움직임 분류에 검증해봤고, 원래의 그래프 구조를 충실히 보존하는 하이퍼볼릭 임베딩을 학습했어.

우리 모델은 기존의 분류나 새로운 분류 카테고리에서 보지 못한 데이터를 잘 인코딩하고, 유클리드 기반 모델이나 VAE 기반 모델보다 성능이 뛰어난 걸 보여줬어. 마지막으로, 개념 증명 실험을 통해 우리가 학습한 임베딩 사이에서 현실적인 경로를 생성하는 데 이 모델을 사용할 수 있다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2306.03865.pdf

Title: Simultaneous Position-and-Stiffness Control of Underactuated Antagonistic Tendon-Driven Continuum Robots

Original Abstract:
Continuum robots have gained widespread popularity due to their inherent compliance and flexibility, particularly their adjustable levels of stiffness for various application scenarios. Despite efforts to dynamic modeling and control synthesis over the past decade, few studies have incorporated stiffness regulation into their feedback control design; however, this is one of the initial motivations to develop continuum robots. This paper addresses the crucial challenge of controlling both the position and stiffness of underactuated continuum robots actuated by antagonistic tendons. We begin by presenting a rigid-link dynamical model that can analyze the open-loop stiffening of tendon-driven continuum robots. Based on this model, we propose a novel passivity-based position-and-stiffness controller that adheres to the non-negative tension constraint. Comprehensive experiments on our continuum robot validate the theoretical results and demonstrate the efficacy and precision of this approach.

Translated Abstract:
연속 로봇은 그 자체의 유연성과 적응성 덕분에 인기가 많아졌어. 특히 다양한 상황에 맞춰 강성을 조절할 수 있다는 점이 좋아. 지난 10년 동안 동적 모델링과 제어 방법에 대한 연구가 있었지만, 강성 조절을 피드백 제어 설계에 포함한 연구는 거의 없었어. 하지만 강성 조절은 연속 로봇을 개발하게 만든 주요 이유 중 하나야.

이 논문은 반작용 힘으로 구동되는 연속 로봇의 위치와 강성을 동시에 제어하는 중요한 문제를 다뤄. 먼저, 힘줄로 구동되는 연속 로봇의 개방 루프 강화를 분석할 수 있는 강체 링크 동적 모델을 제시해. 이 모델을 기반으로, 비음수 장력 제약을 따르는 새로운 수동적 위치 및 강성 제어기를 제안해. 

우리가 만든 연속 로봇에 대한 다양한 실험을 통해 이론적 결과를 검증하고, 이 접근 방식의 효과성과 정확성을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2306.08737.pdf

Title: A Networked Multi-Agent System for Mobile Wireless Infrastructure on Demand

Original Abstract:
Despite the prevalence of wireless connectivity in urban areas around the globe, there remain numerous and diverse situations where connectivity is insufficient or unavailable. To address this, we introduce mobile wireless infrastructure on demand, a system of UAVs that can be rapidly deployed to establish an ad-hoc wireless network. This network has the capability of reconfiguring itself dynamically to satisfy and maintain the required quality of communication. The system optimizes the positions of the UAVs and the routing of data flows throughout the network to achieve this quality of service (QoS). By these means, task agents using the network simply request a desired QoS, and the system adapts accordingly while allowing them to move freely. We have validated this system both in simulation and in real-world experiments. The results demonstrate that our system effectively offers mobile wireless infrastructure on demand, extending the operational range of task agents and supporting complex mobility patterns, all while ensuring connectivity and being resilient to agent failures.

Translated Abstract:
도시 지역에서 무선 연결이 널리 퍼져 있지만, 여전히 연결이 부족하거나 없는 다양한 상황들이 많이 있어. 이를 해결하기 위해, 우리는 필요할 때 즉시 배치할 수 있는 모바일 무선 인프라 시스템을 소개해. 이 시스템은 드론(UAV)으로 구성되어 있어서, 임시 무선 네트워크를 구축할 수 있어.

이 네트워크는 통신 품질을 유지하기 위해 스스로 동적으로 재구성할 수 있는 능력이 있어. 시스템은 드론의 위치와 데이터 흐름의 경로를 최적화해서 서비스 품질(QoS)을 달성해. 네트워크를 사용하는 작업 에이전트는 원하는 QoS를 요청하면, 시스템이 그에 맞춰 조정하면서 자유롭게 이동할 수 있도록 해.

우리는 이 시스템을 시뮬레이션과 실제 실험을 통해 검증했어. 결과적으로, 우리의 시스템은 모바일 무선 인프라를 효과적으로 제공하며, 작업 에이전트의 작동 범위를 넓히고 복잡한 이동 패턴을 지원하면서도 연결성을 유지하고 에이전트의 실패에 강한 모습을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2307.00329.pdf

Title: DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment

Original Abstract:
Large language models (LLMs) encode a vast amount of semantic knowledge and possess remarkable understanding and reasoning capabilities. Previous work has explored how to ground LLMs in robotic tasks to generate feasible and executable textual plans. However, low-level execution in the physical world may deviate from the high-level textual plan due to environmental perturbations or imperfect controller design. In this paper, we propose \textbf{DoReMi}, a novel language model grounding framework that enables immediate Detection and Recovery from Misalignments between plan and execution. Specifically, we leverage LLMs to play a dual role, aiding not only in high-level planning but also generating constraints that can indicate misalignment during execution. Then vision language models (VLMs) are utilized to detect constraint violations continuously. Our pipeline can monitor the low-level execution and enable timely recovery if certain plan-execution misalignment occurs. Experiments on various complex tasks including robot arms and humanoid robots demonstrate that our method can lead to higher task success rates and shorter task completion times. Videos of DoReMi are available at \url{this https URL}.

Translated Abstract:
대형 언어 모델(LLMs)은 방대한 양의 의미적 지식을 담고 있고, 뛰어난 이해력과 추론 능력을 가지고 있어. 이전 연구에서는 LLM을 로봇 작업에 적용해서 실행 가능한 텍스트 계획을 만드는 방법을 탐구했어. 하지만 실제 세계에서의 저수준 실행은 환경 변화나 불완전한 제어기 설계 때문에 고수준 텍스트 계획과 다르게 진행될 수 있어.

이 논문에서는 \textbf{DoReMi}라는 새로운 언어 모델 기반의 프레임워크를 제안해. 이 프레임워크는 계획과 실행 사이의 불일치를 즉시 감지하고 회복할 수 있도록 해줘. 구체적으로, LLM을 활용해서 고수준 계획을 도와줄 뿐만 아니라 실행 중에 불일치를 나타낼 수 있는 제약 조건도 생성해. 그리고 비전 언어 모델(VLM)을 사용해서 제약 조건 위반을 지속적으로 감지해.

우리의 파이프라인은 저수준 실행을 모니터링하고, 특정 계획-실행 불일치가 발생하면 신속하게 회복할 수 있게 해줘. 로봇 팔과 휴머노이드 로봇을 포함한 다양한 복잡한 작업에 대한 실험 결과, 우리의 방법이 더 높은 작업 성공률과 짧은 작업 완료 시간을 가져올 수 있다는 걸 보여줬어. DoReMi에 대한 영상은 \url{this https URL}에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2308.07275.pdf

Title: On Semidefinite Relaxations for Matrix-Weighted State-Estimation Problems in Robotics

Original Abstract:
In recent years, there has been remarkable progress in the development of so-called certifiable perception methods, which leverage semidefinite, convex relaxations to find global optima of perception problems in robotics. However, many of these relaxations rely on simplifying assumptions that facilitate the problem formulation, such as an isotropic measurement noise distribution. In this paper, we explore the tightness of the semidefinite relaxations of matrix-weighted (anisotropic) state-estimation problems and reveal the limitations lurking therein: matrix-weighted factors can cause convex relaxations to lose tightness. In particular, we show that the semidefinite relaxations of localization problems with matrix weights may be tight only for low noise levels. To better understand this issue, we introduce a theoretical connection between the posterior uncertainty of the state estimate and the certificate matrix obtained via convex relaxation. With this connection in mind, we empirically explore the factors that contribute to this loss of tightness and demonstrate that redundant constraints can be used to regain it. As a second technical contribution of this paper, we show that the state-of-the-art relaxation of scalar-weighted SLAM cannot be used when matrix weights are considered. We provide an alternate formulation and show that its SDP relaxation is not tight (even for very low noise levels) unless specific redundant constraints are used. We demonstrate the tightness of our formulations on both simulated and real-world data.

Translated Abstract:
최근 몇 년 동안, 로봇 공학에서 글로벌 최적화를 찾기 위해 반정의(convex) 완화 방법을 활용하는 인증 가능한 인식 방법이 눈에 띄게 발전했어. 하지만 이런 완화 방법 중 많은 것들이 문제를 쉽게 만들기 위해 단순화된 가정에 의존하는데, 예를 들어 등방성(isotropic) 측정 노이즈 분포 같은 것들이야.

이 논문에서는 행렬 가중치가 있는 상태 추정 문제의 반정의 완화의 타이트함을 살펴보고, 그 안에 숨은 한계를 드러내. 행렬 가중치 요소가 반정의 완화의 타이트함을 잃게 만들 수 있다는 거야. 특히, 행렬 가중치가 있는 위치 추정 문제의 반정의 완화는 노이즈 수준이 낮을 때만 타이트할 수 있음을 보여줘.

이 문제를 더 잘 이해하기 위해, 상태 추정의 사후 불확실성과 반정의 완화에서 얻은 증명 행렬 간의 이론적 연결을 소개할게. 이 연결을 염두에 두고, 타이트함을 잃게 만드는 요인들을 실험적으로 조사하고, 중복 제약 조건을 사용해서 다시 타이트함을 회복할 수 있음을 보여줘.

이 논문의 두 번째 기술적 기여로, 행렬 가중치가 고려될 때 최신의 스칼라 가중치 SLAM 반정의 완화는 사용할 수 없다는 점을 보여줘. 대안적인 수식을 제공하고, 특정 중복 제약 조건이 사용되지 않는 한, 그 SDP 완화가 타이트하지 않음을 보여줄 거야. 우리는 시뮬레이션 데이터와 실제 데이터 모두에서 우리의 수식의 타이트함을 입증할 거야.

================================================================================

URL:
https://arxiv.org/pdf/2309.04375.pdf

Title: Data-Driven Batch Localization and SLAM Using Koopman Linearization

Original Abstract:
We present a framework for model-free batch localization and SLAM. We use lifting functions to map a control-affine system into a high-dimensional space, where both the process model and the measurement model are rendered bilinear. During training, we solve a least-squares problem using groundtruth data to compute the high-dimensional model matrices associated with the lifted system purely from data. At inference time, we solve for the unknown robot trajectory and landmarks through an optimization problem, where constraints are introduced to keep the solution on the manifold of the lifting functions. The problem is efficiently solved using a sequential quadratic program (SQP), where the complexity of an SQP iteration scales linearly with the number of timesteps. Our algorithms, called Reduced Constrained Koopman Linearization Localization (RCKL-Loc) and Reduced Constrained Koopman Linearization SLAM (RCKL-SLAM), are validated experimentally in simulation and on two datasets: one with an indoor mobile robot equipped with a laser rangefinder that measures range to cylindrical landmarks, and one on a golf cart equipped with RFID range sensors. We compare RCKL-Loc and RCKL-SLAM with classic model-based nonlinear batch estimation. While RCKL-Loc and RCKL-SLAM have similar performance compared to their model-based counterparts, they outperform the model-based approaches when the prior model is imperfect, showing the potential benefit of the proposed data-driven technique.

Translated Abstract:
우리는 모델이 필요 없는 배치 로컬라이제이션과 SLAM을 위한 프레임워크를 제안해. 여기서 리프팅 함수를 사용해 제어-어파인 시스템을 고차원 공간으로 매핑해. 이때 프로세스 모델과 측정 모델이 둘 다 이차형으로 표현돼.

훈련할 때는, 실제 데이터(ground truth)를 사용해서 최소 제곱 문제를 해결하고, 리프팅된 시스템과 관련된 고차원 모델 행렬을 순수하게 데이터로부터 계산해. 추론할 땐, 최적화 문제를 통해 로봇의 경로와 랜드마크를 찾아. 이때 제약 조건을 추가해서 해결책이 리프팅 함수의 다양체에 머물도록 해. 이 문제는 순차 이차 프로그래밍(SQP)을 사용해서 효율적으로 해결되며, SQP 반복의 복잡도는 시간 단계 수에 따라 선형적으로 늘어나.

우리가 만든 알고리즘은 RCKL-Loc(축소된 제약 조건 쿱만 선형화 로컬라이제이션)과 RCKL-SLAM(축소된 제약 조건 쿱만 선형화 SLAM)이라고 불려. 이 알고리즘은 시뮬레이션과 두 가지 데이터셋에서 실험적으로 검증됐어: 하나는 실내 모바일 로봇이 레이저 거리 측정기를 사용해 원통형 랜드마크까지의 거리를 측정하는 거고, 다른 하나는 RFID 거리 센서를 장착한 골프 카트야.

RCKL-Loc과 RCKL-SLAM을 고전적인 모델 기반 비선형 배치 추정과 비교했어. RCKL-Loc과 RCKL-SLAM이 모델 기반 방법과 비슷한 성능을 보여주지만, 선행 모델이 불완전할 때는 오히려 더 나은 성능을 보여줘. 이건 제안된 데이터 기반 기술의 잠재적인 이점을 나타내는 거야.

================================================================================

URL:
https://arxiv.org/pdf/2309.06664.pdf

Title: A fixed-parameter tractable algorithm for combinatorial filter reduction

Original Abstract:
What is the minimal information that a robot must retain to achieve its task? To design economical robots, the literature dealing with reduction of combinatorial filters approaches this problem algorithmically. As lossless state compression is NP-hard, prior work has examined, along with minimization algorithms, a variety of special cases in which specific properties enable efficient solution. Complementing those findings, this paper refines the present understanding from the perspective of parameterized complexity. We give a fixed-parameter tractable algorithm for the general reduction problem by exploiting a transformation into minimal clique covering. The transformation introduces new constraints that arise from sequential dependencies encoded within the input filter -- some of these constraints can be repaired, others are treated through enumeration. Through this approach, we identify parameters affecting filter reduction that are based upon inter-constraint couplings (expressed as a notion of their height and width), which add to the structural parameters present in the unconstrained problem of minimal clique covering.

Translated Abstract:
로봇이 작업을 수행하기 위해 꼭 기억해야 하는 최소한의 정보는 무엇일까요? 경제적인 로봇을 설계하기 위해, 조합 필터의 축소를 다룬 문헌에서는 이 문제를 알고리즘적으로 접근하고 있어요. 손실 없는 상태 압축은 NP-하드 문제라서, 이전 연구에서는 최소화 알고리즘과 함께 특정 속성이 효율적인 해결을 가능하게 하는 다양한 특별한 경우를 살펴봤어요.

이 논문은 그런 발견을 보완하며 매개변수화된 복잡성의 관점에서 현재의 이해를 더 발전시켜요. 우리는 최소 클리크 커버링으로의 변환을 이용해서 일반적인 축소 문제에 대한 고정 매개변수 처리 가능 알고리즘을 제시해요. 이 변환은 입력 필터 내에서 인코딩된 순차적 의존성에서 발생하는 새로운 제약 조건을 도입해요. 이 제약 조건 중 일부는 수정할 수 있고, 나머지는 열거를 통해 다뤄져요.

이 접근법을 통해, 필터 축소에 영향을 미치는 매개변수를 찾아내는데, 이 매개변수는 제약 조건 간의 상호 연결성(그 높이와 너비 개념으로 표현됨)에 기반해요. 이는 최소 클리크 커버링의 제약 없는 문제에 존재하는 구조적 매개변수에 추가되는 거예요.

================================================================================

URL:
https://arxiv.org/pdf/2310.04266.pdf

Title: DRIFT: Deep Reinforcement Learning for Intelligent Floating Platforms Trajectories

Original Abstract:
This investigation introduces a novel deep reinforcement learning-based suite to control floating platforms in both simulated and real-world environments. Floating platforms serve as versatile test-beds to emulate micro-gravity environments on Earth, useful to test autonomous navigation systems for space applications. Our approach addresses the system and environmental uncertainties in controlling such platforms by training policies capable of precise maneuvers amid dynamic and unpredictable conditions. Leveraging Deep Reinforcement Learning (DRL) techniques, our suite achieves robustness, adaptability, and good transferability from simulation to reality. Our deep reinforcement learning framework provides advantages such as fast training times, large-scale testing capabilities, rich visualization options, and ROS bindings for integration with real-world robotic systems. Being open access, our suite serves as a comprehensive platform for practitioners who want to replicate similar research in their own simulated environments and labs.

Translated Abstract:
이 연구는 부유 플랫폼을 제어하기 위한 새로운 딥 강화 학습 기반 시스템을 소개해. 이 부유 플랫폼은 지구에서 미세 중력 환경을 재현할 수 있는 유용한 테스트 베드로, 우주 응용 프로그램을 위한 자율 내비게이션 시스템을 테스트하는 데 도움이 돼.

우리의 접근 방식은 이런 플랫폼을 제어할 때 발생하는 시스템과 환경의 불확실성을 해결해. 우리는 역동적이고 예측할 수 없는 조건 속에서도 정확한 조작을 할 수 있는 정책을 훈련시켜. 딥 강화 학습(DRL) 기술을 활용해서, 우리의 시스템은 강력함, 적응성, 그리고 시뮬레이션에서 현실로의 좋은 전이 능력을 달성했어.

우리의 딥 강화 학습 프레임워크는 빠른 훈련 시간, 대규모 테스트 가능성, 다양한 시각화 옵션, 그리고 현실의 로봇 시스템과 통합할 수 있는 ROS 바인딩 같은 장점을 제공해. 오픈 액세스인 이 시스템은 비슷한 연구를 자신의 시뮬레이션 환경과 실험실에서 재현하고자 하는 연구자들에게 포괄적인 플랫폼으로 역할을 해.

================================================================================

URL:
https://arxiv.org/pdf/2310.05022.pdf

Title: Fully Spiking Neural Network for Legged Robots

Original Abstract:
Recent advancements in legged robots using deep reinforcement learning have led to significant progress. Quadruped robots can perform complex tasks in challenging environments, while bipedal and humanoid robots have also achieved breakthroughs. Current reinforcement learning methods leverage diverse robot bodies and historical information to perform actions, but previous research has not emphasized the speed and energy consumption of network inference and the biological significance of neural networks. Most networks are traditional artificial neural networks that utilize multilayer perceptrons (MLP). This paper presents a novel Spiking Neural Network (SNN) for legged robots, showing exceptional performance in various simulated terrains. SNNs provide natural advantages in inference speed and energy consumption, and their pulse-form processing enhances biological interpretability. This study presents a highly efficient SNN for legged robots that can be seamless integrated into other learning models.

Translated Abstract:
최근 다리 로봇에 대한 심층 강화 학습의 발전 덕분에 큰 진전을 이뤘어. 네 발로 걷는 로봇은 어려운 환경에서도 복잡한 작업을 수행할 수 있게 되었고, 두 발로 걷는 로봇과 인간형 로봇도 큰 성과를 냈어. 

현재의 강화 학습 방법은 다양한 로봇 구조와 과거 데이터를 활용해서 행동을 수행하지만, 이전 연구들은 네트워크 추론의 속도와 에너지 소비, 그리고 신경망의 생물학적 의미에 대해서는 많이 다루지 않았어. 대부분의 네트워크는 다층 퍼셉트론(MLP)을 이용하는 전통적인 인공 신경망이야. 

이 논문에서는 다리 로봇을 위한 새로운 스파이킹 신경망(SNN)을 소개하고, 다양한 시뮬레이션 환경에서 뛰어난 성능을 보여줘. SNN은 추론 속도와 에너지 소비에서 자연스러운 장점을 제공하고, 펄스 형태의 처리는 생물학적 해석 가능성을 높여. 이 연구는 다른 학습 모델과 쉽게 통합될 수 있는 매우 효율적인 SNN을 다리 로봇을 위해 제시하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2311.02558.pdf

Title: Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity with Free-Flying Robots

Original Abstract:
Assistive free-flyer robots autonomously caring for future crewed outposts -- such as NASA's Astrobee robots on the International Space Station (ISS) -- must be able to detect day-to-day interior changes to track inventory, detect and diagnose faults, and monitor the outpost status. This work presents a framework for multi-agent cooperative mapping and change detection to enable robotic maintenance of space outposts. One agent is used to reconstruct a 3D model of the environment from sequences of images and corresponding depth information. Another agent is used to periodically scan the environment for inconsistencies against the 3D model. Change detection is validated after completing the surveys using real image and pose data collected by Astrobee robots in a ground testing environment and from microgravity aboard the ISS. This work outlines the objectives, requirements, and algorithmic modules for the multi-agent reconstruction system, including recommendations for its use by assistive free-flyers aboard future microgravity outposts.

Translated Abstract:
자율적으로 미래의 유인 기지에서 도움을 주는 자유 비행 로봇들, 예를 들어 NASA의 아스트로비 로봇처럼, 일상적인 내부 변화를 감지할 수 있어야 해. 이렇게 해야 재고를 추적하고, 문제를 감지하고 진단하며, 기지의 상태를 모니터링할 수 있어.

이 연구는 로봇이 우주 기지를 유지할 수 있도록 다중 에이전트 협력 매핑과 변화 감지 프레임워크를 제시해. 한 에이전트는 이미지 시퀀스와 해당 깊이 정보를 바탕으로 환경의 3D 모델을 재구성해. 다른 에이전트는 3D 모델과 비교해 환경에서 불일치를 찾아내기 위해 주기적으로 스캔해.

변화 감지는 아스트로비 로봇이 지상 테스트 환경과 ISS의 미세 중력에서 수집한 실제 이미지와 자세 데이터를 사용해 조사한 후 검증돼. 이 연구는 다중 에이전트 재구성 시스템의 목표, 요구 사항, 알고리즘 모듈을 설명하고, 미래의 미세 중력 기지에서 도움을 주는 자유 비행 로봇들이 사용할 수 있는 추천 사항도 포함하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2311.15330.pdf

Title: Multi-Agent Combinatorial Path Finding with Heterogeneous Task Duration

Original Abstract:
Multi-Agent Combinatorial Path Finding (MCPF) seeks collision-free paths for multiple agents from their initial locations to destinations, visiting a set of intermediate target locations in the middle of the paths, while minimizing the sum of arrival times. While a few approaches have been developed to handle MCPF, most of them simply direct the agent to visit the targets without considering the task duration, i.e., the amount of time needed for an agent to execute the task (such as picking an item) at a target location. MCPF is NP-hard to solve to optimality, and the inclusion of task duration further complicates the problem. This paper investigates heterogeneous task duration, where the duration can be different with respect to both the agents and targets. We develop two methods, where the first method post-processes the paths planned by any MCPF planner to include the task duration and has no solution optimality guarantee; and the second method considers task duration during planning and is able to ensure solution optimality. The numerical and simulation results show that our methods can handle up to 20 agents and 50 targets in the presence of task duration, and can execute the paths subject to robot motion disturbance.

Translated Abstract:
다중 에이전트 조합 경로 찾기(MCPF)는 여러 에이전트가 시작 위치에서 목적지까지 가는 동안 중간 목표 지점을 방문하면서 충돌 없이 경로를 찾고, 도착 시간의 합을 최소화하는 것을 목표로 해. 몇 가지 방법이 MCPF를 처리하기 위해 개발되었지만, 대부분은 에이전트가 목표를 방문하도록 유도하는 데 그치고, 작업 수행 시간(예: 물건을 집는 데 걸리는 시간)은 고려하지 않아.

MCPF 문제는 최적의 해를 찾는 것이 NP-하드하고, 작업 수행 시간을 포함하면 문제는 더 복잡해져. 이 논문은 다양한 작업 수행 시간을 다루는데, 여기서 수행 시간은 에이전트와 목표에 따라 달라질 수 있어. 우리는 두 가지 방법을 개발했어. 첫 번째 방법은 어떤 MCPF 계획자가 계획한 경로를 후처리해서 작업 수행 시간을 포함시키는 거고, 이 방법은 최적의 해를 보장하지 않아. 두 번째 방법은 계획 중에 작업 수행 시간을 고려해서 최적의 해를 보장할 수 있어.

수치적 결과와 시뮬레이션 결과는 우리의 방법이 작업 수행 시간이 있을 때 최대 20개의 에이전트와 50개의 목표를 처리할 수 있고, 로봇의 움직임 방해를 받으면서도 경로를 실행할 수 있음을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2403.07129.pdf

Title: RaceMOP: Mapless Online Path Planning for Multi-Agent Autonomous Racing using Residual Policy Learning

Original Abstract:
The interactive decision-making in multi-agent autonomous racing offers insights valuable beyond the domain of self-driving cars. Mapless online path planning is particularly of practical appeal but poses a challenge for safely overtaking opponents due to the limited planning horizon. To address this, we introduce RaceMOP, a novel method for mapless online path planning designed for multi-agent racing of F1TENTH cars. Unlike classical planners that rely on predefined racing lines, RaceMOP operates without a map, utilizing only local observations to execute high-speed overtaking maneuvers. Our approach combines an artificial potential field method as a base policy with residual policy learning to enable long-horizon planning. We advance the field by introducing a novel approach for policy fusion with the residual policy directly in probability space. Extensive experiments on twelve simulated racetracks validate that RaceMOP is capable of long-horizon decision-making with robust collision avoidance during overtaking maneuvers. RaceMOP demonstrates superior handling over existing mapless planners and generalizes to unknown racetracks, affirming its potential for broader applications in robotics. Our code is available at this http URL.

Translated Abstract:
멀티 에이전트 자율 레이싱에서의 상호작용 의사결정은 자율주행차 분야 외에도 유용한 인사이트를 제공합니다. 지도 없이 온라인 경로 계획을 하는 건 특히 실용적이지만, 제한된 계획 범위 때문에 상대를 안전하게 추월하는 데는 도전이 될 수 있습니다. 

이 문제를 해결하기 위해 RaceMOP이라는 새로운 방법을 소개합니다. 이건 F1TENTH 자동차의 멀티 에이전트 레이싱을 위해 설계된 지도 없는 온라인 경로 계획 방법이에요. 기존의 고전적인 계획자들은 미리 정해진 레이싱 라인에 의존하는데, RaceMOP은 지도가 없이도 작동하고, 오직 지역 관찰만을 이용해 고속 추월 기동을 실행해요. 

우리의 접근 방식은 인공 잠재 필드 방법을 기본 정책으로 사용하고, 잔여 정책 학습을 결합해 긴 범위 계획을 가능하게 합니다. 우리는 확률 공간에서 잔여 정책과의 정책 융합을 위한 새로운 접근 방식을 도입해 이 분야를 발전시켰어요. 12개의 시뮬레이션 레이스트랙에서의 광범위한 실험을 통해 RaceMOP이 추월 기동 중에 견고한 충돌 회피를 하면서 긴 범위 의사결정을 할 수 있다는 것을 확인했습니다. 

RaceMOP은 기존의 지도 없는 계획자들보다 뛰어난 조작성을 보여주고, 알려지지 않은 레이스트랙에도 일반화할 수 있어 로봇 공학에서 더 넓은 응용 가능성을 입증합니다. 우리의 코드는 이 http URL에서 이용할 수 있습니다.

================================================================================

URL:
https://arxiv.org/pdf/2403.07192.pdf

Title: Personalizing Interfaces to Humans with User-Friendly Priors

Original Abstract:
Robots often need to convey information to human users. For example, robots can leverage visual, auditory, and haptic interfaces to display their intent or express their internal state. In some scenarios there are socially agreed upon conventions for what these signals mean: e.g., a red light indicates an autonomous car is slowing down. But as robots develop new capabilities and seek to convey more complex data, the meaning behind their signals is not always mutually understood: one user might think a flashing light indicates the autonomous car is an aggressive driver, while another user might think the same signal means the autonomous car is defensive. In this paper we enable robots to adapt their interfaces to the current user so that the human's personalized interpretation is aligned with the robot's meaning. We start with an information theoretic end-to-end approach, which automatically tunes the interface policy to optimize the correlation between human and robot. But to ensure that this learning policy is intuitive -- and to accelerate how quickly the interface adapts to the human -- we recognize that humans have priors over how interfaces should function. For instance, humans expect interface signals to be proportional and convex. Our approach biases the robot's interface towards these priors, resulting in signals that are adapted to the current user while still following social expectations. Our simulations and user study results across $15$ participants suggest that these priors improve robot-to-human communication. See videos here: this https URL

Translated Abstract:
로봇은 종종 사람에게 정보를 전달해야 해. 예를 들어, 로봇은 시각, 청각, 촉각 인터페이스를 활용해서 자신의 의도를 보여주거나 내부 상태를 표현할 수 있어. 어떤 상황에서는 이러한 신호의 의미에 대해 사회적으로 합의된 규칙이 있지. 예를 들어, 빨간 불은 자율주행차가 속도를 줄이고 있다는 걸 나타내. 

하지만 로봇이 새로운 기능을 개발하고 더 복잡한 데이터를 전달하려고 할 때, 신호의 의미가 항상 서로 이해되지 않을 수 있어. 한 사용자는 깜빡이는 불빛이 자율주행차가 공격적인 운전자를 나타낸다고 생각할 수 있고, 다른 사용자는 같은 신호가 방어적인 운전자를 의미한다고 생각할 수 있어.

이 논문에서는 로봇이 현재 사용자에 맞춰 인터페이스를 조정할 수 있도록 해. 이렇게 하면 사람의 개인적인 해석과 로봇의 의미가 일치하게 돼. 우리는 정보 이론에 기반한 엔드 투 엔드 접근 방식을 사용해서, 인터페이스 정책을 자동으로 조정하여 사람과 로봇 간의 상관관계를 최적화해. 

그리고 이 학습 정책이 직관적이도록 하려면, 인간이 인터페이스가 어떻게 작동해야 하는지에 대한 선입견이 있다는 걸 인식해. 예를 들어, 사람들은 인터페이스 신호가 비례적이고 오목하다고 기대해. 우리의 접근 방식은 로봇의 인터페이스를 이러한 선입견에 맞게 편향시켜, 현재 사용자에게 맞춤화된 신호를 제공하면서도 사회적 기대를 따르게 해.

우리가 진행한 시뮬레이션과 사용자 연구 결과는 총 15명의 참가자를 대상으로 한 것으로, 이러한 선입견이 로봇과 사람 간의 커뮤니케이션을 개선한다는 것을 보여줘. 비디오도 여기에서 볼 수 있어: 이 URL

================================================================================

URL:
https://arxiv.org/pdf/2403.07269.pdf

Title: MPS: A New Method for Selecting the Stable Closed-Loop Equilibrium Attitude-Error Quaternion of a UAV During Flight

Original Abstract:
We present model predictive selection (MPS), a new method for selecting the stable closed-loop (CL) equilibrium attitude-error quaternion (AEQ) of an uncrewed aerial vehicle (UAV) during the execution of high-speed yaw maneuvers. In this approach, we minimize the cost of yawing measured with a performance figure of merit (PFM) that takes into account both the aerodynamic-torque control input and attitude-error state of the UAV. Specifically, this method uses a control law with a term whose sign is dynamically switched in real time to select, between two options, the torque associated with the lesser cost of rotation as predicted by a dynamical model of the UAV derived from first principles. This problem is relevant because the selection of the stable CL equilibrium AEQ significantly impacts the performance of a UAV during high-speed rotational flight, from both the power and control-error perspectives. To test and demonstrate the functionality and performance of the proposed method, we present data collected during one hundred real-time high-speed yaw-tracking flight experiments. These results highlight the superior capabilities of the proposed MPS-based scheme when compared to a benchmark controller commonly used in aerial robotics, as the PFM used to quantify the cost of flight is reduced by 60.30 %, on average. To our best knowledge, these are the first flight-test results that thoroughly demonstrate, evaluate, and compare the performance of a real-time controller capable of selecting the stable CL equilibrium AEQ during operation.

Translated Abstract:
우리는 모델 예측 선택(MPS)이라는 새로운 방법을 제안해. 이 방법은 무인 항공기(UAV)가 고속 회전 조작을 할 때 안정적인 폐쇄 루프(CL) 균형 태세 오차 쿼터니언(AEQ)을 선택하는 데 사용돼. 

이 접근법에서는 공기역학적 토크 제어 입력과 UAV의 태세 오차 상태를 고려한 성능 지표(PFM)를 사용해 회전 비용을 최소화해. 구체적으로, 이 방법은 실시간으로 부호가 동적으로 전환되는 제어 법칙을 사용해서, 두 가지 옵션 중에서 회전 비용이 더 적은 토크를 선택해. 이게 중요한 이유는, 안정적인 CL 균형 AEQ 선택이 고속 회전 비행 중 UAV의 성능에 큰 영향을 미치기 때문이야. 

우리는 제안한 방법의 기능과 성능을 테스트하기 위해 100번의 고속 회전 비행 실험에서 수집한 데이터를 보여줘. 이 결과는 제안한 MPS 기반 방식이 항공 로봇에서 일반적으로 사용하는 벤치마크 제어기와 비교했을 때 우수한 성능을 가지고 있다는 것을 강조해. 비행 비용을 정량화하는 데 사용된 PFM이 평균 60.30% 감소했거든. 우리가 아는 한, 이 실험 결과는 안정적인 CL 균형 AEQ를 선택할 수 있는 실시간 제어기의 성능을 철저히 보여주고 평가한 첫 번째 비행 테스트 결과야.

================================================================================

URL:
https://arxiv.org/pdf/2403.10759.pdf

Title: Fully Distributed Cooperative Multi-agent Underwater Obstacle Avoidance

Original Abstract:
Navigation in cluttered underwater environments is challenging, especially when there are constraints on communication and self-localisation. Part of the fully distributed underwater navigation problem has been resolved by introducing multi-agent robot teams [1], however when the environment becomes cluttered, the problem remains unresolved. In this paper, we first studied the connection between everyday activity of dog walking and the cooperative underwater obstacle avoidance problem. Inspired by this analogy, we propose a novel dog walking paradigm and implement it in a multi-agent underwater system. Simulations were conducted across various scenarios, with performance benchmarked against traditional methods utilising Image-Based Visual Servoing in a multi-agent setup. Results indicate that our dog walking-inspired paradigm significantly enhances cooperative behavior among agents and outperforms the existing approach in navigating through obstacles.

Translated Abstract:
복잡한 수중 환경에서 내비게이션하는 건 정말 어려워. 특히 통신이나 자가 위치 파악에 제약이 있을 때 더 그렇지. 완전히 분산된 수중 내비게이션 문제의 일부는 다중 에이전트 로봇 팀을 도입하면서 해결이 됐지만, 환경이 복잡해지면 여전히 문제가 남아 있어. 

이 논문에서는 먼저 개 산책이라는 일상적인 활동과 협력적인 수중 장애물 회피 문제의 연결점을 연구했어. 이 비유에서 영감을 받아서, 새로운 개 산책 패러다임을 제안하고 이걸 다중 에이전트 수중 시스템에 적용했어. 다양한 시나리오에서 시뮬레이션을 진행했고, 성능은 다중 에이전트 설정에서 이미지 기반 비주얼 서보링을 사용하는 전통적인 방법과 비교했어. 

결과적으로, 우리의 개 산책에서 영감을 받은 패러다임이 에이전트들 간의 협력 행동을 크게 향상시키고, 장애물 사이를 내비게이션하는 데 기존 방법보다 더 뛰어난 성능을 보였어.

================================================================================

URL:
https://arxiv.org/pdf/2403.10996.pdf

Title: A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems

Original Abstract:
Multi-agent reinforcement learning (MARL) systems usually require significantly long training times due to their inherent complexity. Furthermore, deploying them in the real world demands a feature-rich environment along with multiple embodied agents, which may not be feasible due to budget or space limitations, not to mention energy consumption and safety issues. This work tries to address these pain points by presenting a sustainable digital twin framework capable of accelerating MARL training by selectively scaling parallelized workloads on-demand, and transferring the trained policies from simulation to reality using minimal hardware resources. The applicability of the proposed digital twin framework is highlighted through two representative use cases, which cover cooperative as well as competitive classes of MARL problems. We study the effect of agent and environment parallelization on training time and that of systematic domain randomization on zero-shot sim2real transfer across both the case studies. Results indicate up to 76.3% reduction in training time with the proposed parallelization scheme and as low as 2.9% sim2real gap using the suggested deployment method.

Translated Abstract:
멀티 에이전트 강화 학습(MARL) 시스템은 본질적으로 복잡해서 훈련하는 데 시간이 많이 걸려. 게다가 실제 환경에 적용하려면 다양한 기능이 있는 환경과 여러 개의 에이전트가 필요한데, 예산이나 공간 문제 때문에 이게 쉽지 않아. 에너지 소비나 안전 문제도 고려해야 하고.

이 연구는 이런 문제들을 해결하기 위해 지속 가능한 디지털 트윈 프레임워크를 제안해. 이 프레임워크는 필요에 따라 병렬 작업을 조정해서 MARL 훈련 속도를 높일 수 있고, 최소한의 하드웨어 자원으로 시뮬레이션에서 실제로 훈련된 정책을 전이할 수 있어.

제안한 디지털 트윈 프레임워크의 적용 가능성을 두 가지 대표적인 사례를 통해 보여주고, 협력적이거나 경쟁적인 MARL 문제를 다루고 있어. 우리는 에이전트와 환경의 병렬화가 훈련 시간에 미치는 영향과, 체계적인 도메인 랜덤화가 제로샷 시뮬레이션에서 현실로의 전이에 미치는 영향을 두 사례 모두에서 연구했어.

결과적으로, 제안한 병렬화 방식으로 훈련 시간을 최대 76.3% 줄일 수 있었고, 제안한 배포 방법을 사용했을 때 시뮬레이션과 현실 간의 차이는 2.9%까지 낮출 수 있었어.

================================================================================

URL:
https://arxiv.org/pdf/2403.13132.pdf

Title: Wearable Roller Rings to Augment In-Hand Manipulation through Active Surfaces

Original Abstract:
In-hand manipulation is a crucial ability for reorienting and repositioning objects within grasps. The main challenges in this are not only the complexity of the computational models, but also the risks of grasp instability caused by active finger motions, such as rolling, sliding, breaking, and remaking contacts. This paper presents the development of the Roller Ring (RR), a modular robotic attachment with active surfaces that is wearable by both robot and human hands to manipulate without lifting a finger. By installing the angled RRs on hands, such that their spatial motions are not colinear, we derive a general differential motion model for manipulating objects. Our motion model shows that complete in-hand manipulation skill sets can be provided by as few as only 2 RRs through non-holonomic object motions, while more RRs can enable enhanced manipulation dexterity with fewer motion constraints. Through extensive experiments, we test the RRs on both a robot hand and a human hand to evaluate their manipulation capabilities. We show that the RRs can be employed to manipulate arbitrary object shapes to provide dexterous in-hand manipulation.

Translated Abstract:
손 안에서 물체를 조작하는 것은 물체를 방향을 바꾸거나 위치를 옮기는 데 중요한 능력이야. 이 과정에서 어려운 점은 계산 모델의 복잡성뿐만 아니라, 손가락이 움직일 때 생기는 그립 불안정성의 위험이야. 예를 들어, 물체가 굴러가거나 미끄러지거나, 접촉이 끊겼다가 다시 연결되는 경우가 있어.

이 논문에서는 Roller Ring (RR)이라는 모듈식 로봇 장치를 소개해. 이 장치는 로봇이나 사람의 손에 착용할 수 있는 능동적인 표면을 가지고 있어서, 손가락 하나도 들지 않고 물체를 조작할 수 있어. 손에 각도가 있는 RR을 설치하면, 그들의 공간적 움직임이 일직선이 아니게 되는데, 이를 통해 물체를 조작하기 위한 일반적인 미분 운동 모델을 도출했어.

우리의 운동 모델은 비홀로노믹 물체 운동을 통해 단 2개의 RR만으로도 완전한 손 안 조작 기술 세트를 제공할 수 있다는 것을 보여줘. 더 많은 RR을 사용하면 더 적은 움직임 제약으로 더 뛰어난 조작 능력을 발휘할 수 있어. 우리는 광범위한 실험을 통해 로봇 손과 사람 손 모두에서 RR의 조작 능력을 평가했어. 실험 결과, RR을 사용하면 다양한 물체 모양을 조작할 수 있어, 그래서 손 안에서 능숙하게 조작할 수 있다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2403.13348.pdf

Title: MULAN-WC: Multi-Robot Localization Uncertainty-aware Active NeRF with Wireless Coordination

Original Abstract:
This paper presents MULAN-WC, a novel multi-robot 3D reconstruction framework that leverages wireless signal-based coordination between robots and Neural Radiance Fields (NeRF). Our approach addresses key challenges in multi-robot 3D reconstruction, including inter-robot pose estimation, localization uncertainty quantification, and active best-next-view selection. We introduce a method for using wireless Angle-of-Arrival (AoA) and ranging measurements to estimate relative poses between robots, as well as quantifying and incorporating the uncertainty embedded in the wireless localization of these pose estimates into the NeRF training loss to mitigate the impact of inaccurate camera poses. Furthermore, we propose an active view selection approach that accounts for robot pose uncertainty when determining the next-best views to improve the 3D reconstruction, enabling faster convergence through intelligent view selection. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our framework in theory and in practice. Leveraging wireless coordination and localization uncertainty-aware training, MULAN-WC can achieve high-quality 3d reconstruction which is close to applying the ground truth camera poses. Furthermore, the quantification of the information gain from a novel view enables consistent rendering quality improvement with incrementally captured images by commending the robot the novel view position. Our hardware experiments showcase the practicality of deploying MULAN-WC to real robotic systems.

Translated Abstract:
이 논문에서는 MULAN-WC라는 새로운 다중 로봇 3D 재구성 프레임워크를 소개해. 이 시스템은 로봇 간의 무선 신호를 이용한 협력과 Neural Radiance Fields(NeRF)를 활용해. 

우리의 접근법은 다중 로봇 3D 재구성에서 발생하는 주요 문제들, 즉 로봇 간 위치 추정, 위치 불확실성 측정, 그리고 다음에 볼 최적의 위치 선택을 해결해. 우리는 로봇 간의 상대적인 위치를 추정하기 위해 무선 도착 각도(AoA)와 거리 측정값을 사용하는 방법을 소개하고, 이 위치 추정의 무선 로컬라이제이션에서 발생하는 불확실성을 NeRF의 학습 손실에 반영해서 카메라 위치의 부정확성을 줄이는 방법을 제안해.

또한, 우리는 3D 재구성을 개선하기 위해 다음에 볼 최적의 위치를 결정할 때 로봇의 위치 불확실성을 고려하는 능동적 뷰 선택 방법을 제안해. 이렇게 하면 더 스마트한 뷰 선택을 통해 빠르게 수렴할 수 있어. 

합성 데이터와 실제 데이터셋에서 광범위한 실험을 통해 우리 프레임워크의 이론적 및 실제적 효과를 입증했어. MULAN-WC는 무선 협력과 위치 불확실성을 고려한 학습을 활용해 높은 품질의 3D 재구성을 달성할 수 있어, 실제 카메라 위치에 가까운 결과를 만들어내. 

또한, 새로운 뷰에서 얻는 정보의 양을 측정함으로써 로봇이 새로운 뷰 위치를 추천받아 점진적으로 캡처된 이미지의 일관된 렌더링 품질을 개선할 수 있어. 마지막으로, 우리의 하드웨어 실험은 MULAN-WC를 실제 로봇 시스템에 배포하는 것이 얼마나 실용적인지를 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2403.16803.pdf

Title: Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View Planning

Original Abstract:
Object reconstruction is relevant for many autonomous robotic tasks that require interaction with the environment. A key challenge in such scenarios is planning view configurations to collect informative measurements for reconstructing an initially unknown object. One-shot view planning enables efficient data collection by predicting view configurations and planning the globally shortest path connecting all views at once. However, prior knowledge about the object is required to conduct one-shot view planning. In this work, we propose a novel one-shot view planning approach that utilizes the powerful 3D generation capabilities of diffusion models as priors. By incorporating such geometric priors into our pipeline, we achieve effective one-shot view planning starting with only a single RGB image of the object to be reconstructed. Our planning experiments in simulation and real-world setups indicate that our approach balances well between object reconstruction quality and movement cost.

Translated Abstract:
물체 재구성은 환경과 상호작용이 필요한 많은 자율 로봇 작업에 중요해. 이런 상황에서 큰 도전 과제는 처음에 알지 못하는 물체를 재구성하기 위해 정보를 잘 수집할 수 있는 시점 구성을 계획하는 거야. 

원샷 시점 계획은 시점 구성을 예측하고 모든 시점을 한 번에 연결하는 가장 짧은 경로를 계획함으로써 효율적인 데이터 수집을 가능하게 해. 하지만 원샷 시점 계획을 하려면 물체에 대한 사전 정보가 필요해. 

이번 연구에서는 확산 모델의 강력한 3D 생성 능력을 사전 정보로 활용하는 새로운 원샷 시점 계획 방법을 제안해. 이러한 기하학적 사전 정보를 파이프라인에 포함시켜, 재구성할 물체의 단일 RGB 이미지만으로도 효과적인 원샷 시점 계획을 할 수 있어. 

우리의 계획 실험 결과, 시뮬레이션과 실제 환경 모두에서 물체 재구성 품질과 이동 비용 간의 균형이 잘 맞는다는 것을 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2403.19060.pdf

Title: Towards Human-Centered Construction Robotics: A Reinforcement Learning-Driven Companion Robot for Contextually Assisting Carpentry Workers

Original Abstract:
In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a "work companion rover" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive and collaborative human-robot workforce.

Translated Abstract:
전통적인 로봇 통합 방식은 주로 특정 작업을 자동화하는 데 초점을 맞춰왔고, 건설 작업 흐름에서 인간의 복잡성과 변동성을 간과하는 경우가 많았어. 이 논문은 "작업 동반자 로버"라는 인간 중심의 접근 방식을 소개해. 이 로버는 건설 노동자들이 현재의 작업 방식 안에서 도움을 줄 수 있도록 설계되었고, 안전성과 작업 흐름을 개선하는 동시에 건설 노동의 숙련된 특성을 존중하는 걸 목표로 하고 있어.

우리는 목공 형틀 작업에 로봇 시스템을 배치하는 데 대한 심층 연구를 수행했어. 여기서 이동성과 안전성, 그리고 동적인 환경에서 편안한 작업자-로봇 협업을 강조하는 프로토타입을 보여줬어. 이 연구는 건설 분야에서 로봇 응용을 발전시키고, 적응형 로봇이 사람을 대체하는 것이 아니라 지원하는 협력 모델을 지지해. 인간과 로봇이 상호작용하며 협업할 수 있는 가능성을 강조하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2404.03103.pdf

Title: Multi-Robot Planning for Filming Groups of Moving Actors Leveraging Submodularity and Pixel Density

Original Abstract:
Observing and filming a group of moving actors with a team of aerial robots is a challenging problem that combines elements of multi-robot coordination, coverage, and view planning. A single camera may observe multiple actors at once, and a robot team may observe individual actors from multiple views. As actors move about, groups may split, merge, and reform, and robots filming these actors should be able to adapt smoothly to such changes in actor formations. Rather than adopt an approach based on explicit formations or assignments, we propose an approach based on optimizing views directly. We model actors as moving polyhedra and compute approximate pixel densities for each face and camera view. Then, we propose an objective that exhibits diminishing returns as pixel densities increase from repeated observation. This gives rise to a multi-robot perception planning problem that we solve via a combination of value iteration and greedy submodular maximization. We evaluate our approach on challenging scenarios modeled after various social behaviors and featuring different numbers of robots and actors and observe that robot assignments and formations arise implicitly given the movements of groups of actors. Simulation results demonstrate that our approach consistently outperforms baselines, and in addition to performing well with the planner's approximation of pixel densities our approach also performs comparably for evaluation based on rendered views. Overall, the multi-round variant of the sequential planner we propose meets (within 1%) or exceeds formation and assignment baselines in all scenarios.

Translated Abstract:
움직이는 배우들의 그룹을 드론 로봇 팀으로 관찰하고 촬영하는 건 꽤 도전적인 문제야. 이건 여러 로봇이 협력하고, 전체를 커버하며, 촬영 각도를 계획하는 요소들이 섞여 있는 거지. 하나의 카메라로 여러 배우를 동시에 볼 수 있고, 로봇 팀은 각 배우를 여러 각도에서 관찰할 수 있어.

배우들이 움직일 때 그룹이 나뉘기도 하고 합쳐지기도 하니까, 이 배우들을 촬영하는 로봇은 그런 변화에 잘 적응해야 해. 우리는 명확한 형성이나 할당에 기반한 방법 대신, 직접적으로 시점을 최적화하는 방법을 제안해. 배우들을 움직이는 다면체로 모델링하고, 각 면과 카메라 뷰에 대한 픽셀 밀도를 대략 계산해. 그런 다음, 반복적으로 관찰하면서 픽셀 밀도가 증가할 때 수익이 줄어드는 목표를 제안해.

이렇게 해서 우리는 다중 로봇 인식 계획 문제를 만들고, 가치 반복과 탐욕적인 부분 모듈화 최대화를 결합해서 해결해. 다양한 사회적 행동을 모델링한 도전적인 시나리오에서 우리의 접근 방식을 평가해 보았고, 로봇의 할당과 형성이 배우 그룹의 움직임에 따라 자연스럽게 발생한다는 걸 확인했어.

시뮬레이션 결과, 우리의 방법이 기본 모델보다 항상 더 나은 성과를 보였고, 픽셀 밀도 근사치와 잘 작동하는 것뿐만 아니라 렌더링된 뷰를 기반으로 한 평가에서도 비슷한 성과를 냈어. 전반적으로, 우리가 제안한 다중 라운드 변형의 순차 계획자는 모든 시나리오에서 형성과 할당의 기준을 1% 이내로 맞추거나 초과하는 결과를 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2405.01673.pdf

Title: ShadowNav: Autonomous Global Localization for Lunar Navigation in Darkness

Original Abstract:
The ability to determine the pose of a rover in an inertial frame autonomously is a crucial capability necessary for the next generation of surface rover missions on other planetary bodies. Currently, most on-going rover missions utilize ground-in-the-loop interventions to manually correct for drift in the pose estimate and this human supervision bottlenecks the distance over which rovers can operate autonomously and carry out scientific measurements. In this paper, we present ShadowNav, an autonomous approach for global localization on the Moon with an emphasis on driving in darkness and at nighttime. Our approach uses the leading edge of Lunar craters as landmarks and a particle filtering approach is used to associate detected craters with known ones on an offboard map. We discuss the key design decisions in developing the ShadowNav framework for use with a Lunar rover concept equipped with a stereo camera and an external illumination source. Finally, we demonstrate the efficacy of our proposed approach in both a Lunar simulation environment and on data collected during a field test at Cinder Lakes, Arizona.

Translated Abstract:
로버의 자세를 자율적으로 결정하는 능력은 다른 행성에서의 다음 세대 표면 로버 미션에 필수적인 기능이야. 현재 대부분의 로버 미션은 자세 추정의 드리프트를 수동으로 수정하기 위해 사람의 개입을 필요로 해. 이 인간의 감독 덕분에 로버가 자율적으로 작동하고 과학적 측정을 수행할 수 있는 거리가 제한돼.

이 논문에서는 ShadowNav라는 자율적인 방식의 글로벌 위치 추정 방법을 소개해. 이 방법은 달에서 어둠 속에서 주행하는 데 중점을 두고 있어. 우리의 방법은 달의 크레이터의 가장자리를 랜드마크로 사용하고, 입자 필터링 방식을 통해 감지된 크레이터를 외부 맵에 있는 알려진 크레이터와 연결해.

ShadowNav 프레임워크를 개발하면서의 주요 설계 결정을 논의하고, 스테레오 카메라와 외부 조명 장치를 장착한 달 로버 개념에 적용할 수 있도록 했어. 마지막으로, 우리의 제안된 방법의 효과를 달 시뮬레이션 환경과 애리조나의 Cinder Lakes에서 실시한 현장 테스트 데이터에서 보여줄 거야.

================================================================================

URL:
https://arxiv.org/pdf/2405.07392.pdf

Title: NGD-SLAM: Towards Real-Time Dynamic SLAM without GPU

Original Abstract:
Existing SLAM (Simultaneous Localization and Mapping) algorithms have achieved remarkable localization accuracy in dynamic environments by using deep learning techniques to identify dynamic objects. However, they usually require GPUs to operate in real-time. Therefore, this paper proposes an open-source real-time dynamic SLAM system that runs solely on CPU by incorporating a mask prediction mechanism, which allows the deep learning method and the camera tracking to run entirely in parallel at different frequencies. Our SLAM system further introduces a dual-stage optical flow tracking approach and employs a hybrid usage of optical flow and ORB features, enhancing efficiency and robustness by selectively allocating computational resources to input frames. Compared with previous methods, our system maintains high localization accuracy in dynamic environments while achieving a tracking frame rate of 56 FPS on a laptop CPU, proving that deep learning methods are feasible for dynamic SLAM without GPU support. To the best of our knowledge, this is the first SLAM system to achieve this.

Translated Abstract:
기존의 SLAM(동시 위치 측정 및 지도 작성) 알고리즘은 딥러닝 기술을 사용해 동적인 환경에서 뛰어난 위치 측정 정확도를 달성했어. 하지만 보통 실시간으로 작동하려면 GPU가 필요해. 그래서 이 논문에서는 CPU만으로 작동하는 오픈소스 실시간 동적 SLAM 시스템을 제안해. 이 시스템은 마스크 예측 메커니즘을 포함해서 딥러닝 방법과 카메라 추적을 서로 다른 주파수에서 완전히 병렬로 실행할 수 있게 해.

우리 SLAM 시스템은 또한 이중 단계의 광학 흐름 추적 방법을 도입하고, 광학 흐름과 ORB 특징을 혼합해서 사용해. 이렇게 하면 입력 프레임에 컴퓨팅 자원을 선택적으로 할당함으로써 효율성과 강인성을 높일 수 있어. 이전 방법들과 비교했을 때, 우리 시스템은 동적인 환경에서도 높은 위치 측정 정확도를 유지하면서, 노트북 CPU에서 56 FPS의 추적 프레임 속도를 달성했어. 이로써 GPU 지원 없이도 동적 SLAM에 딥러닝 방법을 적용할 수 있다는 걸 증명했어. 우리가 아는 한, 이건 최초의 SLAM 시스템이야.

================================================================================

URL:
https://arxiv.org/pdf/2406.04086.pdf

Title: A Survey of Language-Based Communication in Robotics

Original Abstract:
Embodied robots which can interact with their environment and neighbours are increasingly being used as a test case to develop Artificial Intelligence. This creates a need for multimodal robot controllers that can operate across different types of information, including text. Large Language Models are able to process and generate textual as well as audiovisual data and, more recently, robot actions. Language Models are increasingly being applied to robotic systems; these Language-Based robots leverage the power of language models in a variety of ways. Additionally, the use of language opens up multiple forms of information exchange between members of a human-robot team. This survey motivates the use of language models in robotics, and then delineates works based on the part of the overall control flow in which language is incorporated. Language can be used by human to task a robot, by a robot to inform a human, between robots as a human-like communication medium, and internally for a robot's planning and control. Applications of language-based robots are explored, and numerous limitations and challenges are discussed to provide a summary of the development needed for the future of language-based robotics.

Translated Abstract:
환경과 이웃과 상호작용할 수 있는 로봇들이 인공지능을 개발하기 위한 테스트 케이스로 점점 더 많이 사용되고 있어. 그래서 다양한 정보를 처리할 수 있는 다중 모드 로봇 컨트롤러가 필요해. 대형 언어 모델은 텍스트와 오디오-비주얼 데이터, 최근에는 로봇 동작까지 처리하고 생성할 수 있어. 

언어 모델은 로봇 시스템에 점점 더 많이 적용되고 있어. 이런 언어 기반 로봇들은 언어 모델의 힘을 다양한 방식으로 활용해. 게다가 언어를 사용하면 인간-로봇 팀의 구성원들 간에 여러 형태의 정보 교환이 가능해져. 이 조사는 로봇 공학에서 언어 모델 사용의 필요성을 강조하고, 언어가 통합된 전체 제어 흐름에서의 작업들을 정리해. 

언어는 인간이 로봇에게 작업을 지시할 때, 로봇이 인간에게 정보를 전달할 때, 로봇끼리 인간처럼 소통할 때, 그리고 로봇의 계획과 제어를 위해 내부적으로 사용할 수 있어. 언어 기반 로봇의 응용 사례를 탐구하고, 미래의 언어 기반 로봇 공학을 위해 필요한 발전의 한계를 여러 가지로 논의해.

================================================================================

URL:
https://arxiv.org/pdf/2406.05931.pdf

Title: Differentiable Discrete Elastic Rods for Real-Time Modeling of Deformable Linear Objects

Original Abstract:
This paper addresses the task of modeling Deformable Linear Objects (DLOs), such as ropes and cables, during dynamic motion over long time horizons. This task presents significant challenges due to the complex dynamics of DLOs. To address these challenges, this paper proposes differentiable Discrete Elastic Rods For deformable linear Objects with Real-time Modeling (DEFORM), a novel framework that combines a differentiable physics-based model with a learning framework to model DLOs accurately and in real-time. The performance of DEFORM is evaluated in an experimental setup involving two industrial robots and a variety of sensors. A comprehensive series of experiments demonstrate the efficacy of DEFORM in terms of accuracy, computational speed, and generalizability when compared to state-of-the-art alternatives. To further demonstrate the utility of DEFORM, this paper integrates it into a perception pipeline and illustrates its superior performance when compared to the state-of-the-art methods while tracking a DLO even in the presence of occlusions. Finally, this paper illustrates the superior performance of DEFORM when compared to state-of-the-art methods when it is applied to perform autonomous planning and control of DLOs. Project page: this https URL.

Translated Abstract:
이 논문은 로프나 케이블 같은 변형 가능한 선형 물체(DLOs)를 다루는 방법에 대해 이야기해. 이런 물체들이 동적으로 움직일 때 긴 시간 동안의 모델링이 필요하거든. DLOs의 복잡한 동적 특성 때문에 이 작업은 꽤 도전적이야. 

그래서 이 논문에서는 DEFORM이라는 새로운 프레임워크를 제안해. DEFORM은 변형 가능한 선형 물체를 실시간으로 정확하게 모델링할 수 있도록 차별화된 물리 기반 모델과 학습 프레임워크를 결합한 거야. 

DEFORM의 성능은 두 개의 산업 로봇과 여러 센서를 사용하는 실험 세트업에서 평가했어. 다양한 실험을 통해 DEFORM이 정확성, 계산 속도, 일반화 측면에서 최신 기술과 비교했을 때 얼마나 효과적인지를 보여줬어. 

또한 DEFORM을 인식 파이프라인에 통합해서, 물체가 가려져 있는 상황에서도 DLO를 추적할 때 최신 방법들과 비교해 더 뛰어난 성능을 발휘하는 걸 보여줬어. 마지막으로, DEFORM이 DLO의 자율 계획 및 제어를 수행할 때 최신 방법들과 비교해 얼마나 우수한지를 설명하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2406.08187.pdf

Title: Learning-based Traversability Costmap for Autonomous Off-road Navigation

Original Abstract:
Traversability estimation in off-road terrains is an essential procedure for autonomous navigation. However, creating reliable labels for complex interactions between the robot and the surface is still a challenging problem in learning-based costmap generation. To address this, we propose a method that predicts traversability costmaps by leveraging both visual and geometric information of the environment. To quantify the surface properties like roughness and bumpiness, we introduce a novel way of risk-aware labelling with proprioceptive information for network training. We validate our method in costmap prediction and navigation tasks for complex off-road scenarios. Our results demonstrate that our costmap prediction method excels in terms of average accuracy and MSE. The navigation results indicate that using our learned costmaps leads to safer and smoother driving, outperforming previous methods in terms of the highest success rate, lowest normalized trajectory length, lowest time cost, and highest mean stability across two scenarios.

Translated Abstract:
오프로드 지형에서의 통과 가능성 추정은 자율 주행에 꼭 필요한 과정이야. 하지만 로봇과 지면 사이의 복잡한 상호작용에 대한 신뢰할 수 있는 레이블을 만드는 건 여전히 어려운 문제야. 그래서 우리는 시각적 정보와 기하학적 정보를 활용해서 통과 가능성 비용 맵을 예측하는 방법을 제안해.

지면의 특성, 예를 들어 거칠기나 울퉁불퉁함 같은 걸 정량화하기 위해, 우리는 네트워크 훈련에 필요한 새로운 위험 인식 레이블링 방법을 도입했어. 이 방법은 로봇의 위치 감지 정보를 사용해.

우리는 복잡한 오프로드 시나리오에서 비용 맵 예측과 내비게이션 작업에 대해 우리의 방법을 검증했어. 결과적으로, 우리의 비용 맵 예측 방법은 평균 정확도와 평균 제곱 오차(MSE) 면에서 뛰어난 성능을 보였어.

또한, 내비게이션 결과에서는 우리가 학습한 비용 맵을 사용하면 더 안전하고 부드러운 주행을 할 수 있다는 걸 보여줬어. 이전 방법들보다 성공률이 가장 높고, 정규화된 경로 길이가 가장 짧으며, 시간 비용이 가장 적고, 두 가지 시나리오에서 평균 안정성도 가장 높았어.

================================================================================

URL:
https://arxiv.org/pdf/2407.00014.pdf

Title: Kinetic and Kinematic Sensors-free Approach for Estimation of Continuous Force and Gesture in sEMG Prosthetic Hands

Original Abstract:
Regression-based sEMG prosthetic hands are widely used for their ability to provide continuous kinetic and kinematic parameters. However, establishing these models requires complex sensors systems to collect corresponding kinetic and kinematic data in synchronization with sEMG, which is cumbersome and user-unfriendly. This paper proposes a kinetic and kinematic sensors-free approach for controlling sEMG prosthetic hands, enabling continuous decoding and execution of three hand movements: individual finger flexion/extension, multiple finger flexion/extension, and fist opening/closing. This approach utilizes only two data points (-1 and 1), representing maximal finger flexion force label and extension force label respectively, and their corresponding sEMG data to establish a near-linear model based on sEMG data and labels. The model's output labels values are used to control the direction and magnitude of fingers forces, enabling the estimation of continuous gestures. To validate this approach, we conducted offline and online experiments using four models: Dendritic Net (DD), Linear Net (LN), Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). The offline analysis assessed each model's ability to classify finger force direction and interpolate intermediate force values, while online experiments evaluated real-time control performance in controlling gestures and accurately adjusting forces. Our results demonstrate that the DD and LN models provide excellent real-time control of finger forces and gestures, highlighting the practical potential of this sensors-free approach for prosthetic applications. This study significantly reduces the complexity of collecting kinetic and kinematic parameters in sEMG-based regression prosthetics, thus enhancing the usability and convenience of prosthetic hands.

Translated Abstract:
회귀 기반의 sEMG 의수는 지속적인 운동 및 동작 매개변수를 제공할 수 있어서 많이 사용돼. 하지만 이런 모델을 만들려면 복잡한 센서 시스템이 필요해, sEMG와 동기화된 운동 및 동작 데이터를 수집해야 하니까 좀 번거롭고 사용하기 불편해. 

이 논문에서는 sEMG 의수를 제어하는 데 센서 없는 방법을 제안해. 이 방법은 손가락 개별 굽힘/펼침, 여러 손가락 굽힘/펼침, 주먹 열기/닫기 같은 세 가지 손 동작을 계속해서 해석하고 실행할 수 있게 해. 이 접근법은 최대 손가락 굽힘 힘 레이블과 펼침 힘 레이블을 각각 나타내는 두 개의 데이터 포인트(-1과 1)만 사용해. 그리고 이 데이터와 sEMG 데이터를 기반으로 거의 선형 모델을 만들지. 

모델의 출력 레이블 값들은 손가락 힘의 방향과 크기를 제어하는 데 사용되며, 이를 통해 지속적인 제스처를 추정할 수 있어. 이 방법을 검증하기 위해 Dendritic Net (DD), Linear Net (LN), Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN) 등 네 가지 모델을 사용해 오프라인 및 온라인 실험을 진행했어. 오프라인 분석에서는 각 모델이 손가락 힘의 방향을 분류하고 중간 힘 값을 보간하는 능력을 평가했고, 온라인 실험에서는 제스처를 제어하고 힘을 정확하게 조정하는 실시간 성능을 평가했어. 

우리의 결과에 따르면 DD와 LN 모델이 손가락 힘과 제스처를 실시간으로 잘 제어할 수 있어서 이 센서 없는 접근법이 의수에 실용적인 가능성을 보여줘. 이 연구는 sEMG 기반 회귀 의수에서 운동 및 동작 매개변수를 수집하는 복잡성을 크게 줄여서 의수의 사용성과 편리성을 높여줘.

================================================================================

URL:
https://arxiv.org/pdf/2407.02090.pdf

Title: Universal Plans: One Action Sequence to Solve Them All!

Original Abstract:
This paper introduces the notion of a universal plan, which when executed, is guaranteed to solve all planning problems in a category, regardless of the obstacles, initial state, and goal set. Such plans are specified as a deterministic sequence of actions that are blindly applied without any sensor feedback. Thus, they can be considered as pure exploration in a reinforcement learning context, and we show that with basic memory requirements, they even yield optimal plans. Building upon results in number theory and theory of automata, we provide universal plans both for discrete and continuous (motion) planning and prove their (semi)completeness. The concepts are applied and illustrated through simulation studies, and several directions for future research are sketched.

Translated Abstract:
이 논문은 보편적 계획(universal plan)이라는 개념을 소개해. 이 계획은 실행되면 어떤 장애물이나 초기 상태, 목표 세트와 관계없이 모든 계획 문제를 해결할 수 있어. 이러한 계획은 센서 피드백 없이 무작정 적용되는 결정론적 행동의 순서로 정해져 있어. 그래서 강화 학습의 맥락에서는 순수 탐험(pure exploration)으로 볼 수 있고, 기본적인 메모리 요구사항만으로도 최적의 계획을 만들 수 있다는 걸 보여줘.

우리는 수론과 자동 이론의 결과를 바탕으로 이산(discrete)과 연속(continuous) 계획에 대한 보편적 계획을 제공하고, 그 (부분)완전성을 증명해. 이 개념들은 시뮬레이션 연구를 통해 적용되고 설명되며, 앞으로의 연구 방향도 여러 가지 제시돼.

================================================================================

URL:
https://arxiv.org/pdf/2407.06770.pdf

Title: Pretraining-finetuning Framework for Efficient Co-design: A Case Study on Quadruped Robot Parkour

Original Abstract:
In nature, animals with exceptional locomotion abilities, such as cougars, often possess asymmetric fore and hind legs. This observation inspired us: could optimizing the leg length of quadruped robots endow them with similar locomotive capabilities? In this paper, we propose an approach that co-optimizes the mechanical structure and control policy to boost the locomotive prowess of quadruped robots. Specifically, we introduce a novel pretraining-finetuning framework, which not only guarantees optimal control strategies for each mechanical candidate but also ensures time efficiency. Additionally, we have devised an innovative training method for our pretraining network, integrating spatial domain randomization with regularization methods, markedly improving the network's generalizability. Our experimental results indicate that the proposed pretraining-finetuning framework significantly enhances the overall co-design performance with less time consumption. Moreover, the co-design strategy substantially exceeds the conventional method of independently optimizing control strategies, further improving the robot's locomotive performance and providing an innovative approach to enhancing the extreme parkour capabilities of quadruped robots.

Translated Abstract:
자연에서, 쿠거 같은 뛰어난 이동 능력을 가진 동물들은 종종 비대칭적인 앞다리와 뒷다리를 가지고 있어. 이걸 보고 영감을 받았어: 혹시 네 발 로봇의 다리 길이를 최적화하면 비슷한 이동 능력을 가질 수 있을까? 

이 논문에서는 네 발 로봇의 이동 능력을 향상시키기 위해 기계 구조와 제어 정책을 동시에 최적화하는 방법을 제안해. 구체적으로, 우리는 각 기계 후보에 대해 최적의 제어 전략을 보장하고 시간을 효율적으로 사용할 수 있는 새로운 사전 훈련-세부 조정 프레임워크를 도입했어. 

게다가, 우리는 사전 훈련 네트워크를 위한 혁신적인 훈련 방법도 개발했어. 이 방법은 공간 도메인 무작위화와 정규화 방법을 통합해서 네트워크의 일반화 능력을 크게 향상시켰어. 

실험 결과에 따르면, 제안한 사전 훈련-세부 조정 프레임워크는 전체 공동 설계 성능을 크게 향상시키면서도 시간을 덜 소모해. 게다가, 이 공동 설계 전략은 제어 전략을 독립적으로 최적화하는 전통적인 방법을 훨씬 초월해서 로봇의 이동 성능을 더 개선하고, 네 발 로봇의 극한 파쿠르 능력을 향상시키는 혁신적인 접근법을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2407.07684.pdf

Title: Towards Human-Like Driving: Active Inference in Autonomous Vehicle Control

Original Abstract:
This paper presents a novel approach to Autonomous Vehicle (AV) control through the application of active inference, a theory derived from neuroscience that conceptualizes the brain as a predictive machine. Traditional autonomous driving systems rely heavily on Modular Pipelines, Imitation Learning, or Reinforcement Learning, each with inherent limitations in adaptability, generalization, and computational efficiency. Active inference addresses these challenges by minimizing prediction error (termed "surprise") through a dynamic model that balances perception and action. Our method integrates active inference with deep learning to manage lateral control in AVs, enabling them to perform lane following maneuvers within a simulated urban environment. We demonstrate that our model, despite its simplicity, effectively learns and generalizes from limited data without extensive retraining, significantly reducing computational demands. The proposed approach not only enhances the adaptability and performance of AVs in dynamic scenarios but also aligns closely with human-like driving behavior, leveraging a generative model to predict and adapt to environmental changes. Results from extensive experiments in the CARLA simulator show promising outcomes, outperforming traditional methods in terms of adaptability and efficiency, thereby advancing the potential of active inference in real-world autonomous driving applications.

Translated Abstract:
이 논문은 자율주행차(AV) 제어를 위한 새로운 접근 방식을 제안해. 이 방식은 뇌의 예측 기계로서의 기능을 기반으로 한 '능동 추론(active inference)' 이론을 적용한 거야. 기존의 자율주행 시스템은 모듈 파이프라인, 모방 학습, 강화 학습 같은 방식에 의존하는데, 이들 각각은 적응성, 일반화, 계산 효율성에서 한계가 있어.

능동 추론은 예측 오류(일명 "놀라움")를 최소화하는 동적 모델을 통해 이런 문제를 해결해. 이 모델은 인식과 행동의 균형을 맞추는 방식이야. 우리 방법은 능동 추론과 딥러닝을 결합해서 AV의 측면 제어를 관리하고, 이를 통해 시뮬레이션된 도시 환경에서 차선 유지 조작을 수행할 수 있게 해.

우리가 제안한 모델은 단순함에도 불구하고 제한된 데이터에서 효과적으로 학습하고 일반화할 수 있어. 그래서 광범위한 재훈련 없이도 계산 요구를 크게 줄일 수 있어. 이 접근 방식은 동적 상황에서 AV의 적응성과 성능을 향상시키고, 환경 변화에 적응하기 위해 생성 모델을 활용하면서 인간과 비슷한 운전 행동과도 잘 맞아.

CARLA 시뮬레이터에서 진행한 다양한 실험 결과는 긍정적인 성과를 보여주고, 전통적인 방법보다 적응성과 효율성 면에서 우수하다는 걸 입증했어. 이렇게 해서 능동 추론이 실제 자율주행 응용 분야에서의 가능성을 높이는 데 기여하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2407.16412.pdf

Title: SARO: Space-Aware Robot System for Terrain Crossing via Vision-Language Model

Original Abstract:
The application of vision-language models (VLMs) has achieved impressive success in various robotics tasks. However, there are few explorations for these foundation models used in quadruped robot navigation through terrains in 3D environments. In this work, we introduce SARO (Space Aware Robot System for Terrain Crossing), an innovative system composed of a high-level reasoning module, a closed-loop sub-task execution module, and a low-level control policy. It enables the robot to navigate across 3D terrains and reach the goal position. For high-level reasoning and execution, we propose a novel algorithmic system taking advantage of a VLM, with a design of task decomposition and a closed-loop sub-task execution mechanism. For low-level locomotion control, we utilize the Probability Annealing Selection (PAS) method to effectively train a control policy by reinforcement learning. Numerous experiments show that our whole system can accurately and robustly navigate across several 3D terrains, and its generalization ability ensures the applications in diverse indoor and outdoor scenarios and terrains. Project page: this https URL

Translated Abstract:
비전-언어 모델(VLM)의 활용이 다양한 로봇 작업에서 놀라운 성공을 거두었어. 하지만 3D 환경에서 4족 로봇이 지형을 탐색하는 데 이 기본 모델을 사용한 연구는 거의 없어. 

여기서 우리는 SARO(지형 횡단을 위한 공간 인식 로봇 시스템)라는 혁신적인 시스템을 소개할게. 이 시스템은 고수준 추론 모듈, 폐쇄 루프 하위 작업 실행 모듈, 그리고 저수준 제어 정책으로 구성돼. 이걸 통해 로봇이 3D 지형을 가로질러 목표 지점에 도달할 수 있어. 

고수준 추론과 실행을 위해 우리는 VLM의 장점을 활용한 새로운 알고리즘 시스템을 제안해. 여기에는 작업 분해와 폐쇄 루프 하위 작업 실행 메커니즘이 포함돼. 저수준 이동 제어를 위해서는 확률 어닐링 선택(PAS) 방법을 사용해서 강화 학습으로 제어 정책을 효과적으로 훈련해. 

많은 실험 결과, 우리 시스템이 여러 3D 지형을 정확하고 강력하게 탐색할 수 있다는 걸 보여줬어. 또한, 일반화 능력이 뛰어나서 다양한 실내외 시나리오와 지형에 적용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2407.16811.pdf

Title: Variable Inertia Model Predictive Control for Fast Bipedal Maneuvers

Original Abstract:
This paper proposes a novel control framework for agile and robust bipedal locomotion, addressing model discrepancies between full-body and reduced-order models. Specifically, assumptions such as constant centroidal inertia have introduced significant challenges and limitations in locomotion tasks. To enhance the agility and versatility of full-body humanoid robots, we formalize a Model Predictive Control (MPC) problem that accounts for the variable centroidal inertia of humanoid robots within a convex optimization framework, ensuring computational efficiency for real-time operations. In the proposed formulation, we incorporate a centroidal inertia network designed to predict the variable centroidal inertia over the MPC horizon, taking into account the swing foot trajectories -- an aspect often overlooked in ROM-based MPC frameworks. By integrating the MPC-based contact wrench planning with our low-level whole-body controller, we significantly improve the locomotion performance, achieving stable walking at higher velocities that are not attainable with the baseline method. The effectiveness of our proposed framework is validated through high-fidelity simulations using our full-body bipedal humanoid robot DRACO 3, demonstrating dynamic behaviors.

Translated Abstract:
이 논문에서는 민첩하고 강력한 이족 보행을 위한 새로운 제어 프레임워크를 제안해. 여기서 전체 모델과 축소 모델 간의 차이점을 다루고 있어. 특히, 일정한 중심 관성에 대한 가정이 보행 작업에서 큰 도전과 한계를 가져왔어.

우리는 전체적인 인간형 로봇의 민첩성과 다양성을 높이기 위해, 인간형 로봇의 가변 중심 관성을 고려한 모델 예측 제어(MPC) 문제를 공식화했어. 이건 실시간 작업을 위한 계산 효율성을 보장하는 볼록 최적화 프레임워크 안에서 이루어져. 제안된 형식에서는, MPC 기간 동안 가변 중심 관성을 예측할 수 있도록 설계된 중심 관성 네트워크를 포함했어. 여기서는 흔히 간과되는 스윙 발 궤적도 고려하고 있어.

MPC 기반의 접촉 힘 계획을 저수준 전체 몸 제어기와 통합함으로써, 보행 성능을 크게 향상시켰고, 기본 방법으로는 도달할 수 없는 높은 속도로 안정적인 보행을 달성했어. 우리 프레임워크의 효과는 전체 몸 이족 로봇 DRACO 3를 사용한 고충실도 시뮬레이션을 통해 검증되었고, 동적인 행동을 잘 보여주고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2407.17617.pdf

Title: Adaptive Robot Detumbling of a Non-Rigid Satellite

Original Abstract:
The challenge of satellite stabilization, particularly those with uncertain flexible dynamics, has become a pressing concern in control and robotics. These uncertainties, especially the dynamics of a third-party client satellite, significantly complicate the stabilization task. This paper introduces a novel adaptive detumbling method to handle non-rigid satellites with unknown motion dynamics (translation and rotation). The distinctive feature of our approach is that we model the non-rigid tumbling satellite as a two-link serial chain with unknown stiffness and damping in contrast to previous detumbling research works which consider the satellite a rigid body. We develop a novel adaptive robotics approach to detumble the satellite by using two space tugs as servicer despite the uncertain dynamics in the post-capture case. Notably, the stiffness properties and other physical parameters, including the mass and inertia of the two links, remain unknown to the servicer. Our proposed method addresses the challenges in detumbling tasks and paves the way for advanced manipulation of non-rigid satellites with uncertain dynamics.

Translated Abstract:
위성 안정화, 특히 불확실한 유연 동역학을 가진 위성의 경우, 제어와 로봇 분야에서 큰 문제로 떠오르고 있어. 이런 불확실성, 특히 제3의 클라이언트 위성의 동역학은 안정화 작업을 정말 복잡하게 만들어.

이 논문에서는 모션 동역학(이동과 회전)이 알려지지 않은 비강체 위성을 다루기 위한 새로운 적응형 디탐블링 방법을 소개해. 우리가 제안하는 방법의 특징은 비강체가 회전하는 위성을 이전의 디탐블링 연구들과 달리, 강성과 감쇠가 알려지지 않은 두 개의 링크로 이루어진 직렬 체인으로 모델링한다는 점이야.

우리는 불확실한 동역학이 있는 포획 후 상황에서도 위성을 디탐블링하기 위해 두 개의 우주 구조물(서비스 로봇)을 사용하는 새로운 적응형 로봇 접근 방식을 개발했어. 특히, 두 링크의 질량과 관성 같은 강성 특성과 다른 물리적 매개변수는 서비스 로봇에게 알려지지 않은 상태야. 

우리의 방법은 디탐블링 작업의 도전 과제를 해결하고, 불확실한 동역학을 가진 비강체 위성의 고급 조작 가능성을 열어줄 거야.

================================================================================

URL:
https://arxiv.org/pdf/2408.03539.pdf

Title: Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes

Original Abstract:
Reinforcement learning (RL), particularly its combination with deep neural networks referred to as deep RL (DRL), has shown tremendous promise across a wide range of applications, suggesting its potential for enabling the development of sophisticated robotic behaviors. Robotics problems, however, pose fundamental difficulties for the application of RL, stemming from the complexity and cost of interacting with the physical world. This article provides a modern survey of DRL for robotics, with a particular focus on evaluating the real-world successes achieved with DRL in realizing several key robotic competencies. Our analysis aims to identify the key factors underlying those exciting successes, reveal underexplored areas, and provide an overall characterization of the status of DRL in robotics. We highlight several important avenues for future work, emphasizing the need for stable and sample-efficient real-world RL paradigms, holistic approaches for discovering and integrating various competencies to tackle complex long-horizon, open-world tasks, and principled development and evaluation procedures. This survey is designed to offer insights for both RL practitioners and roboticists toward harnessing RL's power to create generally capable real-world robotic systems.

Translated Abstract:
강화 학습(RL), 특히 심층 신경망과 결합된 심층 강화 학습(DRL)은 다양한 응용 분야에서 큰 가능성을 보이고 있어 복잡한 로봇 행동을 개발하는 데 도움이 될 수 있습니다. 하지만 로봇 문제는 물리적 세계와 상호작용하는 데 복잡성과 비용 때문에 RL 적용에 근본적인 어려움을 겪고 있습니다.

이 글에서는 로봇을 위한 DRL에 대한 최신 조사를 제공하고, DRL이 여러 주요 로봇 능력을 실현하는 데 성공한 실제 사례들을 평가하는 데 초점을 맞추고 있습니다. 우리의 분석은 이러한 흥미로운 성공 뒤에 있는 핵심 요소들을 파악하고, 잘 탐구되지 않은 영역을 드러내며, 로봇 분야에서 DRL의 현재 상태를 전반적으로 설명하는 것을 목표로 합니다.

우리는 안정적이고 샘플 효율적인 실제 RL 패러다임, 복잡한 장기적 오픈 월드 작업을 해결하기 위한 다양한 능력을 발견하고 통합하는 포괄적인 접근 방식, 원칙적인 개발 및 평가 절차와 같은 미래 연구를 위한 몇 가지 중요한 방향을 강조합니다. 이 조사는 RL 실무자와 로봇 공학자들이 RL의 힘을 활용해 일반적으로 능력 있는 실제 로봇 시스템을 만드는 데 도움이 되는 통찰력을 제공하기 위해 설계되었습니다.

================================================================================

URL:
https://arxiv.org/pdf/2408.09251.pdf

Title: V2X-VLM: End-to-End V2X Cooperative Autonomous Driving Through Large Vision-Language Models

Original Abstract:
Advancements in autonomous driving have increasingly focused on end-to-end (E2E) systems that manage the full spectrum of driving tasks, from environmental perception to vehicle navigation and control. This paper introduces V2X-VLM, an innovative E2E vehicle-infrastructure cooperative autonomous driving (VICAD) framework with Vehicle-to-Everything (V2X) systems and large vision-language models (VLMs). V2X-VLM is designed to enhance situational awareness, decision-making, and ultimate trajectory planning by integrating multimodel data from vehicle-mounted cameras, infrastructure sensors, and textual information. The contrastive learning method is further employed to complement VLM by refining feature discrimination, assisting the model to learn robust representations of the driving environment. Evaluations on the DAIR-V2X dataset show that V2X-VLM outperforms state-of-the-art cooperative autonomous driving methods, while additional tests on corner cases validate its robustness in real-world driving conditions.

Translated Abstract:
자율주행 기술이 발전하면서 전체 주행 작업을 관리하는 엔드 투 엔드(E2E) 시스템에 점점 더 집중하고 있어. 여기에는 환경 인식, 차량 내비게이션, 제어 등이 포함돼. 이 논문에서는 V2X-VLM이라는 혁신적인 E2E 차량-인프라 협력 자율주행(VICAD) 프레임워크를 소개해. 이 시스템은 차량과 모든 것(V2X) 간의 통신과 대형 비전-언어 모델(VLM)을 결합한 거야.

V2X-VLM은 차량에 장착된 카메라, 인프라 센서, 텍스트 정보를 통합해서 상황 인식, 의사 결정, 최종 경로 계획을 향상시키도록 설계됐어. 또한, 대조 학습 방법을 사용해서 VLM의 특징 구분을 개선하고, 모델이 주행 환경의 강력한 표현을 배울 수 있도록 도와줘.

DAIR-V2X 데이터셋에서 평가한 결과, V2X-VLM이 기존의 협력 자율주행 방법보다 더 뛰어난 성능을 보여줬어. 추가로 경계 사례에 대한 테스트를 통해 실제 주행 조건에서도 견고함을 검증했어.

================================================================================

URL:
https://arxiv.org/pdf/2408.14997.pdf

Title: Depth Restoration of Hand-Held Transparent Objects for Human-to-Robot Handover

Original Abstract:
Transparent objects are common in daily life, while their optical properties pose challenges for RGB-D cameras to capture accurate depth information. This issue is further amplified when these objects are hand-held, as hand occlusions further complicate depth estimation. For assistant robots, however, accurately perceiving hand-held transparent objects is critical to effective human-robot interaction. This paper presents a Hand-Aware Depth Restoration (HADR) method based on creating an implicit neural representation function from a single RGB-D image. The proposed method utilizes hand posture as an important guidance to leverage semantic and geometric information of hand-object interaction. To train and evaluate the proposed method, we create a high-fidelity synthetic dataset named TransHand-14K with a real-to-sim data generation scheme. Experiments show that our method has better performance and generalization ability compared with existing methods. We further develop a real-world human-to-robot handover system based on HADR, demonstrating its potential in human-robot interaction applications.

Translated Abstract:
투명한 물체는 일상생활에서 흔하게 볼 수 있어. 하지만 이 물체들의 광학적 특성 때문에 RGB-D 카메라가 정확한 깊이 정보를 잡아내기가 어려워. 특히 사람들이 손으로 들고 있을 때 이 문제가 더 심해지는데, 손이 물체를 가려서 깊이 추정이 더욱 복잡해지거든. 

그러나 보조 로봇에게는 손으로 들고 있는 투명한 물체를 정확하게 인식하는 게 인간-로봇 상호작용에 매우 중요해. 이 논문에서는 단일 RGB-D 이미지에서 암묵적 신경 표현 함수를 만들어서 손 인식을 고려한 깊이 복원(HADR) 방법을 제안해. 이 방법은 손 자세를 중요한 지침으로 활용해서 손-물체 상호작용의 의미적이고 기하학적인 정보를 끌어내는 데 사용해.

제안한 방법을 훈련하고 평가하기 위해, 우리는 실제와 비슷한 데이터 생성 방식을 사용해 TransHand-14K라는 고품질 합성 데이터셋을 만들었어. 실험 결과, 우리의 방법이 기존 방법들보다 성능이 좋고 일반화 능력이 뛰어난 것으로 나타났어. 우리는 HADR을 기반으로 한 실제 인간-로봇 물체 전달 시스템도 개발했는데, 이게 인간-로봇 상호작용 응용 프로그램에서의 잠재력을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.03920.pdf

Title: Asymptotically-Optimal Multi-Query Path Planning for a Polygonal Robot

Original Abstract:
Shortest-path roadmaps, also known as reduced visibility graphs, provides a highly efficient multi-query method for computing optimal paths in two-dimensional environments. Combined with Minkowski sum computations, shortest-path roadmaps can compute optimal paths for a translating robot in 2D. In this study, we explore the intuitive idea of stacking up a set of reduced visibility graphs at different orientations for a polygonal holonomic robot to support the fast computation of near-optimal paths, allowing simultaneous 2D translation and rotation. The resulting algorithm, rotation-stacked visibility graph (RVG), is shown to be resolution-complete and asymptotically optimal. Extensive computational experiments show RVG significantly outperforms state-of-the-art single- and multi-query sampling-based methods on both computation time and solution optimality fronts.

Translated Abstract:
최단 경로 로드맵, 즉 축소된 가시성 그래프는 2차원 환경에서 최적 경로를 계산하는 데 매우 효율적인 다중 질의 방법을 제공해. Minkowski 합계산과 결합하면, 최단 경로 로드맵은 2D에서 이동하는 로봇을 위한 최적 경로를 계산할 수 있어.

이 연구에서는 다각형 홀로노믹 로봇을 위해 다양한 방향에서 축소된 가시성 그래프를 쌓는 직관적인 아이디어를 탐구해. 이 방법은 빠르게 근사 최적 경로를 계산할 수 있게 해주고, 2D에서 동시에 이동과 회전을 가능하게 해. 결과적으로 나온 알고리즘인 회전 쌓인 가시성 그래프(RVG)는 해상도 완전하고 점근적으로 최적이라는 걸 보여줘.

광범위한 계산 실험 결과 RVG는 최신 단일 및 다중 질의 샘플링 기반 방법들보다 계산 시간과 솔루션 최적성 두 가지 면에서 확실히 더 뛰어난 성능을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.07914.pdf

Title: InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation

Original Abstract:
We present InterACT: Inter-dependency aware Action Chunking with Hierarchical Attention Transformers, a novel imitation learning framework for bimanual manipulation that integrates hierarchical attention to capture inter-dependencies between dual-arm joint states and visual inputs. InterACT consists of a Hierarchical Attention Encoder and a Multi-arm Decoder, both designed to enhance information aggregation and coordination. The encoder processes multi-modal inputs through segment-wise and cross-segment attention mechanisms, while the decoder leverages synchronization blocks to refine individual action predictions, providing the counterpart's prediction as context. Our experiments on a variety of simulated and real-world bimanual manipulation tasks demonstrate that InterACT significantly outperforms existing methods. Detailed ablation studies validate the contributions of key components of our work, including the impact of CLS tokens, cross-segment encoders, and synchronization blocks.

Translated Abstract:
우리는 InterACT를 소개해. Inter-dependency aware Action Chunking with Hierarchical Attention Transformers라는 이름의 새로운 모방 학습 프레임워크로, 양손 조작을 위해 개발됐어. 이 프레임워크는 두 팔의 관절 상태와 시각적 입력 간의 상호 의존성을 잘 포착하기 위해 계층적 주의를 통합하고 있어.

InterACT는 Hierarchical Attention Encoder와 Multi-arm Decoder로 이루어져 있어. 이 둘은 정보 집합과 조정을 향상시키도록 설계됐어. 인코더는 세그먼트별 및 크로스 세그먼트 주의 메커니즘을 통해 다양한 입력을 처리하고, 디코더는 동기화 블록을 활용해 개별 행동 예측을 개선해. 디코더는 상대의 예측을 맥락으로 사용해.

우리가 여러 가지 시뮬레이션과 실제 양손 조작 작업에서 실험한 결과, InterACT가 기존 방법들보다 훨씬 뛰어난 성능을 보여줬어. 상세한 제거 연구를 통해 우리의 연구에서 중요한 요소들, 즉 CLS 토큰, 크로스 세그먼트 인코더, 동기화 블록의 영향을 검증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08212.pdf

Title: Adaptive Language-Guided Abstraction from Contrastive Explanations

Original Abstract:
Many approaches to robot learning begin by inferring a reward function from a set of human demonstrations. To learn a good reward, it is necessary to determine which features of the environment are relevant before determining how these features should be used to compute reward. End-to-end methods for joint feature and reward learning (e.g., using deep networks or program synthesis techniques) often yield brittle reward functions that are sensitive to spurious state features. By contrast, humans can often generalizably learn from a small number of demonstrations by incorporating strong priors about what features of a demonstration are likely meaningful for a task of interest. How do we build robots that leverage this kind of background knowledge when learning from new demonstrations? This paper describes a method named ALGAE (Adaptive Language-Guided Abstraction from [Contrastive] Explanations) which alternates between using language models to iteratively identify human-meaningful features needed to explain demonstrated behavior, then standard inverse reinforcement learning techniques to assign weights to these features. Experiments across a variety of both simulated and real-world robot environments show that ALGAE learns generalizable reward functions defined on interpretable features using only small numbers of demonstrations. Importantly, ALGAE can recognize when features are missing, then extract and define those features without any human input -- making it possible to quickly and efficiently acquire rich representations of user behavior.

Translated Abstract:
로봇 학습에 대한 많은 접근법은 인간의 시연을 기반으로 보상 함수를 추론하는 것에서 시작해. 좋은 보상을 배우려면, 어떤 환경의 특징이 중요한지를 파악해야 하고, 그 다음에 이 특징들이 보상을 계산하는 데 어떻게 사용될지를 결정해야 해. 

기존의 엔드 투 엔드 방식은 특징과 보상을 함께 배우는 방법인데, 예를 들어 딥 네트워크나 프로그램 합성 기법을 사용해. 하지만 이런 방법은 종종 불안정한 보상 함수를 만들어내고, 잘못된 상태 특징에 민감해. 반면에 인간은 적은 수의 시연으로도 일반화된 학습을 할 수 있는데, 이건 시연에서 어떤 특징이 중요한지에 대한 강한 선험적 지식을 활용하기 때문이야. 

그렇다면, 새로운 시연에서 학습할 때 로봇이 이런 배경 지식을 활용할 수 있도록 어떻게 만들 수 있을까? 이 논문에서는 ALGAE라는 방법을 소개해. 이 방법은 언어 모델을 사용해서 인간에게 의미 있는 특징을 반복적으로 찾아내고, 그 다음에 표준 역 강화 학습 기법을 사용해서 이 특징에 가중치를 부여하는 식으로 진행돼.

실험 결과, 시뮬레이션된 환경과 실제 로봇 환경 모두에서 ALGAE는 적은 수의 시연만으로 해석 가능한 특징을 기반으로 일반화된 보상 함수를 학습한다는 것을 보여줬어. 중요하게도, ALGAE는 특징이 부족할 때 이를 인식하고, 인간의 도움 없이도 그 특징을 추출하고 정의할 수 있어. 이렇게 하면 사용자 행동에 대한 풍부한 표현을 빠르고 효율적으로 얻을 수 있게 돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.08704.pdf

Title: QueryCAD: Grounded Question Answering for CAD Models

Original Abstract:
CAD models are widely used in industry and are essential for robotic automation processes. However, these models are rarely considered in novel AI-based approaches, such as the automatic synthesis of robot programs, as there are no readily available methods that would allow CAD models to be incorporated for the analysis, interpretation, or extraction of information. To address these limitations, we propose QueryCAD, the first system designed for CAD question answering, enabling the extraction of precise information from CAD models using natural language queries. QueryCAD incorporates SegCAD, an open-vocabulary instance segmentation model we developed to identify and select specific parts of the CAD model based on part descriptions. We further propose a CAD question answering benchmark to evaluate QueryCAD and establish a foundation for future research. Lastly, we integrate QueryCAD within an automatic robot program synthesis framework, validating its ability to enhance deep-learning solutions for robotics by enabling them to process CAD models (this https URL).

Translated Abstract:
CAD 모델은 산업에서 널리 사용되며 로봇 자동화 과정에 필수적이야. 하지만 이런 모델들이 자동 로봇 프로그램 합성 같은 새로운 AI 기반 접근법에서는 거의 고려되지 않아. CAD 모델을 분석하거나 해석, 정보 추출에 활용할 만한 방법이 별로 없거든.

이런 한계를 해결하기 위해, 우리는 QueryCAD라는 시스템을 제안해. 이 시스템은 CAD 질문에 답할 수 있도록 설계되어 있어서, 자연어 쿼리를 사용해 CAD 모델에서 정확한 정보를 추출할 수 있어. QueryCAD는 우리가 개발한 SegCAD라는 오픈 어휘 인스턴스 분할 모델을 포함하고 있어, 이 모델은 부품 설명에 기반해 CAD 모델의 특정 부분을 식별하고 선택할 수 있게 해.

또한, QueryCAD를 평가하기 위한 CAD 질문 응답 벤치마크를 제안해, 이걸 통해 미래 연구의 기반을 마련하고자 해. 마지막으로, QueryCAD를 자동 로봇 프로그램 합성 프레임워크에 통합해, CAD 모델을 처리할 수 있도록 해줌으로써 로봇 공학을 위한 딥러닝 솔루션을 향상시킬 수 있는 능력을 검증했어.

================================================================================

URL:
https://arxiv.org/pdf/2210.03437.pdf

Title: PCKRF: Point Cloud Completion and Keypoint Refinement With Fusion Data for 6D Pose Estimation

Original Abstract:
Some robust point cloud registration approaches with controllable pose refinement magnitude, such as ICP and its variants, are commonly used to improve 6D pose estimation accuracy. However, the effectiveness of these methods gradually diminishes with the advancement of deep learning techniques and the enhancement of initial pose accuracy, primarily due to their lack of specific design for pose refinement. In this paper, we propose Point Cloud Completion and Keypoint Refinement with Fusion Data (PCKRF), a new pose refinement pipeline for 6D pose estimation. The pipeline consists of two steps. First, it completes the input point clouds via a novel pose-sensitive point completion network. The network uses both local and global features with pose information during point completion. Then, it registers the completed object point cloud with the corresponding target point cloud by our proposed Color supported Iterative KeyPoint (CIKP) method. The CIKP method introduces color information into registration and registers a point cloud around each keypoint to increase stability. The PCKRF pipeline can be integrated with existing popular 6D pose estimation methods, such as the full flow bidirectional fusion network, to further improve their pose estimation accuracy. Experiments demonstrate that our method exhibits superior stability compared to existing approaches when optimizing initial poses with relatively high precision. Notably, the results indicate that our method effectively complements most existing pose estimation techniques, leading to improved performance in most cases. Furthermore, our method achieves promising results even in challenging scenarios involving textureless and symmetrical objects. Our source code is available at this https URL.

Translated Abstract:
로봇 비전에서 6D 자세 추정 정확도를 높이기 위해 ICP와 그 변형 같은 강력한 포인트 클라우드 등록 방법이 흔히 사용돼. 하지만 이런 방법들은 딥러닝 기술이 발전하고 초기 자세 정확도가 높아짐에 따라 점점 효과가 떨어져. 그 이유는 자세 세밀화에 대한 특별한 설계가 부족하기 때문이야.

이 논문에서는 PCKRF(Point Cloud Completion and Keypoint Refinement with Fusion Data)라는 새로운 자세 세밀화 파이프라인을 제안해. 이 파이프라인은 두 단계로 이루어져 있어. 첫 번째 단계에서는 새로운 포즈 민감 포인트 완성 네트워크를 통해 입력된 포인트 클라우드를 완성해. 이 네트워크는 포인트 완성 중에 포즈 정보를 포함한 지역적과 전역적 특징을 사용해.

두 번째 단계에서는 완성된 객체 포인트 클라우드를 해당하는 타겟 포인트 클라우드와 등록하는데, 여기서 우리가 제안한 색상 지원 반복 키포인트(CIKP) 방법을 이용해. CIKP 방법은 등록 과정에 색상 정보를 도입해서 각 키포인트 주변의 포인트 클라우드를 등록함으로써 안정성을 높여.

PCKRF 파이프라인은 기존의 인기 있는 6D 자세 추정 방법, 예를 들어 전 방향 쌍방향 융합 네트워크와 통합될 수 있어. 이렇게 하면 자세 추정 정확도를 더 높일 수 있지. 실험 결과, 우리의 방법이 초기 자세를 상대적으로 높은 정확도로 최적화할 때 기존 방법들보다 훨씬 더 안정적이라는 것을 보여줬어. 특히, 우리의 방법이 대부분의 기존 자세 추정 기술을 효과적으로 보완해서 대부분의 경우 성능이 향상된다는 결과도 나왔어.

게다가, 우리의 방법은 텍스처가 없고 대칭적인 객체가 있는 어려운 상황에서도 유망한 결과를 보여줘. 우리의 소스 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2309.16024.pdf

Title: Model Predictive Planning: Trajectory Planning in Obstruction-Dense Environments for Low-Agility Aircraft

Original Abstract:
We present Model Predictive Planning (MPP), a trajectory planner for low-agility vehicles such as a fixed-wing aircraft to navigate obstacle-laden environments. MPP consists of (1) a multi-path planning procedure that identifies candidate paths, (2) a raytracing procedure that generates linear constraints around these paths to enforce obstacle avoidance, and (3) a convex quadratic program that finds a feasible trajectory within these constraints if one exists. Low-agility aircraft cannot track arbitrary paths, so refining a given path into a trajectory that respects the vehicle's limited maneuverability and avoids obstacles often leads to an infeasible optimization problem. The critical feature of MPP is that it efficiently considers multiple candidate paths during the refinement process, thereby greatly increasing the chance of finding a feasible and trackable trajectory. We demonstrate the effectiveness of MPP on a longitudinal aircraft model.

Translated Abstract:
우리는 저속 비행기 같은 민첩성이 낮은 차량이 장애물 많은 환경에서 이동할 수 있도록 돕는 경로 계획 방법인 모델 예측 계획(Model Predictive Planning, MPP)을 소개해. 

MPP는 세 가지 주요 요소로 구성되어 있어: 
1) 후보 경로를 찾는 다중 경로 계획 절차, 
2) 이 경로 주변에 장애물 회피를 위한 선형 제약 조건을 생성하는 레이 트레이싱 절차, 
3) 이러한 제약 조건 안에서 실행 가능한 경로를 찾는 볼록 이차 프로그래밍이야. 

민첩성이 낮은 비행기는 임의의 경로를 따라갈 수 없기 때문에, 주어진 경로를 비행기의 제한된 기동성을 존중하면서 장애물을 피하는 경로로 다듬는 게 종종 실행 불가능한 최적화 문제로 이어져. MPP의 중요한 특징은 다듬는 과정에서 여러 후보 경로를 효율적으로 고려한다는 거야. 덕분에 실행 가능하고 추적할 수 있는 경로를 찾을 가능성이 크게 높아져. 

우리는 MPP의 효과를 장거리 비행기 모델을 통해 보여줄 거야.

================================================================================

URL:
https://arxiv.org/pdf/2310.02815.pdf

Title: CoBEV: Elevating Roadside 3D Object Detection with Depth and Height Complementarity

Original Abstract:
Roadside camera-driven 3D object detection is a crucial task in intelligent transportation systems, which extends the perception range beyond the limitations of vision-centric vehicles and enhances road safety. While previous studies have limitations in using only depth or height information, we find both depth and height matter and they are in fact complementary. The depth feature encompasses precise geometric cues, whereas the height feature is primarily focused on distinguishing between various categories of height intervals, essentially providing semantic context. This insight motivates the development of Complementary-BEV (CoBEV), a novel end-to-end monocular 3D object detection framework that integrates depth and height to construct robust BEV representations. In essence, CoBEV estimates each pixel's depth and height distribution and lifts the camera features into 3D space for lateral fusion using the newly proposed two-stage complementary feature selection (CFS) module. A BEV feature distillation framework is also seamlessly integrated to further enhance the detection accuracy from the prior knowledge of the fusion-modal CoBEV teacher. We conduct extensive experiments on the public 3D detection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well as the private Supremind-Road dataset, demonstrating that CoBEV not only achieves the accuracy of the new state-of-the-art, but also significantly advances the robustness of previous methods in challenging long-distance scenarios and noisy camera disturbance, and enhances generalization by a large margin in heterologous settings with drastic changes in scene and camera parameters. For the first time, the vehicle AP score of a camera model reaches 80% on DAIR-V2X-I in terms of easy mode. The source code will be made publicly available at this https URL.

Translated Abstract:
도로변 카메라를 이용한 3D 객체 감지는 지능형 교통 시스템에서 중요한 작업이야. 이 기술은 시각 중심의 차량의 한계를 넘어서서 인식 범위를 확장하고 도로 안전성을 높여줘. 이전 연구들은 깊이 정보나 높이 정보 중 하나만 사용하는 한계가 있었는데, 우리는 깊이와 높이가 모두 중요하고 서로 보완적이라는 걸 발견했어. 깊이 특성은 정밀한 기하학적 단서를 포함하고, 높이 특성은 다양한 높이 범주를 구별하는 데 주로 집중해, 본질적으로 의미론적 맥락을 제공해.

이런 통찰력은 CoBEV라는 새로운 종단 간 단안 3D 객체 감지 프레임워크의 개발을 이끌었어. CoBEV는 깊이와 높이를 통합해서 강력한 BEV 표현을 만들어. 기본적으로 CoBEV는 각 픽셀의 깊이와 높이 분포를 추정하고, 새롭게 제안된 두 단계 보완 특성 선택(CFS) 모듈을 사용해 카메라 특성을 3D 공간으로 변환해.

또한, BEV 특성 증류 프레임워크도 통합되어, CoBEV 교사의 융합 모달에 대한 사전 지식을 통해 감지 정확도를 더욱 향상시켜. 우리는 DAIR-V2X-I와 Rope3D 같은 공개 3D 감지 벤치마크와 개인 데이터셋인 Supremind-Road에서 광범위한 실험을 진행했어. 그 결과 CoBEV는 새로운 최신 기술의 정확도를 달성할 뿐만 아니라, 도전적인 장거리 상황과 시끄러운 카메라 간섭에서도 이전 방법들의 강건성을 크게 향상시켰고, 장면과 카메라 매개변수의 급격한 변화가 있는 다양한 환경에서도 일반화 성능을 크게 높였어.

처음으로, 카메라 모델의 차량 AP 점수가 DAIR-V2X-I에서 쉬운 모드 기준으로 80%에 도달했어. 소스 코드는 이 URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2404.14965.pdf

Title: Vision Beyond Boundaries: An Initial Design Space of Domain-specific Large Vision Models in Human-robot Interaction

Original Abstract:
The emergence of large vision models (LVMs) is following in the footsteps of the recent prosperity of Large Language Models (LLMs) in following years. However, there's a noticeable gap in structured research applying LVMs to human-robot interaction (HRI), despite extensive evidence supporting the efficacy of vision models in enhancing interactions between humans and robots. Recognizing the vast and anticipated potential, we introduce an initial design space that incorporates domain-specific LVMs, chosen for their superior performance over normal models. We delve into three primary dimensions: HRI contexts, vision-based tasks, and specific domains. The empirical evaluation was implemented among 15 experts across six evaluated metrics, showcasing the primary efficacy in relevant decision-making scenarios. We explore the process of ideation and potential application scenarios, envisioning this design space as a foundational guideline for future HRI system design, emphasizing accurate domain alignment and model selection.

Translated Abstract:
대형 비전 모델(LVM)의 발전이 최근 몇 년간 대형 언어 모델(LLM)의 성공을 따라가고 있어. 하지만 LVM을 인간-로봇 상호작용(HRI)에 적용한 연구가 부족해. 비전 모델이 인간과 로봇 간의 상호작용을 개선하는 데 효과적이라는 많은 증거가 있지만 말이야.

우리는 LVM의 잠재력을 인식하고, 일반 모델보다 뛰어난 성능을 가진 도메인 특화 LVM을 포함하는 초기 디자인 공간을 소개해. 여기서 세 가지 주요 차원을 살펴볼 거야: HRI 맥락, 비전 기반 작업, 그리고 특정 도메인.

15명의 전문가를 대상으로 6가지 평가 기준에 따라 실증 평가를 진행했어. 그 결과, 관련 의사 결정 상황에서 주목할 만한 효과를 보여줬어. 우리는 아이디어 구상 과정과 잠재적인 응용 시나리오를 탐구하면서, 이 디자인 공간을 미래 HRI 시스템 디자인을 위한 기초 가이드라인으로 삼고, 정확한 도메인 정렬과 모델 선택의 중요성을 강조하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2406.10115.pdf

Title: Shelf-Supervised Cross-Modal Pre-Training for 3D Object Detection

Original Abstract:
State-of-the-art 3D object detectors are often trained on massive labeled datasets. However, annotating 3D bounding boxes remains prohibitively expensive and time-consuming, particularly for LiDAR. Instead, recent works demonstrate that self-supervised pre-training with unlabeled data can improve detection accuracy with limited labels. Contemporary methods adapt best-practices for self-supervised learning from the image domain to point clouds (such as contrastive learning). However, publicly available 3D datasets are considerably smaller and less diverse than those used for image-based self-supervised learning, limiting their effectiveness. We do note, however, that such data is naturally collected in a multimodal fashion, often paired with images. Rather than pre-training with only self-supervised objectives, we argue that it is better to bootstrap point cloud representations using image-based foundation models trained on internet-scale image data. Specifically, we propose a shelf-supervised approach (e.g. supervised with off-the-shelf image foundation models) for generating zero-shot 3D bounding boxes from paired RGB and LiDAR data. Pre-training 3D detectors with such pseudo-labels yields significantly better semi-supervised detection accuracy than prior self-supervised pretext tasks. Importantly, we show that image-based shelf-supervision is helpful for training LiDAR-only and multi-modal (RGB + LiDAR) detectors. We demonstrate the effectiveness of our approach on nuScenes and WOD, significantly improving over prior work in limited data settings. Our code is available at this https URL

Translated Abstract:
최신 3D 객체 탐지기는 대규모 레이블이 붙은 데이터셋에서 훈련되는 경우가 많아. 하지만 3D 바운딩 박스를 레이블링하는 건 비용도 많이 들고 시간이 너무 걸려, 특히 LiDAR의 경우 특히 그래. 최근 연구들은 레이블이 없는 데이터로 자기 지도 학습을 하면 제한된 레이블로도 탐지 정확도를 높일 수 있다고 보여줘.

요즘 방법들은 이미지 도메인에서 자기 지도 학습의 최선의 방법들을 포인트 클라우드에 맞게 조정하고 있어(예: 대조 학습). 하지만 공개된 3D 데이터셋은 이미지 기반 자기 지도 학습에 사용되는 것보다 훨씬 작고 다양성이 적어서 효과가 제한적이야. 그런데 이런 데이터는 보통 이미지와 함께 멀티모달 방식으로 자연스럽게 수집된다는 점은 주목할 만해.

그래서 우리는 자기 지도 목표만으로 사전 훈련하는 대신, 인터넷 규모의 이미지 데이터로 훈련된 이미지 기반의 기초 모델을 사용해 포인트 클라우드 표현을 부트스트랩하는 게 더 좋다고 주장해. 구체적으로, 우리는 RGB와 LiDAR 데이터가 짝지어진 상태에서 제로샷 3D 바운딩 박스를 생성하기 위한 선반 감독(shelf-supervised) 접근 방식을 제안해. 이렇게 가짜 레이블로 3D 탐지기를 사전 훈련하면 이전의 자기 지도 사전 훈련 작업보다 반지도 학습에서 탐지 정확도가 훨씬 좋아져.

중요한 건, 이미지 기반의 선반 감독이 LiDAR 전용 및 멀티모달(RGB + LiDAR) 탐지기를 훈련하는 데 유용하다는 거야. 우리는 nuScenes와 WOD에서 우리의 접근 방식의 효과를 입증했으며, 제한된 데이터 환경에서 이전 작업보다 크게 개선된 결과를 보여줘. 우리의 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2406.18914.pdf

Title: Verification and Synthesis of Compatible Control Lyapunov and Control Barrier Functions

Original Abstract:
Safety and stability are essential properties of control systems. Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs) are powerful tools to ensure safety and stability respectively. However, previous approaches typically verify and synthesize the CBFs and CLFs separately, satisfying their respective constraints, without proving that the CBFs and CLFs are compatible with each other, namely at every state, there exists control actions within the input limits that satisfy both the CBF and CLF constraints simultaneously. Ignoring the compatibility criteria might cause the CLF-CBF-QP controller to fail at runtime. There exists some recent works that synthesized compatible CLF and CBF, but relying on nominal polynomial or rational controllers, which is just a sufficient but not necessary condition for compatibility. In this work, we investigate verification and synthesis of compatible CBF and CLF independent from any nominal controllers. We derive exact necessary and sufficient conditions for compatibility, and further formulate Sum-Of-Squares programs for the compatibility verification.
Based on our verification framework, we also design a nominal-controller-free synthesis method, which can effectively expands the compatible region, in which the system is guaranteed to be both safe and stable. We evaluate our method on a non-linear toy problem, and also a 3D quadrotor to demonstrate its scalability. The code is open-sourced at \url{this https URL}.

Translated Abstract:
안전성과 안정성은 제어 시스템에서 정말 중요한 특성이야. 제어 장벽 함수(CBF)와 제어 리아푸노프 함수(CLF)는 각각 안전성과 안정성을 보장하는 데 강력한 도구야. 하지만 이전 연구들은 보통 CBF와 CLF를 따로 검증하고 합성하는 데 그쳤고, 이 두 함수가 서로 호환된다는 걸 증명하지 않았어. 즉, 모든 상태에서 입력 한계 내에서 CBF와 CLF의 제약을 동시에 만족하는 제어 동작이 존재해야 하는데, 이걸 무시하면 CLF-CBF-QP 제어기가 실제로 동작할 때 실패할 수 있어.

최근 몇몇 연구에서는 호환되는 CLF와 CBF를 합성했지만, 일반적인 다항식이나 유리 제어기에 의존했어. 이런 건 충분한 조건일 뿐, 필수 조건은 아니야. 이번 연구에서는 어떤 일반 제어기와도 독립적으로 호환되는 CBF와 CLF의 검증과 합성을 조사했어. 우리는 호환성을 위한 정확한 필요 충분 조건을 도출하고, 호환성 검증을 위한 합의 제곱(Sum-Of-Squares) 프로그램을 추가로 세웠어.

우리의 검증 프레임워크를 기반으로, 일반 제어기 없이도 합성할 수 있는 방법을 설계했어. 이 방법은 시스템이 안전하고 안정적일 수 있는 호환 영역을 효과적으로 확장해. 우리는 이 방법을 비선형 장난감 문제와 3D 쿼드로터에 적용해서 확장성을 보여줬어. 코드도 오픈소스로 공개했어.

================================================================================

URL:
https://arxiv.org/pdf/2407.10159.pdf

Title: RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation

Original Abstract:
3D point clouds play a pivotal role in outdoor scene perception, especially in the context of autonomous driving. Recent advancements in 3D LiDAR segmentation often focus intensely on the spatial positioning and distribution of points for accurate segmentation. However, these methods, while robust in variable conditions, encounter challenges due to sole reliance on coordinates and point intensity, leading to poor isometric invariance and suboptimal segmentation. To tackle this challenge, our work introduces Range-Aware Pointwise Distance Distribution (RAPiD) features and the associated RAPiD-Seg architecture. Our RAPiD features exhibit rigid transformation invariance and effectively adapt to variations in point density, with a design focus on capturing the localized geometry of neighboring structures. They utilize inherent LiDAR isotropic radiation and semantic categorization for enhanced local representation and computational efficiency, while incorporating a 4D distance metric that integrates geometric and surface material reflectivity for improved semantic segmentation. To effectively embed high-dimensional RAPiD features, we propose a double-nested autoencoder structure with a novel class-aware embedding objective to encode high-dimensional features into manageable voxel-wise embeddings. Additionally, we propose RAPiD-Seg which incorporates a channel-wise attention fusion and two effective RAPiD-Seg variants, further optimizing the embedding for enhanced performance and generalization. Our method outperforms contemporary LiDAR segmentation work in terms of mIoU on SemanticKITTI (76.1) and nuScenes (83.6) datasets.

Translated Abstract:
3D 포인트 클라우드는 야외 장면 인식에서 중요한 역할을 해, 특히 자율 주행과 관련해서 말이야. 최근 3D LiDAR 분할 기술은 포인트의 공간 위치와 분포에 집중해서 정확한 분할을 목표로 하고 있어. 근데 이런 방법들은 다양한 조건에서 강력하지만, 좌표와 포인트 강도에만 의존하다 보니 동형 불변성이 떨어지고 최적의 분할이 안 되는 문제가 있어.

이 문제를 해결하기 위해 우리는 Range-Aware Pointwise Distance Distribution (RAPiD) 기능과 RAPiD-Seg 아키텍처를 소개해. 우리의 RAPiD 기능은 강체 변환 불변성을 보여주고, 포인트 밀도의 변화에 잘 적응해. 이건 이웃 구조의 국소 기하학을 잘 포착할 수 있도록 설계되었어. 기본적인 LiDAR의 등방성 방사와 의미적 분류를 활용해 지역적 표현과 계산 효율성을 높이고, 기하학적 성질과 표면 재질 반사율을 통합한 4D 거리 지표를 사용해 의미적 분할을 개선해.

고차원 RAPiD 기능을 효과적으로 통합하기 위해, 우리는 새로운 클래스 인식 임베딩 목표를 가진 이중 중첩 오토인코더 구조를 제안했어. 이걸 통해 고차원 기능을 관리 가능한 복셀 단위 임베딩으로 변환할 수 있어. 또, 우리는 채널별 주의 융합을 포함한 RAPiD-Seg와 두 가지 효과적인 RAPiD-Seg 변형을 제안해, 성능과 일반화를 더욱 최적화했어. 우리의 방법은 SemanticKITTI (76.1)와 nuScenes (83.6) 데이터셋에서 최신 LiDAR 분할 작업보다 더 좋은 mIoU 성능을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.01427.pdf

Title: Enhancing Sample Efficiency and Exploration in Reinforcement Learning through the Integration of Diffusion Models and Proximal Policy Optimization

Original Abstract:
Recent advancements in reinforcement learning (RL) have been fueled by large-scale data and deep neural networks, particularly for high-dimensional and complex tasks. Online RL methods like Proximal Policy Optimization (PPO) are effective in dynamic scenarios but require substantial real-time data, posing challenges in resource-constrained or slow simulation environments. Offline RL addresses this by pre-learning policies from large datasets, though its success depends on the quality and diversity of the data. This work proposes a framework that enhances PPO algorithms by incorporating a diffusion model to generate high-quality virtual trajectories for offline datasets. This approach improves exploration and sample efficiency, leading to significant gains in cumulative rewards, convergence speed, and strategy stability in complex tasks. Our contributions are threefold: we explore the potential of diffusion models in RL, particularly for offline datasets, extend the application of online RL to offline environments, and experimentally validate the performance improvements of PPO with diffusion models. These findings provide new insights and methods for applying RL to high-dimensional, complex tasks. Finally, we open-source our code at this https URL

Translated Abstract:
최근 강화 학습(RL) 분야에서 대규모 데이터와 심층 신경망 덕분에 많은 발전이 있었어. 특히 고차원이고 복잡한 작업에서 그렇지. Proximal Policy Optimization (PPO) 같은 온라인 RL 방법은 동적인 상황에서 효과적이지만, 많은 실시간 데이터가 필요해. 그래서 자원이 제한되거나 느린 시뮬레이션 환경에서는 어려움이 있어.

오프라인 RL은 이러한 문제를 해결하는데, 대규모 데이터셋에서 정책을 미리 학습해서 사용해. 하지만 이 방법의 성공은 데이터의 질과 다양성에 달려 있어. 이 연구에서는 PPO 알고리즘을 향상시키기 위해 확산 모델을 활용해 고품질의 가상 경로를 생성하는 프레임워크를 제안해. 이 접근 방식은 탐색과 샘플 효율성을 높여주고, 복잡한 작업에서 누적 보상, 수렴 속도, 전략의 안정성을 크게 개선해.

우리의 기여는 세 가지야: 첫째, 오프라인 데이터셋을 위한 RL에서 확산 모델의 가능성을 탐구하고, 둘째, 온라인 RL의 적용을 오프라인 환경으로 확장하고, 셋째, 확산 모델을 이용한 PPO의 성능 향상을 실험적으로 검증했어. 이 연구 결과는 고차원 복잡한 작업에 RL을 적용하는 데 새로운 통찰력과 방법을 제공해. 마지막으로, 우리의 코드는 이 URL에서 오픈소스로 제공할 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08825.pdf

Title: Flight Testing of Latch Valve with Lightweight LV-Servo Direct Drive Mechanism

Original Abstract:
In the field of rocket technology, the latch valve assumes a pivotal role in regulating the flow of fuel gases and liquids to ensure the requisite energy supply. This project endeavors to innovate by replacing the conventional step motor mechanism with a servo motor for latch valve control. The selected servo motor, boasting a more compact form factor and reduced mass, aligns seamlessly with the project's overarching objectives. While servo motors offer myriad advantages, it is imperative to acknowledge and address the constraints of their maximum output torque to guarantee the latch valve's reliable operation. Furthermore, as a rocket ascends, it encounters significant fluctuations in internal temperature and pressure. Consequently, rigorous environmental testing becomes paramount to validate the servo motor's performance under these dynamic conditions, thus ensuring the latch valve's unwavering functionality. The primary focus of this project is the design and testing of the mechanism's performance in simulated rocket environments, achieved through the implementation of the servo motor for latch valve control. The results reveal that the servo motor demonstrated its effectiveness and reliability in controlling the latch valve under the rigorous environmental conditions of rocket flight.

Translated Abstract:
로켓 기술 분야에서 래치 밸브는 연료 가스와 액체의 흐름을 조절하는 중요한 역할을 해. 이 프로젝트는 기존의 스텝 모터 대신 서보 모터를 사용해서 래치 밸브를 제어하는 새로운 방법을 시도해. 선택된 서보 모터는 더 작고 가벼워서 프로젝트 목표에 잘 맞아.

서보 모터는 여러 가지 장점이 있지만, 최대 출력 토크의 한계를 잘 이해하고 해결하는 게 중요해. 그래야 래치 밸브가 안정적으로 작동할 수 있어. 그리고 로켓이 상승할 때 내부 온도와 압력이 크게 변하니까 서보 모터의 성능을 이런 역동적인 조건에서 테스트하는 게 꼭 필요해. 이렇게 해서 래치 밸브의 기능을 보장할 수 있어.

이 프로젝트의 주요 목표는 서보 모터를 사용해 래치 밸브를 제어하는 메커니즘의 성능을 로켓 환경에서 설계하고 테스트하는 거야. 결과적으로, 서보 모터는 로켓 비행의 까다로운 환경 조건에서도 래치 밸브를 효과적으로 제어할 수 있다는 것을 보여줬어.

================================================================================

