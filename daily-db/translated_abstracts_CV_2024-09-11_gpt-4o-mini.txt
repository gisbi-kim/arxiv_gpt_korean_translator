URL:
https://arxiv.org/pdf/2409.05868.pdf

Title: SpecGaussian with Latent Features: A High-quality Modeling of the View-dependent Appearance for 3D Gaussian Splatting

Original Abstract:
Recently, the 3D Gaussian Splatting (3D-GS) method has achieved great success in novel view synthesis, providing real-time rendering while ensuring high-quality rendering results. However, this method faces challenges in modeling specular reflections and handling anisotropic appearance components, especially in dealing with view-dependent color under complex lighting conditions. Additionally, 3D-GS uses spherical harmonic to learn the color representation, which has limited ability to represent complex scenes. To overcome these challenges, we introduce Lantent-SpecGS, an approach that utilizes a universal latent neural descriptor within each 3D Gaussian. This enables a more effective representation of 3D feature fields, including appearance and geometry. Moreover, two parallel CNNs are designed to decoder the splatting feature maps into diffuse color and specular color separately. A mask that depends on the viewpoint is learned to merge these two colors, resulting in the final rendered image. Experimental results demonstrate that our method obtains competitive performance in novel view synthesis and extends the ability of 3D-GS to handle intricate scenarios with specular reflections.

Translated Abstract:
최근에 3D 가우시안 스플래팅(3D-GS) 방법이 새로운 시점 합성에서 큰 성공을 거두었어. 이 방법은 실시간으로 렌더링하면서도 고품질의 결과를 보장해. 하지만 이 방법은 스페큘러 반사 모델링과 이방성 외관 요소 처리에서 어려움을 겪고 있어. 특히 복잡한 조명 조건에서 시점에 따라 색상이 달라지는 걸 다루는 데 문제가 있어. 게다가 3D-GS는 구형 조화 함수를 사용해서 색상 표현을 학습하는데, 이건 복잡한 장면을 표현하는 데 한계가 있어. 

그래서 우리는 Lantent-SpecGS라는 새로운 접근 방식을 소개해. 이 방법은 각 3D 가우시안 내에서 보편적인 잠재 신경 기술자를 활용해. 이렇게 하면 외관과 기하학을 포함한 3D 특징 필드를 더 효과적으로 표현할 수 있어. 또한, 두 개의 병렬 CNN을 설계해서 스플래팅 특징 맵을 각각 확산 색상과 스페큘러 색상으로 디코딩해. 시점에 따라 달라지는 마스크를 학습해서 이 두 가지 색상을 합쳐서 최종 렌더링 이미지를 만들어. 

실험 결과, 우리 방법이 새로운 시점 합성에서 경쟁력 있는 성능을 보여주고, 스페큘러 반사를 다루는 데 있어 3D-GS의 능력을 확장한다는 걸 증명했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.05875.pdf

Title: Transformer-Enhanced Iterative Feedback Mechanism for Polyp Segmentation

Original Abstract:
Colorectal cancer (CRC) is the third most common cause of cancer diagnosed in the United States and the second leading cause of cancer-related death among both genders. Notably, CRC is the leading cause of cancer in younger men less than 50 years old. Colonoscopy is considered the gold standard for the early diagnosis of CRC. Skills vary significantly among endoscopists, and a high miss rate is reported. Automated polyp segmentation can reduce the missed rates, and timely treatment is possible in the early stage. To address this challenge, we introduce \textit{\textbf{\ac{FANetv2}}}, an advanced encoder-decoder network designed to accurately segment polyps from colonoscopy images. Leveraging an initial input mask generated by Otsu thresholding, FANetv2 iteratively refines its binary segmentation masks through a novel feedback attention mechanism informed by the mask predictions of previous epochs. Additionally, it employs a text-guided approach that integrates essential information about the number (one or many) and size (small, medium, large) of polyps to further enhance its feature representation capabilities. This dual-task approach facilitates accurate polyp segmentation and aids in the auxiliary classification of polyp attributes, significantly boosting the model's performance. Our comprehensive evaluations on the publicly available BKAI-IGH and CVC-ClinicDB datasets demonstrate the superior performance of FANetv2, evidenced by high dice similarity coefficients (DSC) of 0.9186 and 0.9481, along with low Hausdorff distances of 2.83 and 3.19, respectively. The source code for FANetv2 is available at this https URL.

Translated Abstract:
대장암(CRC)은 미국에서 진단되는 암 중 세 번째로 흔하고, 남녀 모두에게서 암으로 인한 사망 원인 중 두 번째입니다. 특히, CRC는 50세 미만의 젊은 남성에게 가장 흔한 암입니다. 대장내시경 검사는 CRC를 조기에 진단하는 데 가장 좋은 방법으로 알려져 있습니다. 하지만 내시경 의사마다 실력이 다르고, 놓치는 경우가 많습니다. 자동 폴립 분할 기술이 놓치는 비율을 줄이고, 조기 치료를 가능하게 할 수 있습니다.

이 문제를 해결하기 위해, 우리는 \textit{\textbf{\ac{FANetv2}}}라는 고급 인코더-디코더 네트워크를 소개합니다. 이 네트워크는 대장내시경 이미지에서 폴립을 정확하게 분할하기 위해 설계되었습니다. 먼저, Otsu 임계처리를 통해 생성된 초기 입력 마스크를 활용하여 FANetv2는 이전 세대의 마스크 예측 정보를 바탕으로 새로운 피드백 주의 메커니즘을 통해 반복적으로 이진 분할 마스크를 개선합니다.

또한, FANetv2는 폴립의 개수(하나 또는 여러 개)와 크기(작음, 중간, 큼)에 대한 중요한 정보를 통합하는 텍스트 안내 방식을 사용하여 기능 표현 능력을 더욱 향상시킵니다. 이런 이중 작업 방식은 폴립 분할을 정확하게 하고, 폴립의 속성을 보조적으로 분류하는 데 도움을 줘서 모델의 성능을 크게 높입니다.

우리가 공개된 BKAI-IGH와 CVC-ClinicDB 데이터셋에서 수행한 종합적인 평가 결과, FANetv2는 높은 Dice 유사도 계수(DSC)인 0.9186과 0.9481, 낮은 하우스도르프 거리인 2.83과 3.19로 뛰어난 성능을 보여주었습니다. FANetv2의 소스 코드는 이 URL에서 확인할 수 있습니다.

================================================================================

URL:
https://arxiv.org/pdf/2409.05985.pdf

Title: Advance and Refinement: The Evolution of UAV Detection and Classification Technologies

Original Abstract:
This review provides a detailed analysis of the advancements in unmanned aerial vehicle (UAV) detection and classification systems from 2020 to today. It covers various detection methodologies such as radar, radio frequency, optical, and acoustic sensors, and emphasizes their integration via sophisticated sensor fusion techniques. The fundamental technologies driving UAV detection and classification are thoroughly examined, with a focus on their accuracy and range. Additionally, the paper discusses the latest innovations in artificial intelligence and machine learning, illustrating their impact on improving the accuracy and efficiency of these systems. The review concludes by predicting further technological developments in UAV detection, which are expected to enhance both performance and reliability.

Translated Abstract:
이 리뷰는 2020년부터 오늘까지의 무인 항공기(UAV) 탐지 및 분류 시스템의 발전을 자세히 분석해. 다양한 탐지 방법론인 레이더, 전파, 광학, 음향 센서를 다루고, 이들을 정교한 센서 융합 기술로 통합하는 것에 중점을 두고 있어.

UAV 탐지와 분류를 이끄는 기본 기술들을 철저히 살펴보고, 이들의 정확성과 범위에 대해 집중적으로 설명해. 또, 인공지능과 머신러닝의 최신 혁신에 대해서도 이야기하며, 이들이 시스템의 정확성과 효율성을 개선하는 데 미치는 영향을 보여줘.

리뷰는 UAV 탐지의 추가 기술 발전을 예측하면서 마무리해. 이러한 발전은 성능과 신뢰성을 모두 향상시킬 것으로 기대돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.06002.pdf

Title: Enhanced Generative Data Augmentation for Semantic Segmentation via Stronger Guidance

Original Abstract:
Data augmentation is a widely used technique for creating training data for tasks that require labeled data, such as semantic segmentation. This method benefits pixel-wise annotation tasks requiring much effort and intensive labor. Traditional data augmentation methods involve simple transformations like rotations and flips to create new images from existing ones. However, these new images may lack diversity along the main semantic axes in the data and not change high-level semantic properties. To address this issue, generative models have emerged as an effective solution for augmenting data by generating synthetic images. Controllable generative models offer a way to augment data for semantic segmentation tasks using a prompt and visual reference from the original image. However, using these models directly presents challenges, such as creating an effective prompt and visual reference to generate a synthetic image that accurately reflects the content and structure of the original. In this work, we introduce an effective data augmentation method for semantic segmentation using the Controllable Diffusion Model. Our proposed method includes efficient prompt generation using Class-Prompt Appending and Visual Prior Combination to enhance attention to labeled classes in real images. These techniques allow us to generate images that accurately depict segmented classes in the real image. In addition, we employ the class balancing algorithm to ensure efficiency when merging the synthetic and original images to generate balanced data for the training dataset. We evaluated our method on the PASCAL VOC datasets and found it highly effective for synthesizing images in semantic segmentation.

Translated Abstract:
데이터 증강(Data augmentation)은 레이블이 있는 데이터가 필요한 작업, 예를 들어 의미 분할(semantic segmentation) 작업을 위해 훈련 데이터를 만드는 데 널리 사용되는 기술이야. 이 방법은 픽셀 단위로 주석을 다는 작업에서 많은 노력과 노동력이 필요하니까 도움이 돼. 

전통적인 데이터 증강 방법은 회전이나 뒤집기 같은 간단한 변환을 통해 기존 이미지에서 새로운 이미지를 만드는 방식이야. 하지만 이렇게 만든 새로운 이미지는 데이터의 주요 의미 축에서 다양성이 부족할 수 있고, 고수준의 의미 속성도 변하지 않아. 이 문제를 해결하기 위해 생성 모델(generative models)이 등장했어. 생성 모델은 합成 이미지를 생성함으로써 데이터를 증강하는 효과적인 방법이야. 

조절 가능한 생성 모델(controllable generative models)은 원본 이미지에서 프롬프트(prompt)와 시각적 참조(visual reference)를 사용해 의미 분할 작업을 위한 데이터를 증강하는 방법을 제공해. 하지만 이 모델을 직접 사용하는 건 효과적인 프롬프트와 시각적 참조를 만드는 것이 어려워서, 원본의 내용과 구조를 정확히 반영하는 합성 이미지를 생성하는 데 도전이 있어. 

이번 연구에서는 조절 가능한 확산 모델(Controllable Diffusion Model)을 사용한 의미 분할을 위한 효과적인 데이터 증강 방법을 소개해. 우리가 제안하는 방법은 클래스 프롬프트 추가(Class-Prompt Appending)와 시각적 우선 조합(Visual Prior Combination)을 사용해 실제 이미지에서 레이블이 붙은 클래스에 대한 주의를 높이는 효율적인 프롬프트 생성을 포함해. 이러한 기술 덕분에 실제 이미지에서 분할된 클래스를 정확히 나타내는 이미지를 생성할 수 있어. 

또한, 우리는 합성 이미지와 원본 이미지를 결합할 때 효율성을 보장하기 위해 클래스 균형 알고리즘(class balancing algorithm)을 사용해. 이 방법을 PASCAL VOC 데이터셋에서 평가해봤고, 의미 분할에서 이미지를 합성하는 데 매우 효과적이라는 결과를 얻었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06037.pdf

Title: Online 3D reconstruction and dense tracking in endoscopic videos

Original Abstract:
3D scene reconstruction from stereo endoscopic video data is crucial for advancing surgical interventions. In this work, we present an online framework for online, dense 3D scene reconstruction and tracking, aimed at enhancing surgical scene understanding and assisting interventions. Our method dynamically extends a canonical scene representation using Gaussian splatting, while modeling tissue deformations through a sparse set of control points. We introduce an efficient online fitting algorithm that optimizes the scene parameters, enabling consistent tracking and accurate reconstruction. Through experiments on the StereoMIS dataset, we demonstrate the effectiveness of our approach, outperforming state-of-the-art tracking methods and achieving comparable performance to offline reconstruction techniques. Our work enables various downstream applications thus contributing to advancing the capabilities of surgical assistance systems.

Translated Abstract:
스테레오 내시경 비디오 데이터를 이용한 3D 장면 재구성은 수술 절차를 개선하는 데 아주 중요해. 이 연구에서는 수술 장면 이해를 높이고 개입을 도와주기 위해 온라인으로 밀집된 3D 장면 재구성과 추적을 할 수 있는 프레임워크를 소개해.

우리 방법은 가우시안 스플래팅을 사용해서 기준 장면 표현을 동적으로 확장하고, 조직 변형은 희소한 제어 점들을 통해 모델링해. 장면 매개변수를 최적화하는 효율적인 온라인 피팅 알고리즘도 도입했어. 이 덕분에 일관된 추적과 정확한 재구성이 가능해.

StereoMIS 데이터셋에서 실험을 통해 우리의 접근 방식이 효과적임을 보여줬고, 최신 추적 방법들보다 더 나은 성능을 보였어. 오프라인 재구성 기법과 비슷한 성능도 달성했어. 우리의 연구는 다양한 후속 응용 프로그램을 가능하게 해서 수술 보조 시스템의 능력을 발전시키는 데 기여하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06065.pdf

Title: DiffusionPen: Towards Controlling the Style of Handwritten Text Generation

Original Abstract:
Handwritten Text Generation (HTG) conditioned on text and style is a challenging task due to the variability of inter-user characteristics and the unlimited combinations of characters that form new words unseen during training. Diffusion Models have recently shown promising results in HTG but still remain under-explored. We present DiffusionPen (DiffPen), a 5-shot style handwritten text generation approach based on Latent Diffusion Models. By utilizing a hybrid style extractor that combines metric learning and classification, our approach manages to capture both textual and stylistic characteristics of seen and unseen words and styles, generating realistic handwritten samples. Moreover, we explore several variation strategies of the data with multi-style mixtures and noisy embeddings, enhancing the robustness and diversity of the generated data. Extensive experiments using IAM offline handwriting database show that our method outperforms existing methods qualitatively and quantitatively, and its additional generated data can improve the performance of Handwriting Text Recognition (HTR) systems. The code is available at: this https URL.

Translated Abstract:
핸드라이팅 텍스트 생성(HTG)은 텍스트와 스타일에 따라 달라지기 때문에 어려운 작업이야. 사용자마다 특징이 다르고, 훈련 중에 보지 못한 새로운 단어를 만드는 조합이 무한대이기 때문이지. 최근에는 확산 모델이 HTG에서 좋은 결과를 보여줬지만, 아직 많이 연구되지 않았어.

우리는 DiffusionPen(DiffPen)이라는 5-shot 스타일 핸드라이팅 텍스트 생성 방법을 제안해. 이 방법은 잠재 확산 모델을 기반으로 해. 메트릭 학습과 분류를 결합한 하이브리드 스타일 추출기를 사용해서, 본 적 있는 단어와 스타일, 본 적 없는 단어와 스타일 모두의 텍스트적이고 스타일적인 특징을 잘 포착해. 그래서 실제 같은 핸드라이팅 샘플을 생성할 수 있어.

또한, 다양한 스타일 혼합과 노이즈가 섞인 임베딩을 사용해 데이터 변형 전략을 여러 가지로 탐색해. 이렇게 해서 생성된 데이터의 강인성과 다양성을 높였어. IAM 오프라인 핸드라이팅 데이터베이스를 이용한 광범위한 실험 결과, 우리의 방법이 기존 방법들보다 질적으로나 양적으로 더 뛰어난 성능을 보였고, 추가로 생성된 데이터는 핸드라이팅 텍스트 인식(HTR) 시스템의 성능도 개선할 수 있어. 코드도 이 링크에서 확인할 수 있어: this https URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.06074.pdf

Title: SVS-GAN: Leveraging GANs for Semantic Video Synthesis

Original Abstract:
In recent years, there has been a growing interest in Semantic Image Synthesis (SIS) through the use of Generative Adversarial Networks (GANs) and diffusion models. This field has seen innovations such as the implementation of specialized loss functions tailored for this task, diverging from the more general approaches in Image-to-Image (I2I) translation. While the concept of Semantic Video Synthesis (SVS)$\unicode{x2013}$the generation of temporally coherent, realistic sequences of images from semantic maps$\unicode{x2013}$is newly formalized in this paper, some existing methods have already explored aspects of this field. Most of these approaches rely on generic loss functions designed for video-to-video translation or require additional data to achieve temporal coherence. In this paper, we introduce the SVS-GAN, a framework specifically designed for SVS, featuring a custom architecture and loss functions. Our approach includes a triple-pyramid generator that utilizes SPADE blocks. Additionally, we employ a U-Net-based network for the image discriminator, which performs semantic segmentation for the OASIS loss. Through this combination of tailored architecture and objective engineering, our framework aims to bridge the existing gap between SIS and SVS, outperforming current state-of-the-art models on datasets like Cityscapes and KITTI-360.

Translated Abstract:
최근 몇 년간, 생성적 적대 신경망(GANs)과 확산 모델을 이용한 의미적 이미지 합성(SIS)에 대한 관심이 커지고 있어. 이 분야에서는 이 작업에 맞춘 특수 손실 함수들이 도입되면서, 이미지-이미지(I2I) 번역에서 일반적으로 사용되는 접근 방식과는 다르게 발전하고 있어.

이 논문에서는 의미적 비디오 합성(SVS)이라는 개념을 새롭게 정의했어. SVS는 의미 맵에서 시간적으로 일관되고 현실적인 이미지 시퀀스를 생성하는 걸 말해. 이미 몇몇 기존 방법들이 이 분야의 일부 측면을 탐구했지만, 대부분의 접근 방식은 비디오-비디오 번역을 위해 설계된 일반 손실 함수에 의존하거나 시간적 일관성을 얻기 위해 추가 데이터를 필요로 해.

우리는 SVS를 위해 특별히 설계된 SVS-GAN이라는 프레임워크를 소개해. 이 프레임워크는 맞춤형 아키텍처와 손실 함수를 특징으로 하고 있어. 우리의 접근 방식은 SPADE 블록을 사용하는 삼중 피라미드 생성기를 포함해. 또한, OASIS 손실을 위해 의미 분할을 수행하는 이미지 판별기를 위한 U-Net 기반 네트워크를 사용하고 있어.

이 맞춤형 아키텍처와 목표 공학의 조합을 통해, 우리의 프레임워크는 SIS와 SVS 사이의 기존 격차를 메우고, Cityscapes와 KITTI-360 같은 데이터셋에서 현재 최첨단 모델들을 능가하는 것을 목표로 하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06104.pdf

Title: LSE-NeRF: Learning Sensor Modeling Errors for Deblured Neural Radiance Fields with RGB-Event Stereo

Original Abstract:
We present a method for reconstructing a clear Neural Radiance Field (NeRF) even with fast camera motions. To address blur artifacts, we leverage both (blurry) RGB images and event camera data captured in a binocular configuration. Importantly, when reconstructing our clear NeRF, we consider the camera modeling imperfections that arise from the simple pinhole camera model as learned embeddings for each camera measurement, and further learn a mapper that connects event camera measurements with RGB data. As no previous dataset exists for our binocular setting, we introduce an event camera dataset with captures from a 3D-printed stereo configuration between RGB and event cameras. Empirically, we evaluate our introduced dataset and EVIMOv2 and show that our method leads to improved reconstructions. Our code and dataset are available at this https URL.

Translated Abstract:
우리는 빠른 카메라 움직임에서도 선명한 신경 복사 필드(NeRF)를 재구성하는 방법을 제안해. 흐릿한 아티팩트를 해결하기 위해, 우리는 쌍안경 설정에서 촬영된 (흐릿한) RGB 이미지와 이벤트 카메라 데이터를 활용해.

중요한 건, 선명한 NeRF를 재구성할 때 단순한 핀홀 카메라 모델 때문에 생기는 카메라 모델링의 불완전성을 고려해. 각 카메라 측정을 위한 학습된 임베딩을 만들고, 이벤트 카메라 측정과 RGB 데이터를 연결하는 매퍼도 학습해.

우리의 쌍안경 설정에 대한 이전 데이터셋이 없으니, RGB와 이벤트 카메라 간의 3D 프린팅 스테레오 구성에서 촬영한 이벤트 카메라 데이터셋을 소개할 거야. 우리는 소개한 데이터셋과 EVIMOv2를 평가해봤고, 우리의 방법이 재구성을 개선하는 데 도움이 된다는 걸 보여줬어.

우리의 코드와 데이터셋은 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06105.pdf

Title: SGC-VQGAN: Towards Complex Scene Representation via Semantic Guided Clustering Codebook

Original Abstract:
Vector quantization (VQ) is a method for deterministically learning features through discrete codebook representations. Recent works have utilized visual tokenizers to discretize visual regions for self-supervised representation learning. However, a notable limitation of these tokenizers is lack of semantics, as they are derived solely from the pretext task of reconstructing raw image pixels in an auto-encoder paradigm. Additionally, issues like imbalanced codebook distribution and codebook collapse can adversely impact performance due to inefficient codebook utilization. To address these challenges, We introduce SGC-VQGAN through Semantic Online Clustering method to enhance token semantics through Consistent Semantic Learning. Utilizing inference results from segmentation model , our approach constructs a temporospatially consistent semantic codebook, addressing issues of codebook collapse and imbalanced token semantics. Our proposed Pyramid Feature Learning pipeline integrates multi-level features to capture both image details and semantics simultaneously. As a result, SGC-VQGAN achieves SOTA performance in both reconstruction quality and various downstream tasks. Its simplicity, requiring no additional parameter learning, enables its direct application in downstream tasks, presenting significant potential.

Translated Abstract:
벡터 양자화(VQ)는 불연속 코드북 표현을 통해 특징을 결정적으로 학습하는 방법이야. 최근에는 비주얼 토크나이저를 사용해서 시각적 영역을 이산화하여 자기 지도 학습을 하고 있어. 하지만 이 토크나이저의 큰 단점은 의미가 부족하다는 거야. 그 이유는 단순히 원본 이미지 픽셀을 재구성하는 프리텍스트 작업에서 파생되기 때문이야.

또한, 코드북 분포 불균형이나 코드북 붕괴 같은 문제도 성능에 나쁜 영향을 줄 수 있어. 이런 도전 과제를 해결하기 위해 우리는 일관된 의미 학습을 통해 토큰의 의미를 강화하는 SGC-VQGAN을 도입했어. 세분화 모델의 추론 결과를 활용해서, 우리 방법은 시간적, 공간적으로 일관된 의미 코드북을 만들어서 코드북 붕괴와 불균형한 토큰 의미 문제를 해결해.

우리가 제안한 파라미드 특징 학습 파이프라인은 다층 특징을 통합해서 이미지의 세부사항과 의미를 동시에 잡아내. 그 결과, SGC-VQGAN은 재구성 품질과 다양한 다운스트림 작업에서 최첨단 성능을 달성했어. 추가적인 파라미터 학습이 필요 없어서 다운스트림 작업에 직접 적용할 수 있다는 점에서 큰 잠재력을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.06129.pdf

Title: DECOLLAGE: 3D Detailization by Controllable, Localized, and Learned Geometry Enhancement

Original Abstract:
We present a 3D modeling method which enables end-users to refine or detailize 3D shapes using machine learning, expanding the capabilities of AI-assisted 3D content creation. Given a coarse voxel shape (e.g., one produced with a simple box extrusion tool or via generative modeling), a user can directly "paint" desired target styles representing compelling geometric details, from input exemplar shapes, over different regions of the coarse shape. These regions are then up-sampled into high-resolution geometries which adhere with the painted styles. To achieve such controllable and localized 3D detailization, we build on top of a Pyramid GAN by making it masking-aware. We devise novel structural losses and priors to ensure that our method preserves both desired coarse structures and fine-grained features even if the painted styles are borrowed from diverse sources, e.g., different semantic parts and even different shape categories. Through extensive experiments, we show that our ability to localize details enables novel interactive creative workflows and applications. Our experiments further demonstrate that in comparison to prior techniques built on global detailization, our method generates structure-preserving, high-resolution stylized geometries with more coherent shape details and style transitions.

Translated Abstract:
우리는 사용자들이 머신러닝을 사용해서 3D 형태를 더 정교하게 다듬을 수 있는 3D 모델링 방법을 소개해. 이 방법은 AI가 도와주는 3D 콘텐츠 제작의 가능성을 확장하는 거야.

사용자가 거친 복셀 형태(예를 들어, 간단한 박스 압출 도구로 만든 것이나 생성 모델링을 통해 만든 것)를 가져오면, 원하는 스타일을 직접 "그릴" 수 있어. 이 스타일은 매력적인 기하학적 세부 사항을 나타내는 입력 예시 형태에서 가져오는 거야. 사용자가 그린 스타일을 바탕으로, 그 부분들이 고해상도 기하학으로 업샘플링돼. 

이런 제어 가능하고 지역적인 3D 세부 묘사를 위해 우리는 Pyramid GAN을 기반으로 하면서 마스킹을 인식하도록 개선했어. 그리고 다양한 소스에서 가져온 스타일을 사용하더라도 원하는 거친 구조와 세밀한 특징을 유지할 수 있도록 새로운 구조적 손실과 우선 순위를 개발했어. 

많은 실험을 통해 우리는 세부 묘사를 지역화하는 능력이 새로운 인터랙티브 창작 워크플로우와 응용 프로그램을 가능하게 한다는 걸 보여줬어. 또한, 기존의 전역 세부 묘사 기법과 비교했을 때, 우리의 방법이 구조를 유지하면서 고해상도의 스타일화된 기하학을 생성하고 더 일관된 형태 세부 사항과 스타일 전환을 제공한다는 것도 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06154.pdf

Title: UniLearn: Enhancing Dynamic Facial Expression Recognition through Unified Pre-Training and Fine-Tuning on Images and Videos

Original Abstract:
Dynamic facial expression recognition (DFER) is essential for understanding human emotions and behavior. However, conventional DFER methods, which primarily use dynamic facial data, often underutilize static expression images and their labels, limiting their performance and robustness. To overcome this, we introduce UniLearn, a novel unified learning paradigm that integrates static facial expression recognition (SFER) data to enhance DFER task. UniLearn employs a dual-modal self-supervised pre-training method, leveraging both facial expression images and videos to enhance a ViT model's spatiotemporal representation capability. Then, the pre-trained model is fine-tuned on both static and dynamic expression datasets using a joint fine-tuning strategy. To prevent negative transfer during joint fine-tuning, we introduce an innovative Mixture of Adapter Experts (MoAE) module that enables task-specific knowledge acquisition and effectively integrates information from both static and dynamic expression data. Extensive experiments demonstrate UniLearn's effectiveness in leveraging complementary information from static and dynamic facial data, leading to more accurate and robust DFER. UniLearn consistently achieves state-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, with weighted average recall (WAR) of 53.65\%, 58.44\%, and 76.68\%, respectively. The source code and model weights will be publicly available at \url{this https URL}.

Translated Abstract:
동적 얼굴 표정 인식(DFER)은 인간의 감정과 행동을 이해하는 데 중요해. 그런데 기존 DFER 방법들은 주로 동적인 얼굴 데이터를 사용하고, 정적인 표정 이미지와 레이블을 잘 활용하지 않아서 성능과 안정성이 떨어지는 문제가 있어. 

이 문제를 해결하기 위해 우리는 UniLearn이라는 새로운 통합 학습 방식을 제안해. 이 방법은 정적인 얼굴 표정 인식(SFER) 데이터를 통합해서 DFER 작업을 강화해. UniLearn은 얼굴 표정 이미지와 비디오를 모두 활용하는 이중 모드 자기 지도 사전 학습 방법을 사용해서 ViT 모델의 시공간 표현 능력을 향상시켜. 이후, 사전 학습된 모델은 정적 및 동적 표정 데이터셋을 사용해 공동 미세 조정 전략으로 조정돼.

공동 미세 조정 중에 부정적인 전이를 방지하기 위해 우리는 Mixture of Adapter Experts (MoAE)라는 혁신적인 모듈을 도입했어. 이 모듈은 작업별 지식 습득을 가능하게 하고, 정적 및 동적 표정 데이터에서 정보를 잘 통합할 수 있게 해. 

많은 실험 결과 UniLearn이 정적과 동적 얼굴 데이터에서 보완적인 정보를 잘 활용해서 더 정확하고 안정적인 DFER을 가능하게 한다는 걸 보여줬어. UniLearn은 FERV39K, MAFW, DFEW 벤치마크에서 최첨단 성능을 꾸준히 달성했으며, 가중 평균 재현율(WAR)은 각각 53.65%, 58.44%, 76.68%야. 소스 코드와 모델 가중치는 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06166.pdf

Title: Revisiting Prompt Pretraining of Vision-Language Models

Original Abstract:
Prompt learning is an effective method to customize Vision-Language Models (VLMs) for various downstream tasks, involving tuning very few parameters of input prompt tokens. Recently, prompt pretraining in large-scale dataset (e.g., ImageNet-21K) has played a crucial role in prompt learning for universal visual discrimination. However, we revisit and observe that the limited learnable prompts could face underfitting risks given the extensive images during prompt pretraining, simultaneously leading to poor generalization. To address the above issues, in this paper, we propose a general framework termed Revisiting Prompt Pretraining (RPP), which targets at improving the fitting and generalization ability from two aspects: prompt structure and prompt supervision. For prompt structure, we break the restriction in common practice where query, key, and value vectors are derived from the shared learnable prompt token. Instead, we introduce unshared individual query, key, and value learnable prompts, thereby enhancing the model's fitting capacity through increased parameter diversity. For prompt supervision, we additionally utilize soft labels derived from zero-shot probability predictions provided by a pretrained Contrastive Language Image Pretraining (CLIP) teacher model. These soft labels yield more nuanced and general insights into the inter-class relationships, thereby endowing the pretraining process with better generalization ability. RPP produces a more resilient prompt initialization, enhancing its robust transferability across diverse visual recognition tasks. Experiments across various benchmarks consistently confirm the state-of-the-art (SOTA) performance of our pretrained prompts. Codes and models will be made available soon.

Translated Abstract:
프롬프트 학습은 시각-언어 모델(VLMs)을 다양한 후속 작업에 맞추는 효과적인 방법인데, 입력 프롬프트 토큰의 매개변수를 아주 적게 조정하는 방식이야. 최근에는 대규모 데이터셋(예: ImageNet-21K)에서 프롬프트 사전 학습이 보편적인 시각적 구분을 위한 프롬프트 학습에 중요한 역할을 하고 있어.

하지만 우리는 제한된 학습 가능한 프롬프트가 많은 이미지들을 다루는 프롬프트 사전 학습에서 언더피팅의 위험에 직면할 수 있다는 것을 다시 살펴보았고, 이로 인해 일반화가 잘 안 되는 문제도 발생한다고 관찰했어. 이런 문제를 해결하기 위해, 이 논문에서는 RPP(프롬프트 재검토 사전 학습)라는 일반적인 프레임워크를 제안해. 이 프레임워크는 두 가지 측면에서 적합성과 일반화 능력을 개선하는 데 초점을 맞추고 있어: 프롬프트 구조와 프롬프트 감독.

프롬프트 구조에 대해서는, 일반적인 관행에서 쿼리, 키, 값 벡터가 공유 학습 가능한 프롬프트 토큰에서 파생되는 제한을 깨버렸어. 대신에, 우리는 개별 쿼리, 키, 값 학습 가능한 프롬프트를 도입하여 매개변수 다양성을 증가시킴으로써 모델의 적합 능력을 높였어.

프롬프트 감독 측면에서는, 사전 학습된 대조 언어 이미지 사전 학습(CLIP) 교사 모델이 제공하는 제로샷 확률 예측에서 유도된 소프트 레이블을 추가로 활용했어. 이 소프트 레이블은 클래스 간 관계에 대한 더 미세하고 일반적인 통찰을 제공해서, 사전 학습 과정에 더 나은 일반화 능력을 부여해.

RPP는 더욱 견고한 프롬프트 초기화를 생성하여 다양한 시각 인식 작업에 대한 강력한 전이 가능성을 높여. 여러 벤치마크에서 실험을 진행한 결과, 우리의 사전 학습된 프롬프트가 최첨단(SOTA) 성능을 지속적으로 확인해. 코드와 모델은 곧 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06171.pdf

Title: Loss Distillation via Gradient Matching for Point Cloud Completion with Weighted Chamfer Distance

Original Abstract:
3D point clouds enhanced the robot's ability to perceive the geometrical information of the environments, making it possible for many downstream tasks such as grasp pose detection and scene understanding. The performance of these tasks, though, heavily relies on the quality of data input, as incomplete can lead to poor results and failure cases. Recent training loss functions designed for deep learning-based point cloud completion, such as Chamfer distance (CD) and its variants (\eg HyperCD ), imply a good gradient weighting scheme can significantly boost performance. However, these CD-based loss functions usually require data-related parameter tuning, which can be time-consuming for data-extensive tasks. To address this issue, we aim to find a family of weighted training losses ({\em weighted CD}) that requires no parameter tuning. To this end, we propose a search scheme, {\em Loss Distillation via Gradient Matching}, to find good candidate loss functions by mimicking the learning behavior in backpropagation between HyperCD and weighted CD. Once this is done, we propose a novel bilevel optimization formula to train the backbone network based on the weighted CD loss. We observe that: (1) with proper weighted functions, the weighted CD can always achieve similar performance to HyperCD, and (2) the Landau weighted CD, namely {\em Landau CD}, can outperform HyperCD for point cloud completion and lead to new state-of-the-art results on several benchmark datasets. {\it Our demo code is available at \url{this https URL}.}

Translated Abstract:
3D 포인트 클라우드는 로봇이 환경의 기하학적 정보를 인식하는 능력을 키워줘서, 물체 잡기 자세 감지나 장면 이해 같은 여러 작업을 가능하게 해. 하지만 이런 작업들의 성과는 입력 데이터의 품질에 많이 의존해. 데이터가 불완전하면 결과가 나빠지거나 실패할 수 있어.

최근에 딥러닝 기반 포인트 클라우드 완성을 위한 훈련 손실 함수들이 개발됐는데, 예를 들면 챔퍼 거리(Chamfer distance, CD)와 그 변형들(예: HyperCD)이 있어. 이런 CD 기반 손실 함수들은 좋은 그래디언트 가중치 방식을 사용하면 성능을 크게 개선할 수 있다는 걸 보여줘. 하지만 보통 이런 CD 기반 손실 함수는 데이터 관련 파라미터 조정이 필요해서, 데이터가 많은 작업에서는 시간이 많이 걸릴 수 있어.

그래서 우리는 파라미터 조정이 필요 없는 가중치 훈련 손실 함수의 가족({\em weighted CD})을 찾는 걸 목표로 해. 이를 위해서 우리는 HyperCD와 weighted CD 사이의 역전파 학습 행동을 모방해서 좋은 후보 손실 함수를 찾는 방법인 {\em Loss Distillation via Gradient Matching}을 제안해. 이 작업이 끝나면, 우리는 weighted CD 손실을 기반으로 백본 네트워크를 훈련하기 위한 새로운 이층 최적화 공식을 제안해.

우리가 관찰한 바는: (1) 적절한 가중치 함수를 사용하면, weighted CD가 HyperCD와 비슷한 성능을 항상 달성할 수 있고, (2) Landau weighted CD, 즉 {\em Landau CD}는 포인트 클라우드 완성에서 HyperCD보다 더 나은 성능을 보이며, 여러 벤치마크 데이터셋에서 새로운 최첨단 결과를 이끌어낼 수 있다는 거야. {\it 우리의 데모 코드는 \url{이 URL}에서 확인할 수 있어.}

================================================================================

URL:
https://arxiv.org/pdf/2409.06183.pdf

Title: EDADepth: Enhanced Data Augmentation for Monocular Depth Estimation

Original Abstract:
Due to their text-to-image synthesis feature, diffusion models have recently seen a rise in visual perception tasks, such as depth estimation. The lack of good-quality datasets makes the extraction of a fine-grain semantic context challenging for the diffusion models. The semantic context with fewer details further worsens the process of creating effective text embeddings that will be used as input for diffusion models. In this paper, we propose a novel EDADepth, an enhanced data augmentation method to estimate monocular depth without using additional training data. We use Swin2SR, a super-resolution model, to enhance the quality of input images. We employ the BEiT pre-trained semantic segmentation model for better extraction of text embeddings. We introduce BLIP-2 tokenizer to generate tokens from these text embeddings. The novelty of our approach is the introduction of Swin2SR, the BEiT model, and the BLIP-2 tokenizer in the diffusion-based pipeline for the monocular depth estimation. Our model achieves state-of-the-art results (SOTA) on the {\delta}3 metric on NYUv2 and KITTI datasets. It also achieves results comparable to those of the SOTA models in the RMSE and REL metrics. Finally, we also show improvements in the visualization of the estimated depth compared to the SOTA diffusion-based monocular depth estimation models. Code: this https URL.

Translated Abstract:
최근에 확산 모델들이 텍스트-이미지 합성 기능 덕분에 깊이 추정 같은 시각 인식 작업에서 주목받고 있어. 하지만 고품질 데이터셋이 부족해서 세밀한 의미 맥락을 추출하는 게 어려워. 의미 맥락이 덜 상세하면, 확산 모델에 입력할 효과적인 텍스트 임베딩을 만드는 과정이 더 나빠져.

이 논문에서는 추가 훈련 데이터 없이 단안 깊이를 추정하기 위한 새로운 데이터 증강 방법인 EDADepth를 제안해. 입력 이미지의 품질을 높이기 위해 Swin2SR이라는 초해상도 모델을 사용해. 텍스트 임베딩을 더 잘 추출하기 위해 BEiT라는 사전 훈련된 의미 분할 모델을 활용해. 텍스트 임베딩에서 토큰을 생성하기 위해 BLIP-2 토크나이저를 도입했어.

우리 접근법의 새로움은 단안 깊이 추정을 위한 확산 기반 파이프라인에 Swin2SR, BEiT 모델, BLIP-2 토크나이저를 도입한 거야. 우리 모델은 NYUv2와 KITTI 데이터셋에서 {\delta}3 메트릭에서 최첨단 결과(SOTA)를 달성했어. RMSE와 REL 메트릭에서도 SOTA 모델들과 비슷한 결과를 보여줬고, 마지막으로, 추정된 깊이의 시각화에서도 SOTA 확산 기반 단안 깊이 추정 모델들보다 개선된 모습을 보여줬어. 코드: 이 URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.06187.pdf

Title: Bottleneck-based Encoder-decoder ARchitecture (BEAR) for Learning Unbiased Consumer-to-Consumer Image Representations

Original Abstract:
Unbiased representation learning is still an object of study under specific applications and contexts. Novel architectures are usually crafted to resolve particular problems using mixtures of fundamental pieces. This paper presents different image feature extraction mechanisms that work together with residual connections to encode perceptual image information in an autoencoder configuration. We use image data that aims to support a larger research agenda dealing with issues regarding criminal activity in consumer-to-consumer online platforms. Preliminary results suggest that the proposed architecture can learn rich spaces using ours and other image datasets resolving important challenges that are identified.

Translated Abstract:
편향 없는 표현 학습은 여전히 특정 응용 프로그램과 상황에서 연구되고 있어. 보통 새로운 구조가 특정 문제를 해결하기 위해 기본 요소들을 조합해서 만들어져. 

이 논문에서는 잔여 연결(residual connections)과 함께 작동하는 다양한 이미지 특징 추출 메커니즘을 소개해. 이 메커니즘은 오토인코더 구성에서 감각적인 이미지 정보를 인코딩하는 데 사용돼. 우리는 소비자 간 온라인 플랫폼에서의 범죄 활동 관련 문제를 다루는 더 큰 연구 계획을 지원하는 이미지 데이터를 사용하고 있어. 

초기 결과는 제안된 구조가 우리와 다른 이미지 데이터셋을 사용하여 중요한 문제들을 해결하면서 풍부한 공간을 학습할 수 있다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.06189.pdf

Title: MyGo: Consistent and Controllable Multi-View Driving Video Generation with Camera Control

Original Abstract:
High-quality driving video generation is crucial for providing training data for autonomous driving models. However, current generative models rarely focus on enhancing camera motion control under multi-view tasks, which is essential for driving video generation. Therefore, we propose MyGo, an end-to-end framework for video generation, introducing motion of onboard cameras as conditions to make progress in camera controllability and multi-view consistency. MyGo employs additional plug-in modules to inject camera parameters into the pre-trained video diffusion model, which retains the extensive knowledge of the pre-trained model as much as possible. Furthermore, we use epipolar constraints and neighbor view information during the generation process of each view to enhance spatial-temporal consistency. Experimental results show that MyGo has achieved state-of-the-art results in both general camera-controlled video generation and multi-view driving video generation tasks, which lays the foundation for more accurate environment simulation in autonomous driving. Project page: \href{this https URL}{this http URL\_project/MyGo/page.html}

Translated Abstract:
고품질의 드라이빙 비디오 생성은 자율주행 모델 훈련 데이터 제공에 정말 중요해. 그런데 현재의 생성 모델들은 멀티뷰 작업에서 카메라 움직임 제어를 향상시키는 데 별로 집중하지 않고 있어. 이게 드라이빙 비디오 생성에는 꼭 필요하거든. 그래서 우리는 MyGo라는 비디오 생성용 엔드 투 엔드 프레임워크를 제안해. 이 모델은 온보드 카메라의 움직임을 조건으로 사용해서 카메라 제어 가능성과 멀티뷰 일관성을 높이는 데 도움을 줘.

MyGo는 추가 플러그인 모듈을 사용해서 카메라 파라미터를 미리 훈련된 비디오 확산 모델에 주입해. 이렇게 하면 미리 훈련된 모델의 방대한 지식을 최대한 유지할 수 있어. 게다가 우리는 각 뷰를 생성하는 과정에서 에피폴라 제약 조건과 이웃 뷰 정보를 사용해서 공간-시간 일관성을 높여.

실험 결과, MyGo는 일반 카메라 제어 비디오 생성과 멀티뷰 드라이빙 비디오 생성 작업 모두에서 최신 기술 수준의 결과를 달성했어. 이게 자율주행에서 더 정확한 환경 시뮬레이션의 기초를 다지는 데 도움을 줄 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06197.pdf

Title: UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised

Original Abstract:
Road segmentation is a critical task for autonomous driving systems, requiring accurate and robust methods to classify road surfaces from various environmental data. Our work introduces an innovative approach that integrates LiDAR point cloud data, visual image, and relative depth maps derived from images. The integration of multiple data sources in road segmentation presents both opportunities and challenges. One of the primary challenges is the scarcity of large-scale, accurately labeled datasets that are necessary for training robust deep learning models. To address this, we have developed the [UdeerLID+] framework under a semi-supervised learning paradigm. Experiments results on KITTI datasets validate the superior performance.

Translated Abstract:
도로 분할은 자율주행 시스템에서 중요한 작업이야. 다양한 환경 데이터를 이용해 도로 표면을 정확하게 분류해야 하거든. 우리의 연구는 LiDAR 포인트 클라우드 데이터, 시각 이미지, 그리고 이미지에서 유도된 상대 깊이 맵을 통합하는 새로운 접근 방식을 소개해.

여러 데이터 소스를 도로 분할에 통합하는 건 기회와 도전 과제가 있어. 그 중 하나는 강력한 딥러닝 모델을 훈련시키기 위해 필요한 대규모의 정확하게 레이블이 붙은 데이터셋이 부족하다는 거야. 이 문제를 해결하기 위해 우리는 반지도 학습 패러다임 아래 [UdeerLID+] 프레임워크를 개발했어. KITTI 데이터셋에서 실험한 결과, 우리의 방법이 우수한 성능을 보였다는 걸 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06198.pdf

Title: Deep kernel representations of latent space features for low-dose PET-MR imaging robust to variable dose reduction

Original Abstract:
Low-dose positron emission tomography (PET) image reconstruction methods have potential to significantly improve PET as an imaging modality. Deep learning provides a promising means of incorporating prior information into the image reconstruction problem to produce quantitatively accurate images from compromised signal. Deep learning-based methods for low-dose PET are generally poorly conditioned and perform unreliably on images with features not present in the training distribution. We present a method which explicitly models deep latent space features using a robust kernel representation, providing robust performance on previously unseen dose reduction factors. Additional constraints on the information content of deep latent features allow for tuning in-distribution accuracy and generalisability. Tests with out-of-distribution dose reduction factors ranging from $\times 10$ to $\times 1000$ and with both paired and unpaired MR, demonstrate significantly improved performance relative to conventional deep-learning methods trained using the same data. Code:this https URL

Translated Abstract:
저용량 양전자 방출 단층촬영(PET) 이미지를 복원하는 방법들은 PET의 이미징 방식 개선에 큰 잠재력을 가지고 있어. 딥 러닝은 손상된 신호로부터 정량적으로 정확한 이미지를 만들기 위해 기존 정보를 이미지 복원 문제에 통합하는 유망한 방법이야. 

하지만 저용량 PET에 대한 딥 러닝 기반 방법들은 일반적으로 상태가 좋지 않고, 훈련 데이터에 없는 특징이 있는 이미지에서는 신뢰성이 떨어져. 우리는 강력한 커널 표현을 사용해서 딥 잠재 공간의 특징을 명시적으로 모델링하는 방법을 제안해. 이 방법은 이전에 본 적 없는 용량 감소 인자에서도 강력한 성능을 보여줘.

딥 잠재 특징의 정보 내용에 대한 추가 제약을 통해, 훈련 데이터에 대한 정확도와 일반화 가능성을 조정할 수 있어. $\times 10$에서 $\times 1000$까지의 범위에서 훈련 데이터와 연결된 MR 이미지와 연결되지 않은 MR 이미지를 사용한 테스트 결과, 기존의 딥 러닝 방법들에 비해 성능이 크게 개선된 것을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06202.pdf

Title: RealisDance: Equip controllable character animation with realistic hands

Original Abstract:
Controllable character animation is an emerging task that generates character videos controlled by pose sequences from given character images. Although character consistency has made significant progress via reference UNet, another crucial factor, pose control, has not been well studied by existing methods yet, resulting in several issues: 1) The generation may fail when the input pose sequence is corrupted. 2) The hands generated using the DWPose sequence are blurry and unrealistic. 3) The generated video will be shaky if the pose sequence is not smooth enough. In this paper, we present RealisDance to handle all the above issues. RealisDance adaptively leverages three types of poses, avoiding failed generation caused by corrupted pose sequences. Among these pose types, HaMeR provides accurate 3D and depth information of hands, enabling RealisDance to generate realistic hands even for complex gestures. Besides using temporal attention in the main UNet, RealisDance also inserts temporal attention into the pose guidance network, smoothing the video from the pose condition aspect. Moreover, we introduce pose shuffle augmentation during training to further improve generation robustness and video smoothness. Qualitative experiments demonstrate the superiority of RealisDance over other existing methods, especially in hand quality.

Translated Abstract:
제어 가능한 캐릭터 애니메이션은 주어진 캐릭터 이미지에서 포즈 시퀀스를 이용해 캐릭터 비디오를 만드는 새롭게 떠오르는 작업이야. 캐릭터 일관성은 참조 UNet 덕분에 많이 발전했지만, 포즈 제어는 기존 방법에서 잘 연구되지 않아서 몇 가지 문제가 생기고 있어. 

1) 입력 포즈 시퀀스가 손상되면 생성이 실패할 수 있어.  
2) DWPose 시퀀스를 사용해 생성된 손은 흐릿하고 비현실적이야.  
3) 포즈 시퀀스가 매끄럽지 않으면 생성된 비디오가 흔들려.  

이 논문에서는 이런 모든 문제를 해결하기 위해 RealisDance를 제안해. RealisDance는 세 가지 유형의 포즈를 적절히 활용해서 손상된 포즈 시퀀스로 인한 생성 실패를 피할 수 있어. 이 포즈 유형 중 HaMeR는 손의 정확한 3D 및 깊이 정보를 제공해서, 복잡한 제스처에 대해서도 RealisDance가 사실적인 손을 생성할 수 있게 해. 

또한, RealisDance는 주요 UNet에서 시간 주의를 사용하고, 포즈 가이드 네트워크에도 시간 주의를 추가해서 포즈 조건 측면에서 비디오를 부드럽게 만들어. 그리고 훈련 중에 포즈 셔플 증강을 도입해 생성의 강인성과 비디오의 부드러움을 더 향상시켜. 질적인 실험 결과, RealisDance가 기존의 다른 방법들보다 특히 손 품질에서 우수하다는 것을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.06206.pdf

Title: AgileIR: Memory-Efficient Group Shifted Windows Attention for Agile Image Restoration

Original Abstract:
Image Transformers show a magnificent success in Image Restoration tasks. Nevertheless, most of transformer-based models are strictly bounded by exorbitant memory occupancy. Our goal is to reduce the memory consumption of Swin Transformer and at the same time speed up the model during training process. Thus, we introduce AgileIR, group shifted attention mechanism along with window attention, which sparsely simplifies the model in architecture. We propose Group Shifted Window Attention (GSWA) to decompose Shift Window Multi-head Self Attention (SW-MSA) and Window Multi-head Self Attention (W-MSA) into groups across their attention heads, contributing to shrinking memory usage in back propagation. In addition to that, we keep shifted window masking and its shifted learnable biases during training, in order to induce the model interacting across windows within the channel. We also re-allocate projection parameters to accelerate attention matrix calculation, which we found a negligible decrease in performance. As a result of experiment, compared with our baseline SwinIR and other efficient quantization models, AgileIR keeps the performance still at 32.20 dB on Set5 evaluation dataset, exceeding other methods with tailor-made efficient methods and saves over 50% memory while a large batch size is employed.

Translated Abstract:
이미지 변환기는 이미지 복원 작업에서 멋진 성과를 보여줬어. 하지만 대부분의 변환기 기반 모델은 정말 많은 메모리를 사용해야 해. 우리의 목표는 Swin Transformer의 메모리 소비를 줄이고, 훈련 과정에서 모델 속도를 높이는 거야. 

그래서 AgileIR이라는 걸 도입했어. 이건 그룹 이동 주의 메커니즘과 창 주의 메커니즘을 결합해서 모델 구조를 간소화하는 거야. 우리는 그룹 이동 창 주의(GSWA)를 제안해서 이동 창 다중 헤드 자기 주의(SW-MSA)와 창 다중 헤드 자기 주의(W-MSA)를 주의 헤드별로 그룹으로 나누어 메모리 사용량을 줄였어. 

또한, 훈련 중에 이동 창 마스킹과 그 이동 가능한 편향을 유지해서 모델이 채널 내에서 창 간 상호작용을 하도록 유도했어. 우리는 주의 행렬 계산을 가속하기 위해 프로젝션 매개변수를 재배치했는데, 성능 저하는 거의 없었어. 

실험 결과, 우리 기본 모델인 SwinIR와 다른 효율적인 양자화 모델들과 비교했을 때, AgileIR은 Set5 평가 데이터셋에서 여전히 32.20 dB의 성능을 유지했어. 맞춤형 효율적 방법으로 다른 방법들을 초과하면서도, 큰 배치 사이즈를 쓸 때 50% 이상의 메모리를 절약했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06210.pdf

Title: INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding

Original Abstract:
Affordance denotes the potential interactions inherent in objects. The perception of affordance can enable intelligent agents to navigate and interact with new environments efficiently. Weakly supervised affordance grounding teaches agents the concept of affordance without costly pixel-level annotations, but with exocentric images. Although recent advances in weakly supervised affordance grounding yielded promising results, there remain challenges including the requirement for paired exocentric and egocentric image dataset, and the complexity in grounding diverse affordances for a single object. To address them, we propose INTeraction Relationship-aware weakly supervised Affordance grounding (INTRA). Unlike prior arts, INTRA recasts this problem as representation learning to identify unique features of interactions through contrastive learning with exocentric images only, eliminating the need for paired datasets. Moreover, we leverage vision-language model embeddings for performing affordance grounding flexibly with any text, designing text-conditioned affordance map generation to reflect interaction relationship for contrastive learning and enhancing robustness with our text synonym augmentation. Our method outperformed prior arts on diverse datasets such as AGD20K, IIT-AFF, CAD and UMD. Additionally, experimental results demonstrate that our method has remarkable domain scalability for synthesized images / illustrations and is capable of performing affordance grounding for novel interactions and objects.

Translated Abstract:
어포던스는 물체에 내재된 잠재적 상호작용을 의미해. 어포던스를 인식하면 지능형 에이전트가 새로운 환경에서 더 효율적으로 탐색하고 상호작용할 수 있어. 약하게 감독된 어포던스 그라운딩은 비싼 픽셀 수준의 주석 없이 에이전트에게 어포던스 개념을 가르치는데, 이때 외부 이미지(exocentric images)를 사용해. 최근 약하게 감독된 어포던스 그라운딩의 발전이 좋은 결과를 보여줬지만, 여전히 외부 이미지와 자아 중심 이미지(egocentric image) 데이터셋이 쌍으로 필요하고, 단일 물체에 대한 다양한 어포던스를 그라운딩하는 복잡함 같은 문제들이 남아 있어.

이 문제를 해결하기 위해 우리는 INTeraction 관계 인식 약하게 감독된 어포던스 그라운딩(INTRA)을 제안해. 이전 연구들과 달리, INTRA는 이 문제를 표현 학습으로 바꿔서 외부 이미지만 사용해 상호작용의 고유한 특징을 식별하도록 해. 이렇게 하면 쌍 데이터셋이 필요 없어. 게다가 우리는 비전-언어 모델 임베딩을 활용해 어떤 텍스트로도 어포던스 그라운딩을 유연하게 수행할 수 있도록 해. 텍스트 조건에 맞춘 어포던스 맵 생성을 디자인해서 상호작용 관계를 반영하고, 텍스트 동의어 증강을 통해 더 강력한 성능을 끌어내고 있어.

우리 방법은 AGD20K, IIT-AFF, CAD, UMD 같은 다양한 데이터셋에서 이전 연구들보다 뛰어난 성과를 보여줬어. 실험 결과는 우리 방법이 합성 이미지나 일러스트에 대해 놀라운 도메인 확장성을 가지며, 새로운 상호작용과 물체에 대해서도 어포던스 그라운딩을 수행할 수 있음을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.06214.pdf

Title: Towards Generalizable Scene Change Detection

Original Abstract:
Scene Change Detection (SCD) is vital for applications such as visual surveillance and mobile robotics. However, current SCD methods exhibit a bias to the temporal order of training datasets and limited performance on unseen domains; coventional SCD benchmarks are not able to evaluate generalization or temporal consistency. To tackle these limitations, we introduce a Generalizable Scene Change Detection Framework (GeSCF) in this work. The proposed GeSCF leverages localized semantics of a foundation model without any re-training or fine-tuning -- for generalization over unseen domains. Specifically, we design an adaptive thresholding of the similarity distribution derived from facets of the pre-trained foundation model to generate initial pseudo-change mask. We further utilize Segment Anything Model's (SAM) class-agnostic masks to refine pseudo-masks. Moreover, our proposed framework maintains commutative operations in all settings to ensure complete temporal consistency. Finally, we define new metrics, evaluation dataset, and evaluation protocol for Generalizable Scene Change Detection (GeSCD). Extensive experiments demonstrate that GeSCF excels across diverse and challenging environments -- establishing a new benchmark for SCD performance.

Translated Abstract:
장면 변화 감지는 비주얼 감시나 모바일 로봇 같은 분야에서 정말 중요해. 하지만 현재의 장면 변화 감지 방법들은 훈련 데이터셋의 시간적 순서에 편향되어 있고, 보지 못한 도메인에서는 성능이 떨어져. 기존의 장면 변화 감지 기준들은 일반화나 시간적 일관성을 평가할 수 없어서 문제가 있어.

이런 한계를 해결하기 위해, 우리는 일반화 가능한 장면 변화 감지 프레임워크(GeSCF)를 제안해. 이 GeSCF는 재훈련이나 미세 조정 없이 기본 모델의 지역적 의미를 활용해 보지 못한 도메인에서도 일반화할 수 있어. 구체적으로는, 사전 훈련된 기본 모델의 다양한 요소에서 얻은 유사성 분포의 적응형 임계값을 설계해서 초기 의사 변화 마스크를 만들어.

또한, Segment Anything Model(SAM)의 클래스 비관성 마스크를 활용해 의사 마스크를 다듬어. 게다가, 우리가 제안한 프레임워크는 모든 설정에서 교환 가능한 연산을 유지해서 완전한 시간적 일관성을 보장해. 마지막으로, 일반화 가능한 장면 변화 감지(GeSCD)를 위한 새로운 메트릭, 평가 데이터셋, 평가 프로토콜을 정의해.

많은 실험 결과, GeSCF가 다양한 도전적인 환경에서 뛰어난 성능을 보여줘서 장면 변화 감지 성능의 새로운 기준을 세웠어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06217.pdf

Title: DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online Surgical Phase Recognition

Original Abstract:
Surgical phase recognition has become a crucial requirement in laparoscopic surgery, enabling various clinical applications like surgical risk forecasting. Current methods typically identify the surgical phase using individual frame-wise embeddings as the fundamental unit for time modeling. However, this approach is overly sensitive to current observations, often resulting in discontinuous and erroneous predictions within a complete surgical phase. In this paper, we propose DACAT, a novel dual-stream model that adaptively learns clip-aware context information to enhance the temporal relationship. In one stream, DACAT pretrains a frame encoder, caching all historical frame-wise features. In the other stream, DACAT fine-tunes a new frame encoder to extract the frame-wise feature at the current moment. Additionally, a max clip-response read-out (Max-R) module is introduced to bridge the two streams by using the current frame-wise feature to adaptively fetch the most relevant past clip from the feature cache. The clip-aware context feature is then encoded via cross-attention between the current frame and its fetched adaptive clip, and further utilized to enhance the time modeling for accurate online surgical phase recognition. The benchmark results on three public datasets, i.e., Cholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed DACAT over existing state-of-the-art methods, with improvements in Jaccard scores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have been released at this https URL.

Translated Abstract:
수술 단계 인식은 복강경 수술에서 중요한 요구사항이 되었어. 이 기술 덕분에 수술 위험 예측 같은 여러 임상 응용이 가능해. 현재 방법들은 보통 각 프레임의 정보로 수술 단계를 파악하는데, 이 방식은 현재의 관찰에 너무 민감해서 전체 수술 단계에서 불연속적이고 잘못된 예측을 자주 하게 돼.

이 논문에서는 DACAT이라는 새로운 이중 스트림 모델을 제안해. 이 모델은 시간 관계를 향상시키기 위해 클립 인식 컨텍스트 정보를 적응적으로 학습해. 한 스트림에서는 DACAT이 프레임 인코더를 사전 훈련시켜서 모든 이전 프레임 정보를 저장해. 다른 스트림에서는 DACAT이 현재 시점의 프레임 정보를 추출하기 위해 새로운 프레임 인코더를 미세 조정해.

추가로, Max-R이라는 모듈을 도입해서 두 스트림을 연결해. 이 모듈은 현재 프레임 정보를 사용해서 가장 관련성 높은 이전 클립을 적응적으로 불러오는 역할을 해. 이렇게 얻은 클립 인식 컨텍스트 정보는 현재 프레임과 불러온 클립 간의 교차 주의를 통해 인코딩되고, 이를 통해 정확한 온라인 수술 단계 인식을 위해 시간 모델링을 개선해.

세 개의 공개 데이터셋인 Cholec80, M2CAI16, AutoLaparo에서의 벤치마크 결과는 DACAT이 기존의 최신 방법들에 비해 우수함을 보여줘. Jaccard 점수가 각각 최소 4.5%, 4.6%, 2.7% 향상됐어. 우리 코드와 모델은 이 URL에서 공개했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06220.pdf

Title: CerviXpert: A Multi-Structural Convolutional Neural Network for Predicting Cervix Type and Cervical Cell Abnormalities

Original Abstract:
Cervical cancer affects millions of women worldwide and has a significantly higher survival rate when diagnosed early. Pap smears and cervical biopsies are vital screening tools for detecting such cancer. However, the success of these screening processes depends on the skills of cytologists. A recent trend in diagnostic cytology is to apply machine-learning-based models to classify cancer using cell images. These automated models have been shown to perform just as well as, or even better than, expert cytologists. Some notable methods for classifying cervix cancers include ResNet50, VGG16, MobileNetV2, and InceptionV3, based on deep convolutional neural networks (CNN). However, these methods are computationally expensive. We present CerviXpert, a multi-structural Convolutional Neural Network, to identify cervix cancer. We perform extensive experiments on a publicly available dataset, SiPaKMeD, to show the efficacy of our method. CerviXpert presents a promising solution for efficient cervical cancer screening and diagnosis by striking a balance between accuracy and practical feasibility.

Translated Abstract:
자궁경부암은 전 세계 수백만 여성에게 영향을 미치고 있고, 조기에 진단하면 생존율이 훨씬 높아져. 파파니콜로 검사와 자궁경부 생검은 이런 암을 발견하는 데 중요한 검사 방법이야. 하지만 이 검사들이 잘 작동하려면 세포학자의 기술이 중요해. 최근 진단 세포학에서 머신러닝 기반 모델을 활용해 세포 이미지를 통해 암을 분류하는 경향이 있어. 이러한 자동화된 모델들은 전문가 세포학자들보다 같거나 더 잘 성능을 보인다고 해.

자궁경부암을 분류하는 몇 가지 주목할 만한 방법으로는 ResNet50, VGG16, MobileNetV2, InceptionV3 같은 딥 컨볼루션 신경망(CNN) 기반의 방법들이 있어. 하지만 이런 방법들은 계산 비용이 많이 들어. 그래서 우리는 CerviXpert라는 다중 구조 컨볼루션 신경망을 제안해. SiPaKMeD라는 공공 데이터셋을 사용해서 우리의 방법의 효과를 검토했어. 

CerviXpert는 정확성과 실용성을 잘 조화시켜 효율적인 자궁경부암 검진과 진단을 위한 유망한 해결책을 제시해.

================================================================================

URL:
https://arxiv.org/pdf/2409.06224.pdf

Title: MIP-GAF: A MLLM-annotated Benchmark for Most Important Person Localization and Group Context Understanding

Original Abstract:
Estimating the Most Important Person (MIP) in any social event setup is a challenging problem mainly due to contextual complexity and scarcity of labeled data. Moreover, the causality aspects of MIP estimation are quite subjective and diverse. To this end, we aim to address the problem by annotating a large-scale `in-the-wild' dataset for identifying human perceptions about the `Most Important Person (MIP)' in an image. The paper provides a thorough description of our proposed Multimodal Large Language Model (MLLM) based data annotation strategy, and a thorough data quality analysis. Further, we perform a comprehensive benchmarking of the proposed dataset utilizing state-of-the-art MIP localization methods, indicating a significant drop in performance compared to existing datasets. The performance drop shows that the existing MIP localization algorithms must be more robust with respect to `in-the-wild' situations. We believe the proposed dataset will play a vital role in building the next-generation social situation understanding methods. The code and data is available at this https URL.

Translated Abstract:
사회적 이벤트에서 가장 중요한 사람(MIP)을 추정하는 것은 복잡한 맥락과 라벨이 붙은 데이터 부족 때문에 어려운 문제야. 게다가 MIP 추정의 인과 관계는 주관적이고 다양해.

우리는 이 문제를 해결하기 위해 사람들의 인식이 담긴 대규모 '자연 상황' 데이터셋을 주석 처리하는 작업을 하고 있어. 이 논문에서는 우리가 제안한 다중 모달 대형 언어 모델(MLLM) 기반의 데이터 주석 전략과 데이터 품질 분석에 대해 자세히 설명해.

또한, 최신 MIP 위치 추정 방법을 활용해서 제안된 데이터셋의 포괄적인 벤치마킹도 진행했어. 그 결과, 기존 데이터셋과 비교했을 때 성능이 크게 떨어지는 걸 확인했어. 이 성능 저하는 기존 MIP 위치 추정 알고리즘이 '자연 상황'에 대해 더 강건해야 한다는 걸 보여줘.

우리는 제안된 데이터셋이 다음 세대 사회적 상황 이해 방법을 구축하는 데 중요한 역할을 할 거라고 믿어. 코드와 데이터는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06231.pdf

Title: A Latent Implicit 3D Shape Model for Multiple Levels of Detail

Original Abstract:
Implicit neural representations map a shape-specific latent code and a 3D coordinate to its corresponding signed distance (SDF) value. However, this approach only offers a single level of detail. Emulating low levels of detail can be achieved with shallow networks, but the generated shapes are typically not smooth. Alternatively, some network designs offer multiple levels of detail, but are limited to overfitting a single object.
To address this, we propose a new shape modeling approach, which enables multiple levels of detail and guarantees a smooth surface at each level. At the core, we introduce a novel latent conditioning for a multiscale and bandwith-limited neural architecture. This results in a deep parameterization of multiple shapes, where early layers quickly output approximated SDF values. This allows to balance speed and accuracy within a single network and enhance the efficiency of implicit scene rendering. We demonstrate that by limiting the bandwidth of the network, we can maintain smooth surfaces across all levels of detail. At finer levels, reconstruction quality is on par with the state of the art models, which are limited to a single level of detail.

Translated Abstract:
암묵적 신경 표현은 특정 형태의 잠재 코드와 3D 좌표를 해당하는 부호화 거리(SDF) 값에 매핑해. 하지만 이 방법은 단일 세부 수준만 제공해. 낮은 세부 수준을 모방하는 건 얕은 네트워크로 가능하지만, 이때 생성된 형태는 일반적으로 부드럽지 않아. 반면, 일부 네트워크 설계는 여러 세부 수준을 제공하지만, 한 객체에만 과적합되는 한계가 있어.

이를 해결하기 위해, 우리는 여러 세부 수준을 가능하게 하고 각 수준에서 부드러운 표면을 보장하는 새로운 형태 모델링 접근 방식을 제안해. 핵심은 다중 스케일 및 대역폭 제한된 신경 구조를 위한 새로운 잠재 조건을 도입하는 거야. 이로 인해 여러 형태의 깊은 매개변수를 얻을 수 있고, 초기 레이어에서 빠르게 근사된 SDF 값을 출력할 수 있어. 이렇게 하면 단일 네트워크 내에서 속도와 정확성을 균형 있게 유지할 수 있고, 암묵적 장면 렌더링의 효율성을 높일 수 있어. 

네트워크의 대역폭을 제한함으로써, 모든 세부 수준에서 부드러운 표면을 유지할 수 있다는 걸 보여줘. 더 세밀한 수준에서는 재구성 품질이 단일 세부 수준에 제한된 최신 모델과 동등해.

================================================================================

URL:
https://arxiv.org/pdf/2409.06235.pdf

Title: Recurrent Neural Networks for Still Images

Original Abstract:
In this paper, we explore the application of Recurrent Neural Network (RNN) for still images. Typically, Convolutional Neural Networks (CNNs) are the prevalent method applied for this type of data, and more recently, transformers have gained popularity, although they often require large models. Unlike these methods, RNNs are generally associated with processing sequences over time rather than single images. We argue that RNNs can effectively handle still images by interpreting the pixels as a sequence. This approach could be particularly advantageous for compact models designed for embedded systems, where resources are limited. Additionally, we introduce a novel RNN design tailored for two-dimensional inputs, such as images, and a custom version of BiDirectional RNN (BiRNN) that is more memory-efficient than traditional implementations. In our research, we have tested these layers in Convolutional Recurrent Neural Networks (CRNNs), predominantly composed of Conv2D layers, with RNN layers at or close to the end. Experiments on the COCO and CIFAR100 datasets show better results, particularly for small networks.

Translated Abstract:
이 논문에서는 정지 이미지에 대한 순환 신경망(RNN)의 적용을 탐구해. 보통 정지 이미지를 처리할 때는 합성곱 신경망(CNN)을 주로 사용하고, 최근에는 트랜스포머도 많이 쓰이지만, 이건 모델이 커야 하는 경우가 많아. RNN은 일반적으로 시간에 따른 시퀀스를 처리하는 데 관련이 있지만, 우리는 RNN이 정지 이미지를 효과적으로 처리할 수 있다고 주장해. 픽셀을 시퀀스로 해석하는 방식이야. 

이 접근법은 자원이 제한된 임베디드 시스템을 위해 설계된 컴팩트 모델에 특히 유리할 수 있어. 또한, 우리는 2차원 입력, 즉 이미지에 맞춘 새로운 RNN 디자인과 전통적인 구현보다 메모리를 더 효율적으로 사용하는 맞춤형 양방향 RNN(BiRNN)을 소개해. 

우리 연구에서는 이 레이어들을 합성곱 순환 신경망(CRNN)에서 테스트했어. CRNN은 주로 Conv2D 레이어로 구성되고, RNN 레이어는 마지막 쪽에 있어. COCO와 CIFAR100 데이터셋에서 실험한 결과, 특히 작은 네트워크에서 더 좋은 결과를 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06240.pdf

Title: Test-Time Certifiable Self-Supervision to Bridge the Sim2Real Gap in Event-Based Satellite Pose Estimation

Original Abstract:
Deep learning plays a critical role in vision-based satellite pose estimation. However, the scarcity of real data from the space environment means that deep models need to be trained using synthetic data, which raises the Sim2Real domain gap problem. A major cause of the Sim2Real gap are novel lighting conditions encountered during test time. Event sensors have been shown to provide some robustness against lighting variations in vision-based pose estimation. However, challenging lighting conditions due to strong directional light can still cause undesirable effects in the output of commercial off-the-shelf event sensors, such as noisy/spurious events and inhomogeneous event densities on the object. Such effects are non-trivial to simulate in software, thus leading to Sim2Real gap in the event domain. To close the Sim2Real gap in event-based satellite pose estimation, the paper proposes a test-time self-supervision scheme with a certifier module. Self-supervision is enabled by an optimisation routine that aligns a dense point cloud of the predicted satellite pose with the event data to attempt to rectify the inaccurately estimated pose. The certifier attempts to verify the corrected pose, and only certified test-time inputs are backpropagated via implicit differentiation to refine the predicted landmarks, thus improving the pose estimates and closing the Sim2Real gap. Results show that the our method outperforms established test-time adaptation schemes.

Translated Abstract:
딥러닝은 비전 기반 위성 자세 추정에서 중요한 역할을 해. 하지만 우주 환경에서 실제 데이터가 부족해서 딥 모델을 합성 데이터를 사용해 훈련해야 해. 이게 Sim2Real 도메인 갭 문제를 일으켜. Sim2Real 갭의 주요 원인 중 하나는 테스트할 때 발생하는 새로운 조명 조건이야.

이벤트 센서는 조명 변화에 대해 약간의 강인성을 제공하는 것이 입증되었어. 하지만 강한 방향성 조명 때문에 어려운 조명 조건이 발생하면 상용 이벤트 센서의 출력에 원치 않는 영향을 줄 수 있어. 예를 들어, 잡음이 섞인 이벤트나 물체에 고르지 않은 이벤트 밀도를 초래할 수 있어. 이런 효과는 소프트웨어로 시뮬레이션하기도 쉽지 않아, 그래서 이벤트 도메인에서 Sim2Real 갭이 생기게 돼.

이 논문은 이벤트 기반 위성 자세 추정에서 Sim2Real 갭을 줄이기 위해 테스트 시간 셀프 슈퍼비전 방안을 제안해. 이건 인증 모듈을 포함하고 있어. 셀프 슈퍼비전은 예측된 위성 자세의 밀집 포인트 클라우드를 이벤트 데이터와 정렬하는 최적화 과정을 통해 가능해. 이렇게 해서 부정확하게 추정된 자세를 수정하려고 해.

인증자는 수정된 자세를 검증하려고 하고, 인증된 테스트 시간 입력만이 암묵적인 미분을 통해 다시 전파되어 예측된 랜드마크를 정교화해. 이렇게 하면 자세 추정이 개선되고 Sim2Real 갭이 줄어들어. 결과적으로, 우리의 방법이 기존의 테스트 시간 적응 방식보다 성능이 뛰어난 것으로 나타났어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06259.pdf

Title: ALSS-YOLO: An Adaptive Lightweight Channel Split and Shuffling Network for TIR Wildlife Detection in UAV Imagery

Original Abstract:
Unmanned aerial vehicles (UAVs) equipped with thermal infrared (TIR) cameras play a crucial role in combating nocturnal wildlife poaching. However, TIR images often face challenges such as jitter, and wildlife overlap, necessitating UAVs to possess the capability to identify blurred and overlapping small targets. Current traditional lightweight networks deployed on UAVs struggle to extract features from blurry small targets. To address this issue, we developed ALSS-YOLO, an efficient and lightweight detector optimized for TIR aerial images. Firstly, we propose a novel Adaptive Lightweight Channel Split and Shuffling (ALSS) module. This module employs an adaptive channel split strategy to optimize feature extraction and integrates a channel shuffling mechanism to enhance information exchange between channels. This improves the extraction of blurry features, crucial for handling jitter-induced blur and overlapping targets. Secondly, we developed a Lightweight Coordinate Attention (LCA) module that employs adaptive pooling and grouped convolution to integrate feature information across dimensions. This module ensures lightweight operation while maintaining high detection precision and robustness against jitter and target overlap. Additionally, we developed a single-channel focus module to aggregate the width and height information of each channel into four-dimensional channel fusion, which improves the feature representation efficiency of infrared images. Finally, we modify the localization loss function to emphasize the loss value associated with small objects to improve localization accuracy. Extensive experiments on the BIRDSAI and ISOD TIR UAV wildlife datasets show that ALSS-YOLO achieves state-of-the-art performance, Our code is openly available at this https URL.

Translated Abstract:
무인 항공기(UAV)에 장착된 열 적외선(TIR) 카메라는 야간 야생동물 밀렵을 방지하는 데 중요한 역할을 해. 하지만 TIR 이미지는 흔들림이나 야생동물의 겹침 같은 문제에 직면하는 경우가 많아서, UAV가 흐릿하고 겹쳐진 작은 목표를 인식할 수 있는 능력을 가져야 해.

현재 UAV에 배치된 전통적인 경량 네트워크는 흐릿한 작은 목표에서 특징을 추출하는 데 어려움을 겪고 있어. 이 문제를 해결하기 위해 우리는 TIR 공중 이미지에 최적화된 효율적이고 경량의 탐지기인 ALSS-YOLO를 개발했어.

첫째로, 우리는 새로운 적응형 경량 채널 분할 및 셔플링(ALSS) 모듈을 제안해. 이 모듈은 특징 추출을 최적화하기 위해 적응형 채널 분할 전략을 사용하고, 채널 간 정보 교환을 강화하기 위해 채널 셔플링 메커니즘을 통합해. 이 덕분에 흐릿한 특징을 더 잘 추출할 수 있어, 흔들림으로 인한 흐림이나 겹치는 목표를 다루는 데 중요해.

둘째로, 우리는 경량 좌표 주의(LCA) 모듈을 개발했어. 이 모듈은 적응형 풀링과 그룹화된 합성을 사용해서 차원 간 특징 정보를 통합해. 이 모듈은 경량으로 작동하면서도 높은 탐지 정밀도와 흔들림 및 목표 겹침에 대한 강건성을 유지해.

또한, 우리는 각 채널의 너비와 높이 정보를 네 차원 채널 융합으로 집계하는 단일 채널 포커스 모듈을 개발했어. 이 모듈은 적외선 이미지의 특징 표현 효율을 개선해.

마지막으로, 우리는 작은 객체와 관련된 손실 값을 강조하기 위해 위치 손실 함수를 수정했어. 이로 인해 위치 정확도를 높일 수 있어. BIRDSAI와 ISOD TIR UAV 야생동물 데이터셋에서의 광범위한 실험 결과 ALSS-YOLO가 최첨단 성능을 달성했어. 우리의 코드는 이 URL에서 공개되고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06267.pdf

Title: Mahalanobis k-NN: A Statistical Lens for Robust Point-Cloud Registrations

Original Abstract:
In this paper, we discuss Mahalanobis k-NN: a statistical lens designed to address the challenges of feature matching in learning-based point cloud registration when confronted with an arbitrary density of point clouds, either in the source or target point cloud. We tackle this by adopting Mahalanobis k-NN's inherent property to capture the distribution of the local neighborhood and surficial geometry. Our method can be seamlessly integrated into any local-graph-based point cloud analysis method. In this paper, we focus on two distinct methodologies: Deep Closest Point (DCP) and Deep Universal Manifold Embedding (DeepUME). Our extensive benchmarking on the ModelNet40 and Faust datasets highlights the efficacy of the proposed method in point cloud registration tasks. Moreover, we establish for the first time that the features acquired through point cloud registration inherently can possess discriminative capabilities. This is evident by a substantial improvement of about 20\% in the average accuracy observed in the point cloud few-shot classification task benchmarked on ModelNet40 and ScanObjectNN. The code is publicly available at this https URL

Translated Abstract:
이 논문에서는 Mahalanobis k-NN에 대해 이야기해. 이건 학습 기반의 점군 등록에서 특징 매칭의 문제를 해결하기 위해 만든 통계적 방법이야. 점군의 밀도가 불규칙할 때, 원본이나 목표 점군에서 발생하는 문제를 다루는 거지. 

우리는 Mahalanobis k-NN의 고유한 특성을 활용해서 지역 이웃의 분포와 표면 기하학을 포착해. 이 방법은 어떤 로컬 그래프 기반의 점군 분석 방법에도 쉽게 통합할 수 있어. 

논문에서는 두 가지 방법론에 집중했어: Deep Closest Point (DCP)와 Deep Universal Manifold Embedding (DeepUME). ModelNet40과 Faust 데이터셋에서의 광범위한 벤치마킹을 통해 우리가 제안한 방법의 효과를 보여줬어. 

또한, 점군 등록을 통해 얻은 특징들이 본래 차별화된 능력을 가질 수 있다는 걸 처음으로 증명했어. 이는 ModelNet40과 ScanObjectNN에서 점군 몇 샷 분류 작업의 평균 정확도가 약 20% 개선된 것으로 나타났어. 코드는 이 URL에서 공개되고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06285.pdf

Title: Context Enhancement with Reconstruction as Sequence for Unified Unsupervised Anomaly Detection

Original Abstract:
Unsupervised anomaly detection (AD) aims to train robust detection models using only normal samples, while can generalize well to unseen anomalies. Recent research focuses on a unified unsupervised AD setting in which only one model is trained for all classes, i.e., n-class-one-model paradigm. Feature-reconstruction-based methods achieve state-of-the-art performance in this scenario. However, existing methods often suffer from a lack of sufficient contextual awareness, thereby compromising the quality of the reconstruction. To address this issue, we introduce a novel Reconstruction as Sequence (RAS) method, which enhances the contextual correspondence during feature reconstruction from a sequence modeling perspective. In particular, based on the transformer technique, we integrate a specialized RASFormer block into RAS. This block enables the capture of spatial relationships among different image regions and enhances sequential dependencies throughout the reconstruction process. By incorporating the RASFormer block, our RAS method achieves superior contextual awareness capabilities, leading to remarkable performance. Experimental results show that our RAS significantly outperforms competing methods, well demonstrating the effectiveness and superiority of our method. Our code is available at this https URL.

Translated Abstract:
비지도 이상 탐지(AD)는 정상 샘플만 사용해서 강력한 탐지 모델을 훈련시키는 걸 목표로 해. 이 모델은 보지 못한 이상치에도 잘 일반화되야 해. 최근 연구들은 모든 클래스를 위해 단 하나의 모델만 훈련하는 통합 비지도 AD 설정에 집중하고 있어. 이걸 n-class-one-model 패러다임이라고 해. 

특징 재구성 기반 방법들이 이 상황에서 최첨단 성능을 보여주고 있는데, 기존 방법들은 충분한 맥락 인식이 부족해서 재구성 품질이 떨어지는 문제가 있어. 이 문제를 해결하기 위해, 우리는 새로운 재구성 방법인 'Sequence로서의 재구성(RAS)'을 소개해. 이 방법은 특징 재구성 과정에서 맥락의 일치를 강화해. 

특히, 변환기(트랜스포머) 기술을 바탕으로, RAS에 전문화된 RASFormer 블록을 통합했어. 이 블록은 서로 다른 이미지 영역 간의 공간적 관계를 포착하고 재구성 과정 전반에 걸쳐 순차적 종속성을 향상시켜. RASFormer 블록을 포함함으로써, 우리의 RAS 방법은 맥락 인식 능력이 뛰어나게 발휘되어서 성능이 크게 개선돼. 

실험 결과는 우리 RAS 방법이 경쟁 방법들보다 훨씬 우수하다는 걸 보여줘, 이로써 우리의 방법의 효과성과 우수성을 잘 증명했어. 우리의 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06290.pdf

Title: EntAugment: Entropy-Driven Adaptive Data Augmentation Framework for Image Classification

Original Abstract:
Data augmentation (DA) has been widely used to improve the generalization of deep neural networks. While existing DA methods have proven effective, they often rely on augmentation operations with random magnitudes to each sample. However, this approach can inadvertently introduce noise, induce distribution shifts, and increase the risk of overfitting. In this paper, we propose EntAugment, a tuning-free and adaptive DA framework. Unlike previous work, EntAugment dynamically assesses and adjusts the augmentation magnitudes for each sample during training, leveraging insights into both the inherent complexities of training samples and the evolving status of deep models. Specifically, in EntAugment, the magnitudes are determined by the information entropy derived from the probability distribution obtained by applying the softmax function to the model's output. In addition, to further enhance the efficacy of EntAugment, we introduce a novel entropy regularization term, EntLoss, which complements the EntAugment approach. Theoretical analysis further demonstrates that EntLoss, compared to traditional cross-entropy loss, achieves closer alignment between the model distributions and underlying dataset distributions. Moreover, EntAugment and EntLoss can be utilized separately or jointly. We conduct extensive experiments across multiple image classification tasks and network architectures with thorough comparisons of existing DA methods. Importantly, the proposed methods outperform others without introducing any auxiliary models or noticeable extra computational costs, highlighting both effectiveness and efficiency. Code is available at this https URL.

Translated Abstract:
데이터 증강(DA)은 딥 뉴럴 네트워크의 일반화를 개선하는 데 널리 사용되고 있어. 기존의 DA 방법들이 효과적이긴 하지만, 보통 각 샘플에 대해 랜덤한 크기의 증강 작업에 의존해. 그런데 이런 접근 방식은 의도치 않게 노이즈를 추가하거나 분포 변화를 유발할 수 있고, 과적합의 위험도 증가시킬 수 있어.

그래서 우리는 EntAugment라는 조정이 필요 없는 적응형 DA 프레임워크를 제안해. 이전 연구들과 달리, EntAugment는 훈련 중 각 샘플에 대한 증강 크기를 동적으로 평가하고 조정해. 이 과정에서 훈련 샘플의 복잡성과 딥 모델의 상태 변화를 반영해. 구체적으로 EntAugment에서는 모델의 출력에 소프트맥스 함수를 적용해 얻은 확률 분포에서 파생된 정보 엔트로피를 통해 크기를 결정해.

또한, EntAugment의 효과를 더욱 높이기 위해 새로운 엔트로피 정규화 항인 EntLoss를 도입해. 이 EntLoss는 EntAugment 방법을 보완해줘. 이론적 분석을 통해 EntLoss가 전통적인 교차 엔트로피 손실보다 모델 분포와 기본 데이터셋 분포 사이의 정렬을 더 잘 이룬다는 걸 보여줘. 게다가 EntAugment와 EntLoss는 따로 또는 함께 사용할 수 있어.

우리는 여러 이미지 분류 작업과 네트워크 구조에서 광범위한 실험을 진행했어. 그리고 기존 DA 방법들과 철저히 비교했지. 중요한 건, 우리가 제안한 방법들이 다른 방법들보다 더 뛰어난 성능을 보이면서 추가적인 모델이나 눈에 띄는 계산 비용 없이도 효과적이고 효율적이라는 거야. 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06299.pdf

Title: Enhancing Long Video Understanding via Hierarchical Event-Based Memory

Original Abstract:
Recently, integrating visual foundation models into large language models (LLMs) to form video understanding systems has attracted widespread attention. Most of the existing models compress diverse semantic information within the whole video and feed it into LLMs for content comprehension. While this method excels in short video understanding, it may result in a blend of multiple event information in long videos due to coarse compression, which causes information redundancy. Consequently, the semantics of key events might be obscured within the vast information that hinders the model's understanding capabilities. To address this issue, we propose a Hierarchical Event-based Memory-enhanced LLM (HEM-LLM) for better understanding of long videos. Firstly, we design a novel adaptive sequence segmentation scheme to divide multiple events within long videos. In this way, we can perform individual memory modeling for each event to establish intra-event contextual connections, thereby reducing information redundancy. Secondly, while modeling current event, we compress and inject the information of the previous event to enhance the long-term inter-event dependencies in videos. Finally, we perform extensive experiments on various video understanding tasks and the results show that our model achieves state-of-the-art performances.

Translated Abstract:
최근에 시각 기반 모델을 대형 언어 모델(LLMs)과 통합해 비디오 이해 시스템을 만드는 것에 대한 관심이 많아졌어. 기존의 많은 모델들은 전체 비디오 안의 다양한 의미 정보를 압축해서 LLM에 넣어 콘텐츠를 이해하는 방식을 사용해. 이 방법은 짧은 비디오를 이해하는 데는 잘 작동하지만, 긴 비디오에서는 여러 사건 정보가 혼합될 수 있어서 정보가 중복되는 문제가 생겨. 그래서 중요한 사건의 의미가 방대한 정보 속에서 가려져서 모델이 이해하기 어려워질 수 있어.

이 문제를 해결하기 위해 우리는 긴 비디오를 더 잘 이해할 수 있도록 '계층적 사건 기반 메모리 강화 LLM(HEM-LLM)'을 제안해. 먼저, 긴 비디오 안의 여러 사건을 나누기 위해 새로운 적응형 시퀀스 분할 방식을 설계했어. 이렇게 하면 각 사건에 대해 개별적으로 메모리를 모델링할 수 있어서 사건 간의 맥락 연결을 구축하고, 정보 중복을 줄일 수 있어. 

다음으로, 현재 사건을 모델링할 때 이전 사건의 정보를 압축해서 주입함으로써 비디오 안의 장기적인 사건 간 의존성을 강화해. 마지막으로, 다양한 비디오 이해 작업에 대해 광범위한 실험을 수행했는데, 결과적으로 우리 모델이 최첨단 성능을 달성했다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06300.pdf

Title: An Attribute-Enriched Dataset and Auto-Annotated Pipeline for Open Detection

Original Abstract:
Detecting objects of interest through language often presents challenges, particularly with objects that are uncommon or complex to describe, due to perceptual discrepancies between automated models and human annotators. These challenges highlight the need for comprehensive datasets that go beyond standard object labels by incorporating detailed attribute descriptions. To address this need, we introduce the Objects365-Attr dataset, an extension of the existing Objects365 dataset, distinguished by its attribute annotations. This dataset reduces inconsistencies in object detection by integrating a broad spectrum of attributes, including color, material, state, texture and tone. It contains an extensive collection of 5.6M object-level attribute descriptions, meticulously annotated across 1.4M bounding boxes. Additionally, to validate the dataset's effectiveness, we conduct a rigorous evaluation of YOLO-World at different scales, measuring their detection performance and demonstrating the dataset's contribution to advancing object detection.

Translated Abstract:
물체를 언어로 탐지하는 건 주로 어려운 점이 많아. 특히 드물거나 설명하기 복잡한 물체는 자동화된 모델과 사람 주석자 간에 인식 차이가 있어서 더 힘들어. 이런 문제들은 우리가 일반적인 물체 라벨을 넘어서서 자세한 속성 설명까지 포함하는 포괄적인 데이터셋이 필요하다는 걸 보여줘.

그래서 우리는 Objects365-Attr 데이터셋을 소개해. 이건 기존의 Objects365 데이터셋을 확장한 건데, 속성 주석이 추가된 게 특징이야. 이 데이터셋은 색상, 재질, 상태, 질감, 톤 같은 다양한 속성을 통합해서 물체 탐지에서의 불일치를 줄여줘. 총 5.6M 개의 물체 수준 속성 설명이 1.4M 개의 바운딩 박스에 정밀하게 주석 처리되어 있어.

또한, 이 데이터셋의 효과를 검증하기 위해 다양한 스케일에서 YOLO-World의 성능을 철저히 평가했어. 이를 통해 탐지 성능을 측정하고, 데이터셋이 물체 탐지를 발전시키는 데 어떻게 기여하는지를 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06305.pdf

Title: High-Performance Few-Shot Segmentation with Foundation Models: An Empirical Study

Original Abstract:
Existing few-shot segmentation (FSS) methods mainly focus on designing novel support-query matching and self-matching mechanisms to exploit implicit knowledge in pre-trained backbones. However, the performance of these methods is often constrained by models pre-trained on classification tasks. The exploration of what types of pre-trained models can provide more beneficial implicit knowledge for FSS remains limited. In this paper, inspired by the representation consistency of foundational computer vision models, we develop a FSS framework based on foundation models. To be specific, we propose a simple approach to extract implicit knowledge from foundation models to construct coarse correspondence and introduce a lightweight decoder to refine coarse correspondence for fine-grained segmentation. We systematically summarize the performance of various foundation models on FSS and discover that the implicit knowledge within some of these models is more beneficial for FSS than models pre-trained on classification tasks. Extensive experiments on two widely used datasets demonstrate the effectiveness of our approach in leveraging the implicit knowledge of foundation models. Notably, the combination of DINOv2 and DFN exceeds previous state-of-the-art methods by 17.5% on COCO-20i. Code is available at this https URL.

Translated Abstract:
기존의 몇 샷 분할(FSS) 방법들은 주로 새로운 지원-쿼리 매칭과 자기 매칭 메커니즘을 설계하는 데 집중해, 사전 훈련된 백본에서 암묵적인 지식을 활용하려고 해. 하지만 이런 방법들의 성능은 분류 작업에 대해 사전 훈련된 모델에 의해 제한되는 경우가 많아. 어떤 종류의 사전 훈련된 모델이 FSS에 더 유용한 암묵적 지식을 제공할 수 있는지에 대한 탐색이 아직 부족해.

이 논문에서는 기본 컴퓨터 비전 모델의 표현 일관성에 영감을 받아, 기초 모델을 기반으로 한 FSS 프레임워크를 개발했어. 구체적으로는, 기초 모델에서 암묵적인 지식을 뽑아내서 대략적인 대응 관계를 만들고, 경량 디코더를 도입해 대략적인 대응 관계를 세밀하게 다듬는 간단한 접근 방식을 제안해. 우리는 다양한 기초 모델의 FSS 성능을 체계적으로 정리하고, 이들 중 일부 모델의 암묵적인 지식이 분류 작업으로 사전 훈련된 모델보다 FSS에 더 유리하다는 것을 발견했어.

두 개의 널리 사용되는 데이터셋에서 진행한 광범위한 실험은 기초 모델의 암묵적 지식을 활용하는 우리의 접근 방식이 효과적임을 보여줘. 특히, DINOv2와 DFN의 조합은 COCO-20i에서 이전의 최첨단 방법보다 17.5% 더 나은 성능을 기록했어. 코드도 이 URL에서 사용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06309.pdf

Title: PPMamba: A Pyramid Pooling Local Auxiliary SSM-Based Model for Remote Sensing Image Semantic Segmentation

Original Abstract:
Semantic segmentation is a vital task in the field of remote sensing (RS). However, conventional convolutional neural network (CNN) and transformer-based models face limitations in capturing long-range dependencies or are often computationally intensive. Recently, an advanced state space model (SSM), namely Mamba, was introduced, offering linear computational complexity while effectively establishing long-distance dependencies. Despite their advantages, Mamba-based methods encounter challenges in preserving local semantic information. To cope with these challenges, this paper proposes a novel network called Pyramid Pooling Mamba (PPMamba), which integrates CNN and Mamba for RS semantic segmentation tasks. The core structure of PPMamba, the Pyramid Pooling-State Space Model (PP-SSM) block, combines a local auxiliary mechanism with an omnidirectional state space model (OSS) that selectively scans feature maps from eight directions, capturing comprehensive feature information. Additionally, the auxiliary mechanism includes pyramid-shaped convolutional branches designed to extract features at multiple scales. Extensive experiments on two widely-used datasets, ISPRS Vaihingen and LoveDA Urban, demonstrate that PPMamba achieves competitive performance compared to state-of-the-art models.

Translated Abstract:
의미 분할은 원격 감지(RS) 분야에서 중요한 작업이야. 하지만 기존의 합성곱 신경망(CNN)이나 트랜스포머 기반 모델들은 긴 거리 의존성을 잘 잡지 못하거나 계산량이 많아서 문제야. 최근에는 Mamba라는 고급 상태 공간 모델(SSM)이 소개됐는데, 이 모델은 선형 계산 복잡도를 가지면서 긴 거리 의존성을 효과적으로 설정할 수 있어.

그런데 Mamba 기반 방법들은 지역적인 의미 정보를 잘 보존하는 데 어려움이 있어. 이 문제를 해결하기 위해, 이 논문에서는 Pyramid Pooling Mamba(PPMamba)라는 새로운 네트워크를 제안해. PPMamba는 CNN과 Mamba를 결합해서 RS 의미 분할 작업을 수행해.

PPMamba의 핵심 구조인 Pyramid Pooling-State Space Model (PP-SSM) 블록은 지역 보조 메커니즘과 팔 방향에서 특징 맵을 선택적으로 스캔하는 전방향 상태 공간 모델(OSS)을 결합해. 이걸 통해 더 많은 특징 정보를 잡을 수 있어. 게다가 보조 메커니즘은 여러 스케일에서 특징을 추출하기 위해 피라미드 형태의 합성곱 가지를 포함하고 있어.

ISPRS Vaihingen과 LoveDA Urban이라는 두 개의 널리 사용되는 데이터셋에서 광범위한 실험을 진행했는데, PPMamba가 최신 모델들과 비교해도 경쟁력 있는 성능을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06311.pdf

Title: Seam Carving as Feature Pooling in CNN

Original Abstract:
This work investigates the potential of seam carving as a feature pooling technique within Convolutional Neural Networks (CNNs) for image classification tasks. We propose replacing the traditional max pooling layer with a seam carving operation. Our experiments on the Caltech-UCSD Birds 200-2011 dataset demonstrate that the seam carving-based CNN achieves better performance compared to the model utilizing max pooling, based on metrics such as accuracy, precision, recall, and F1-score. We further analyze the behavior of both approaches through feature map visualizations, suggesting that seam carving might preserve more structural information during the pooling process. Additionally, we discuss the limitations of our approach and propose potential future directions for research.

Translated Abstract:
이 연구는 이미지 분류 작업에서 합성곱 신경망(CNN) 내에서 특성 풀링 기법으로서 시암 카빙의 가능성을 조사해. 우리는 전통적인 맥스 풀링 레이어를 시암 카빙 작업으로 교체하는 걸 제안해.

Caltech-UCSD Birds 200-2011 데이터셋에서 실험한 결과, 시암 카빙 기반의 CNN이 맥스 풀링을 사용하는 모델보다 더 좋은 성능을 보여줬어. 이건 정확도, 정밀도, 재현율, F1 점수 같은 지표를 기준으로 한 거야. 

두 접근 방식의 행동을 특성 맵 시각화를 통해 분석하면서, 시암 카빙이 풀링 과정에서 구조적인 정보를 더 잘 유지할 수 있을 것 같다는 제안을 했어. 마지막으로, 우리의 접근 방식의 한계를 논의하고 향후 연구 방향에 대한 가능성도 제시했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06322.pdf

Title: G3PT: Unleash the power of Autoregressive Modeling in 3D Generation via Cross-scale Querying Transformer

Original Abstract:
Autoregressive transformers have revolutionized generative models in language processing and shown substantial promise in image and video generation. However, these models face significant challenges when extended to 3D generation tasks due to their reliance on next-token prediction to learn token sequences, which is incompatible with the unordered nature of 3D data. Instead of imposing an artificial order on 3D data, in this paper, we introduce G3PT, a scalable coarse-to-fine 3D generative model utilizing a cross-scale querying transformer. The key is to map point-based 3D data into discrete tokens with different levels of detail, naturally establishing a sequential relationship between different levels suitable for autoregressive modeling. Additionally, the cross-scale querying transformer connects tokens globally across different levels of detail without requiring an ordered sequence. Benefiting from this approach, G3PT features a versatile 3D generation pipeline that effortlessly supports diverse conditional structures, enabling the generation of 3D shapes from various types of conditions. Extensive experiments demonstrate that G3PT achieves superior generation quality and generalization ability compared to previous 3D generation methods. Most importantly, for the first time in 3D generation, scaling up G3PT reveals distinct power-law scaling behaviors.

Translated Abstract:
자기 회귀 변환기(autoregressive transformers)는 언어 처리에서 생성 모델을 혁신적으로 변화시켰고, 이미지와 비디오 생성에서도 큰 가능성을 보여줬어. 하지만 이 모델들이 3D 생성 작업으로 확장될 때는 몇 가지 큰 문제에 직면해. 그 이유는 이 모델들이 다음 토큰 예측에 의존하기 때문에 3D 데이터의 비순서적인 특성과 맞지 않기 때문이야.

그래서 이번 논문에서는 G3PT라는 스케일 조정 가능한 3D 생성 모델을 소개해. 이 모델은 크로스 스케일 쿼리 변환기(cross-scale querying transformer)를 활용해. 중요한 점은 포인트 기반 3D 데이터를 서로 다른 디테일 수준의 이산 토큰으로 변환하는 거야. 이렇게 하면 서로 다른 디테일 수준 간에 자연스럽게 순차적 관계가 형성돼서 자기 회귀 모델링에 적합해.

또한, 크로스 스케일 쿼리 변환기는 정해진 순서 없이도 서로 다른 디테일 수준의 토큰을 전 세계적으로 연결해. 이 접근 덕분에 G3PT는 다양한 조건 구조를 지원하는 유연한 3D 생성 파이프라인을 갖추고 있어, 여러 종류의 조건으로부터 3D 형태를 생성할 수 있게 해줘.

많은 실험 결과, G3PT는 이전의 3D 생성 방법들에 비해 더 뛰어난 생성 품질과 일반화 능력을 보여줬어. 가장 중요한 점은, G3PT를 확장해보니 3D 생성에서 처음으로 뚜렷한 파워-로우 스케일링 행동이 드러났다는 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06324.pdf

Title: SDF-Net: A Hybrid Detection Network for Mediastinal Lymph Node Detection on Contrast CT Images

Original Abstract:
Accurate lymph node detection and quantification are crucial for cancer diagnosis and staging on contrast-enhanced CT images, as they impact treatment planning and prognosis. However, detecting lymph nodes in the mediastinal area poses challenges due to their low contrast, irregular shapes and dispersed distribution. In this paper, we propose a Swin-Det Fusion Network (SDF-Net) to effectively detect lymph nodes. SDF-Net integrates features from both segmentation and detection to enhance the detection capability of lymph nodes with various shapes and sizes. Specifically, an auto-fusion module is designed to merge the feature maps of segmentation and detection networks at different levels. To facilitate effective learning without mask annotations, we introduce a shape-adaptive Gaussian kernel to represent lymph node in the training stage and provide more anatomical information for effective learning. Comparative results demonstrate promising performance in addressing the complex lymph node detection problem.

Translated Abstract:
림프절을 정확하게 찾고 수량을 측정하는 건, 암 진단과 병기 설정에 정말 중요해. 특히, 조영제 CT 이미지에서는 치료 계획과 예후에 큰 영향을 미치거든. 근데, 흉부 중앙에 있는 림프절은 대비가 낮고, 모양도 불규칙하며, 분포가 흩어져 있어서 찾기가 힘들어.

이 논문에서는 림프절을 효과적으로 찾아내기 위해 Swin-Det Fusion Network (SDF-Net)라는 방법을 제안해. SDF-Net은 분할과 탐지에서 나오는 특징을 통합해서 다양한 모양과 크기의 림프절을 더 잘 감지할 수 있게 해. 특히, 자동 융합 모듈을 설계해서 분할 네트워크와 탐지 네트워크의 특징 맵을 서로 다른 레벨에서 합쳐.

또한, 마스크 주석 없이 효과적으로 학습할 수 있도록 훈련 단계에서 림프절을 표현하기 위해 모양에 적응하는 가우시안 커널을 도입했어. 이 방법은 효과적인 학습을 위해 더 많은 해부학적 정보를 제공해. 비교 결과, 복잡한 림프절 탐지 문제를 해결하는 데 좋은 성과를 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06334.pdf

Title: Multi-Weather Image Restoration via Histogram-Based Transformer Feature Enhancement

Original Abstract:
Currently, the mainstream restoration tasks under adverse weather conditions have predominantly focused on single-weather scenarios. However, in reality, multiple weather conditions always coexist and their degree of mixing is usually unknown. Under such complex and diverse weather conditions, single-weather restoration models struggle to meet practical demands. This is particularly critical in fields such as autonomous driving, where there is an urgent need for a model capable of effectively handling mixed weather conditions and enhancing image quality in an automated manner. In this paper, we propose a Task Sequence Generator module that, in conjunction with the Task Intra-patch Block, effectively extracts task-specific features embedded in degraded images. The Task Intra-patch Block introduces an external learnable sequence that aids the network in capturing task-specific information. Additionally, we employ a histogram-based transformer module as the backbone of our network, enabling the capture of both global and local dynamic range features. Our proposed model achieves state-of-the-art performance on public datasets.

Translated Abstract:
현재, 악천후에서의 복원 작업은 주로 단일 날씨 시나리오에 집중되고 있어. 하지만 실제로는 여러 날씨 조건이 항상 함께 존재하고 그 혼합 정도는 보통 알기 어려워. 이런 복잡하고 다양한 날씨 조건에서는 단일 날씨 복원 모델이 실제 요구를 충족하는 데 어려움을 겪어. 특히 자율 주행 같은 분야에서는 혼합된 날씨 조건을 효과적으로 처리하고 이미지 품질을 자동으로 향상시킬 수 있는 모델이 절실해.

이 논문에서는 Task Sequence Generator 모듈을 제안해. 이 모듈은 Task Intra-patch Block과 함께 사용돼서, 손상된 이미지에서 작업에 맞는 특징을 효과적으로 추출해. Task Intra-patch Block은 네트워크가 작업에 특화된 정보를 포착하는 데 도움을 주는 외부 학습 가능한 시퀀스를 도입해. 게다가, 우리는 히스토그램 기반의 트랜스포머 모듈을 네트워크의 백본으로 사용해, 전역 및 지역 동적 범위 특징을 모두 포착할 수 있게 해.

우리의 제안한 모델은 공개 데이터셋에서 최첨단 성능을 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06355.pdf

Title: DiffQRCoder: Diffusion-based Aesthetic QR Code Generation with Scanning Robustness Guided Iterative Refinement

Original Abstract:
With the success of Diffusion Models for image generation, the technologies also have revolutionized the aesthetic Quick Response (QR) code generation. Despite significant improvements in visual attractiveness for the beautified codes, their scannabilities are usually sacrificed and thus hinder their practical uses in real-world scenarios. To address this issue, we propose a novel Diffusion-based QR Code generator (DiffQRCoder) to effectively craft both scannable and visually pleasing QR codes. The proposed approach introduces Scanning-Robust Perceptual Guidance (SRPG), a new diffusion guidance for Diffusion Models to guarantee the generated aesthetic codes to obey the ground-truth QR codes while maintaining their attractiveness during the denoising process. Additionally, we present another post-processing technique, Scanning Robust Manifold Projected Gradient Descent (SR-MPGD), to further enhance their scanning robustness through iterative latent space optimization. With extensive experiments, the results demonstrate that our approach not only outperforms other compared methods in Scanning Success Rate (SSR) with better or comparable CLIP aesthetic score (CLIP-aes.) but also significantly improves the SSR of the ControlNet-only approach from 60% to 99%. The subjective evaluation indicates that our approach achieves promising visual attractiveness to users as well. Finally, even with different scanning angles and the most rigorous error tolerance settings, our approach robustly achieves over 95% SSR, demonstrating its capability for real-world applications.

Translated Abstract:
이미지 생성에 대한 디퓨전 모델의 성공 덕분에, 이 기술들이 QR 코드 생성에서도 큰 변화를 가져왔어. 예쁘게 꾸민 QR 코드들은 시각적으로는 매력적이지만, 보통 스캔 성공률이 떨어져서 실제 사용에 제약이 있어. 

이 문제를 해결하기 위해, 우리는 스캔 가능하면서도 시각적으로 아름다운 QR 코드를 만들 수 있는 새로운 디퓨전 기반 QR 코드 생성기(DiffQRCoder)를 제안해. 이 방법은 Scanning-Robust Perceptual Guidance (SRPG)라는 새로운 디퓨전 가이드를 도입해서, 생성된 아름다운 코드가 실제 QR 코드를 따르도록 보장하면서도, 노이즈 제거 과정에서 매력을 유지할 수 있도록 해.

또한, Scanning Robust Manifold Projected Gradient Descent (SR-MPGD)라는 후처리 기술도 소개하는데, 이건 반복적인 잠재 공간 최적화를 통해 스캔 강인성을 더 향상시켜. 여러 실험 결과를 보면, 우리의 방법이 다른 방법들보다 스캔 성공률(SSR)에서 더 잘 작동하고, CLIP 미적 점수(CLIP-aes.)에서도 비슷하게 좋거나 더 나은 결과를 보여줘. 특히, ControlNet만 사용했을 때 SSR이 60%에서 99%로 크게 개선되었어.

주관적인 평가에서도 우리의 방법이 사용자들에게 매력적인 비주얼을 제공한다는 결과가 나왔어. 마지막으로, 다양한 스캔 각도와 가장 엄격한 오류 허용 설정에서도, 우리 방법은 95% 이상의 SSR을 안정적으로 달성해서 실제 응용에 적합하다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.06367.pdf

Title: Texture-AD: An Anomaly Detection Dataset and Benchmark for Real Algorithm Development

Original Abstract:
Anomaly detection is a crucial process in industrial manufacturing and has made significant advancements recently. However, there is a large variance between the data used in the development and the data collected by the production environment. Therefore, we present the Texture-AD benchmark based on representative texture-based anomaly detection to evaluate the effectiveness of unsupervised anomaly detection algorithms in real-world applications. This dataset includes images of 15 different cloth, 14 semiconductor wafers and 10 metal plates acquired under different optical schemes. In addition, it includes more than 10 different types of defects produced during real manufacturing processes, such as scratches, wrinkles, color variations and point defects, which are often more difficult to detect than existing datasets. All anomalous areas are provided with pixel-level annotations to facilitate comprehensive evaluation using anomaly detection models. Specifically, to adapt to diverse products in automated pipelines, we present a new evaluation method and results of baseline algorithms. The experimental results show that Texture-AD is a difficult challenge for state-of-the-art algorithms. To our knowledge, Texture-AD is the first dataset to be devoted to evaluating industrial defect detection algorithms in the real world. The dataset is available at https://XXX.

Translated Abstract:
이상 탐지는 산업 제조에서 중요한 과정이고 최근에 많은 발전이 있었어. 하지만 개발에 사용된 데이터와 실제 생산 환경에서 수집된 데이터 사이에는 큰 차이가 있어. 그래서 우리는 대표적인 텍스처 기반 이상 탐지를 바탕으로 한 Texture-AD 벤치마크를 제안해. 이 벤치마크는 실제 응용 프로그램에서 비지도 이상 탐지 알고리즘의 효과를 평가하는 데 사용돼.

이 데이터셋에는 서로 다른 광학 방식으로 촬영된 15종의 천, 14종의 반도체 웨이퍼, 10종의 금속판 이미지가 포함되어 있어. 그리고 스크래치, 주름, 색상 변화, 점 결함 같은 10종 이상의 결함도 포함되어 있어. 이런 결함들은 기존 데이터셋보다 탐지하기 더 어려운 경우가 많아. 모든 이상 영역은 픽셀 수준의 주석이 제공되어서 이상 탐지 모델을 사용한 포괄적인 평가를 쉽게 할 수 있어.

특히, 자동화된 파이프라인에서 다양한 제품에 적응하기 위해 새로운 평가 방법과 기준 알고리즘의 결과를 제시해. 실험 결과에 따르면 Texture-AD는 최신 알고리즘에게 어려운 도전 과제가 되는 것으로 나타났어. 우리가 아는 한, Texture-AD는 실제 산업 결함 탐지 알고리즘을 평가하기 위해 처음으로 만들어진 데이터셋이야. 이 데이터셋은 https://XXX에서 사용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06371.pdf

Title: Distilling Generative-Discriminative Representations for Very Low-Resolution Face Recognition

Original Abstract:
Very low-resolution face recognition is challenging due to the serious loss of informative facial details in resolution degradation. In this paper, we propose a generative-discriminative representation distillation approach that combines generative representation with cross-resolution aligned knowledge distillation. This approach facilitates very low-resolution face recognition by jointly distilling generative and discriminative models via two distillation modules. Firstly, the generative representation distillation takes the encoder of a diffusion model pretrained for face super-resolution as the generative teacher to supervise the learning of the student backbone via feature regression, and then freezes the student backbone. After that, the discriminative representation distillation further considers a pretrained face recognizer as the discriminative teacher to supervise the learning of the student head via cross-resolution relational contrastive distillation. In this way, the general backbone representation can be transformed into discriminative head representation, leading to a robust and discriminative student model for very low-resolution face recognition. Our approach improves the recovery of the missing details in very low-resolution faces and achieves better knowledge transfer. Extensive experiments on face datasets demonstrate that our approach enhances the recognition accuracy of very low-resolution faces, showcasing its effectiveness and adaptability.

Translated Abstract:
아주 낮은 해상도의 얼굴 인식은 얼굴 세부 정보가 많이 사라져서 어려워. 이 논문에서는 생성적-판별적 표현 증류 방법을 제안해. 이 방법은 생성적 표현과 크로스 해상도 정렬 지식 증류를 결합해서 아주 낮은 해상도의 얼굴 인식을 돕는 거야. 이 과정은 두 개의 증류 모듈을 통해 생성적 모델과 판별적 모델을 함께 증류하는 방식이야.

먼저, 생성적 표현 증류는 얼굴 초해상도를 위해 사전 훈련된 확산 모델의 인코더를 생성적 선생님으로 사용해. 이 선생님이 학생 모델의 학습을 피처 회귀를 통해 감독하고, 그 다음 학생 모델은 고정돼. 그 후, 판별적 표현 증류는 사전 훈련된 얼굴 인식기를 판별적 선생님으로 삼아 학생 헤드의 학습을 크로스 해상도 관계 대조 증류를 통해 감독해. 이런 식으로 일반적인 백본 표현이 판별적 헤드 표현으로 변환돼, 아주 낮은 해상도의 얼굴 인식에 강력하고 판별적인 학생 모델이 만들어지는 거지.

우리 방법은 아주 낮은 해상도의 얼굴에서 잃어버린 세부 정보를 회복하는 데 도움을 주고, 더 나은 지식 전이를 이뤄내. 여러 얼굴 데이터셋에서 실험한 결과, 우리 방법이 아주 낮은 해상도의 얼굴 인식 정확도를 높인다는 걸 보여줬어. 이로써 효과적이고 적응력이 뛰어난 방법임을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06381.pdf

Title: A Cross-Font Image Retrieval Network for Recognizing Undeciphered Oracle Bone Inscriptions

Original Abstract:
Oracle Bone Inscription (OBI) is the earliest mature writing system known in China to date, which represents a crucial stage in the development of hieroglyphs. Nevertheless, the substantial quantity of undeciphered OBI characters continues to pose a persistent challenge for scholars, while conventional methods of ancient script research are both time-consuming and labor-intensive. In this paper, we propose a cross-font image retrieval network (CFIRN) to decipher OBI characters by establishing associations between OBI characters and other script forms, simulating the interpretive behavior of paleography scholars. Concretely, our network employs a siamese framework to extract deep features from character images of various fonts, fully exploring structure clues with different resolution by designed multiscale feature integration (MFI) module and multiscale refinement classifier (MRC). Extensive experiments on three challenging cross-font image retrieval datasets demonstrate that, given undeciphered OBI characters, our CFIRN can effectively achieve accurate matches with characters from other gallery fonts.

Translated Abstract:
오래된 뼈에 새겨진 글자(OBI)는 현재까지 중국에서 알려진 가장 초기의 성숙한 문자 체계야. 이건 상형문자의 발전에 있어서 중요한 단계로 여겨져. 하지만 아직 해독되지 않은 OBI 문자들이 많아서 연구자들에게 계속 어려운 문제로 남아있어. 전통적인 고대 문자 연구 방법은 시간도 많이 들고 힘도 많이 들어.

이번 논문에서는 OBI 문자를 해독하기 위해 크로스 폰트 이미지 검색 네트워크(CFIRN)를 제안해. 이 네트워크는 OBI 문자와 다른 문자 형태 간의 관계를 성립시켜서 고대 문헌 연구자들이 해석하는 방식을 모방해. 구체적으로, 우리 네트워크는 시암 네트워크 프레임워크를 사용해서 다양한 폰트의 문자 이미지에서 깊은 특징을 추출해. 그리고 다중 해상도를 통해 구조적 단서를 완전히 탐색하기 위해 설계된 다중 스케일 특징 통합(MFI) 모듈과 다중 스케일 세분화 분류기(MRC)를 활용해.

세 개의 도전적인 크로스 폰트 이미지 검색 데이터셋에서 광범위한 실험을 해봤는데, 해독되지 않은 OBI 문자가 주어졌을 때, 우리의 CFIRN이 다른 갤러리 폰트의 문자와 정확하게 매칭할 수 있다는 걸 효과적으로 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06385.pdf

Title: AMNS: Attention-Weighted Selective Mask and Noise Label Suppression for Text-to-Image Person Retrieval

Original Abstract:
Text-to-image person retrieval aims to retrieve images of person given textual descriptions, and most methods implicitly assume that the training image-text pairs are correctly aligned, but in practice, under-correlated and false-correlated problems arise for image-text pairs due to poor image quality and mislabeling. Meanwhile, the random masking augmentation strategy may incorrectly discard semantic content resulting in the problem of generating noisy pairings between image lexical elements and text descriptions. To solve these two problems, we propose a new noise label suppression method and alleviate the problem generated by random mask through an attention-weighted selective mask strategy. In the proposed noise label suppression method, the effect of noise labels is suppressed by preventing the model from being overconfident by considering the inverse KL scatter loss, which is combined with the weight adjustment focus loss to further improve the model's recognition ability on difficult samples. On the other hand, Attention-Weighted Selective Mask processes the raw image through the EMA version of the image encoder, retaining some of the tokens with strong semantic associations with the corresponding text descriptions in order to extract better features. Numerous experiments validate the effectiveness of our approach in terms of dealing with noisy problems. The code will be available soon at this https URL.

Translated Abstract:
텍스트-이미지 인물 검색은 텍스트 설명을 기반으로 인물의 이미지를 찾는 거야. 대부분의 방법들은 훈련 이미지-텍스트 쌍이 제대로 정렬되어 있다고 가정하는데, 실제로는 이미지 품질이 나쁘거나 잘못 라벨링된 이유로 이미지-텍스트 쌍에서 잘못된 상관관계 문제가 생길 수 있어. 

게다가, 랜덤 마스킹 증강 전략은 의미 있는 내용을 잘못 버릴 수 있어서 이미지 요소와 텍스트 설명 간의 노이즈가 많은 쌍을 생성하는 문제가 생길 수 있어. 이런 두 가지 문제를 해결하기 위해, 우리는 새로운 노이즈 라벨 억제 방법을 제안하고, 랜덤 마스크로 인해 발생하는 문제를 해결하기 위해 주의 가중 선택 마스크 전략을 사용해.

우리의 노이즈 라벨 억제 방법에서는 노이즈 라벨의 영향을 줄이기 위해 모델이 지나치게 자신감이 생기지 않도록 역 KL 산포 손실을 고려해. 이걸 가중치 조정 집중 손실과 결합해서 모델이 어려운 샘플에서도 인식 능력을 더 높일 수 있도록 해. 

한편, 주의 가중 선택 마스크는 이미지 인코더의 EMA 버전을 통해 원본 이미지를 처리해. 이 과정에서 해당 텍스트 설명과 강한 의미적 연관을 가진 일부 토큰을 유지해서 더 나은 특징을 추출할 수 있게 해. 

많은 실험을 통해 우리가 제안한 방법이 노이즈 문제를 다루는 데 효과적이라는 걸 확인했어. 코드도 곧 이 URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06407.pdf

Title: Sources of Uncertainty in 3D Scene Reconstruction

Original Abstract:
The process of 3D scene reconstruction can be affected by numerous uncertainty sources in real-world scenes. While Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (GS) achieve high-fidelity rendering, they lack built-in mechanisms to directly address or quantify uncertainties arising from the presence of noise, occlusions, confounding outliers, and imprecise camera pose inputs. In this paper, we introduce a taxonomy that categorizes different sources of uncertainty inherent in these methods. Moreover, we extend NeRF- and GS-based methods with uncertainty estimation techniques, including learning uncertainty outputs and ensembles, and perform an empirical study to assess their ability to capture the sensitivity of the reconstruction. Our study highlights the need for addressing various uncertainty aspects when designing NeRF/GS-based methods for uncertainty-aware 3D reconstruction.

Translated Abstract:
3D 장면 재구성 과정은 실제 장면에서 여러 가지 불확실성 때문에 영향을 받을 수 있어. Neural Radiance Fields (NeRFs)나 3D Gaussian Splatting (GS)는 고품질 렌더링을 잘 하지만, 노이즈, 가림 현상, 혼란스러운 아웃라이어, 그리고 부정확한 카메라 포즈 입력 때문에 생기는 불확실성을 직접 다루거나 측정하는 방법이 없어.

이 논문에서는 이런 방법들에 내재된 다양한 불확실성 원인을 분류하는 방법을 제안해. 그리고 NeRF와 GS 기반 방법에 불확실성 추정 기술을 추가했어. 여기에는 불확실성 출력 학습과 앙상블 기법이 포함돼. 우리는 재구성이 얼마나 민감한지를 평가하기 위한 실험도 진행했어.

이 연구는 NeRF/GS 기반의 불확실성 인식 3D 재구성을 설계할 때, 다양한 불확실성 요소를 다루는 게 필요하다는 점을 강조해.

================================================================================

URL:
https://arxiv.org/pdf/2409.06424.pdf

Title: A Likelihood Ratio-Based Approach to Segmenting Unknown Objects

Original Abstract:
Addressing the Out-of-Distribution (OoD) segmentation task is a prerequisite for perception systems operating in an open-world environment. Large foundational models are frequently used in downstream tasks, however, their potential for OoD remains mostly unexplored. We seek to leverage a large foundational model to achieve robust representation. Outlier supervision is a widely used strategy for improving OoD detection of the existing segmentation networks. However, current approaches for outlier supervision involve retraining parts of the original network, which is typically disruptive to the model's learned feature representation. Furthermore, retraining becomes infeasible in the case of large foundational models. Our goal is to retrain for outlier segmentation without compromising the strong representation space of the foundational model. To this end, we propose an adaptive, lightweight unknown estimation module (UEM) for outlier supervision that significantly enhances the OoD segmentation performance without affecting the learned feature representation of the original network. UEM learns a distribution for outliers and a generic distribution for known classes. Using the learned distributions, we propose a likelihood-ratio-based outlier scoring function that fuses the confidence of UEM with that of the pixel-wise segmentation inlier network to detect unknown objects. We also propose an objective to optimize this score directly. Our approach achieves a new state-of-the-art across multiple datasets, outperforming the previous best method by 5.74% average precision points while having a lower false-positive rate. Importantly, strong inlier performance remains unaffected.

Translated Abstract:
오픈 월드 환경에서 작동하는 인식 시스템을 위해 Out-of-Distribution (OoD) 세분화 작업을 해결하는 것은 필수적이야. 대규모 기본 모델들이 다운스트림 작업에 자주 사용되지만, OoD에 대한 잠재력은 아직 많이 탐색되지 않았어. 우리는 강력한 표현을 얻기 위해 대규모 기본 모델을 활용하려고 해.

아웃라이어 감독은 기존 세분화 네트워크의 OoD 감지를 개선하기 위해 널리 사용되는 전략이야. 하지만 현재의 아웃라이어 감독 방식은 원래 네트워크의 일부를 재훈련해야 해서 모델이 학습한 특성 표현에 방해가 될 수 있어. 게다가 대규모 기본 모델의 경우에는 재훈련이 사실상 불가능해. 우리의 목표는 기본 모델의 강력한 표현 공간을 손상시키지 않으면서 아웃라이어 세분화를 위해 재훈련하는 거야.

이걸 위해 우리는 아웃라이어 감독을 위한 적응형 경량 미지수 추정 모듈(UEM)을 제안해. 이 모듈은 원래 네트워크의 학습된 특성 표현에 영향을 주지 않으면서 OoD 세분화 성능을 크게 향상시켜. UEM은 아웃라이어에 대한 분포와 알려진 클래스에 대한 일반적인 분포를 학습해. 학습된 분포를 사용해, 우리는 UEM의 신뢰도와 픽셀 단위 세분화 내부 네트워크의 신뢰도를 결합해 알려지지 않은 객체를 탐지하는 가능도 비율 기반 아웃라이어 점수 함수를 제안해.

또한 이 점수를 직접 최적화하기 위한 목표도 제안해. 우리의 접근 방식은 여러 데이터셋에서 새로운 최첨단 성과를 달성했고, 이전 최고의 방법보다 5.74% 평균 정밀도를 높이면서 잘못된 긍정률은 낮춰. 중요한 것은 강력한 내부 성능은 영향을 받지 않았다는 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06442.pdf

Title: Prompt2Fashion: An automatically generated fashion dataset

Original Abstract:
Despite the rapid evolution and increasing efficacy of language and vision generative models, there remains a lack of comprehensive datasets that bridge the gap between personalized fashion needs and AI-driven design, limiting the potential for truly inclusive and customized fashion solutions. In this work, we leverage generative models to automatically construct a fashion image dataset tailored to various occasions, styles, and body types as instructed by users. We use different Large Language Models (LLMs) and prompting strategies to offer personalized outfits of high aesthetic quality, detail, and relevance to both expert and non-expert users' requirements, as demonstrated by qualitative analysis. Up until now the evaluation of the generated outfits has been conducted by non-expert human subjects. Despite the provided fine-grained insights on the quality and relevance of generation, we extend the discussion on the importance of expert knowledge for the evaluation of artistic AI-generated datasets such as this one. Our dataset is publicly available on GitHub at this https URL.

Translated Abstract:
언어와 비전 생성 모델이 빠르게 발전하고 효과성이 높아지고 있지만, 개인의 패션 요구와 AI 기반 디자인을 연결하는 종합적인 데이터셋은 여전히 부족해. 이로 인해 진정한 맞춤형 패션 솔루션을 만드는 데 한계가 있어. 

이 연구에서는 생성 모델을 활용해 사용자 지침에 맞춰 다양한 경우, 스타일, 체형에 맞춘 패션 이미지 데이터셋을 자동으로 만들어. 여러 개의 대형 언어 모델(LLM)과 프롬프트 전략을 사용해서 전문가와 비전문가 모두의 요구에 맞춘 고품질의 개인화된 의상을 제공해. 질적 분석을 통해 이 점을 입증했어.

지금까지 생성된 의상의 평가는 비전문가인 사람들에 의해 이루어졌어. 생성물의 품질과 관련성에 대한 세부적인 통찰이 제공되긴 했지만, 이런 예술적 AI 생성 데이터셋의 평가에서 전문가의 지식이 얼마나 중요한지도 논의할 필요가 있어. 

우리의 데이터셋은 GitHub에서 공개되어 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06443.pdf

Title: Knowledge Distillation via Query Selection for Detection Transformer

Original Abstract:
Transformers have revolutionized the object detection landscape by introducing DETRs, acclaimed for their simplicity and efficacy. Despite their advantages, the substantial size of these models poses significant challenges for practical deployment, particularly in resource-constrained environments. This paper addresses the challenge of compressing DETR by leveraging knowledge distillation, a technique that holds promise for maintaining model performance while reducing size. A critical aspect of DETRs' performance is their reliance on queries to interpret object representations accurately. Traditional distillation methods often focus exclusively on positive queries, identified through bipartite matching, neglecting the rich information present in hard-negative queries. Our visual analysis indicates that hard-negative queries, focusing on foreground elements, are crucial for enhancing distillation outcomes. To this end, we introduce a novel Group Query Selection strategy, which diverges from traditional query selection in DETR distillation by segmenting queries based on their Generalized Intersection over Union (GIoU) with ground truth objects, thereby uncovering valuable hard-negative queries for distillation. Furthermore, we present the Knowledge Distillation via Query Selection for DETR (QSKD) framework, which incorporates Attention-Guided Feature Distillation (AGFD) and Local Alignment Prediction Distillation (LAPD). These components optimize the distillation process by focusing on the most informative aspects of the teacher model's intermediate features and output. Our comprehensive experimental evaluation of the MS-COCO dataset demonstrates the effectiveness of our approach, significantly improving average precision (AP) across various DETR architectures without incurring substantial computational costs. Specifically, the AP of Conditional DETR ResNet-18 increased from 35.8 to 39.9.

Translated Abstract:
트랜스포머는 DETR을 도입하면서 객체 감지 분야에 혁신을 가져왔고, 이 모델들은 간단하면서도 효과적이라는 평가를 받고 있어. 하지만 이러한 모델의 크기가 크기 때문에 실제 환경에서 사용하기가 어렵다는 문제가 있어, 특히 자원이 제한된 환경에서는 더 그렇고. 

이 논문에서는 DETR을 압축하는 문제를 다뤄. 지식 증류라는 기법을 활용하는데, 이 방법은 모델의 성능을 유지하면서 크기를 줄일 수 있는 가능성이 있어. DETR의 성능에서 중요한 부분은 객체 표현을 정확히 해석하기 위한 쿼리의 의존성이야. 전통적인 증류 방법은 주로 양성 쿼리, 즉 이분 매칭을 통해 추출된 쿼리에만 초점을 맞추고, 어려운 부정 쿼리에서 나오는 풍부한 정보를 무시해. 우리의 시각적 분석에 따르면, 배경 요소에 초점을 맞춘 어려운 부정 쿼리가 증류 결과를 향상시키는 데 매우 중요해.

그래서 우리는 새로운 그룹 쿼리 선택 전략을 소개해. 이 전략은 DETR 증류에서 전통적인 쿼리 선택 방식과는 다르게, 쿼리를 실제 객체와의 일반화된 교차 비율(GIoU)에 따라 나누어 어려운 부정 쿼리를 찾는 방법이야. 그리고 우리는 DETR을 위한 쿼리 선택을 통한 지식 증류(QSKD) 프레임워크를 제시해. 이 프레임워크는 주의 기반 특징 증류(AGFD)와 지역 정렬 예측 증류(LAPD)를 포함하고 있어. 이러한 요소들은 교사 모델의 중간 특징과 출력을 가장 유용한 부분에 집중함으로써 증류 과정을 최적화해. 

MS-COCO 데이터셋에 대한 우리의 종합적인 실험 평가에서, 우리의 접근 방식이 효과적임을 보여주었고, 다양한 DETR 아키텍처에서 평균 정밀도(AP)를 크게 향상시켰어, 큰 계산 비용 없이 말이지. 특히, Conditional DETR ResNet-18의 AP는 35.8에서 39.9로 증가했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06445.pdf

Title: Learning Generative Interactive Environments By Trained Agent Exploration

Original Abstract:
World models are increasingly pivotal in interpreting and simulating the rules and actions of complex environments. Genie, a recent model, excels at learning from visually diverse environments but relies on costly human-collected data. We observe that their alternative method of using random agents is too limited to explore the environment. We propose to improve the model by employing reinforcement learning based agents for data generation. This approach produces diverse datasets that enhance the model's ability to adapt and perform well across various scenarios and realistic actions within the environment. In this paper, we first release the model GenieRedux - an implementation based on Genie. Additionally, we introduce GenieRedux-G, a variant that uses the agent's readily available actions to factor out action prediction uncertainty during validation. Our evaluation, including a replication of the Coinrun case study, shows that GenieRedux-G achieves superior visual fidelity and controllability using the trained agent exploration. The proposed approach is reproducable, scalable and adaptable to new types of environments. Our codebase is available at this https URL .

Translated Abstract:
월드 모델은 복잡한 환경의 규칙과 행동을 해석하고 시뮬레이션하는 데 점점 더 중요해지고 있어. 최근 모델인 Genie는 시각적으로 다양한 환경에서 잘 배우지만, 비싼 인간 수집 데이터를 필요로 해. 우리는 랜덤 에이전트를 사용하는 대안 방법이 환경을 탐험하는 데 너무 제한적이라고 봤어.

그래서 우리는 데이터 생성을 위해 강화 학습 기반 에이전트를 사용하는 방법을 제안해. 이 방법은 다양한 데이터셋을 만들어 모델이 다양한 상황과 실제 행동에서도 잘 적응하고 성능을 발휘할 수 있도록 도와줘.

이번 논문에서는 먼저 Genie에 기반한 구현 모델인 GenieRedux를 발표해. 그리고 에이전트의 쉽게 구할 수 있는 행동을 사용해서 검증 중에 행동 예측 불확실성을 줄이는 변형 모델인 GenieRedux-G도 소개해. 

우리의 평가 결과, Coinrun 사례 연구를 복제한 결과 GenieRedux-G가 훈련된 에이전트 탐사를 통해 시각적 품질과 제어 가능성이 뛰어나다는 걸 보여줬어. 제안한 방법은 재현 가능하고, 확장 가능하며, 새로운 환경 유형에 잘 적응해. 우리 코드베이스는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06471.pdf

Title: Weakly-supervised Camera Localization by Ground-to-satellite Image Registration

Original Abstract:
The ground-to-satellite image matching/retrieval was initially proposed for city-scale ground camera localization. This work addresses the problem of improving camera pose accuracy by ground-to-satellite image matching after a coarse location and orientation have been obtained, either from the city-scale retrieval or from consumer-level GPS and compass sensors. Existing learning-based methods for solving this task require accurate GPS labels of ground images for network training. However, obtaining such accurate GPS labels is difficult, often requiring an expensive {\color{black}Real Time Kinematics (RTK)} setup and suffering from signal occlusion, multi-path signal disruptions, \etc. To alleviate this issue, this paper proposes a weakly supervised learning strategy for ground-to-satellite image registration when only noisy pose labels for ground images are available for network training. It derives positive and negative satellite images for each ground image and leverages contrastive learning to learn feature representations for ground and satellite images useful for translation estimation. We also propose a self-supervision strategy for cross-view image relative rotation estimation, which trains the network by creating pseudo query and reference image pairs. Experimental results show that our weakly supervised learning strategy achieves the best performance on cross-area evaluation compared to recent state-of-the-art methods that are reliant on accurate pose labels for supervision.

Translated Abstract:
지상-위성 이미지 매칭/검색은 처음에 도시 규모의 지상 카메라 위치 추적을 위해 제안되었어. 이 연구는 대략적인 위치와 방향이 도시 규모 검색이나 소비자 GPS 및 나침반 센서로부터 얻어진 후, 지상-위성 이미지 매칭을 통해 카메라 자세 정확도를 높이는 문제를 다루고 있어. 

기존의 학습 기반 방법들은 네트워크 훈련을 위해 지상 이미지의 정확한 GPS 레이블이 필요해. 하지만 이런 정확한 GPS 레이블을 얻는 건 어려워. 보통 비싼 실시간 동역학(RTK) 장비가 필요하고, 신호가 가려지거나 멀티패스 신호 방해 같은 문제도 있어. 

이 문제를 해결하기 위해, 이 논문은 지상 이미지에 대해 노이즈가 있는 자세 레이블만 있을 때 지상-위성 이미지 등록을 위한 약한 감독 학습 전략을 제안해. 각 지상 이미지에 대해 긍정적 및 부정적 위성 이미지를 도출하고, 대조 학습을 활용해서 지상 및 위성 이미지의 특징 표현을 학습해. 이 특징 표현은 위치 추정에 유용해. 

또한, 교차 뷰 이미지 상대 회전 추정을 위한 자기 감독 전략도 제안해. 이 방법은 의사 쿼리 및 참조 이미지 쌍을 만들어 네트워크를 훈련해. 실험 결과, 우리의 약한 감독 학습 전략이 최근의 최첨단 방법들과 비교했을 때, 정확한 자세 레이블에 의존하지 않고도 교차 영역 평가에서 가장 좋은 성능을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06481.pdf

Title: NeIn: Telling What You Don't Want

Original Abstract:
Negation is a fundamental linguistic concept used by humans to convey information that they do not desire. Despite this, there has been minimal research specifically focused on negation within vision-language tasks. This lack of research means that vision-language models (VLMs) may struggle to understand negation, implying that they struggle to provide accurate results. One barrier to achieving human-level intelligence is the lack of a standard collection by which research into negation can be evaluated. This paper presents the first large-scale dataset, Negative Instruction (NeIn), for studying negation within the vision-language domain. Our dataset comprises 530,694 quadruples, i.e., source image, original caption, negative sentence, and target image in total, including 495,694 queries for training and 35,000 queries for benchmarking across multiple vision-language tasks. Specifically, we automatically generate NeIn based on a large, existing vision-language dataset, MS-COCO, via two steps: generation and filtering. During the generation phase, we leverage two VLMs, BLIP and MagicBrush, to generate the target image and a negative clause that expresses the content of the source image. In the subsequent filtering phase, we apply BLIP to remove erroneous samples. Additionally, we introduce an evaluation protocol for negation understanding of image editing models. Extensive experiments using our dataset across multiple VLMs for instruction-based image editing tasks demonstrate that even recent state-of-the-art VLMs struggle to understand negative queries. The project page is: this https URL

Translated Abstract:
부정은 사람들이 원하지 않는 정보를 전달할 때 사용하는 기본적인 언어 개념이야. 그런데 비전-언어 작업에서 부정에 초점을 맞춘 연구가 거의 없었어. 이게 문제인 게, 비전-언어 모델(VLMs)이 부정을 이해하는 데 어려움을 겪을 수 있어서, 정확한 결과를 내는 데도 힘들다는 거야. 인간 수준의 지능에 도달하는 데 하나의 장벽이 바로 부정에 대한 연구를 평가할 수 있는 표준화된 데이터셋이 부족하다는 점이야.

이 논문은 비전-언어 분야에서 부정을 연구하기 위한 첫 번째 대규모 데이터셋인 Negative Instruction(NeIn)을 소개해. 우리 데이터셋은 총 530,694개의 쿼드러플로 구성돼, 즉 소스 이미지, 원본 캡션, 부정 문장, 목표 이미지가 포함되어 있어. 여기에는 495,694개의 훈련용 쿼리와 35,000개의 여러 비전-언어 작업을 위한 벤치마킹 쿼리가 포함돼 있어. 

특히, 우리는 기존의 대규모 비전-언어 데이터셋인 MS-COCO를 바탕으로, 두 가지 단계인 생성과 필터링을 통해 NeIn을 자동 생성해. 생성 단계에서는 두 개의 VLM인 BLIP와 MagicBrush를 활용해서 목표 이미지와 소스 이미지의 내용을 표현하는 부정 절을 만들어. 이후 필터링 단계에서는 BLIP를 사용해서 잘못된 샘플을 제거해. 

또한, 우리는 이미지 편집 모델의 부정 이해를 평가하기 위한 평가 프로토콜도 도입했어. 우리 데이터셋을 이용한 여러 VLMs에 대한 광범위한 실험 결과, 최신의 최첨단 VLM조차 부정 쿼리를 이해하는 데 어려움을 겪는다는 걸 보여줘. 프로젝트 페이지는 이 URL이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06485.pdf

Title: Mitigating Hallucination in Visual-Language Models via Re-Balancing Contrastive Decoding

Original Abstract:
Although Visual-Language Models (VLMs) have shown impressive capabilities in tasks like visual question answering and image captioning, they still struggle with hallucinations. Analysis of attention distribution in these models shows that VLMs tend to processing textual tokens rather than visual tokens. This imbalance of attention distribution causes VLMs to favor textual knowledge in the case of multimodal knowledge conflicts, resulting in differences from the image information. In this paper, we propose Re-Balancing Contrastive Decoding (RBD) method, which employs textual and visual branches to recalibrate attention distribution in VLMs. Specifically, the textual branch injects image noise to stimulate the model's dependency on text, thereby reducing textual bias. Concurrently, the visual branch focuses on the selection of significant tokens, refining the attention mechanism to highlight the primary subject. This dual-branch strategy enables the RBD method to diminish textual bias while enhancing visual information. Experimental results demonstrate that our method, RBD, outperforms the existing methods by the CHAIR and POPE metrics, mitigate hallucinations without reducing the model's general capabilities.

Translated Abstract:
비주얼-언어 모델(Visual-Language Models, VLMs)은 시각적 질문 답변이나 이미지 캡션 생성 같은 작업에서 뛰어난 성능을 보였지만, 여전히 허위 정보 생성 문제에서 어려움을 겪고 있어. 이 모델의 주의 분포를 분석해보면, VLMs는 시각적 요소보다 텍스트 요소를 처리하는 경향이 있어. 이런 주의 분포의 불균형 때문에, VLM은 멀티모달 지식 충돌이 발생했을 때 텍스트 지식을 더 선호하게 되어서 이미지 정보와 차이가 생기게 돼.

이 논문에서는 주의 분포를 조정하기 위해 텍스트와 시각적 브랜치를 사용하는 재균형 대조 디코딩(Re-Balancing Contrastive Decoding, RBD) 방법을 제안해. 구체적으로, 텍스트 브랜치는 이미지 노이즈를 주입해서 모델이 텍스트에 의존하도록 자극하고, 이로 인해 텍스트 편향을 줄여. 동시에, 시각적 브랜치는 중요한 토큰을 선택하는 데 집중해서 주의 메커니즘을 개선하고 주요 주제를 강조해.

이런 두 가지 브랜치 전략 덕분에 RBD 방법은 텍스트 편향을 줄이면서 시각적 정보를 강화할 수 있어. 실험 결과를 보면, 우리 방법인 RBD가 CHAIR와 POPE 지표에서 기존 방법들보다 더 나은 성능을 보였고, 모델의 일반적인 능력을 줄이지 않으면서 허위 정보를 완화하는 데 성공했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06490.pdf

Title: UAVDB: Trajectory-Guided Adaptable Bounding Boxes for UAV Detection

Original Abstract:
With the rapid development of drone technology, accurate detection of Unmanned Aerial Vehicles (UAVs) has become essential for applications such as surveillance, security, and airspace management. In this paper, we propose a novel trajectory-guided method, the Patch Intensity Convergence (PIC) technique, which generates high-fidelity bounding boxes for UAV detection tasks and no need for the effort required for labeling. The PIC technique forms the foundation for developing UAVDB, a database explicitly created for UAV detection. Unlike existing datasets, which often use low-resolution footage or focus on UAVs in simple backgrounds, UAVDB employs high-resolution video to capture UAVs at various scales, ranging from hundreds of pixels to nearly single-digit sizes. This broad-scale variation enables comprehensive evaluation of detection algorithms across different UAV sizes and distances. Applying the PIC technique, we can also efficiently generate detection datasets from trajectory or positional data, even without size information. We extensively benchmark UAVDB using YOLOv8 series detectors, offering a detailed performance analysis. Our findings highlight UAVDB's potential as a vital database for advancing UAV detection, particularly in high-resolution and long-distance tracking scenarios.

Translated Abstract:
드론 기술이 급격히 발전하면서, 무인 항공기(UAV)를 정확하게 탐지하는 것이 감시, 보안, 공중 관리와 같은 분야에서 필수적이 되었어. 이 논문에서는 새로운 방법인 패치 강도 수렴(PIC) 기법을 제안하는데, 이 방법은 UAV 탐지 작업을 위한 고해상도 경계 상자를 생성하고, 라벨링에 필요한 노력을 줄여줘.

PIC 기법은 UAV 탐지를 위해 특별히 만들어진 UAVDB라는 데이터베이스를 개발하는 기초가 돼. 기존 데이터셋들은 저해상도 영상이나 단순한 배경에서 UAV에 집중하는 경우가 많았지만, UAVDB는 다양한 크기의 UAV를 고해상도 비디오로 캡처해. 이 크기는 수백 픽셀에서 거의 한 자리 숫자 크기까지 다양해. 이렇게 다양한 크기로 촬영하면, 서로 다른 UAV 크기와 거리에서 탐지 알고리즘을 종합적으로 평가할 수 있어.

PIC 기법을 사용하면, 크기 정보가 없어도 궤적이나 위치 데이터를 바탕으로 탐지 데이터셋을 효율적으로 생성할 수 있어. 우리는 YOLOv8 시리즈 탐지기를 사용해 UAVDB를 광범위하게 벤치마킹하고, 자세한 성능 분석을 제공했어. 우리의 연구 결과는 UAVDB가 고해상도 및 장거리 추적 상황에서 UAV 탐지를 발전시키는 데 중요한 데이터베이스로서의 잠재력을 강조해.

================================================================================

URL:
https://arxiv.org/pdf/2409.06493.pdf

Title: Elucidating Optimal Reward-Diversity Tradeoffs in Text-to-Image Diffusion Models

Original Abstract:
Text-to-image (T2I) diffusion models have become prominent tools for generating high-fidelity images from text prompts. However, when trained on unfiltered internet data, these models can produce unsafe, incorrect, or stylistically undesirable images that are not aligned with human preferences. To address this, recent approaches have incorporated human preference datasets to fine-tune T2I models or to optimize reward functions that capture these preferences. Although effective, these methods are vulnerable to reward hacking, where the model overfits to the reward function, leading to a loss of diversity in the generated images. In this paper, we prove the inevitability of reward hacking and study natural regularization techniques like KL divergence and LoRA scaling, and their limitations for diffusion models. We also introduce Annealed Importance Guidance (AIG), an inference-time regularization inspired by Annealed Importance Sampling, which retains the diversity of the base model while achieving Pareto-Optimal reward-diversity tradeoffs. Our experiments demonstrate the benefits of AIG for Stable Diffusion models, striking the optimal balance between reward optimization and image diversity. Furthermore, a user study confirms that AIG improves diversity and quality of generated images across different model architectures and reward functions.

Translated Abstract:
텍스트-이미지(T2I) 확산 모델은 텍스트 프롬프트로부터 고품질 이미지를 생성하는 데 중요한 도구가 되었어. 하지만 인터넷에서 필터링 없이 수집한 데이터를 통해 훈련된 모델은 사람의 선호와 맞지 않는 안전하지 않거나 잘못된, 또는 스타일적으로 바람직하지 않은 이미지를 만들 수 있어.

이에 대한 해결책으로 최근에는 인간의 선호 데이터를 활용해서 T2I 모델을 미세 조정하거나 이 선호를 반영한 보상 함수 최적화를 시도했어. 이런 방법들이 효과적이긴 하지만, 보상 해킹에 취약해. 보상 해킹은 모델이 보상 함수에 과적합되면서 생성된 이미지의 다양성이 줄어드는 현상이야.

이 논문에서는 보상 해킹이 불가피하다는 것을 증명하고, KL 발산이나 LoRA 스케일링 같은 자연스러운 정규화 기법과 그 한계에 대해 연구했어. 그리고 Annealed Importance Sampling에서 영감을 받은 Annealed Importance Guidance(AIG)라는 추론 시간 정규화 기법을 소개했어. 이 방법은 기본 모델의 다양성을 유지하면서 보상과 다양성의 최적 거래를 달성할 수 있어.

실험 결과, AIG가 Stable Diffusion 모델에서 보상 최적화와 이미지 다양성 간에 최적의 균형을 이룬다는 것을 보여줬어. 게다가 사용자 연구에서도 AIG가 다양한 모델 아키텍처와 보상 함수에서 생성된 이미지의 다양성과 품질을 향상시킨다는 것을 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06506.pdf

Title: Neural Laplacian Operator for 3D Point Clouds

Original Abstract:
The discrete Laplacian operator holds a crucial role in 3D geometry processing, yet it is still challenging to define it on point clouds. Previous works mainly focused on constructing a local triangulation around each point to approximate the underlying manifold for defining the Laplacian operator, which may not be robust or accurate. In contrast, we simply use the K-nearest neighbors (KNN) graph constructed from the input point cloud and learn the Laplacian operator on the KNN graph with graph neural networks (GNNs). However, the ground-truth Laplacian operator is defined on a manifold mesh with a different connectivity from the KNN graph and thus cannot be directly used for training. To train the GNN, we propose a novel training scheme by imitating the behavior of the ground-truth Laplacian operator on a set of probe functions so that the learned Laplacian operator behaves similarly to the ground-truth Laplacian operator. We train our network on a subset of ShapeNet and evaluate it across a variety of point clouds. Compared with previous methods, our method reduces the error by an order of magnitude and excels in handling sparse point clouds with thin structures or sharp features. Our method also demonstrates a strong generalization ability to unseen shapes. With our learned Laplacian operator, we further apply a series of Laplacian-based geometry processing algorithms directly to point clouds and achieve accurate results, enabling many exciting possibilities for geometry processing on point clouds. The code and trained models are available at this https URL.

Translated Abstract:
이산 라플라시안 연산자는 3D 기하학 처리에서 중요한 역할을 하는데, 점 구름에서 정의하기가 여전히 어렵다. 이전 연구들은 주로 각 점 주위에 로컬 삼각 분할을 만들어서 기본 매니폴드를 근사하는 데 집중했는데, 이 방식은 안정적이지 않거나 정확하지 않을 수 있다.

반면에 우리는 입력 점 구름에서 만든 K-최근접 이웃(KNN) 그래프를 단순히 사용하고, 그래프 신경망(GNN)을 통해 KNN 그래프에서 라플라시안 연산자를 학습한다. 하지만 실제 라플라시안 연산자는 KNN 그래프와 연결성이 다른 매니폴드 메시에서 정의되기 때문에, 직접적으로 학습에 사용할 수는 없다.

그래서 GNN을 훈련시키기 위해, 실제 라플라시안 연산자의 행동을 프로브 함수 집합에서 모방하는 새로운 훈련 방식을 제안한다. 이렇게 하면 학습된 라플라시안 연산자가 실제 라플라시안 연산자와 비슷하게 작동하도록 한다. 우리는 ShapeNet의 서브셋에서 네트워크를 훈련시키고, 다양한 점 구름에서 평가했다.

이전 방법들과 비교했을 때, 우리의 방법은 오류를 한 차원 줄이고, 얇은 구조나 뾰족한 특징이 있는 드문 점 구름을 처리하는 데 뛰어난 성능을 보인다. 또한, 우리의 방법은 보지 못한 형태에 대해서도 강력한 일반화 능력을 보여준다. 학습된 라플라시안 연산자를 활용해, 우리는 라플라시안 기반의 기하학 처리 알고리즘을 점 구름에 직접 적용하고 정확한 결과를 얻었다. 이로 인해 점 구름에서 기하학 처리를 위한 많은 흥미로운 가능성이 열렸다. 코드와 학습된 모델은 이 링크에서 확인할 수 있다.

================================================================================

URL:
https://arxiv.org/pdf/2409.06509.pdf

Title: Aligning Machine and Human Visual Representations across Abstraction Levels

Original Abstract:
Deep neural networks have achieved success across a wide range of applications, including as models of human behavior in vision tasks. However, neural network training and human learning differ in fundamental ways, and neural networks often fail to generalize as robustly as humans do, raising questions regarding the similarity of their underlying representations. What is missing for modern learning systems to exhibit more human-like behavior? We highlight a key misalignment between vision models and humans: whereas human conceptual knowledge is hierarchically organized from fine- to coarse-scale distinctions, model representations do not accurately capture all these levels of abstraction. To address this misalignment, we first train a teacher model to imitate human judgments, then transfer human-like structure from its representations into pretrained state-of-the-art vision foundation models. These human-aligned models more accurately approximate human behavior and uncertainty across a wide range of similarity tasks, including a new dataset of human judgments spanning multiple levels of semantic abstractions. They also perform better on a diverse set of machine learning tasks, increasing generalization and out-of-distribution robustness. Thus, infusing neural networks with additional human knowledge yields a best-of-both-worlds representation that is both more consistent with human cognition and more practically useful, thus paving the way toward more robust, interpretable, and human-like artificial intelligence systems.

Translated Abstract:
딥 뉴럴 네트워크는 다양한 분야에서 성공을 거두었고, 특히 시각 작업에서 인간 행동 모델로 사용되기도 해. 하지만, 뉴럴 네트워크의 학습 방식과 인간의 학습 방식은 근본적으로 다르고, 뉴럴 네트워크는 종종 인간처럼 잘 일반화하지 못해. 그래서 그들의 기본적인 표현 방식이 얼마나 유사한지에 대한 의문이 생겨. 현대 학습 시스템이 더 인간처럼 행동하기 위해선 뭘 보완해야 할까?

우리는 시각 모델과 인간 사이의 중요한 불일치를 강조해. 인간의 개념적 지식은 세밀한 구분에서부터 거친 구분으로 계층적으로 조직되어 있지만, 모델의 표현은 이런 모든 추상 수준을 정확하게 포착하지 못해. 이 불일치를 해결하기 위해, 먼저 인간의 판단을 모방하는 교사 모델을 훈련시키고, 그 모델의 표현에서 인간적인 구조를 최신의 비전 모델에 전이해. 이렇게 인간과 정렬된 모델은 다양한 유사성 작업에서 인간 행동과 불확실성을 더 정확히 근사해. 여기에는 여러 수준의 의미적 추상화를 포함한 인간 판단의 새로운 데이터셋도 포함돼.

이 모델들은 다양한 머신러닝 작업에서도 더 잘 수행되며, 일반화 능력과 분포 외 강인성을 높여줘. 그래서 뉴럴 네트워크에 추가적인 인간 지식을 주입하는 것은 인간의 인지와 더 일치하고 실용적으로도 유용한, 양쪽의 장점을 가진 표현 방식을 만들어내. 이는 더 강력하고 해석 가능하며 인간 같은 인공지능 시스템으로 나아가는 길을 열어줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.06520.pdf

Title: In Flight Boresight Rectification for Lightweight Airborne Pushbroom Imaging Spectrometry

Original Abstract:
Hyperspectral cameras have recently been miniaturized for operation on lightweight airborne platforms such as UAV or small aircraft. Unlike frame cameras (RGB or Multispectral), many hyperspectral sensors use a linear array or 'push-broom' scanning design. This design presents significant challenges for image rectification and the calibration of the intrinsic and extrinsic camera parameters. Typically, methods employed to address such tasks rely on a precise GPS/INS estimate of the airborne platform trajectory and a detailed terrain model. However, inaccuracies in the trajectory or surface model information can introduce systematic errors and complicate geometric modeling which ultimately degrade the quality of the rectification. To overcome these challenges, we propose a method for tie point extraction and camera calibration for 'push-broom' hyperspectral sensors using only the raw spectral imagery and raw, possibly low quality, GPS/INS trajectory. We demonstrate that our approach allows for the automatic calibration of airborne systems with hyperspectral cameras, outperforms other state-of-the-art automatic rectification methods and reaches an accuracy on par with manual calibration methods.

Translated Abstract:
하이퍼스펙트럼 카메라가 최근에 UAV나 작은 항공기 같은 가벼운 비행 플랫폼에서 사용할 수 있도록 작아졌어. 일반적인 프레임 카메라(RGB나 멀티스펙트럼)와는 달리, 많은 하이퍼스펙트럼 센서는 선형 배열 또는 '푸시-브룸' 스캐닝 디자인을 사용해. 이 디자인 때문에 이미지 보정이나 카메라의 내재적 및 외재적 파라미터를 조정하는 데 큰 도전이 있어.

보통 이런 작업을 해결하기 위해서는 비행 플랫폼의 궤적을 정확하게 파악할 수 있는 GPS/INS 추정치와 자세한 지형 모델이 필요해. 하지만 궤적이나 표면 모델 정보에 오류가 있으면 체계적인 오류가 생기고 기하학적 모델링이 복잡해져서 보정 품질이 떨어질 수 있어.

이런 문제를 해결하기 위해, 우리는 원시 스펙트럼 이미지와 저품질일 수도 있는 GPS/INS 궤적만으로 '푸시-브룸' 하이퍼스펙트럼 센서를 위한 tie point 추출 및 카메라 보정 방법을 제안해. 우리의 방법은 하이퍼스펙트럼 카메라가 장착된 비행 시스템의 자동 보정을 가능하게 하고, 다른 최신 자동 보정 방법보다 성능이 뛰어나며, 수동 보정 방법과 비슷한 정확도를 달성하는 것을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06535.pdf

Title: PoseEmbroider: Towards a 3D, Visual, Semantic-aware Human Pose Representation

Original Abstract:
Aligning multiple modalities in a latent space, such as images and texts, has shown to produce powerful semantic visual representations, fueling tasks like image captioning, text-to-image generation, or image grounding. In the context of human-centric vision, albeit CLIP-like representations encode most standard human poses relatively well (such as standing or sitting), they lack sufficient acuteness to discern detailed or uncommon ones. Actually, while 3D human poses have been often associated with images (e.g. to perform pose estimation or pose-conditioned image generation), or more recently with text (e.g. for text-to-pose generation), they have seldom been paired with both. In this work, we combine 3D poses, person's pictures and textual pose descriptions to produce an enhanced 3D-, visual- and semantic-aware human pose representation. We introduce a new transformer-based model, trained in a retrieval fashion, which can take as input any combination of the aforementioned modalities. When composing modalities, it outperforms a standard multi-modal alignment retrieval model, making it possible to sort out partial information (e.g. image with the lower body occluded). We showcase the potential of such an embroidered pose representation for (1) SMPL regression from image with optional text cue; and (2) on the task of fine-grained instruction generation, which consists in generating a text that describes how to move from one 3D pose to another (as a fitness coach). Unlike prior works, our model can take any kind of input (image and/or pose) without retraining.

Translated Abstract:
여러 가지 형태의 데이터를 하나의 공간에 맞추는 것은 이미지와 텍스트 같은 것들이 강력한 의미론적 시각 표현을 만들어내는 데 효과적이었어. 이로 인해 이미지 캡셔닝, 텍스트-이미지 생성, 이미지 그라운딩 같은 작업들이 가능해졌지. 

인간 중심의 비전에서는 CLIP 같은 표현이 일반적인 인간의 자세(예: 서있거나 앉아있는 모습)는 꽤 잘 인식하지만, 자세히 보거나 흔치 않은 자세를 구별하는 데는 부족해. 실제로 3D 인간 자세는 보통 이미지를 통해 연결되거나(예: 자세 추정이나 자세 기반 이미지 생성), 최근에는 텍스트와 연결되기도 했지만, 두 가지를 함께 사용하는 경우는 드물었어.

이번 연구에서는 3D 자세, 사람의 사진, 텍스트로 된 자세 설명을 결합해서 더 나은 3D, 시각적, 의미적으로 인식할 수 있는 인간 자세 표현을 만들었어. 우리는 새로운 변환기 기반 모델을 도입했는데, 이 모델은 위에서 언급한 여러 형태의 데이터를 조합해서 사용할 수 있어. 

이 모델은 기존의 멀티 모달 정렬 검색 모델보다 성능이 좋고, 부분 정보(예: 하체가 가려진 이미지)를 정리할 수 있는 가능성을 보여줘. 우리는 이러한 복합적인 자세 표현이 (1) 선택적인 텍스트 힌트를 가진 이미지에서 SMPL 회귀를 수행하는 데; (2) 한 3D 자세에서 다른 3D 자세로 이동하는 방법을 설명하는 텍스트를 생성하는 미세한 지시 생성 작업에 어떻게 활용될 수 있는지를 보여줬어. 

이전 연구들과 달리, 우리의 모델은 재훈련 없이 어떤 종류의 입력(이미지 및/또는 자세)을 받아들일 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06579.pdf

Title: Quantifying and Enabling the Interpretability of CLIP-like Models

Original Abstract:
CLIP is one of the most popular foundational models and is heavily used for many vision-language tasks. However, little is known about the inner workings of CLIP. To bridge this gap we propose a study to quantify the interpretability in CLIP like models. We conduct this study on six different CLIP models from OpenAI and OpenCLIP which vary by size, type of pre-training data and patch size. Our approach begins with using the TEXTSPAN algorithm and in-context learning to break down individual attention heads into specific properties. We then evaluate how easily these heads can be interpreted using new metrics which measure property consistency within heads and property disentanglement across heads. Our findings reveal that larger CLIP models are generally more interpretable than their smaller counterparts. To further assist users in understanding the inner workings of CLIP models, we introduce CLIP-InterpreT, a tool designed for interpretability analysis. CLIP-InterpreT offers five types of analyses: property-based nearest neighbor search, per-head topic segmentation, contrastive segmentation, per-head nearest neighbors of an image, and per-head nearest neighbors of text.

Translated Abstract:
CLIP는 요즘 가장 인기 있는 기본 모델 중 하나로, 많은 비전-언어 작업에 많이 사용돼. 하지만 CLIP의 내부 작동 방식에 대해서는 잘 알려진 게 없어. 이 문제를 해결하기 위해 우리는 CLIP 같은 모델의 해석 가능성을 측정하는 연구를 제안해. 

우리는 OpenAI와 OpenCLIP에서 제공하는 여섯 가지 다른 CLIP 모델을 대상으로 연구를 진행했어. 이 모델들은 크기, 사전 학습 데이터의 종류, 패치 크기 등에서 차이가 나. 우리의 접근 방식은 TEXTSPAN 알고리즘과 인-컨텍스트 학습을 이용해 개별 주의 헤드를 특정 속성으로 나누는 거야. 그런 다음, 새로운 지표를 사용해 이 헤드들이 얼마나 쉽게 해석될 수 있는지 평가했어. 이 지표는 헤드 내 속성 일관성과 헤드 간 속성 분리를 측정해.

우리의 연구 결과는 더 큰 CLIP 모델이 일반적으로 작은 모델보다 더 해석 가능하다는 걸 보여줘. 사용자가 CLIP 모델의 내부 작동 방식을 이해하는 데 도움을 주기 위해, 우리는 해석 가능성 분석을 위한 도구인 CLIP-InterpreT를 소개해. 

CLIP-InterpreT는 다섯 가지 종류의 분석을 제공해: 속성 기반 최근접 이웃 검색, 헤드별 주제 분할, 대조 분할, 이미지에 대한 헤드별 최근접 이웃, 텍스트에 대한 헤드별 최근접 이웃.

================================================================================

URL:
https://arxiv.org/pdf/2409.06583.pdf

Title: Semi-Supervised 3D Object Detection with Chanel Augmentation using Transformation Equivariance

Original Abstract:
Accurate 3D object detection is crucial for autonomous vehicles and robots to navigate and interact with the environment safely and effectively. Meanwhile, the performance of 3D detector relies on the data size and annotation which is expensive. Consequently, the demand of training with limited labeled data is growing. We explore a novel teacher-student framework employing channel augmentation for 3D semi-supervised object detection. The teacher-student SSL typically adopts a weak augmentation and strong augmentation to teacher and student, respectively. In this work, we apply multiple channel augmentations to both networks using the transformation equivariance detector (TED). The TED allows us to explore different combinations of augmentation on point clouds and efficiently aggregates multi-channel transformation equivariance features. In principle, by adopting fixed channel augmentations for the teacher network, the student can train stably on reliable pseudo-labels. Adopting strong channel augmentations can enrich the diversity of data, fostering robustness to transformations and enhancing generalization performance of the student network. We use SOTA hierarchical supervision as a baseline and adapt its dual-threshold to TED, which is called channel IoU consistency. We evaluate our method with KITTI dataset, and achieved a significant performance leap, surpassing SOTA 3D semi-supervised object detection models.

Translated Abstract:
정확한 3D 물체 감지는 자율주행차와 로봇이 환경을 안전하고 효과적으로 탐색하고 상호작용하는 데 매우 중요해. 그런데 3D 탐지기의 성능은 데이터 크기와 주석에 의존하는데, 이건 비용이 많이 들어. 그래서 제한된 라벨이 있는 데이터로 훈련하려는 수요가 점점 커지고 있어.

우리는 3D 반자동 물체 감지를 위해 채널 증강을 사용하는 새로운 교사-학생 프레임워크를 살펴봤어. 일반적으로 교사-학생 SSL은 각각 약한 증강과 강한 증강을 사용해. 이번 연구에서는 변환 등가 탐지기(TED)를 사용해서 두 네트워크 모두에 여러 채널 증강을 적용했어. TED는 포인트 클라우드에서 다양한 증강 조합을 탐색할 수 있게 해주고, 다채널 변환 등가 특성을 효율적으로 집계할 수 있어.

원칙적으로, 교사 네트워크에 고정된 채널 증강을 적용하면 학생 네트워크는 신뢰할 수 있는 가짜 라벨로 안정적으로 훈련할 수 있어. 강한 채널 증강을 사용하면 데이터의 다양성이 풍부해져서 변환에 대한 강인함을 키우고, 학생 네트워크의 일반화 성능도 향상시킬 수 있어. 우리는 SOTA 계층적 감독을 기준으로 삼고, 이중 임계값을 TED에 적응시켰는데, 이를 채널 IoU 일관성이라고 불러.

우리 방법을 KITTI 데이터셋으로 평가해봤고, SOTA 3D 반자동 물체 감지 모델들을 능가하는 성능 향상을 이뤘어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06584.pdf

Title: Transtreaming: Adaptive Delay-aware Transformer for Real-time Streaming Perception

Original Abstract:
Real-time object detection is critical for the decision-making process for many real-world applications, such as collision avoidance and path planning in autonomous driving. This work presents an innovative real-time streaming perception method, Transtreaming, which addresses the challenge of real-time object detection with dynamic computational delay. The core innovation of Transtreaming lies in its adaptive delay-aware transformer, which can concurrently predict multiple future frames and select the output that best matches the real-world present time, compensating for any system-induced computation delays. The proposed model outperforms the existing state-of-the-art methods, even in single-frame detection scenarios, by leveraging a transformer-based methodology. It demonstrates robust performance across a range of devices, from powerful V100 to modest 2080Ti, achieving the highest level of perceptual accuracy on all platforms. Unlike most state-of-the-art methods that struggle to complete computation within a single frame on less powerful devices, Transtreaming meets the stringent real-time processing requirements on all kinds of devices. The experimental results emphasize the system's adaptability and its potential to significantly improve the safety and reliability for many real-world systems, such as autonomous driving.

Translated Abstract:
실시간 물체 감지는 자율 주행에서 충돌 회피나 경로 계획 같은 많은 실제 응용 프로그램에서 의사 결정에 정말 중요해. 이 연구는 Transtreaming이라는 혁신적인 실시간 스트리밍 인식 방법을 제안해, 동적인 계산 지연 문제를 해결하는 데 초점을 맞추고 있어.

Transtreaming의 핵심 혁신은 적응형 지연 인식 변환기야. 이 변환기는 여러 미래 프레임을 동시에 예측하고, 현재 시간에 가장 잘 맞는 출력을 선택해서 시스템에서 발생하는 계산 지연을 보완해. 이 모델은 단일 프레임 감지에서도 기존의 최첨단 방법보다 더 나은 성능을 보여줘. 변환기 기반 방법론을 활용해서 모든 플랫폼에서 인식 정확도를 최고 수준으로 달성하고 있어.

대부분의 최첨단 방법들은 성능이 약한 장치에서 단일 프레임 내에 계산을 완료하는 데 어려움을 겪는데, Transtreaming은 모든 종류의 장치에서 엄격한 실시간 처리 요구 사항을 충족해. 실험 결과는 이 시스템의 적응성뿐만 아니라 자율 주행 같은 많은 실제 시스템의 안전성과 신뢰성을 크게 향상시킬 수 있는 잠재력을 강조하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06589.pdf

Title: Seg-HGNN: Unsupervised and Light-Weight Image Segmentation with Hyperbolic Graph Neural Networks

Original Abstract:
Image analysis in the euclidean space through linear hyperspaces is well studied. However, in the quest for more effective image representations, we turn to hyperbolic manifolds. They provide a compelling alternative to capture complex hierarchical relationships in images with remarkably small dimensionality. To demonstrate hyperbolic embeddings' competence, we introduce a light-weight hyperbolic graph neural network for image segmentation, encompassing patch-level features in a very small embedding size. Our solution, Seg-HGNN, surpasses the current best unsupervised method by 2.5\%, 4\% on VOC-07, VOC-12 for localization, and by 0.8\%, 1.3\% on CUB-200, ECSSD for segmentation, respectively. With less than 7.5k trainable parameters, Seg-HGNN delivers effective and fast ($\approx 2$ images/second) results on very standard GPUs like the GTX1650. This empirical evaluation presents compelling evidence of the efficacy and potential of hyperbolic representations for vision tasks.

Translated Abstract:
유클리드 공간에서의 이미지 분석은 잘 연구되어 왔어. 하지만 더 효과적인 이미지 표현을 찾기 위해 우리는 쌍곡면을 살펴보게 되었어. 쌍곡면은 이미지 안의 복잡한 계층 관계를 아주 작은 차원으로 잘 포착할 수 있는 매력적인 대안이야.

이번 연구에서는 쌍곡 임베딩의 능력을 보여주기 위해 이미지 분할을 위한 경량 쌍곡 그래프 신경망을 소개할 거야. 이 네트워크는 아주 작은 임베딩 크기로 패치 수준의 특징을 포함하고 있어. 우리의 솔루션인 Seg-HGNN은 현재 가장 좋은 비지도 학습 방법보다 VOC-07과 VOC-12에서 각각 2.5%와 4% 더 나은 성능을 보여주고, CUB-200과 ECSSD에서는 각각 0.8%와 1.3% 더 뛰어난 성과를 냈어.

Seg-HGNN은 7.5k 미만의 학습 가능한 파라미터로, GTX1650 같은 일반적인 GPU에서 약 2장의 이미지를 초당 처리하는 효과적이고 빠른 결과를 제공해. 이 실증적 평가는 비전 작업에서 쌍곡 표현의 효과와 잠재력을 강하게 입증해.

================================================================================

URL:
https://arxiv.org/pdf/2409.06590.pdf

Title: Lightweight Multiscale Feature Fusion Super-Resolution Network Based on Two-branch Convolution and Transformer

Original Abstract:
The single image super-resolution(SISR) algorithms under deep learning currently have two main models, one based on convolutional neural networks and the other based on Transformer. The former uses the stacking of convolutional layers with different convolutional kernel sizes to design the model, which enables the model to better extract the local features of the image; the latter uses the self-attention mechanism to design the model, which allows the model to establish long-distance dependencies between image pixel points through the self-attention mechanism and then better extract the global features of the image. However, both of the above methods face their problems. Based on this, this paper proposes a new lightweight multi-scale feature fusion network model based on two-way complementary convolutional and Transformer, which integrates the respective features of Transformer and convolutional neural networks through a two-branch network architecture, to realize the mutual fusion of global and local information. Meanwhile, considering the partial loss of information caused by the low-pixel images trained by the deep neural network, this paper designs a modular connection method of multi-stage feature supplementation to fuse the feature maps extracted from the shallow stage of the model with those extracted from the deep stage of the model, to minimize the loss of the information in the feature images that is beneficial to the image restoration as much as possible, to facilitate the obtaining of a higher-quality restored image. The practical results finally show that the model proposed in this paper is optimal in image recovery performance when compared with other lightweight models with the same amount of parameters.

Translated Abstract:
단일 이미지 초해상도(SISR) 알고리즘은 현재 딥러닝 기반으로 두 가지 주요 모델이 있어. 하나는 합성곱 신경망(CNN) 기반이고, 다른 하나는 트랜스포머 기반이야. 

합성곱 신경망은 서로 다른 크기의 합성곱 커널을 쌓아서 모델을 설계하는데, 이 덕분에 이미지의 지역적인 특징을 더 잘 추출할 수 있어. 반면, 트랜스포머는 자기 주의 메커니즘을 사용해서 모델을 설계하는데, 이를 통해 이미지의 픽셀 간의 장거리 의존성을 잘 설정할 수 있어. 그래서 글로벌 특징을 더 잘 추출할 수 있게 돼. 

하지만 두 방법 모두 문제점이 있어. 그래서 이 논문에서는 양방향 상호 보완적인 합성곱과 트랜스포머를 기반으로 한 새로운 경량 다중 스케일 특징 융합 네트워크 모델을 제안해. 이 모델은 두 가지 특징을 두 개의 분기 네트워크 구조로 통합해서 글로벌 정보와 로컬 정보를 서로 융합할 수 있도록 해. 

또한, 딥 뉴럴 네트워크로 훈련된 저해상도 이미지에서 정보가 일부 손실되는 것을 고려해서, 이 논문에서는 다단계 특징 보완을 위한 모듈 연결 방법을 설계했어. 이를 통해 모델의 얕은 단계에서 추출된 특징 맵과 깊은 단계에서 추출된 특징 맵을 융합해서 이미지 복원에 도움이 되는 정보 손실을 최소화하려고 해. 이렇게 하면 더 높은 품질의 복원 이미지를 얻는 데 도움이 돼. 

마지막으로, 이 논문에서 제안한 모델은 같은 양의 파라미터를 가진 다른 경량 모델들과 비교했을 때 이미지 복원 성능이 최적이라는 것을 실제 결과로 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.06603.pdf

Title: A Practical Gated Recurrent Transformer Network Incorporating Multiple Fusions for Video Denoising

Original Abstract:
State-of-the-art (SOTA) video denoising methods employ multi-frame simultaneous denoising mechanisms, resulting in significant delays (e.g., 16 frames), making them impractical for real-time cameras. To overcome this limitation, we propose a multi-fusion gated recurrent Transformer network (GRTN) that achieves SOTA denoising performance with only a single-frame delay. Specifically, the spatial denoising module extracts features from the current frame, while the reset gate selects relevant information from the previous frame and fuses it with current frame features via the temporal denoising module. The update gate then further blends this result with the previous frame features, and the reconstruction module integrates it with the current frame. To robustly compute attention for noisy features, we propose a residual simplified Swin Transformer with Euclidean distance (RSSTE) in the spatial and temporal denoising modules. Comparative objective and subjective results show that our GRTN achieves denoising performance comparable to SOTA multi-frame delay networks, with only a single-frame delay.

Translated Abstract:
최신 비디오 노이즈 제거 방법들은 여러 프레임을 동시에 처리하는 방식을 사용하는데, 이로 인해 16프레임처럼 큰 지연이 발생해 실시간 카메라에는 사용하기 어려워. 이 문제를 해결하기 위해, 우리는 단일 프레임 지연으로 SOTA 수준의 노이즈 제거 성능을 달성하는 멀티퓨전 게이트 순환 변환기 네트워크(GRTN)를 제안해.

구체적으로, 공간 노이즈 제거 모듈은 현재 프레임에서 특징을 추출하고, 리셋 게이트는 이전 프레임에서 관련 정보를 선택해 현재 프레임의 특징과 결합해. 그런 다음 업데이트 게이트가 이 결과를 이전 프레임의 특징과 더 섞고, 재구성 모듈이 이 정보를 현재 프레임과 통합해.

노이즈가 있는 특징에 대한 주의를 강하게 계산하기 위해, 우리는 공간 및 시간 노이즈 제거 모듈에서 유클리드 거리를 이용한 잔여 간소화 스윈 변환기(RSSTE)를 제안해. 객관적이고 주관적인 비교 결과에 따르면, 우리의 GRTN은 단일 프레임 지연만으로 SOTA 다중 프레임 지연 네트워크와 비슷한 노이즈 제거 성능을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.06609.pdf

Title: Improving the Precision of CNNs for Magnetic Resonance Spectral Modeling

Original Abstract:
Magnetic resonance spectroscopic imaging is a widely available imaging modality that can non-invasively provide a metabolic profile of the tissue of interest, yet is challenging to integrate clinically. One major reason is the expensive, expert data processing and analysis that is required. Using machine learning to predict MRS-related quantities offers avenues around this problem, but deep learning models bring their own challenges, especially model trust. Current research trends focus primarily on mean error metrics, but comprehensive precision metrics are also needed, e.g. standard deviations, confidence intervals, etc.. This work highlights why more comprehensive error characterization is important and how to improve the precision of CNNs for spectral modeling, a quantitative task. The results highlight advantages and trade-offs of these techniques that should be considered when addressing such regression tasks with CNNs. Detailed insights into the underlying mechanisms of each technique, and how they interact with other techniques, are discussed in depth.

Translated Abstract:
자기 공명 분광 이미징(MRSI)은 비침습적으로 관심 있는 조직의 대사 프로필을 제공할 수 있는 널리 사용되는 이미징 방법이야. 그런데 임상에서 적용하기는 쉽지 않아. 그 이유 중 하나는 비싸고 전문적인 데이터 처리와 분석이 필요하기 때문이야.

기계 학습을 사용해 MRS 관련 수치를 예측하면 이 문제를 해결할 수 있는 방법이 생기지만, 딥러닝 모델은 신뢰성 문제 같은 다른 도전 과제를 가지고 있어. 현재 연구는 주로 평균 오차 지표에 초점을 맞추고 있지만, 표준 편차나 신뢰 구간 같은 더 포괄적인 정밀도 지표도 필요해.

이 연구는 왜 더 포괄적인 오차 특성이 중요한지, 그리고 스펙트럼 모델링을 위한 CNN의 정밀도를 어떻게 향상시킬 수 있는지를 강조해. 결과는 이런 기술의 장점과 단점을 보여줬고, CNN을 사용해 회귀 작업을 해결할 때 고려해야 할 점들을 다뤘어. 각 기술의 기본 메커니즘과 다른 기술들과의 상호작용에 대한 자세한 통찰도 깊이 있게 논의했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06617.pdf

Title: When to Extract ReID Features: A Selective Approach for Improved Multiple Object Tracking

Original Abstract:
Extracting and matching Re-Identification (ReID) features is used by many state-of-the-art (SOTA) Multiple Object Tracking (MOT) methods, particularly effective against frequent and long-term occlusions. While end-to-end object detection and tracking have been the main focus of recent research, they have yet to outperform traditional methods in benchmarks like MOT17 and MOT20. Thus, from an application standpoint, methods with separate detection and embedding remain the best option for accuracy, modularity, and ease of implementation, though they are impractical for edge devices due to the overhead involved. In this paper, we investigate a selective approach to minimize the overhead of feature extraction while preserving accuracy, modularity, and ease of implementation. This approach can be integrated into various SOTA methods. We demonstrate its effectiveness by applying it to StrongSORT and Deep OC-SORT. Experiments on MOT17, MOT20, and DanceTrack datasets show that our mechanism retains the advantages of feature extraction during occlusions while significantly reducing runtime. Additionally, it improves accuracy by preventing confusion in the feature-matching stage, particularly in cases of deformation and appearance similarity, which are common in DanceTrack. this https URL, this https URL

Translated Abstract:
다중 객체 추적(MOT) 방법 중 많은 최신 기술들이 재식별(ReID) 특징을 추출하고 매칭하는 방식을 사용해. 이 방법은 자주 발생하는 가림 현상과 장기적인 가림에 특히 효과적이야. 최근 연구들은 종단 간 객체 감지와 추적에 주력했지만, MOT17과 MOT20 같은 벤치마크에서는 여전히 전통적인 방법을 이기지 못하고 있어. 그래서 실제 적용 측면에서는 감지와 임베딩이 분리된 방법이 정확성, 모듈성, 구현의 용이성 면에서 가장 좋은 선택이야. 하지만 이런 방법은 오버헤드 때문에 엣지 디바이스에는 실용적이지 않아.

이 논문에서는 정확성, 모듈성, 구현의 용이성을 유지하면서 특징 추출의 오버헤드를 최소화하는 선택적 접근 방식을 조사해. 이 방식을 다양한 최신 기술에 통합할 수 있어. StrongSORT와 Deep OC-SORT에 적용해 효과를 보여줬어. MOT17, MOT20, DanceTrack 데이터셋에서 실험한 결과, 우리의 메커니즘이 가림 현상 동안 특징 추출의 장점을 유지하면서 실행 시간을 크게 줄였어. 게다가, DanceTrack에서 흔히 발생하는 변형과 외형 유사성 문제로 인한 혼란을 방지해서 정확성을 향상시켰어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06618.pdf

Title: Hierarchical Multi-Label Classification with Missing Information for Benthic Habitat Imagery

Original Abstract:
In this work, we apply state-of-the-art self-supervised learning techniques on a large dataset of seafloor imagery, \textit{BenthicNet}, and study their performance for a complex hierarchical multi-label (HML) classification downstream task. In particular, we demonstrate the capacity to conduct HML training in scenarios where there exist multiple levels of missing annotation information, an important scenario for handling heterogeneous real-world data collected by multiple research groups with differing data collection protocols. We find that, when using smaller one-hot image label datasets typical of local or regional scale benthic science projects, models pre-trained with self-supervision on a larger collection of in-domain benthic data outperform models pre-trained on ImageNet. In the HML setting, we find the model can attain a deeper and more precise classification if it is pre-trained with self-supervision on in-domain data. We hope this work can establish a benchmark for future models in the field of automated underwater image annotation tasks and can guide work in other domains with hierarchical annotations of mixed resolution.

Translated Abstract:
이번 연구에서는 최신 자기 지도 학습 기법을 사용해 해저 이미지 데이터셋인 \textit{BenthicNet}에 적용하고, 복잡한 계층 다중 레이블(HML) 분류 작업에서의 성능을 살펴봤어. 특히, 여러 레벨의 주석 정보가 누락된 상황에서도 HML 학습을 진행할 수 있는 능력을 보여줬어. 이건 여러 연구 그룹이 서로 다른 데이터 수집 방식으로 모은 비슷한 현실 세계 데이터를 처리할 때 중요해.

우리가 발견한 건, 지역적 또는 지역적 규모의 저서 생태학 프로젝트에서 일반적으로 사용하는 작은 원-핫 이미지 레이블 데이터셋을 사용할 때, 더 큰 도메인 내 저서 데이터로 자기 지도 학습을 통해 사전 훈련된 모델이 ImageNet으로 사전 훈련된 모델보다 성능이 더 좋다는 거야. HML 설정에서는 도메인 내 데이터로 자기 지도 학습을 통해 사전 훈련된 모델이 더 깊고 정확한 분류를 할 수 있다는 것도 알게 됐어.

이 연구가 자동화된 수중 이미지 주석 작업 분야에서 미래 모델의 기준을 세우고, 혼합 해상도의 계층 주석이 있는 다른 도메인에서도 도움이 되기를 바래.

================================================================================

URL:
https://arxiv.org/pdf/2409.06620.pdf

Title: MVGaussian: High-Fidelity text-to-3D Content Generation with Multi-View Guidance and Surface Densification

Original Abstract:
The field of text-to-3D content generation has made significant progress in generating realistic 3D objects, with existing methodologies like Score Distillation Sampling (SDS) offering promising guidance. However, these methods often encounter the "Janus" problem-multi-face ambiguities due to imprecise guidance. Additionally, while recent advancements in 3D gaussian splitting have shown its efficacy in representing 3D volumes, optimization of this representation remains largely unexplored. This paper introduces a unified framework for text-to-3D content generation that addresses these critical gaps. Our approach utilizes multi-view guidance to iteratively form the structure of the 3D model, progressively enhancing detail and accuracy. We also introduce a novel densification algorithm that aligns gaussians close to the surface, optimizing the structural integrity and fidelity of the generated models. Extensive experiments validate our approach, demonstrating that it produces high-quality visual outputs with minimal time cost. Notably, our method achieves high-quality results within half an hour of training, offering a substantial efficiency gain over most existing methods, which require hours of training time to achieve comparable results.

Translated Abstract:
텍스트를 3D 콘텐츠로 변환하는 분야에서 현실적인 3D 객체를 생성하는 데 큰 발전이 있었어. 기존의 방법들, 예를 들어 Score Distillation Sampling (SDS) 같은 것들이 괜찮은 방향을 제시해주고 있어. 하지만 이런 방법들은 종종 "Janus" 문제에 부딪히는데, 이건 여러 얼굴의 모호함 때문에 생기는 문제야. 그리고 최근에 3D 가우시안 분할 기술이 3D 부피를 표현하는 데 효과적이라는 게 밝혀졌지만, 이 표현을 최적화하는 건 여전히 많이 연구되지 않았어.

이 논문에서는 텍스트를 3D 콘텐츠로 변환하는 통합 프레임워크를 소개해. 이 프레임워크는 이런 중요한 문제들을 해결해. 우리의 접근 방식은 다중 시점 지침을 활용해서 3D 모델의 구조를 반복적으로 형성하고, 점차적으로 세부사항과 정확도를 높여. 또한, 표면에 가까운 가우시안을 정렬하는 새로운 밀도화 알고리즘도 도입했어. 이걸 통해 생성된 모델의 구조적 완전성과 충실도를 최적화할 수 있어.

우리는 많은 실험을 통해 우리의 접근 방식을 검증했어. 결과적으로 높은 품질의 비주얼 출력을 최소한의 시간 비용으로 생성할 수 있음을 보여줬어. 특히, 우리의 방법은 훈련하는 데 단 30분 안에 고품질 결과를 얻을 수 있어서, 기존의 방법들보다 효율성이 크게 향상됐어. 기존 방법들은 비슷한 결과를 얻기 위해 몇 시간의 훈련 시간이 필요했거든.

================================================================================

URL:
https://arxiv.org/pdf/2409.06625.pdf

Title: Towards Localizing Structural Elements: Merging Geometrical Detection with Semantic Verification in RGB-D Data

Original Abstract:
RGB-D cameras supply rich and dense visual and spatial information for various robotics tasks such as scene understanding, map reconstruction, and localization. Integrating depth and visual information can aid robots in localization and element mapping, advancing applications like 3D scene graph generation and Visual Simultaneous Localization and Mapping (VSLAM). While point cloud data containing such information is primarily used for enhanced scene understanding, exploiting their potential to capture and represent rich semantic information has yet to be adequately targeted. This paper presents a real-time pipeline for localizing building components, including wall and ground surfaces, by integrating geometric calculations for pure 3D plane detection followed by validating their semantic category using point cloud data from RGB-D cameras. It has a parallel multi-thread architecture to precisely estimate poses and equations of all the planes detected in the environment, filters the ones forming the map structure using a panoptic segmentation validation, and keeps only the validated building components. Incorporating the proposed method into a VSLAM framework confirmed that constraining the map with the detected environment-driven semantic elements can improve scene understanding and map reconstruction accuracy. It can also ensure (re-)association of these detected components into a unified 3D scene graph, bridging the gap between geometric accuracy and semantic understanding. Additionally, the pipeline allows for the detection of potential higher-level structural entities, such as rooms, by identifying the relationships between building components based on their layout.

Translated Abstract:
RGB-D 카메라는 장면 이해, 지도 재구성, 위치 추정 같은 다양한 로봇 작업을 위해 풍부하고 밀집된 시각적 및 공간 정보를 제공해. 깊이 정보와 시각 정보를 통합하면 로봇이 위치를 파악하고 요소를 맵핑하는 데 도움이 되는데, 이건 3D 장면 그래프 생성이나 Visual Simultaneous Localization and Mapping (VSLAM) 같은 응용 분야를 발전시켜.

하지만 이런 정보를 담고 있는 포인트 클라우드 데이터는 주로 장면 이해를 향상시키기 위해 사용되고, 그 잠재력을 활용해 풍부한 의미 정보를 캡처하고 표현하는 건 아직 충분히 다뤄지지 않았어. 이 논문은 순수 3D 평면 감지를 위한 기하학적 계산을 통합하고, RGB-D 카메라로부터 얻은 포인트 클라우드 데이터를 활용해 의미 카테고리를 검증하는 실시간 파이프라인을 제안해.

이 시스템은 환경에서 감지된 모든 평면의 자세와 방정식을 정확히 추정하기 위해 병렬 멀티스레드 구조를 가지고 있어. 그리고 맵 구조를 형성하는 평면들만 필터링해서, 검증된 건축 요소들만 유지해. 제안된 방법을 VSLAM 프레임워크에 통합해 보니, 감지된 환경 기반의 의미 요소로 맵을 제약하면 장면 이해와 맵 재구성 정확도를 높일 수 있다는 걸 확인했어.

게다가 이 파이프라인은 건축 요소 간의 관계를 파악해 방 같은 더 높은 수준의 구조적 엔티티를 감지할 수 있도록 해. 이렇게 하면 기하학적 정확성과 의미적 이해 사이의 간극을 메울 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06633.pdf

Title: SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation

Original Abstract:
In recent years, the development of diffusion models has led to significant progress in image and video generation tasks, with pre-trained models like the Stable Diffusion series playing a crucial role. Inspired by model pruning which lightens large pre-trained models by removing unimportant parameters, we propose a novel model fine-tuning method to make full use of these ineffective parameters and enable the pre-trained model with new task-specified capabilities. In this work, we first investigate the importance of parameters in pre-trained diffusion models, and discover that the smallest 10% to 20% of parameters by absolute values do not contribute to the generation process. Based on this observation, we propose a method termed SaRA that re-utilizes these temporarily ineffective parameters, equating to optimizing a sparse weight matrix to learn the task-specific knowledge. To mitigate overfitting, we propose a nuclear-norm-based low-rank sparse training scheme for efficient fine-tuning. Furthermore, we design a new progressive parameter adjustment strategy to make full use of the re-trained/finetuned parameters. Finally, we propose a novel unstructural backpropagation strategy, which significantly reduces memory costs during fine-tuning. Our method enhances the generative capabilities of pre-trained models in downstream applications and outperforms traditional fine-tuning methods like LoRA in maintaining model's generalization ability. We validate our approach through fine-tuning experiments on SD models, demonstrating significant improvements. SaRA also offers a practical advantage that requires only a single line of code modification for efficient implementation and is seamlessly compatible with existing methods.

Translated Abstract:
최근 몇 년 동안, 확산 모델의 발전 덕분에 이미지와 비디오 생성 작업에서 큰 발전이 있었어. 특히 Stable Diffusion 시리즈 같은 사전 훈련된 모델이 중요한 역할을 했지. 우리는 모델 프루닝에서 영감을 얻었는데, 이건 큰 사전 훈련 모델에서 중요하지 않은 매개변수를 제거해 가볍게 만드는 방법이야. 그래서 우리는 이 비효율적인 매개변수를 최대한 활용하고, 사전 훈련된 모델을 새로운 작업에 맞게 조정하는 새로운 방법을 제안해.

이 연구에서는 먼저 사전 훈련된 확산 모델의 매개변수 중요성을 조사했어. 그 결과, 절대값 기준으로 가장 작은 10%에서 20%의 매개변수는 생성 과정에 기여하지 않는다는 걸 발견했지. 이 관찰에 기반해서, 우리는 SaRA라는 방법을 제안했어. 이 방법은 일시적으로 비효율적인 매개변수를 다시 활용해서, 작업에 특화된 지식을 학습하는 희소 가중치 행렬을 최적화하는 거야.

과적합을 줄이기 위해, 우리는 효율적인 미세 조정을 위한 핵 노름 기반의 저랭크 희소 훈련 방식을 제안했어. 게다가, 재훈련/미세 조정된 매개변수를 최대한 활용하기 위한 새로운 점진적 매개변수 조정 전략도 설계했어. 마지막으로, 메모리 비용을 크게 줄이는 새로운 비구조적 역전파 전략도 제안했어. 우리 방법은 하위 응용 프로그램에서 사전 훈련된 모델의 생성 능력을 향상시키고, 모델의 일반화 능력을 유지하는 데 있어 전통적인 미세 조정 방법인 LoRA보다 성능이 더 좋아.

우리는 SD 모델에 대한 미세 조정 실험을 통해 우리의 접근 방식을 검증했으며, 상당한 개선을 보여줬어. SaRA는 효율적인 구현을 위해 단 한 줄의 코드 수정을 요구하는 실용적인 장점도 있고, 기존 방법들과도 원활하게 호환돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.06644.pdf

Title: EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis

Original Abstract:
Early detection of eye diseases like glaucoma, macular degeneration, and diabetic retinopathy is crucial for preventing vision loss. While artificial intelligence (AI) foundation models hold significant promise for addressing these challenges, existing ophthalmic foundation models primarily focus on a single modality, whereas diagnosing eye diseases requires multiple modalities. A critical yet often overlooked aspect is harnessing the multi-view information across various modalities for the same patient. Additionally, due to the long-tail nature of ophthalmic diseases, standard fully supervised or unsupervised learning approaches often struggle. Therefore, it is essential to integrate clinical text to capture a broader spectrum of diseases. We propose EyeCLIP, a visual-language foundation model developed using over 2.77 million multi-modal ophthalmology images with partial text data. To fully leverage the large multi-modal unlabeled and labeled data, we introduced a pretraining strategy that combines self-supervised reconstructions, multi-modal image contrastive learning, and image-text contrastive learning to learn a shared representation of multiple modalities. Through evaluation using 14 benchmark datasets, EyeCLIP can be transferred to a wide range of downstream tasks involving ocular and systemic diseases, achieving state-of-the-art performance in disease classification, visual question answering, and cross-modal retrieval. EyeCLIP represents a significant advancement over previous methods, especially showcasing few-shot, even zero-shot capabilities in real-world long-tail scenarios.

Translated Abstract:
눈 질병, 예를 들어 녹내장, 황반변성, 당뇨병성 망막병증 등을 조기에 발견하는 건 시력을 잃지 않기 위해 정말 중요해. 인공지능(AI) 기본 모델이 이런 문제를 해결하는 데 큰 가능성을 가지고 있지만, 기존의 안과 모델들은 주로 한 가지 방식만 집중하고 있어. 그런데 눈 질병을 진단하려면 여러 가지 방식이 필요해. 그래서 여러 방식에서 같은 환자에 대한 정보를 활용하는 게 중요한데, 이 부분은 자주 간과되고 있어.

또한, 안과 질병은 종류가 많아서 기존의 완전 감독 학습이나 비감독 학습 방법이 잘 작동하지 않아. 그래서 더 다양한 질병을 포착하기 위해 임상 텍스트를 통합하는 게 필수적이야. 우리는 EyeCLIP이라는 시각-언어 모델을 제안해. 이 모델은 277만 개 이상의 다중 모달 안과 이미지를 부분 텍스트 데이터와 함께 사용해서 개발했어. 

EyeCLIP은 큰 다중 모달 비라벨 및 라벨 데이터의 힘을 최대한 활용하기 위해 자가 감독 재구성, 다중 모달 이미지 대조 학습, 이미지-텍스트 대조 학습을 결합한 사전 학습 전략을 도입했어. 14개의 벤치마크 데이터셋을 사용한 평가에서 EyeCLIP은 안과 및 전신 질병과 관련된 다양한 다운스트림 작업으로 잘 전이될 수 있고, 질병 분류, 시각 질문 응답, 크로스 모달 검색에서 최첨단 성능을 달성했어. EyeCLIP은 이전 방법들에 비해 중요한 발전을 나타내며, 특히 실제 세계의 긴 꼬리 문제에서도 몇 번의 학습만으로, 심지어 제로샷 능력을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.06648.pdf

Title: Image Vectorization with Depth: convexified shape layers with depth ordering

Original Abstract:
Image vectorization is a process to convert a raster image into a scalable vector graphic format. Objective is to effectively remove the pixelization effect while representing boundaries of image by scaleable parameterized curves. We propose new image vectorization with depth which considers depth ordering among shapes and use curvature-based inpainting for convexifying shapes in vectorization process.From a given color quantized raster image, we first define each connected component of the same color as a shape layer, and construct depth ordering among them using a newly proposed depth ordering energy. Global depth ordering among all shapes is described by a directed graph, and we propose an energy to remove cycle within the graph. After constructing depth ordering of shapes, we convexify occluded regions by Euler's elastica curvature-based variational inpainting, and leverage on the stability of Modica-Mortola double-well potential energy to inpaint large regions. This is following human vision perception that boundaries of shapes extend smoothly, and we assume shapes are likely to be convex. Finally, we fit Bézier curves to the boundaries and save vectorization as a SVG file which allows superposition of curvature-based inpainted shapes following the depth ordering. This is a new way to vectorize images, by decomposing an image into scalable shape layers with computed depth ordering. This approach makes editing shapes and images more natural and intuitive. We also consider grouping shape layers for semantic vectorization. We present various numerical results and comparisons against recent layer-based vectorization methods to validate the proposed model.

Translated Abstract:
이미지 벡터화는 래스터 이미지를 확장 가능한 벡터 그래픽 형식으로 변환하는 과정이야. 목표는 픽셀화 효과를 효과적으로 제거하면서 이미지의 경계를 스케일 가능한 매개변수 곡선으로 표현하는 거야.

우리는 깊이를 고려한 새로운 이미지 벡터화를 제안해. 여기서는 형태들 사이의 깊이 순서를 생각하고, 벡터화 과정에서 볼록한 형태로 만들기 위해 곡률 기반의 인페인팅을 사용해. 주어진 색상 양자화된 래스터 이미지에서, 먼저 같은 색상의 연결된 구성 요소를 형태 레이어로 정의하고, 새로 제안한 깊이 순서 에너지를 사용해 그들 간의 깊이 순서를 구성해.

모든 형태 간의 글로벌 깊이 순서는 방향 그래프로 설명되며, 우리는 그래프 내에서 사이클을 제거하기 위한 에너지를 제안해. 형태의 깊이 순서를 구성한 후에는 오클루드(가려진) 영역을 유일한 엘라스티카 곡률 기반 변분 인페인팅으로 볼록하게 만들어. 그리고 Modica-Mortola 이중 우물 잠재 에너지의 안정성을 이용해 큰 영역을 인페인팅해. 이는 경계가 부드럽게 확장된다고 보는 인간의 시각 인식을 따르는 것이고, 우리는 형태가 볼록할 가능성이 높다고 가정해.

마지막으로, 우리는 경계에 베지에 곡선을 맞추고 벡터화를 SVG 파일로 저장해. 이렇게 하면 깊이 순서에 따라 곡률 기반으로 인페인팅된 형태를 겹칠 수 있어. 이 방법은 이미지를 깊이 순서로 계산된 확장 가능한 형태 레이어로 분해함으로써 이미지를 벡터화하는 새로운 방식을 보여줘. 이 접근 방식은 형태와 이미지를 더 자연스럽고 직관적으로 편집할 수 있게 해. 우리는 또한 의미론적 벡터화를 위해 형태 레이어를 그룹화하는 것도 고려해. 마지막으로, 제안한 모델을 검증하기 위해 다양한 수치 결과와 최근의 레이어 기반 벡터화 방법들과의 비교를 제시해.

================================================================================

URL:
https://arxiv.org/pdf/2409.06662.pdf

Title: World-Grounded Human Motion Recovery via Gravity-View Coordinates

Original Abstract:
We present a novel method for recovering world-grounded human motion from monocular video. The main challenge lies in the ambiguity of defining the world coordinate system, which varies between sequences. Previous approaches attempt to alleviate this issue by predicting relative motion in an autoregressive manner, but are prone to accumulating errors. Instead, we propose estimating human poses in a novel Gravity-View (GV) coordinate system, which is defined by the world gravity and the camera view direction. The proposed GV system is naturally gravity-aligned and uniquely defined for each video frame, largely reducing the ambiguity of learning image-pose mapping. The estimated poses can be transformed back to the world coordinate system using camera rotations, forming a global motion sequence. Additionally, the per-frame estimation avoids error accumulation in the autoregressive methods. Experiments on in-the-wild benchmarks demonstrate that our method recovers more realistic motion in both the camera space and world-grounded settings, outperforming state-of-the-art methods in both accuracy and speed. The code is available at this https URL.

Translated Abstract:
우리는 단안 비디오에서 세계에 기반한 인간의 움직임을 회복하는 새로운 방법을 제안해. 가장 큰 문제는 세계 좌표계를 정의하는 것이 모호하다는 건데, 이건 시퀀스마다 다르게 나타나. 이전 방법들은 상대적인 움직임을 예측하는 방식으로 이 문제를 해결하려고 했지만, 오류가 쌓일 위험이 있어.

그래서 우리는 인간의 자세를 새로운 중력-뷰(GV) 좌표계에서 추정하는 방법을 제안해. 이 GV 시스템은 세계의 중력과 카메라의 시야 방향에 의해 정의되며, 각 비디오 프레임에 대해 고유하게 정의되기 때문에 이미지와 자세 매핑의 모호성을 크게 줄여. 추정된 자세는 카메라 회전을 사용해 세계 좌표계로 다시 변환할 수 있어서, 전역적인 움직임 시퀀스를 형성할 수 있어. 게다가, 프레임별로 추정하는 방식 덕분에 이전 방법에서 발생하는 오류 누적을 피할 수 있어.

실험 결과, 우리의 방법이 카메라 공간과 세계 기반 설정 모두에서 더 현실적인 움직임을 회복한다는 걸 보여줬어. 정확성과 속도 면에서 최신 방법들을 능가하는 성능을 보였고. 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06665.pdf

Title: Data Collection-free Masked Video Modeling

Original Abstract:
Pre-training video transformers generally requires a large amount of data, presenting significant challenges in terms of data collection costs and concerns related to privacy, licensing, and inherent biases. Synthesizing data is one of the promising ways to solve these issues, yet pre-training solely on synthetic data has its own challenges. In this paper, we introduce an effective self-supervised learning framework for videos that leverages readily available and less costly static images. Specifically, we define the Pseudo Motion Generator (PMG) module that recursively applies image transformations to generate pseudo-motion videos from images. These pseudo-motion videos are then leveraged in masked video modeling. Our approach is applicable to synthetic images as well, thus entirely freeing video pre-training from data collection costs and other concerns in real data. Through experiments in action recognition tasks, we demonstrate that this framework allows effective learning of spatio-temporal features through pseudo-motion videos, significantly improving over existing methods which also use static images and partially outperforming those using both real and synthetic videos. These results uncover fragments of what video transformers learn through masked video modeling.

Translated Abstract:
비디오 트랜스포머를 사전 학습하려면 대량의 데이터가 필요해. 이로 인해 데이터 수집 비용이나 개인정보, 라이선스 문제, 그리고 내재된 편향 같은 여러 도전 과제가 생겨. 데이터 합성이 이러한 문제를 해결할 수 있는 방법 중 하나인데, 합성 데이터만으로 사전 학습하는 건 또 다른 어려움이 있어. 

이 논문에서는 저렴하고 쉽게 구할 수 있는 정적인 이미지를 활용한 효과적인 자기 지도 학습 프레임워크를 소개해. 구체적으로는, 이미지 변환을 반복 적용해서 이미지에서 의사 움직임 비디오를 생성하는 '가짜 움직임 생성기(PMG)' 모듈을 정의했어. 이렇게 생성된 의사 움직임 비디오는 마스킹된 비디오 모델링에 사용돼. 이 방법은 합성 이미지에도 적용 가능해서, 실제 데이터 수집 비용이나 다른 걱정에서 완전히 벗어날 수 있어.

우리는 행동 인식 작업에서 실험을 통해 이 프레임워크가 의사 움직임 비디오를 통해 시공간 특징을 효과적으로 학습할 수 있게 해준다는 걸 보여줬어. 기존에 정적인 이미지를 사용한 방법보다 상당히 개선되었고, 실제 비디오와 합성 비디오를 모두 사용하는 방법보다 부분적으로 더 나은 성능을 냈어. 이 결과들은 마스킹된 비디오 모델링을 통해 비디오 트랜스포머가 무엇을 배우는지를 일부 보여주고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06671.pdf

Title: A Semantic Segmentation Approach on Sweet Orange Leaf Diseases Detection Utilizing YOLO

Original Abstract:
This research introduces an advanced method for diagnosing diseases in sweet orange leaves by utilising advanced artificial intelligence models like YOLOv8 . Due to their significance as a vital agricultural product, sweet oranges encounter significant threats from a variety of diseases that harmfully affect both their yield and quality. Conventional methods for disease detection primarily depend on manual inspection which is ineffective and frequently leads to errors, resulting in delayed treatment and increased financial losses. In response to this challenge, the research utilized YOLOv8 , harnessing their proficiencies in detecting objects and analyzing images. YOLOv8 is recognized for its rapid and precise performance, while VIT is acknowledged for its detailed feature extraction abilities. Impressively, during both the training and validation stages, YOLOv8 exhibited a perfect accuracy of 80.4%, while VIT achieved an accuracy of 99.12%, showcasing their potential to transform disease detection in agriculture. The study comprehensively examined the practical challenges related to the implementation of AI technologies in agriculture, encompassing the computational demands and user accessibility, and offering viable solutions for broader usage. Moreover, it underscores the environmental considerations, particularly the potential for reduced pesticide usage, thereby promoting sustainable farming and environmental conservation. These findings provide encouraging insights into the application of AI in agriculture, suggesting a transition towards more effective, sustainable, and technologically advanced farming methods. This research not only highlights the efficacy of YOLOv8 within a specific agricultural domain but also lays the foundation for further studies that encompass a broader application in crop management and sustainable agricultural practices.

Translated Abstract:
이 연구는 요즘 인공지능 모델인 YOLOv8을 이용해서 감귤 잎의 질병을 진단하는 고급 방법을 소개해. 감귤은 중요한 농산물이기 때문에 여러 질병의 위협을 많이 받고, 이 질병들이 수확량과 품질에 나쁜 영향을 미쳐.

기존의 질병 탐지 방법은 주로 수작업 검사를 기반으로 하는데, 이건 비효율적이고 자주 실수를 해서 치료가 늦어지고 금전적인 손실이 커져. 그래서 이 연구에서는 YOLOv8을 활용했어. YOLOv8은 물체를 탐지하고 이미지를 분석하는 데 강점을 가지고 있어. YOLOv8은 빠르고 정확한 성능으로 유명하고, VIT는 세부적인 특징을 잘 추출하는 능력이 있어.

훈련과 검증 단계에서 YOLOv8은 80.4%의 완벽한 정확도를 보였고, VIT는 99.12%의 정확도를 기록했어. 이건 농업에서 질병 탐지를 혁신할 가능성을 보여줘. 이 연구는 농업에서 AI 기술을 적용하는 데 실질적인 도전 과제들을 자세히 살펴봤어. 여기에는 계산 요구사항과 사용자 접근성 문제가 포함되고, 더 넓은 사용을 위한 실현 가능한 해결책도 제시했어.

또한, 환경적인 고려 사항도 강조했는데, 특히 농약 사용을 줄일 수 있는 가능성을 보여줌으로써 지속 가능한 농업과 환경 보호를 촉진할 수 있어. 이 연구 결과는 농업에서 AI를 활용하는 데 희망적인 통찰을 제공하고, 더 효과적이고 지속 가능하며 기술적으로 진보한 농업 방법으로의 전환 가능성을 제안해.

이 연구는 YOLOv8의 효능을 특정 농업 분야에서 강조할 뿐만 아니라, 작물 관리와 지속 가능한 농업 관행의 더 넓은 적용을 위한 추가 연구의 기초도 마련했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06683.pdf

Title: Alignist: CAD-Informed Orientation Distribution Estimation by Fusing Shape and Correspondences

Original Abstract:
Object pose distribution estimation is crucial in robotics for better path planning and handling of symmetric objects. Recent distribution estimation approaches employ contrastive learning-based approaches by maximizing the likelihood of a single pose estimate in the absence of a CAD model. We propose a pose distribution estimation method leveraging symmetry respecting correspondence distributions and shape information obtained using a CAD model. Contrastive learning-based approaches require an exhaustive amount of training images from different viewpoints to learn the distribution properly, which is not possible in realistic scenarios. Instead, we propose a pipeline that can leverage correspondence distributions and shape information from the CAD model, which are later used to learn pose distributions. Besides, having access to pose distribution based on correspondences before learning pose distributions conditioned on images, can help formulate the loss between distributions. The prior knowledge of distribution also helps the network to focus on getting sharper modes instead. With the CAD prior, our approach converges much faster and learns distribution better by focusing on learning sharper distribution near all the valid modes, unlike contrastive approaches, which focus on a single mode at a time. We achieve benchmark results on SYMSOL-I and T-Less datasets.

Translated Abstract:
물체 자세 분포 추정은 로봇이 더 나은 경로 계획과 대칭 물체 처리를 위해 매우 중요해. 최근에는 CAD 모델 없이 단일 자세 추정의 가능성을 극대화하는 대비 학습 기반 방법들이 사용되고 있어. 우리는 대칭성을 고려한 대응 분포와 CAD 모델을 통해 얻은 형태 정보를 활용한 자세 분포 추정 방법을 제안해.

대비 학습 기반 방법은 분포를 제대로 배우기 위해 다양한 시점에서 많은 훈련 이미지를 필요로 하는데, 현실적인 상황에서는 이게 불가능해. 대신, 우리는 CAD 모델에서 얻은 대응 분포와 형태 정보를 활용하는 파이프라인을 제안해. 이 정보는 나중에 자세 분포를 배우는 데 사용돼. 게다가, 이미지를 기반으로 한 자세 분포를 배우기 전에 대응에 기반한 자세 분포에 접근할 수 있으면, 분포 간의 손실을 정리하는 데 도움이 돼. 사전 지식이 있으면 네트워크가 더 뚜렷한 모드에 집중할 수 있게 해.

CAD 사전 지식을 활용하면, 우리의 방법이 훨씬 더 빠르게 수렴하고 모든 유효한 모드 근처의 뚜렷한 분포를 배우는 데 집중할 수 있어. 반면에 대비 기반 방법은 한 번에 하나의 모드에만 집중하지. 우리는 SYMSOL-I와 T-Less 데이터셋에서 기준 성과를 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06685.pdf

Title: GigaGS: Scaling up Planar-Based 3D Gaussians for Large Scene Surface Reconstruction

Original Abstract:
3D Gaussian Splatting (3DGS) has shown promising performance in novel view synthesis. Previous methods adapt it to obtaining surfaces of either individual 3D objects or within limited scenes. In this paper, we make the first attempt to tackle the challenging task of large-scale scene surface reconstruction. This task is particularly difficult due to the high GPU memory consumption, different levels of details for geometric representation, and noticeable inconsistencies in appearance. To this end, we propose GigaGS, the first work for high-quality surface reconstruction for large-scale scenes using 3DGS. GigaGS first applies a partitioning strategy based on the mutual visibility of spatial regions, which effectively grouping cameras for parallel processing. To enhance the quality of the surface, we also propose novel multi-view photometric and geometric consistency constraints based on Level-of-Detail representation. In doing so, our method can reconstruct detailed surface structures. Comprehensive experiments are conducted on various datasets. The consistent improvement demonstrates the superiority of GigaGS.

Translated Abstract:
3D 가우시안 스플래팅(3DGS)은 새로운 시점 합성에서 좋은 성능을 보여줬어. 이전 방법들은 이 기술을 개별 3D 객체나 제한된 장면의 표면을 얻는 데 적용했어. 

이번 논문에서는 대규모 장면의 표면 재구성이라는 도전적인 작업에 처음으로 도전해봤어. 이 작업은 높은 GPU 메모리 소비, 기하학적 표현을 위한 다양한 세부 수준, 그리고 눈에 띄는 외관 불일치 때문에 특히 어려워. 

그래서 우리는 GigaGS라는 것을 제안했어. GigaGS는 대규모 장면에 대한 고품질 표면 재구성을 위한 첫 번째 연구야. GigaGS는 먼저 공간 영역의 상호 가시성에 기반한 분할 전략을 적용해서 카메라를 효과적으로 그룹화하고 병렬 처리를 가능하게 해. 

표면의 품질을 높이기 위해, 우리는 레벨 오브 디테일 표현에 기반한 새로운 다중 뷰 광학적 및 기하학적 일관성 제약도 제안했어. 이렇게 해서 우리 방법은 상세한 표면 구조를 재구성할 수 있어. 

다양한 데이터셋에서 포괄적인 실험을 진행했어. 일관된 개선 결과가 GigaGS의 우수성을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.06702.pdf

Title: Hint-AD: Holistically Aligned Interpretability in End-to-End Autonomous Driving

Original Abstract:
End-to-end architectures in autonomous driving (AD) face a significant challenge in interpretability, impeding human-AI trust. Human-friendly natural language has been explored for tasks such as driving explanation and 3D captioning. However, previous works primarily focused on the paradigm of declarative interpretability, where the natural language interpretations are not grounded in the intermediate outputs of AD systems, making the interpretations only declarative. In contrast, aligned interpretability establishes a connection between language and the intermediate outputs of AD systems. Here we introduce Hint-AD, an integrated AD-language system that generates language aligned with the holistic perception-prediction-planning outputs of the AD model. By incorporating the intermediate outputs and a holistic token mixer sub-network for effective feature adaptation, Hint-AD achieves desirable accuracy, achieving state-of-the-art results in driving language tasks including driving explanation, 3D dense captioning, and command prediction. To facilitate further study on driving explanation task on nuScenes, we also introduce a human-labeled dataset, Nu-X. Codes, dataset, and models will be publicly available.

Translated Abstract:
자율주행(AD)에서의 엔드 투 엔드 아키텍처는 해석 가능성에서 큰 도전에 직면해 있어, 사람과 AI 간의 신뢰를 방해하고 있어. 사람 친화적인 자연어는 주행 설명이나 3D 캡셔닝 같은 작업을 위해 연구되어 왔어. 하지만 이전 연구들은 주로 선언적 해석 가능성 패러다임에 집중했는데, 여기서 자연어 해석이 AD 시스템의 중간 출력에 기반하지 않아서 해석이 단지 선언적일 뿐이었어.

반대로, 정렬된 해석 가능성은 언어와 AD 시스템의 중간 출력 간의 연결을 확립해. 여기서 우리는 Hint-AD를 소개하는데, 이건 AD 모델의 전체적인 인식-예측-계획 출력에 맞춰 언어를 생성하는 통합 AD-언어 시스템이야. 중간 출력을 포함하고 효과적인 특성 적응을 위한 전체 토큰 믹서 서브 네트워크를 통해 Hint-AD는 원하는 정확도를 달성했어. 이 시스템은 주행 설명, 3D 밀집 캡셔닝, 명령 예측 같은 주행 언어 작업에서 최첨단 결과를 얻었어.

또한, nuScenes에서 주행 설명 작업에 대한 추가 연구를 돕기 위해 사람 레이블이 붙은 데이터셋인 Nu-X도 소개할 거야. 코드, 데이터셋, 모델은 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06703.pdf

Title: LEIA: Latent View-invariant Embeddings for Implicit 3D Articulation

Original Abstract:
Neural Radiance Fields (NeRFs) have revolutionized the reconstruction of static scenes and objects in 3D, offering unprecedented quality. However, extending NeRFs to model dynamic objects or object articulations remains a challenging problem. Previous works have tackled this issue by focusing on part-level reconstruction and motion estimation for objects, but they often rely on heuristics regarding the number of moving parts or object categories, which can limit their practical use. In this work, we introduce LEIA, a novel approach for representing dynamic 3D objects. Our method involves observing the object at distinct time steps or "states" and conditioning a hypernetwork on the current state, using this to parameterize our NeRF. This approach allows us to learn a view-invariant latent representation for each state. We further demonstrate that by interpolating between these states, we can generate novel articulation configurations in 3D space that were previously unseen. Our experimental results highlight the effectiveness of our method in articulating objects in a manner that is independent of the viewing angle and joint configuration. Notably, our approach outperforms previous methods that rely on motion information for articulation registration.

Translated Abstract:
신경 방사 필드(NeRF)는 정적인 장면과 물체를 3D로 재구성하는 데 혁신을 가져왔고, 그 품질이 정말 뛰어나. 하지만 NeRF를 동적인 물체나 물체의 움직임을 모델링하는 데 확장하는 건 여전히 어려운 문제야. 이전 연구들은 물체의 부분별 재구성과 움직임 추정에 초점을 맞춰 이 문제를 해결하려 했지만, 많은 경우에 움직이는 부분의 수나 물체의 종류에 대한 경험적 규칙에 의존하기 때문에 실용성이 떨어질 수 있어.

이번 연구에서는 LEIA라는 새로운 접근 방식을 소개해. 이 방법은 물체를 다양한 시간 단계나 "상태"에서 관찰하고, 현재 상태를 기반으로 하이퍼네트워크를 조정해. 이렇게 해서 우리의 NeRF를 매개변수화할 수 있어. 이 접근 방식 덕분에 각 상태에 대해 시점에 구애받지 않는 잠재 표현을 학습할 수 있어. 

또한, 이 상태들 사이를 보간(interpolating)함으로써, 이전에 보지 못했던 새로운 움직임 구성을 3D 공간에서 생성할 수 있다는 점도 보여줘. 실험 결과에 따르면, 우리의 방법이 보는 각도와 관절 구성이 독립적이면서 물체를 잘 움직일 수 있게 해준다는 게 드러났어. 특히, 우리의 접근 방식은 움직임 정보를 이용한 기존 방법들보다 더 나은 성능을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06704.pdf

Title: GeoCalib: Learning Single-image Calibration with Geometric Optimization

Original Abstract:
From a single image, visual cues can help deduce intrinsic and extrinsic camera parameters like the focal length and the gravity direction. This single-image calibration can benefit various downstream applications like image editing and 3D mapping. Current approaches to this problem are based on either classical geometry with lines and vanishing points or on deep neural networks trained end-to-end. The learned approaches are more robust but struggle to generalize to new environments and are less accurate than their classical counterparts. We hypothesize that they lack the constraints that 3D geometry provides. In this work, we introduce GeoCalib, a deep neural network that leverages universal rules of 3D geometry through an optimization process. GeoCalib is trained end-to-end to estimate camera parameters and learns to find useful visual cues from the data. Experiments on various benchmarks show that GeoCalib is more robust and more accurate than existing classical and learned approaches. Its internal optimization estimates uncertainties, which help flag failure cases and benefit downstream applications like visual localization. The code and trained models are publicly available at this https URL.

Translated Abstract:
단일 이미지에서 시각적 단서를 통해 카메라의 내적 및 외적 파라미터, 예를 들어 초점 거리와 중력 방향을 추정할 수 있어. 이런 단일 이미지 보정은 이미지 편집이나 3D 맵핑 같은 여러 후속 작업에 도움이 돼. 

현재 이 문제에 대한 접근 방식은 고전 기하학에 기반한 선과 소실점 또는 엔드 투 엔드로 훈련된 딥 뉴럴 네트워크를 사용하는 방법이 있어. 학습된 방법은 더 견고하지만 새로운 환경에 일반화하는 데 어려움을 겪고, 고전 방법보다 정확도가 떨어져. 우리는 3D 기하학이 제공하는 제약이 부족하다고 생각해. 

이번 연구에서는 GeoCalib이라는 딥 뉴럴 네트워크를 소개해. 이 네트워크는 최적화 과정을 통해 3D 기하학의 보편적인 규칙을 활용해. GeoCalib는 카메라 파라미터를 추정하기 위해 엔드 투 엔드로 훈련되고, 데이터에서 유용한 시각적 단서를 찾는 방법을 배워. 

다양한 벤치마크 실험 결과 GeoCalib는 기존의 고전적 방법이나 학습된 방법보다 더 견고하고 정확하다는 걸 보여줘. 내부 최적화는 불확실성을 추정해 실패 사례를 표시하는 데 도움을 주고, 시각적 위치 파악 같은 후속 작업에도 유익해. 코드와 훈련된 모델은 이 URL에서 공개돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.05900.pdf

Title: Memory-Optimized Once-For-All Network

Original Abstract:
Deploying Deep Neural Networks (DNNs) on different hardware platforms is challenging due to varying resource constraints. Besides handcrafted approaches aiming at making deep models hardware-friendly, Neural Architectures Search is rising as a toolbox to craft more efficient DNNs without sacrificing performance. Among these, the Once-For-All (OFA) approach offers a solution by allowing the sampling of well-performing sub-networks from a single supernet -- this leads to evident advantages in terms of computation. However, OFA does not fully utilize the potential memory capacity of the target device, focusing instead on limiting maximum memory usage per layer. This leaves room for an unexploited potential in terms of model generalizability. In this paper, we introduce a Memory-Optimized OFA (MOOFA) supernet, designed to enhance DNN deployment on resource-limited devices by maximizing memory usage (and for instance, features diversity) across different configurations. Tested on ImageNet, our MOOFA supernet demonstrates improvements in memory exploitation and model accuracy compared to the original OFA supernet. Our code is available at this https URL.

Translated Abstract:
다양한 하드웨어 플랫폼에 딥 뉴럴 네트워크(DNN)를 배포하는 건 자원 제약 때문에 어려워. 딥 모델이 하드웨어에 잘 맞도록 손으로 만든 방법도 있지만, 성능을 희생하지 않고 더 효율적인 DNN을 만드는 도구로 신경 구조 검색(Neural Architecture Search)이 각광받고 있어. 

그 중에서도 Once-For-All (OFA) 접근법은 하나의 슈퍼넷에서 잘 작동하는 서브 네트워크를 샘플링할 수 있게 해줘. 이 덕분에 계산 측면에서 분명한 장점이 있어. 하지만 OFA는 최대 메모리 사용량을 각 레이어별로 제한하는 데 집중해서, 목표 장치의 메모리 용량을 충분히 활용하지 못하고 있어. 그래서 모델의 일반화 가능성을 더 높일 수 있는 여지가 남아있어.

이 논문에서는 메모리 최적화된 OFA (MOOFA) 슈퍼넷을 소개해. 이 슈퍼넷은 다양한 구성에서 메모리 사용량을 극대화하고, 예를 들어 기능 다양성을 높여서 자원이 제한된 장치에서 DNN 배포를 개선하도록 설계되었어. ImageNet에서 테스트해본 결과, 우리의 MOOFA 슈퍼넷은 원래 OFA 슈퍼넷보다 메모리 활용과 모델 정확도에서 개선된 성능을 보여줬어. 우리의 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.05905.pdf

Title: Towards Narrowing the Generalization Gap in Deep Boolean Networks

Original Abstract:
The rapid growth of the size and complexity in deep neural networks has sharply increased computational demands, challenging their efficient deployment in real-world scenarios. Boolean networks, constructed with logic gates, offer a hardware-friendly alternative that could enable more efficient implementation. However, their ability to match the performance of traditional networks has remained uncertain. This paper explores strategies to enhance deep Boolean networks with the aim of surpassing their traditional counterparts. We propose novel methods, including logical skip connections and spatiality preserving sampling, and validate them on vision tasks using widely adopted datasets, demonstrating significant improvement over existing approaches. Our analysis shows how deep Boolean networks can maintain high performance while minimizing computational costs through 1-bit logic operations. These findings suggest that Boolean networks are a promising direction for efficient, high-performance deep learning models, with significant potential for advancing hardware-accelerated AI applications.

Translated Abstract:
딥 뉴럴 네트워크의 크기와 복잡성이 빠르게 증가하면서 컴퓨터 자원에 대한 요구도 크게 늘어났어. 이런 상황에서 현실 세계에 효과적으로 배포하기가 어려워졌지. 불리언 네트워크는 논리 게이트를 사용해 만들어진 것으로, 하드웨어에 친화적인 대안이 될 수 있어. 하지만 기존 네트워크와 비슷한 성능을 낼 수 있을지는 아직 확실하지 않아.

이 논문은 딥 불리언 네트워크의 성능을 향상시키기 위한 전략을 탐구해. 목표는 기존 네트워크를 뛰어넘는 거야. 우리는 논리적 스킵 연결과 공간성을 유지하는 샘플링 같은 새로운 방법을 제안하고, 널리 사용되는 데이터셋을 통해 비전 작업에서 검증했어. 그 결과 기존 방법들보다 상당한 개선을 보여줬지.

우리 분석은 딥 불리언 네트워크가 1비트 논리 연산을 통해 높은 성능을 유지하면서도 계산 비용을 최소화할 수 있는 방법을 보여줘. 이러한 결과는 불리언 네트워크가 효율적이고 고성능의 딥 러닝 모델을 위한 유망한 방향임을 시사해. 하드웨어 가속 AI 응용 프로그램을 발전시킬 수 있는 큰 잠재력이 있다고 할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.05982.pdf

Title: Enhancing Cross-Modality Synthesis: Subvolume Merging for MRI-to-CT Conversion

Original Abstract:
Providing more precise tissue attenuation information, synthetic computed tomography (sCT) generated from magnetic resonance imaging (MRI) contributes to improved radiation therapy treatment planning. In our study, we employ the advanced SwinUNETR framework for synthesizing CT from MRI images. Additionally, we introduce a three-dimensional subvolume merging technique in the prediction process. By selecting an optimal overlap percentage for adjacent subvolumes, stitching artifacts are effectively mitigated, leading to a decrease in the mean absolute error (MAE) between sCT and the labels from 52.65 HU to 47.75 HU. Furthermore, implementing a weight function with a gamma value of 0.9 results in the lowest MAE within the same overlap area. By setting the overlap percentage between 50% and 70%, we achieve a balance between image quality and computational efficiency.

Translated Abstract:
자기공명영상(MRI)에서 생성된 합성 전산 단층촬영(sCT)은 더 정확한 조직 감쇠 정보를 제공해서 방사선 치료 계획을 개선하는 데 도움을 줍니다. 우리 연구에서는 MRI 이미지를 사용해 CT를 합성하기 위해 고급 SwinUNETR 프레임워크를 사용했어요. 

또한 예측 과정에서 3차원 서브볼륨 병합 기술을 도입했습니다. 인접한 서브볼륨의 최적 중첩 비율을 선택함으로써, 이어붙임 아티팩트를 효과적으로 줄일 수 있었고, 그 결과 sCT와 레이블 간의 평균 절대 오차(MAE)가 52.65 HU에서 47.75 HU로 감소했어요. 게다가, 감마 값이 0.9인 가중치 함수를 적용하니 같은 중첩 영역 내에서 MAE가 가장 낮아졌습니다. 

중첩 비율을 50%에서 70% 사이로 설정함으로써 이미지 품질과 계산 효율성 간의 균형을 이룰 수 있었어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.06013.pdf

Title: Improved Visually Prompted Keyword Localisation in Real Low-Resource Settings

Original Abstract:
Given an image query, visually prompted keyword localisation (VPKL) aims to find occurrences of the depicted word in a speech collection. This can be useful when transcriptions are not available for a low-resource language (e.g. if it is unwritten). Previous work showed that VPKL can be performed with a visually grounded speech model trained on paired images and unlabelled speech. But all experiments were done on English. Moreover, transcriptions were used to get positive and negative pairs for the contrastive loss. This paper introduces a few-shot learning scheme to mine pairs automatically without transcriptions. On English, this results in only a small drop in performance. We also - for the first time - consider VPKL on a real low-resource language, Yoruba. While scores are reasonable, here we see a bigger drop in performance compared to using ground truth pairs because the mining is less accurate in Yoruba.

Translated Abstract:
이미지 쿼리가 주어지면, 시각적 키워드 위치 지정(VPKL)은 해당 단어가 음성 데이터 모음에 얼마나 나타나는지를 찾는 걸 목표로 해. 이게 필요한 이유는 저자원 언어(예를 들어, 쓰여지지 않은 언어)의 경우 전사본이 없을 때 유용할 수 있어.

이전 연구에서는 짝지어진 이미지와 라벨이 없는 음성으로 훈련된 시각적 기반 음성 모델을 사용해서 VPKL을 수행할 수 있다고 보여줬어. 하지만 모든 실험이 영어로 진행됐고, 전사본을 사용해서 대조 손실을 위한 긍정과 부정 쌍을 만들었어.

이번 논문에서는 전사본 없이 쌍을 자동으로 만드는 몇 샷 학습 방식을 도입했어. 영어에서는 성능이 약간 떨어지는 정도로 결과가 나왔어. 그리고 이번에는 실제 저자원 언어인 요루바에 대해 VPKL을 처음으로 고려했어. 성능 점수가 괜찮긴 했지만, 요루바에서의 쌍 찾기가 덜 정확해서 실제 쌍을 사용할 때보다 성능이 더 크게 떨어지는 걸 볼 수 있었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06018.pdf

Title: Pioneering Precision in Lumbar Spine MRI Segmentation with Advanced Deep Learning and Data Enhancement

Original Abstract:
This study presents an advanced approach to lumbar spine segmentation using deep learning techniques, focusing on addressing key challenges such as class imbalance and data preprocessing. Magnetic resonance imaging (MRI) scans of patients with low back pain are meticulously preprocessed to accurately represent three critical classes: vertebrae, spinal canal, and intervertebral discs (IVDs). By rectifying class inconsistencies in the data preprocessing stage, the fidelity of the training data is ensured. The modified U-Net model incorporates innovative architectural enhancements, including an upsample block with leaky Rectified Linear Units (ReLU) and Glorot uniform initializer, to mitigate common issues such as the dying ReLU problem and improve stability during training. Introducing a custom combined loss function effectively tackles class imbalance, significantly improving segmentation accuracy. Evaluation using a comprehensive suite of metrics showcases the superior performance of this approach, outperforming existing methods and advancing the current techniques in lumbar spine segmentation. These findings hold significant advancements for enhanced lumbar spine MRI and segmentation diagnostic accuracy.

Translated Abstract:
이 연구는 심층 학습 기법을 사용해 허리 척추를 분할하는 새로운 방법을 제시해. 여기서 주요 문제점인 클래스 불균형과 데이터 전처리를 해결하는 데 중점을 두고 있어. 

저희는 허리 통증 환자들의 자기 공명 영상(MRI)을 꼼꼼하게 전처리해서 세 가지 중요한 클래스를 정확히 나타냈어: 척추뼈, 척추관, 그리고 추간판(IVD). 데이터 전처리 단계에서 클래스 불일치를 수정함으로써 훈련 데이터의 신뢰성을 높였지.

수정된 U-Net 모델은 훈련 중 안정성을 높이고 '죽은 ReLU 문제' 같은 일반적인 문제를 완화하기 위해 리키 Rectified Linear Units(ReLU)와 Glorot 균일 초기화를 포함한 혁신적인 구조 개선을 도입했어. 커스텀 결합 손실 함수를 도입해서 클래스 불균형 문제를 효과적으로 해결하고, 분할 정확도를 크게 향상시켰어.

종합적인 평가 지표를 통해 이 접근법의 우수한 성능을 보여주고, 기존 방법보다 더 나은 결과를 내고 있어. 이 연구 결과는 허리 척추 MRI와 분할 진단 정확도를 높이는 데 큰 발전이 될 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06030.pdf

Title: NESI: Shape Representation via Neural Explicit Surface Intersection

Original Abstract:
Compressed representations of 3D shapes that are compact, accurate, and can be processed efficiently directly in compressed form, are extremely useful for digital media applications. Recent approaches in this space focus on learned implicit or parametric representations. While implicits are well suited for tasks such as in-out queries, they lack natural 2D parameterization, complicating tasks such as texture or normal mapping. Conversely, parametric representations support the latter tasks but are ill-suited for occupancy queries. We propose a novel learned alternative to these approaches, based on intersections of localized explicit, or height-field, surfaces. Since explicits can be trivially expressed both implicitly and parametrically, NESI directly supports a wider range of processing operations than implicit alternatives, including occupancy queries and parametric access. We represent input shapes using a collection of differently oriented height-field bounded half-spaces combined using volumetric Boolean intersections. We first tightly bound each input using a pair of oppositely oriented height-fields, forming a Double Height-Field (DHF) Hull. We refine this hull by intersecting it with additional localized height-fields (HFs) that capture surface regions in its interior. We minimize the number of HFs necessary to accurately capture each input and compactly encode both the DHF hull and the local HFs as neural functions defined over subdomains of R^2. This reduced dimensionality encoding delivers high-quality compact approximations. Given similar parameter count, or storage capacity, NESI significantly reduces approximation error compared to the state of the art, especially at lower parameter counts.

Translated Abstract:
3D 형태의 압축 표현은 작고 정확하며 압축된 형태로 효율적으로 처리할 수 있기 때문에 디지털 미디어 응용 프로그램에 매우 유용해. 최근 연구들은 학습된 암묵적 또는 매개변수적 표현에 집중하고 있어. 암묵적 표현은 입출력 쿼리 같은 작업에 잘 맞지만, 자연스러운 2D 매개변수화가 부족해서 텍스처나 노멀 맵핑 같은 작업이 복잡해져. 반면에 매개변수적 표현은 후자의 작업을 지원하지만 점유 쿼리에는 적합하지 않아.

우리는 이러한 접근 방식에 대한 새로운 학습된 대안을 제안해. 이 방법은 국소적으로 명시적인 표면, 즉 높이 필드의 교차를 기반으로 해. 명시적인 표현들은 암묵적이고 매개변수적으로 쉽게 표현될 수 있어서, NESI는 점유 쿼리와 매개변수 접근 같은 다양한 처리 작업을 직접 지원해.

우리는 입력 형태를 서로 다른 방향으로 정렬된 높이 필드로 경계가 있는 반공간의 모음으로 표현해. 먼저, 서로 반대 방향으로 정렬된 높이 필드를 사용해 각 입력을 꽉 잡아줘서 더블 높이 필드(DHF) 외피를 형성해. 그 외피는 내부의 표면 영역을 캡처하는 추가적인 국소 높이 필드(HFs)와 교차시켜서 다듬어. 각 입력을 정확하게 캡처하는 데 필요한 HFs의 수를 최소화하고, DHF 외피와 국소 HFs를 R^2의 부분 영역에서 정의된 신경 함수로 간결하게 인코딩해. 이렇게 차원을 줄인 인코딩은 고품질의 압축된 근사치를 제공해. 비슷한 매개변수 수나 저장 용량이 주어졌을 때, NESI는 최첨단 기술에 비해 근사 오류를 크게 줄여줘, 특히 낮은 매개변수 수에서 더 효과적이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06035.pdf

Title: Analyzing Tumors by Synthesis

Original Abstract:
Computer-aided tumor detection has shown great potential in enhancing the interpretation of over 80 million CT scans performed annually in the United States. However, challenges arise due to the rarity of CT scans with tumors, especially early-stage tumors. Developing AI with real tumor data faces issues of scarcity, annotation difficulty, and low prevalence. Tumor synthesis addresses these challenges by generating numerous tumor examples in medical images, aiding AI training for tumor detection and segmentation. Successful synthesis requires realistic and generalizable synthetic tumors across various organs. This chapter reviews AI development on real and synthetic data and summarizes two key trends in synthetic data for cancer imaging research: modeling-based and learning-based approaches. Modeling-based methods, like Pixel2Cancer, simulate tumor development over time using generic rules, while learning-based methods, like DiffTumor, learn from a few annotated examples in one organ to generate synthetic tumors in others. Reader studies with expert radiologists show that synthetic tumors can be convincingly realistic. We also present case studies in the liver, pancreas, and kidneys reveal that AI trained on synthetic tumors can achieve performance comparable to, or better than, AI only trained on real data. Tumor synthesis holds significant promise for expanding datasets, enhancing AI reliability, improving tumor detection performance, and preserving patient privacy.

Translated Abstract:
컴퓨터 지원 종양 탐지는 미국에서 매년 8천만 건 이상의 CT 스캔 해석을 개선하는 데 큰 가능성을 보여주고 있어. 하지만 초기 단계의 종양이 포함된 CT 스캔은 드물어서 몇 가지 어려움이 있어. 실제 종양 데이터로 AI를 개발하는 데는 데이터 부족, 주석 달기 어려움, 그리고 낮은 발생률 같은 문제가 있거든. 

종양 합성은 이런 문제를 해결하기 위해 의료 이미지에서 많은 종양 예시를 생성해주고, 이걸 통해 AI 훈련을 도와줘. 성공적인 합성을 위해서는 다양한 장기에 걸쳐 사실적이고 일반화 가능한 합성 종양이 필요해. 이 장에서는 실제 데이터와 합성 데이터로 AI를 개발한 내용을 리뷰하고, 암 이미징 연구를 위한 합성 데이터의 두 가지 주요 경향을 정리할 거야: 모델링 기반 방법과 학습 기반 방법이야. 

모델링 기반 방법인 Pixel2Cancer는 일반적인 규칙을 사용해 시간에 따라 종양 발달을 시뮬레이션해. 반면에 학습 기반 방법인 DiffTumor는 한 장기에서 몇 개의 주석 달린 예시를 배워서 다른 장기에서 합성 종양을 생성해. 전문가 방사선의사들과의 연구 결과에 따르면, 합성 종양이 매우 사실적으로 보일 수 있다고 해. 

우리는 간, 췌장, 신장에서의 사례 연구를 통해 합성 종양으로 훈련된 AI가 실제 데이터로만 훈련된 AI와 비슷하거나 더 나은 성능을 낼 수 있다는 것을 보여줘. 종양 합성은 데이터셋을 확장하고, AI 신뢰성을 높이며, 종양 탐지 성능을 개선하고, 환자 개인정보를 보호하는 데 큰 가능성을 가지고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06111.pdf

Title: PaRCE: Probabilistic and Reconstruction-Based Competency Estimation for Safe Navigation Under Perception Uncertainty

Original Abstract:
Perception-based navigation systems are useful for unmanned ground vehicle (UGV) navigation in complex terrains, where traditional depth-based navigation schemes are insufficient. However, these data-driven methods are highly dependent on their training data and can fail in surprising and dramatic ways with little warning. To ensure the safety of the vehicle and the surrounding environment, it is imperative that the navigation system is able to recognize the predictive uncertainty of the perception model and respond safely and effectively in the face of uncertainty. In an effort to enable safe navigation under perception uncertainty, we develop a probabilistic and reconstruction-based competency estimation (PaRCE) method to estimate the model's level of familiarity with an input image as a whole and with specific regions in the image. We find that the overall competency score can correctly predict correctly classified, misclassified, and out-of-distribution (OOD) samples. We also confirm that the regional competency maps can accurately distinguish between familiar and unfamiliar regions across images. We then use this competency information to develop a planning and control scheme that enables effective navigation while maintaining a low probability of error. We find that the competency-aware scheme greatly reduces the number of collisions with unfamiliar obstacles, compared to a baseline controller with no competency awareness. Furthermore, the regional competency information is very valuable in enabling efficient navigation.

Translated Abstract:
지각 기반 내비게이션 시스템은 복잡한 지형에서 무인 지상 차량(UGV)의 내비게이션에 유용해. 전통적인 깊이 기반 내비게이션 방식은 이런 환경에서는 부족하거든. 하지만 이런 데이터 기반 방법들은 훈련 데이터에 많이 의존하고, 경고 없이 예상치 못한 방식으로 실패할 수 있어. 차량과 주변 환경의 안전을 위해서는 내비게이션 시스템이 지각 모델의 예측 불확실성을 인식하고, 불확실한 상황에서 안전하고 효과적으로 대응할 수 있어야 해.

그래서 우리는 안전한 내비게이션을 가능하게 하기 위해, 입력 이미지 전체와 특정 지역에 대한 모델의 익숙함 수준을 추정하는 확률적 재구성 기반 역량 추정 방법(PaRCE)을 개발했어. 이 방법으로 전체 역량 점수가 잘 분류된 샘플, 잘못 분류된 샘플, 그리고 분포 밖의 샘플(OOD)을 정확하게 예측할 수 있다는 걸 발견했어. 또한, 지역 역량 맵이 이미지 간에 익숙한 지역과 낯선 지역을 정확하게 구별할 수 있다는 것도 확인했어.

그 다음, 이 역량 정보를 사용해서 효과적인 내비게이션을 하면서 오류 확률을 낮출 수 있는 계획 및 제어 방식을 개발했어. 역량을 인식한 방식은 익숙하지 않은 장애물과의 충돌 수를 크게 줄여주는 걸 발견했어. 게다가, 지역 역량 정보는 효율적인 내비게이션을 가능하게 하는 데 매우 유용해.

================================================================================

URL:
https://arxiv.org/pdf/2409.06135.pdf

Title: Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis

Original Abstract:
Foley is a term commonly used in filmmaking, referring to the addition of daily sound effects to silent films or videos to enhance the auditory experience. Video-to-Audio (V2A), as a particular type of automatic foley task, presents inherent challenges related to audio-visual synchronization. These challenges encompass maintaining the content consistency between the input video and the generated audio, as well as the alignment of temporal and loudness properties within the video. To address these issues, we construct a controllable video-to-audio synthesis model, termed Draw an Audio, which supports multiple input instructions through drawn masks and loudness signals. To ensure content consistency between the synthesized audio and target video, we introduce the Mask-Attention Module (MAM), which employs masked video instruction to enable the model to focus on regions of interest. Additionally, we implement the Time-Loudness Module (TLM), which uses an auxiliary loudness signal to ensure the synthesis of sound that aligns with the video in both loudness and temporal dimensions. Furthermore, we have extended a large-scale V2A dataset, named VGGSound-Caption, by annotating caption prompts. Extensive experiments on challenging benchmarks across two large-scale V2A datasets verify Draw an Audio achieves the state-of-the-art. Project page: this https URL.

Translated Abstract:
폴리(Foley)는 영화 제작에서 자주 사용되는 용어로, 무성 영화나 비디오에 일상적인 음향 효과를 추가해 청각 경험을 향상시키는 걸 말해. 비디오-오디오(Video-to-Audio, V2A)는 자동 폴리 작업의 일종으로, 오디오와 비디오의 동기화에 어려움이 있어. 

이런 어려움은 입력 비디오와 생성된 오디오 사이의 내용 일관성을 유지하는 것과 비디오의 시간적 및 음량 속성을 맞추는 것과 관련돼. 이 문제를 해결하기 위해, 우리는 여러 입력 지시를 지원하는 'Draw an Audio'라는 조절 가능한 비디오-오디오 합성 모델을 만들었어. 이 모델은 그려진 마스크와 음량 신호를 통해 다양한 지시를 받을 수 있어.

합성된 오디오와 목표 비디오 간의 내용 일관성을 보장하기 위해, 우리는 'Mask-Attention Module (MAM)'을 도입했어. 이 모듈은 마스크된 비디오 지시를 사용해서 모델이 관심 있는 영역에 집중할 수 있게 해. 

또한, 'Time-Loudness Module (TLM)'을 구현했는데, 이 모듈은 보조 음량 신호를 사용해 비디오의 음량과 시간 차원 모두에 맞는 소리를 합성하도록 도와줘. 

마지막으로, 우리는 'VGGSound-Caption'이라는 대규모 V2A 데이터셋을 확장해서 자막 프롬프트를 주석 처리했어. 두 개의 대규모 V2A 데이터셋에 대한 다양한 벤치마크 실험을 통해, 'Draw an Audio'가 최신 기술 수준에 도달했음을 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06219.pdf

Title: Denoising: A Powerful Building-Block for Imaging, Inverse Problems, and Machine Learning

Original Abstract:
Denoising, the process of reducing random fluctuations in a signal to emphasize essential patterns, has been a fundamental problem of interest since the dawn of modern scientific inquiry. Recent denoising techniques, particularly in imaging, have achieved remarkable success, nearing theoretical limits by some measures. Yet, despite tens of thousands of research papers, the wide-ranging applications of denoising beyond noise removal have not been fully recognized. This is partly due to the vast and diverse literature, making a clear overview challenging.
This paper aims to address this gap. We present a comprehensive perspective on denoisers, their structure, and desired properties. We emphasize the increasing importance of denoising and showcase its evolution into an essential building block for complex tasks in imaging, inverse problems, and machine learning. Despite its long history, the community continues to uncover unexpected and groundbreaking uses for denoising, further solidifying its place as a cornerstone of scientific and engineering practice.

Translated Abstract:
잡음 제거는 신호의 무작위 변동을 줄여서 중요한 패턴을 강조하는 과정으로, 현대 과학 연구가 시작된 이후로 기본적인 관심사였어. 최근의 잡음 제거 기법, 특히 이미지 처리 분야에서 큰 성공을 거두었고, 어떤 측면에서는 이론적인 한계에 근접하고 있어. 하지만 수만 개의 연구 논문이 있음에도 불구하고, 잡음 제거의 다양한 응용이 소음 제거를 넘어서 충분히 인식되지 않은 상태야. 이는 방대한 문헌으로 인해 전체적인 개요를 파악하기가 어려운 부분도 있어.

이 논문은 이러한 갭을 메우는 걸 목표로 하고 있어. 우리는 잡음 제거기(denoiser)에 대한 포괄적인 관점을 제시하고, 그 구조와 원하는 특성에 대해서도 설명해. 잡음 제거의 중요성이 점점 커지고 있다는 점을 강조하고, 이미지 처리, 역문제, 기계 학습에서 복잡한 작업을 위한 필수 요소로 발전해온 과정을 보여줘. 긴 역사에도 불구하고, 연구자들은 여전히 예상치 못한 혁신적인 잡음 제거의 사용법을 발견하고 있어서, 과학과 공학 분야에서의 잡음 제거의 중요성이 더욱 확고해지고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06270.pdf

Title: Towards Robust Uncertainty-Aware Incomplete Multi-View Classification

Original Abstract:
Handling incomplete data in multi-view classification is challenging, especially when traditional imputation methods introduce biases that compromise uncertainty estimation. Existing Evidential Deep Learning (EDL) based approaches attempt to address these issues, but they often struggle with conflicting evidence due to the limitations of the Dempster-Shafer combination rule, leading to unreliable decisions. To address these challenges, we propose the Alternating Progressive Learning Network (APLN), specifically designed to enhance EDL-based methods in incomplete MVC scenarios. Our approach mitigates bias from corrupted observed data by first applying coarse imputation, followed by mapping the data to a latent space. In this latent space, we progressively learn an evidence distribution aligned with the target domain, incorporating uncertainty considerations through EDL. Additionally, we introduce a conflict-aware Dempster-Shafer combination rule (DSCR) to better handle conflicting evidence. By sampling from the learned distribution, we optimize the latent representations of missing views, reducing bias and enhancing decision-making robustness. Extensive experiments demonstrate that APLN, combined with DSCR, significantly outperforms traditional methods, particularly in environments characterized by high uncertainty and conflicting evidence, establishing it as a promising solution for incomplete multi-view classification.

Translated Abstract:
다중 뷰 분류에서 불완전한 데이터를 다루는 건 어렵고, 전통적인 보간 방법들은 편향을 유발해 불확실성 추정에 문제를 일으켜. 현재 사용되고 있는 증거 기반 심층 학습(Evidential Deep Learning, EDL) 방법들이 이런 문제를 해결하려고 하지만, Dempster-Shafer 결합 규칙의 한계 때문에 상충되는 증거로 인해 신뢰할 수 없는 결정을 내리는 경우가 많아.

이런 문제를 해결하기 위해 우리는 교차 진행 학습 네트워크(Alternating Progressive Learning Network, APLN)를 제안해. 이건 불완전한 다중 뷰 분류(MVC) 상황에서 EDL 기반 방법을 강화하도록 특별히 설계됐어. 우리 접근법은 먼저 대충의 보간을 적용해서 손상된 관측 데이터에서 편향을 줄이고, 그 다음 데이터는 잠재 공간으로 매핑해. 이 잠재 공간에서 우리는 목표 도메인에 맞춰 증거 분포를 점진적으로 학습하고, EDL을 통해 불확실성도 고려해.

또한 상충되는 증거를 더 잘 처리하기 위해 갈등 인식 Dempster-Shafer 결합 규칙(DSCR)을 도입했어. 학습한 분포에서 샘플링을 통해 누락된 뷰의 잠재 표현을 최적화하고, 편향을 줄이며 결정의 강인성을 높여. 여러 실험 결과, APLN과 DSCR을 결합한 방법이 전통적인 방법들보다 훨씬 뛰어난 성능을 보여줬어. 특히 높은 불확실성과 상충되는 증거가 많은 환경에서 확실한 해결책으로 자리잡을 수 있을 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.06420.pdf

Title: Unrevealed Threats: A Comprehensive Study of the Adversarial Robustness of Underwater Image Enhancement Models

Original Abstract:
Learning-based methods for underwater image enhancement (UWIE) have undergone extensive exploration. However, learning-based models are usually vulnerable to adversarial examples so as the UWIE models. To the best of our knowledge, there is no comprehensive study on the adversarial robustness of UWIE models, which indicates that UWIE models are potentially under the threat of adversarial attacks. In this paper, we propose a general adversarial attack protocol. We make a first attempt to conduct adversarial attacks on five well-designed UWIE models on three common underwater image benchmark datasets. Considering the scattering and absorption of light in the underwater environment, there exists a strong correlation between color correction and underwater image enhancement. On the basis of that, we also design two effective UWIE-oriented adversarial attack methods Pixel Attack and Color Shift Attack targeting different color spaces. The results show that five models exhibit varying degrees of vulnerability to adversarial attacks and well-designed small perturbations on degraded images are capable of preventing UWIE models from generating enhanced results. Further, we conduct adversarial training on these models and successfully mitigated the effectiveness of adversarial attacks. In summary, we reveal the adversarial vulnerability of UWIE models and propose a new evaluation dimension of UWIE models.

Translated Abstract:
수중 이미지 향상을 위한 학습 기반 방법(UWIE)은 많은 연구가 진행되었어. 하지만 학습 기반 모델은 일반적으로 적대적 예제에 취약한데, UWIE 모델도 마찬가지야. 우리가 아는 한, UWIE 모델의 적대적 강인성에 대한 종합적인 연구는 없어서, UWIE 모델이 적대적 공격의 위험에 처해 있다는 걸 의미해.

이 논문에서는 일반적인 적대적 공격 프로토콜을 제안해. 우리는 세 가지 일반적인 수중 이미지 벤치마크 데이터셋에서 다섯 개의 잘 설계된 UWIE 모델에 대해 적대적 공격을 시도해본 첫 번째 사례야. 수중 환경에서 빛의 산란과 흡수를 고려할 때, 색상 보정과 수중 이미지 향상 사이에는 강한 상관관계가 있어. 이를 바탕으로, 우리는 서로 다른 색 공간을 겨냥한 두 가지 효과적인 UWIE 지향의 적대적 공격 방법인 Pixel Attack과 Color Shift Attack도 설계했어.

결과를 보니, 다섯 개의 모델이 적대적 공격에 대해 각기 다른 정도로 취약성을 보였고, 저하된 이미지에 잘 설계된 작은 변화를 주면 UWIE 모델이 향상된 결과를 생성하지 못하게 할 수 있었어. 그리고 우리는 이 모델들에 대해 적대적 훈련을 진행해서 적대적 공격의 효과를 성공적으로 줄였어. 요약하자면, 우리는 UWIE 모델의 적대적 취약성을 밝혀냈고, UWIE 모델의 새로운 평가 차원을 제안했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06455.pdf

Title: Continual Domain Incremental Learning for Privacy-aware Digital Pathology

Original Abstract:
In recent years, there has been remarkable progress in the field of digital pathology, driven by the ability to model complex tissue patterns using advanced deep-learning algorithms. However, the robustness of these models is often severely compromised in the presence of data shifts (e.g., different stains, organs, centers, etc.). Alternatively, continual learning (CL) techniques aim to reduce the forgetting of past data when learning new data with distributional shift conditions. Specifically, rehearsal-based CL techniques, which store some past data in a buffer and then replay it with new data, have proven effective in medical image analysis tasks. However, privacy concerns arise as these approaches store past data, prompting the development of our novel Generative Latent Replay-based CL (GLRCL) approach. GLRCL captures the previous distribution through Gaussian Mixture Models instead of storing past samples, which are then utilized to generate features and perform latent replay with new data. We systematically evaluate our proposed framework under different shift conditions in histopathology data, including stain and organ shift. Our approach significantly outperforms popular buffer-free CL approaches and performs similarly to rehearsal-based CL approaches that require large buffers causing serious privacy violations.

Translated Abstract:
최근 몇 년 동안 디지털 병리학 분야에서 괄목할 만한 발전이 있었어. 이건 복잡한 조직 패턴을 고급 딥러닝 알고리즘으로 모델링할 수 있게 되면서 가능해졌지. 하지만 이런 모델들은 데이터가 바뀌면 (예: 다른 염색, 장기, 병원 등) 성능이 많이 떨어지는 문제가 있어.

그래서 지속적 학습(Continual Learning, CL) 기법이 등장했어. 이 기술은 새로운 데이터가 들어올 때 과거 데이터를 잊어버리지 않도록 도와줘. 특히, 리허설 기반 CL 기법은 과거 데이터를 버퍼에 저장하고 이를 새로운 데이터와 함께 재생해서 사용하는 방식인데, 의료 이미지 분석 작업에서 효과가 입증됐어. 하지만 이 방법은 과거 데이터를 저장해야 해서 개인정보 보호 문제가 생겨.

그래서 우리는 새로운 Generative Latent Replay 기반 CL(GLRCL) 방법을 개발했어. GLRCL은 과거 샘플을 저장하는 대신 가우시안 혼합 모델을 이용해 이전 분포를 포착해. 그 후 이 정보를 사용해 특징을 생성하고 새로운 데이터와 함께 잠재적 재생(latent replay)을 수행해. 

우리는 조직병리학 데이터에서 염색 및 장기 변화 같은 다양한 변화 조건 하에서 제안한 프레임워크를 체계적으로 평가했어. 우리의 접근법은 인기 있는 버퍼 없는 CL 방법들보다 크게 성능이 뛰어나고, 큰 버퍼가 필요해 심각한 개인정보 침해를 일으키는 리허설 기반 CL 방법들과 비슷한 성능을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.06476.pdf

Title: Multi-scale Cycle Tracking in Dynamic Planar Graphs

Original Abstract:
This paper presents a nested tracking framework for analyzing cycles in 2D force networks within granular materials. These materials are composed of interacting particles, whose interactions are described by a force network. Understanding the cycles within these networks at various scales and their evolution under external loads is crucial, as they significantly contribute to the mechanical and kinematic properties of the system. Our approach involves computing a cycle hierarchy by partitioning the 2D domain into segments bounded by cycles in the force network. We can adapt concepts from nested tracking graphs originally developed for merge trees by leveraging the duality between this partitioning and the cycles. We demonstrate the effectiveness of our method on two force networks derived from experiments with photoelastic disks.

Translated Abstract:
이 논문은 입자 물질 내에서 2D 힘 네트워크의 사이클을 분석하기 위한 중첩 추적 프레임워크를 제시해. 이런 물질은 서로 상호작용하는 입자로 구성되어 있고, 그 상호작용은 힘 네트워크로 설명돼. 다양한 스케일에서 이 네트워크의 사이클을 이해하고 외부 하중에 따른 변화 과정을 아는 게 중요한데, 이게 시스템의 기계적 및 운동학적 특성에 큰 영향을 미치거든.

우리의 접근 방식은 힘 네트워크의 사이클로 경계가 정해진 세그먼트로 2D 영역을 나누어서 사이클 계층을 계산하는 거야. 이 분할과 사이클 간의 이중성을 활용해서 원래 병합 트리를 위해 개발된 중첩 추적 그래프 개념을 적용할 수 있어. 우리는 포토엘라스틱 디스크 실험에서 얻은 두 개의 힘 네트워크를 통해 우리의 방법의 효과를 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06605.pdf

Title: Interactive 3D Segmentation for Primary Gross Tumor Volume in Oropharyngeal Cancer

Original Abstract:
The main treatment modality for oropharyngeal cancer (OPC) is radiotherapy, where accurate segmentation of the primary gross tumor volume (GTVp) is essential. However, accurate GTVp segmentation is challenging due to significant interobserver variability and the time-consuming nature of manual annotation, while fully automated methods can occasionally fail. An interactive deep learning (DL) model offers the advantage of automatic high-performance segmentation with the flexibility for user correction when necessary. In this study, we examine interactive DL for GTVp segmentation in OPC. We implement state-of-the-art algorithms and propose a novel two-stage Interactive Click Refinement (2S-ICR) framework. Using the 2021 HEad and neCK TumOR (HECKTOR) dataset for development and an external dataset from The University of Texas MD Anderson Cancer Center for evaluation, the 2S-ICR framework achieves a Dice similarity coefficient of 0.713 $\pm$ 0.152 without user interaction and 0.824 $\pm$ 0.099 after five interactions, outperforming existing methods in both cases.

Translated Abstract:
구강인두암(OPC)의 주요 치료 방법은 방사선 치료인데, 여기서 주된 종양 부피(GTVp)를 정확하게 분할하는 게 중요해. 하지만 GTVp 분할은 관찰자 간의 차이가 크고, 수작업으로 주석을 달다 보면 시간이 많이 걸려서 어렵거든. 완전 자동화된 방법도 가끔 실패할 수 있어. 

그래서 우리가 제안하는 건, 사용자 수정이 가능하면서도 자동으로 높은 성능의 분할을 제공하는 인터랙티브 딥러닝(DL) 모델이야. 이 연구에서는 OPC의 GTVp 분할을 위해 인터랙티브 DL을 살펴봤어. 최신 알고리즘을 사용하고, 새로운 두 단계 인터랙티브 클릭 정제(2S-ICR) 프레임워크를 제안했어.

우리는 2021 HEad and neCK TumOR (HECKTOR) 데이터셋을 개발에 사용하고, 텍사스 대학교 MD 앤더슨 암 센터의 외부 데이터셋을 평가에 활용했어. 그 결과, 2S-ICR 프레임워크는 사용자 상호작용 없이 Dice 유사도 계수가 0.713 ± 0.152를 기록했고, 다섯 번의 상호작용 후에는 0.824 ± 0.099로 기존 방법들을 능가했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06676.pdf

Title: Constructing an Interpretable Deep Denoiser by Unrolling Graph Laplacian Regularizer

Original Abstract:
An image denoiser can be used for a wide range of restoration problems via the Plug-and-Play (PnP) architecture. In this paper, we propose a general framework to build an interpretable graph-based deep denoiser (GDD) by unrolling a solution to a maximum a posteriori (MAP) problem equipped with a graph Laplacian regularizer (GLR) as signal prior. Leveraging a recent theorem showing that any (pseudo-)linear denoiser $\boldsymbol \Psi$, under mild conditions, can be mapped to a solution of a MAP denoising problem regularized using GLR, we first initialize a graph Laplacian matrix $\mathbf L$ via truncated Taylor Series Expansion (TSE) of $\boldsymbol \Psi^{-1}$. Then, we compute the MAP linear system solution by unrolling iterations of the conjugate gradient (CG) algorithm into a sequence of neural layers as a feed-forward network -- one that is amenable to parameter tuning. The resulting GDD network is "graph-interpretable", low in parameter count, and easy to initialize thanks to $\mathbf L$ derived from a known well-performing denoiser $\boldsymbol \Psi$. Experimental results show that GDD achieves competitive image denoising performance compared to competitors, but employing far fewer parameters, and is more robust to covariate shift.

Translated Abstract:
이미지 노이즈 제거기는 Plug-and-Play (PnP) 구조를 통해 다양한 복원 문제에 사용될 수 있어. 이 논문에서는 그래프 기반의 깊은 노이즈 제거기(GDD)를 만드는 일반적인 프레임워크를 제안해. 이건 그래프 라플라시안 정규화기(GLR)를 신호 사전으로 사용해서 최대 사후 확률(MAP) 문제의 해를 전개하는 방식이야.

최근의 정리에 따르면, 어떤 (유사)선형 노이즈 제거기 $\boldsymbol \Psi$는 조건이 맞으면 GLR로 정규화된 MAP 노이즈 제거 문제의 해로 변환될 수 있어. 그래서 먼저, $\boldsymbol \Psi^{-1}$의 절단된 테일러 급수 전개(TSE)를 통해 그래프 라플라시안 행렬 $\mathbf L$을 초기화해. 그 다음, 공액 경량(CG) 알고리즘의 반복을 신경망의 피드포워드 네트워크로 전개해서 MAP 선형 시스템 해를 계산해. 이 네트워크는 파라미터 조정이 가능해.

결과적으로 GDD 네트워크는 "그래프 해석 가능"하고, 파라미터 수가 적으며, 이미 잘 동작하는 노이즈 제거기 $\boldsymbol \Psi$로부터 유도된 $\mathbf L$ 덕분에 초기화하기 쉬워. 실험 결과에 따르면, GDD는 경쟁자들에 비해 경쟁력 있는 이미지 노이즈 제거 성능을 보여주면서도 훨씬 적은 파라미터를 사용하고, 공변량 변화에 더 강한 모습을 보여.

================================================================================

URL:
https://arxiv.org/pdf/2409.06687.pdf

Title: A study on deep feature extraction to detect and classify Acute Lymphoblastic Leukemia (ALL)

Original Abstract:
Acute lymphoblastic leukaemia (ALL) is a blood malignancy that mainly affects adults and children. This study looks into the use of deep learning, specifically Convolutional Neural Networks (CNNs), for the detection and classification of ALL. Conventional techniques for ALL diagnosis, such bone marrow biopsy, are costly and prone to mistakes made by hand. By utilising automated technologies, the research seeks to improve diagnostic accuracy. The research uses a variety of pre-trained CNN models, such as InceptionV3, ResNet101, VGG19, DenseNet121, MobileNetV2, and DenseNet121, to extract characteristics from pictures of blood smears. ANOVA, Recursive Feature Elimination (RFE), Random Forest, Lasso, and Principal Component Analysis (PCA) are a few of the selection approaches used to find the most relevant features after feature extraction. Following that, machine learning methods like Naïve Bayes, Random Forest, Support Vector Machine (SVM), and K-Nearest Neighbours (KNN) are used to classify these features. With an 87% accuracy rate, the ResNet101 model produced the best results, closely followed by DenseNet121 and VGG19. According to the study, CNN-based models have the potential to decrease the need for medical specialists by increasing the speed and accuracy of ALL diagnosis. To improve model performance, the study also recommends expanding and diversifying datasets and investigating more sophisticated designs such as transformers. This study highlights how well automated deep learning systems do medical diagnosis.

Translated Abstract:
급성 림프구 백혈병(ALL)은 주로 성인과 어린이에 영향을 미치는 혈액 악성종양이야. 이 연구는 ALL을 발견하고 분류하는 데 딥러닝, 특히 합성곱 신경망(CNN)을 사용하는 방법을 살펴보고 있어.

전통적인 ALL 진단 방법은 골수 생검 같은데, 이건 비용이 비싸고 사람의 실수로 인해 오류가 발생할 수 있어. 자동화된 기술을 활용함으로써 진단의 정확성을 높이려는 게 이 연구의 목표야. 연구에서는 InceptionV3, ResNet101, VGG19, DenseNet121, MobileNetV2 같은 여러 사전 훈련된 CNN 모델을 사용해서 혈액 도말 사진에서 특징을 추출했어.

특징 추출 후에는 ANOVA, 재귀적 특징 제거(RFE), 랜덤 포레스트, 라쏘, 주성분 분석(PCA) 같은 방법을 써서 가장 관련 있는 특징들을 찾아냈어. 그 다음에는 나이브 베이즈, 랜덤 포레스트, 서포트 벡터 머신(SVM), K-최근접 이웃(KNN) 같은 머신러닝 방법을 이용해 이 특징들을 분류했지. ResNet101 모델이 87%의 정확도로 가장 좋은 결과를 냈고, DenseNet121과 VGG19가 그 뒤를 이었어.

이 연구에 따르면, CNN 기반 모델이 ALL 진단의 속도와 정확성을 높여서 의료 전문가의 필요성을 줄일 수 있는 가능성이 있어. 모델 성능을 향상시키기 위해 데이터셋을 확장하고 다양화하며, 트랜스포머 같은 더 복잡한 디자인을 연구할 것도 추천하고 있어. 이 연구는 자동화된 딥러닝 시스템이 의료 진단에서 얼마나 잘 작동하는지를 강조하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06689.pdf

Title: A comprehensive study on Blood Cancer detection and classification using Convolutional Neural Network

Original Abstract:
Over the years in object detection several efficient Convolutional Neural Networks (CNN) networks, such as DenseNet201, InceptionV3, ResNet152v2, SEresNet152, VGG19, Xception gained significant attention due to their performance. Moreover, CNN paradigms have expanded to transfer learning and ensemble models from original CNN architectures. Research studies suggest that transfer learning and ensemble models are capable of increasing the accuracy of deep learning (DL) models. However, very few studies have conducted comprehensive experiments utilizing these techniques in detecting and localizing blood malignancies. Realizing the gap, this study conducted three experiments; in the first experiment -- six original CNNs were used, in the second experiment -- transfer learning and, in the third experiment a novel ensemble model DIX (DenseNet201, InceptionV3, and Xception) was developed to detect and classify blood cancer. The statistical result suggests that DIX outperformed the original and transfer learning performance, providing an accuracy of 99.12%. However, this study also provides a negative result in the case of transfer learning, as the transfer learning did not increase the accuracy of the original CNNs. Like many other cancers, blood cancer diseases require timely identification for effective treatment plans and increased survival possibilities. The high accuracy in detecting and categorization blood cancer detection using CNN suggests that the CNN model is promising in blood cancer disease detection. This research is significant in the fields of biomedical engineering, computer-aided disease diagnosis, and ML-based disease detection.

Translated Abstract:
최근 몇 년간 물체 탐지 분야에서는 DenseNet201, InceptionV3, ResNet152v2, SEresNet152, VGG19, Xception 같은 효율적인 합성곱 신경망(CNN)들이 성능 덕분에 많은 주목을 받았어. 게다가 CNN 구조는 전이 학습과 앙상블 모델로도 확장되고 있어. 연구에 따르면, 전이 학습과 앙상블 모델이 딥러닝(DL) 모델의 정확도를 높일 수 있다고 해. 하지만 혈액 악성 종양을 탐지하고 위치를 찾기 위한 실험은 아주 드물었어.

이런 gap을 인식하고, 이 연구에서는 세 가지 실험을 진행했어. 첫 번째 실험에서는 여섯 개의 원래 CNN을 사용했고, 두 번째 실험에서는 전이 학습을 적용했지. 세 번째 실험에서는 혈액암을 탐지하고 분류하기 위해 새로운 앙상블 모델 DIX(즉, DenseNet201, InceptionV3, Xception)를 개발했어. 통계 결과에 따르면, DIX는 원래 모델과 전이 학습보다 성능이 뛰어나서 99.12%의 정확도를 기록했어. 하지만 전이 학습의 경우에는 원래 CNN의 정확도를 높이지 못했다는 부정적인 결과도 있었어.

다른 암들과 마찬가지로, 혈액암 질환은 효과적인 치료 계획과 생존 가능성을 높이기 위해 신속한 식별이 필요해. CNN을 사용한 혈액암 탐지의 높은 정확도는 CNN 모델이 혈액암 탐지에 유망하다는 걸 보여주고 있어. 이 연구는 생의학 공학, 컴퓨터 지원 질병 진단, 그리고 ML 기반 질병 탐지 분야에서 중요해.

================================================================================

URL:
https://arxiv.org/pdf/2409.06699.pdf

Title: A study on Deep Convolutional Neural Networks, Transfer Learning and Ensemble Model for Breast Cancer Detection

Original Abstract:
In deep learning, transfer learning and ensemble models have shown promise in improving computer-aided disease diagnosis. However, applying the transfer learning and ensemble model is still relatively limited. Moreover, the ensemble model's development is ad-hoc, overlooks redundant layers, and suffers from imbalanced datasets and inadequate augmentation. Lastly, significant Deep Convolutional Neural Networks (D-CNNs) have been introduced to detect and classify breast cancer. Still, very few comparative studies were conducted to investigate the accuracy and efficiency of existing CNN architectures. Realising the gaps, this study compares the performance of D-CNN, which includes the original CNN, transfer learning, and an ensemble model, in detecting breast cancer. The comparison study of this paper consists of comparison using six CNN-based deep learning architectures (SE-ResNet152, MobileNetV2, VGG19, ResNet18, InceptionV3, and DenseNet-121), a transfer learning, and an ensemble model on breast cancer detection. Among the comparison of these models, the ensemble model provides the highest detection and classification accuracy of 99.94% for breast cancer detection and classification. However, this study also provides a negative result in the case of transfer learning, as the transfer learning did not increase the accuracy of the original SE-ResNet152, MobileNetV2, VGG19, ResNet18, InceptionV3, and DenseNet-121 model. The high accuracy in detecting and categorising breast cancer detection using CNN suggests that the CNN model is promising in breast cancer disease detection. This research is significant in biomedical engineering, computer-aided disease diagnosis, and ML-based disease detection.

Translated Abstract:
딥러닝에서 전이 학습과 앙상블 모델이 컴퓨터 기반 질병 진단을 개선하는 데 가능성을 보여줬어. 하지만 전이 학습과 앙상블 모델을 적용하는 건 아직 제한적이야. 게다가 앙상블 모델의 개발은 임시방편적이고, 중복된 레이어를 무시하며, 불균형 데이터셋과 부족한 데이터 증강 문제를 겪고 있어.

최근에 유방암을 감지하고 분류하기 위해 중요한 딥 컨볼루션 신경망(D-CNN)이 도입됐어. 하지만 기존 CNN 아키텍처의 정확성과 효율성을 조사한 비교 연구는 거의 없어. 이 연구는 D-CNN, 즉 원래 CNN, 전이 학습, 앙상블 모델을 비교해서 유방암을 감지하는 성능을 평가해.

이 논문에서는 유방암 감지를 위해 여섯 가지 CNN 기반 딥러닝 아키텍처(SE-ResNet152, MobileNetV2, VGG19, ResNet18, InceptionV3, DenseNet-121)와 전이 학습, 앙상블 모델을 비교했어. 이 모델들 중에서 앙상블 모델이 유방암 감지와 분류에서 99.94%의 가장 높은 정확도를 보여줬어. 하지만 전이 학습의 경우, SE-ResNet152, MobileNetV2, VGG19, ResNet18, InceptionV3, DenseNet-121 원래 모델의 정확도를 높이지 못해 부정적인 결과도 있었어.

CNN을 사용한 유방암 감지와 분류의 높은 정확도는 CNN 모델이 유방암 질병 감지에 가능성이 있다는 걸 보여줘. 이 연구는 생물 의공학, 컴퓨터 기반 질병 진단, 그리고 ML 기반 질병 감지에 중요한 의미가 있어.

================================================================================

URL:
https://arxiv.org/pdf/2110.07588.pdf

Title: Playing for 3D Human Recovery

Original Abstract:
Image- and video-based 3D human recovery (i.e., pose and shape estimation) have achieved substantial progress. However, due to the prohibitive cost of motion capture, existing datasets are often limited in scale and diversity. In this work, we obtain massive human sequences by playing the video game with automatically annotated 3D ground truths. Specifically, we contribute GTA-Human, a large-scale 3D human dataset generated with the GTA-V game engine, featuring a highly diverse set of subjects, actions, and scenarios. More importantly, we study the use of game-playing data and obtain five major insights. First, game-playing data is surprisingly effective. A simple frame-based baseline trained on GTA-Human outperforms more sophisticated methods by a large margin. For video-based methods, GTA-Human is even on par with the in-domain training set. Second, we discover that synthetic data provides critical complements to the real data that is typically collected indoor. Our investigation into domain gap provides explanations for our data mixture strategies that are simple yet useful. Third, the scale of the dataset matters. The performance boost is closely related to the additional data available. A systematic study reveals the model sensitivity to data density from multiple key aspects. Fourth, the effectiveness of GTA-Human is also attributed to the rich collection of strong supervision labels (SMPL parameters), which are otherwise expensive to acquire in real datasets. Fifth, the benefits of synthetic data extend to larger models such as deeper convolutional neural networks (CNNs) and Transformers, for which a significant impact is also observed. We hope our work could pave the way for scaling up 3D human recovery to the real world. Homepage: this https URL

Translated Abstract:
3D 인간 복구(즉, 포즈와 형태 추정)에 대한 이미지와 비디오 기반 연구가 큰 발전을 이뤘어. 하지만, 모션 캡처의 비용이 너무 비싸서 기존 데이터셋은 규모와 다양성이 한정적이야. 그래서 우리는 자동으로 주석이 달린 3D 실제 데이터를 사용해서 비디오 게임을 통해 방대한 인간 시퀀스를 얻었어. 

특히, 우리는 GTA-Human이라는 대규모 3D 인간 데이터셋을 만들었고, 이건 GTA-V 게임 엔진을 사용해서 생성된 거야. 이 데이터셋은 다양한 주제, 행동, 시나리오를 포함하고 있어. 더 중요한 건, 게임을 통해 얻은 데이터의 활용에 대한 다섯 가지 주요 통찰을 얻었다는 거야. 

첫째, 게임 데이터를 사용하니까 의외로 효과적이었어. GTA-Human에서 간단한 프레임 기반 모델이 더 복잡한 방법보다 훨씬 더 나은 성능을 보여줬어. 비디오 기반 방법에서도 GTA-Human은 도메인 내 훈련 세트와 거의 동등한 성능을 보여줬어. 

둘째, 합성 데이터가 일반적으로 실내에서 수집되는 실제 데이터에 중요한 보완이 된다는 걸 발견했어. 도메인 간의 간극을 조사해보니, 우리의 데이터 혼합 전략이 간단하면서도 유용하다는 설명이 나왔어. 

셋째, 데이터셋의 규모가 중요해. 성능 향상은 추가 데이터와 밀접하게 관련되어 있어. 체계적인 연구를 통해 모델이 데이터 밀도에 얼마나 민감한지를 여러 측면에서 보여줬어. 

넷째, GTA-Human의 효과는 강력한 감독 레이블(SMPL 파라미터)의 풍부한 수집 덕분이야. 이걸 실제 데이터셋에서 얻으려면 비용이 많이 드니까. 

다섯째, 합성 데이터의 이점은 더 깊은 컨볼루션 신경망(CNN)과 트랜스포머 같은 더 큰 모델에도 확장돼. 여기서도 상당한 영향을 확인했어. 우리는 우리의 연구가 3D 인간 복구를 실제 세계로 확장하는 데 도움이 되기를 바래.

================================================================================

URL:
https://arxiv.org/pdf/2210.11795.pdf

Title: PoseScript: Linking 3D Human Poses and Natural Language

Original Abstract:
Natural language plays a critical role in many computer vision applications, such as image captioning, visual question answering, and cross-modal retrieval, to provide fine-grained semantic information. Unfortunately, while human pose is key to human understanding, current 3D human pose datasets lack detailed language descriptions. To address this issue, we have introduced the PoseScript dataset. This dataset pairs more than six thousand 3D human poses from AMASS with rich human-annotated descriptions of the body parts and their spatial relationships. Additionally, to increase the size of the dataset to a scale that is compatible with data-hungry learning algorithms, we have proposed an elaborate captioning process that generates automatic synthetic descriptions in natural language from given 3D keypoints. This process extracts low-level pose information, known as "posecodes", using a set of simple but generic rules on the 3D keypoints. These posecodes are then combined into higher level textual descriptions using syntactic rules. With automatic annotations, the amount of available data significantly scales up (100k), making it possible to effectively pretrain deep models for finetuning on human captions. To showcase the potential of annotated poses, we present three multi-modal learning tasks that utilize the PoseScript dataset. Firstly, we develop a pipeline that maps 3D poses and textual descriptions into a joint embedding space, allowing for cross-modal retrieval of relevant poses from large-scale datasets. Secondly, we establish a baseline for a text-conditioned model generating 3D poses. Thirdly, we present a learned process for generating pose descriptions. These applications demonstrate the versatility and usefulness of annotated poses in various tasks and pave the way for future research in the field.

Translated Abstract:
자연어는 이미지 캡셔닝, 시각 질문 답변, 크로스 모달 검색 같은 많은 컴퓨터 비전 응용 프로그램에서 중요한 역할을 해. 이 과정에서 세밀한 의미 정보를 제공하는데, 인간의 포즈는 사람을 이해하는 데 필수적이지만, 현재의 3D 인간 포즈 데이터셋은 자세한 언어 설명이 부족해.

이 문제를 해결하기 위해 우리는 PoseScript 데이터셋을 만들었어. 이 데이터셋은 AMASS에서 가져온 6천 개 이상의 3D 인간 포즈와 몸의 각 부분과 그 공간적 관계에 대한 풍부한 인간 주석 설명을 쌍으로 제공해. 그리고 데이터셋의 크기를 늘려서 데이터가 많이 필요한 학습 알고리즘과 잘 맞도록 하기 위해, 주어진 3D 키포인트에서 자연어로 자동 합성 설명을 생성하는 정교한 캡셔닝 프로세스를 제안했어.

이 프로세스는 3D 키포인트에서 "포즈코드"라고 불리는 저수준의 포즈 정보를 추출해. 이 포즈코드는 간단하지만 일반적인 규칙을 이용해 생성돼. 그런 다음 이 포즈코드를 문법적 규칙을 사용해 더 높은 수준의 텍스트 설명으로 결합해. 자동 주석 덕분에 사용 가능한 데이터 양이 크게 늘어나서 (10만 개) 인간 캡션에 맞춰 딥 모델을 효과적으로 사전 훈련할 수 있게 돼.

우리는 주석이 달린 포즈의 잠재력을 보여주기 위해 PoseScript 데이터셋을 활용한 세 가지 다중 모달 학습 작업을 제안해. 첫째, 우리는 3D 포즈와 텍스트 설명을 공동 임베딩 공간으로 매핑하는 파이프라인을 개발했어. 이걸 통해 대규모 데이터셋에서 관련 포즈를 크로스 모달로 검색할 수 있어. 둘째, 텍스트에 조건을 둔 모델로 3D 포즈를 생성하는 기준선을 설정했어. 셋째, 포즈 설명을 생성하는 학습된 프로세스를 제시했어. 

이러한 응용 프로그램은 다양한 작업에서 주석이 달린 포즈의 다재다능함과 유용성을 보여주고, 이 분야의 미래 연구를 위한 길을 열어줘.

================================================================================

URL:
https://arxiv.org/pdf/2301.02315.pdf

Title: TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction

Original Abstract:
Deep saliency prediction algorithms complement the object recognition features, they typically rely on additional information, such as scene context, semantic relationships, gaze direction, and object dissimilarity. However, none of these models consider the temporal nature of gaze shifts during image observation. We introduce a novel saliency prediction model that learns to output saliency maps in sequential time intervals by exploiting human temporal attention patterns. Our approach locally modulates the saliency predictions by combining the learned temporal maps. Our experiments show that our method outperforms the state-of-the-art models, including a multi-duration saliency model, on the SALICON benchmark. Our code will be publicly available on GitHub.

Translated Abstract:
딥 세일리언시 예측 알고리즘은 물체 인식 기능을 보완해주는데, 일반적으로 장면 맥락, 의미적 관계, 시선 방향, 물체의 차별성 같은 추가 정보를 필요로 해. 하지만 기존 모델들은 이미지 관찰 중 시선 이동의 시간적 특성을 고려하지 않았어. 

우리는 인간의 시간적 주의 패턴을 활용해서 순차적인 시간 간격으로 세일리언시 맵을 출력하는 새로운 세일리언시 예측 모델을 소개해. 우리 방법은 학습된 시간 맵을 결합해서 세일리언시 예측을 지역적으로 조정해. 실험 결과, 우리 방법이 SALICON 벤치마크에서 다중 지속 시간 세일리언시 모델을 포함한 최첨단 모델들보다 성능이 뛰어난 것을 보여줬어. 

우리 코드는 GitHub에서 공개할 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2303.08727.pdf

Title: Improving Out-of-Distribution Detection with Disentangled Foreground and Background Features

Original Abstract:
Detecting out-of-distribution (OOD) inputs is a principal task for ensuring the safety of deploying deep-neural-network classifiers in open-set scenarios. OOD samples can be drawn from arbitrary distributions and exhibit deviations from in-distribution (ID) data in various dimensions, such as foreground features (e.g., objects in CIFAR100 images vs. those in CIFAR10 images) and background features (e.g., textural images vs. objects in CIFAR10). Existing methods can confound foreground and background features in training, failing to utilize the background features for OOD detection. This paper considers the importance of feature disentanglement in out-of-distribution detection and proposes the simultaneous exploitation of both foreground and background features to support the detection of OOD inputs in in out-of-distribution detection. To this end, we propose a novel framework that first disentangles foreground and background features from ID training samples via a dense prediction approach, and then learns a new classifier that can evaluate the OOD scores of test images from both foreground and background features. It is a generic framework that allows for a seamless combination with various existing OOD detection methods. Extensive experiments show that our approach 1) can substantially enhance the performance of four different state-of-the-art (SotA) OOD detection methods on multiple widely-used OOD datasets with diverse background features, and 2) achieves new SotA performance on these benchmarks.

Translated Abstract:
딥러닝 분류기를 오픈셋 상황에 배포할 때, 분포 밖(out-of-distribution, OOD) 입력을 감지하는 것은 안전성을 보장하는 중요한 작업이야. OOD 샘플은 임의의 분포에서 나올 수 있고, 여러 면에서 인-디스트리뷰션(in-distribution, ID) 데이터와 차이를 보여. 예를 들어, CIFAR100 이미지의 객체와 CIFAR10 이미지의 객체처럼 전경 특징이 다를 수 있고, 질감 이미지와 CIFAR10의 객체처럼 배경 특징도 다를 수 있어.

기존 방법들은 훈련 시 전경과 배경 특징을 혼동해서 OOD 감지를 위해 배경 특징을 잘 활용하지 못해. 이 논문은 OOD 감지에서 특징 분리의 중요성을 다루고, OOD 입력 감지를 위해 전경과 배경 특징을 동시에 활용하는 방법을 제안해.

이를 위해, 우리는 새로운 프레임워크를 제안하는데, 이 프레임워크는 먼저 ID 훈련 샘플에서 전경과 배경 특징을 분리하고, 그 다음 두 특징을 모두 활용해 테스트 이미지의 OOD 점수를 평가할 수 있는 새로운 분류기를 학습해. 이 프레임워크는 다양한 기존 OOD 감지 방법과 원활하게 결합할 수 있는 범용적인 구조야. 

많은 실험 결과, 우리의 접근 방식이 1) 여러 다양한 배경 특징을 가진 OOD 데이터셋에서 네 가지 최신 OOD 감지 방법의 성능을 크게 향상시킬 수 있고, 2) 이러한 벤치마크에서 새로운 최첨단 성능을 달성한다는 것을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2305.11891.pdf

Title: Unlocking the Use of Raw Multispectral Earth Observation Imagery for Onboard Artificial Intelligence

Original Abstract:
Nowadays, there is growing interest in applying Artificial Intelligence (AI) on board Earth Observation (EO) satellites for time-critical applications, such as natural disaster response. However, the unavailability of raw satellite data currently hinders research on lightweight pre-processing techniques and limits the exploration of end-to-end pipelines, which could offer more efficient and accurate extraction of insights directly from the source data. To fill this gap, this work presents a novel methodology to automate the creation of datasets for the detection of target events (e.g., warm thermal hotspots) or objects (e.g., vessels) from Sentinel-2 raw data and other multispectral EO pushbroom raw imagery. The presented approach first processes the raw data by applying a pipeline consisting of spatial band registration and georeferencing of the raw data pixels. Then, it detects the target events by leveraging event-specific state-of-the-art algorithms on the Level-1C products, which are mosaicked and cropped on the georeferenced correspondent raw granule area. The detected events are finally re-projected back onto the corresponding raw images. We apply the proposed methodology to realize THRawS (Thermal Hotspots in Raw Sentinel-2 data), the first dataset of Sentinel-2 raw data containing warm thermal hotspots. THRawS includes 1090 samples containing wildfires, volcanic eruptions, and 33,335 event-free acquisitions to enable thermal hotspot detection and general classification applications. This dataset and associated toolkits provide the community with both an immediately useful resource as well as a framework and methodology acting as a template for future additions. With this work, we hope to pave the way for research on energy-efficient pre-processing algorithms and AI-based end-to-end processing systems on board EO satellites.

Translated Abstract:
요즘 인공지능(AI)을 지구 관측(EO) 위성에 적용하는 것에 대한 관심이 커지고 있어. 특히 자연 재해 대응 같은 시간에 민감한 응용에서 말이야. 하지만 현재 원시 위성 데이터가 없어서, 가벼운 전처리 기법에 대한 연구가 어려워지고, 소스 데이터에서 직접 통찰력을 더 효율적이고 정확하게 추출할 수 있는 엔드 투 엔드 파이프라인을 탐색하는 데도 제약이 있어. 

이 문제를 해결하기 위해, 우리는 센티넬-2 원시 데이터와 다른 다중 스펙트럼 EO 푸시브룸 원시 이미지를 사용해서 목표 이벤트(예: 따뜻한 열점)나 객체(예: 선박)를 탐지하기 위한 데이터셋을 자동으로 만드는 새로운 방법론을 제시했어. 이 방법은 먼저 원시 데이터를 처리해. 여기서는 공간 밴드 등록과 원시 데이터 픽셀의 지리 참조를 포함하는 파이프라인을 적용해. 그 다음, 이벤트별 최신 알고리즘을 활용해서 Level-1C 제품에서 목표 이벤트를 탐지해. 이 제품은 지리 참조된 원시 그래뉼 영역에 맞춰 모자이크하고 잘라낸 거야. 탐지된 이벤트는 다시 해당 원시 이미지에 재투영돼.

우리는 이 방법론을 이용해 THRawS(센티넬-2 원시 데이터의 열점)를 만들어냈어. 이 데이터셋은 따뜻한 열점이 포함된 첫 번째 센티넬-2 원시 데이터셋이야. THRawS는 1090개의 샘플을 포함하고 있으며, 여기에는 산불, 화산 폭발, 그리고 33,335개의 이벤트가 없는 수집이 포함돼. 이 데이터셋은 열점 탐지와 일반 분류 응용을 가능하게 해. 이 데이터셋과 관련된 툴킷은 커뮤니티에 즉시 유용한 자원을 제공하고, 앞으로의 추가 작업을 위한 프레임워크와 방법론의 템플릿 역할을 해. 

이 연구를 통해 우리는 에너지 효율적인 전처리 알고리즘과 AI 기반의 엔드 투 엔드 처리 시스템에 대한 연구를 촉진하고 싶어.

================================================================================

URL:
https://arxiv.org/pdf/2305.17644.pdf

Title: Caterpillar: A Pure-MLP Architecture with Shifted-Pillars-Concatenation

Original Abstract:
Modeling in Computer Vision has evolved to MLPs. Vision MLPs naturally lack local modeling capability, to which the simplest treatment is combined with convolutional layers. Convolution, famous for its sliding window scheme, also suffers from this scheme of redundancy and lower parallel computation. In this paper, we seek to dispense with the windowing scheme and introduce a more elaborate and parallelizable method to exploit locality. To this end, we propose a new MLP module, namely Shifted-Pillars-Concatenation (SPC), that consists of two steps of processes: (1) Pillars-Shift, which generates four neighboring maps by shifting the input image along four directions, and (2) Pillars-Concatenation, which applies linear transformations and concatenation on the maps to aggregate local features. SPC module offers superior local modeling power and performance gains, making it a promising alternative to the convolutional layer. Then, we build a pure-MLP architecture called Caterpillar by replacing the convolutional layer with the SPC module in a hybrid model of sMLPNet. Extensive experiments show Caterpillar's excellent performance on both small-scale and ImageNet-1k classification benchmarks, with remarkable scalability and transfer capability possessed as well. The code is available at this https URL.

Translated Abstract:
컴퓨터 비전에서 모델링이 MLP(다층 퍼셉트론)로 발전했어. 비전 MLP는 지역적인 모델링 능력이 부족한데, 이걸 해결하기 위해 가장 간단한 방법은 합성곱 층을 결합하는 거야. 합성곱은 슬라이딩 윈도우 방식으로 유명하지만, 이 방식도 중복성과 낮은 병렬 계산 문제를 겪고 있어.

이 논문에서는 윈도우 방식을 없애고 지역성을 활용하기 위해 더 정교하고 병렬화 가능한 방법을 소개하려고 해. 그래서 우리가 제안하는 새로운 MLP 모듈은 'Shifted-Pillars-Concatenation' (SPC)라는 이름을 가지고 있어. 이 모듈은 두 단계로 이루어져 있어: (1) Pillars-Shift는 입력 이미지를 네 방향으로 이동시켜서 네 개의 이웃 맵을 생성하고, (2) Pillars-Concatenation은 이 맵에 선형 변환과 연결을 적용해서 지역 특징을 모아.

SPC 모듈은 지역 모델링 능력과 성능이 뛰어나, 합성곱 층의 유망한 대안이 될 수 있어. 그리고 우리는 합성곱 층을 SPC 모듈로 바꿔서 'Caterpillar'라는 순수 MLP 아키텍처를 만들었어. 이건 sMLPNet의 하이브리드 모델에서 온 거야. 많은 실험을 통해 Caterpillar가 소규모와 ImageNet-1k 분류 벤치마크에서 뛰어난 성능을 보였고, 확장성과 전이 능력도 뛰어난 걸 확인했어. 코드도 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2309.05388.pdf

Title: Robust Single Rotation Averaging Revisited

Original Abstract:
In this work, we propose a novel method for robust single rotation averaging that can efficiently handle an extremely large fraction of outliers. Our approach is to minimize the total truncated least unsquared deviations (TLUD) cost of geodesic distances. The proposed algorithm consists of three steps: First, we consider each input rotation as a potential initial solution and choose the one that yields the least sum of truncated chordal deviations. Next, we obtain the inlier set using the initial solution and compute its chordal $L_2$-mean. Finally, starting from this estimate, we iteratively compute the geodesic $L_1$-mean of the inliers using the Weiszfeld algorithm on $SO(3)$. An extensive evaluation shows that our method is robust against up to 99% outliers given a sufficient number of accurate inliers, outperforming the current state of the art.

Translated Abstract:
이 연구에서는 많은 아웃라이어를 효율적으로 처리할 수 있는 강력한 단일 회전 평균화 방법을 제안해. 우리의 접근법은 지오데식 거리의 총 절단 최소 제곱 편차(TLUD) 비용을 최소화하는 거야.

제안한 알고리즘은 세 단계로 이루어져 있어. 첫 번째로, 각 입력 회전을 잠재적인 초기 솔루션으로 고려하고, 절단된 코드 편차의 합이 가장 적은 것을 선택해. 다음으로, 초기 솔루션을 사용해 인라이어 세트를 얻고, 그 코드 $L_2$ 평균을 계산해.

마지막으로, 이 추정값에서 시작해서 Weiszfeld 알고리즘을 이용해 인라이어의 지오데식 $L_1$ 평균을 반복적으로 계산해. 광범위한 평가 결과, 우리의 방법은 충분한 정확한 인라이어가 주어지면 최대 99%의 아웃라이어에 대해 강력하게 작동하며, 현재의 최첨단 기술보다 더 뛰어난 성능을 보여.

================================================================================

URL:
https://arxiv.org/pdf/2309.09947.pdf

Title: Deep Visual Odometry with Events and Frames

Original Abstract:
Visual Odometry (VO) is crucial for autonomous robotic navigation, especially in GPS-denied environments like planetary terrains. To improve robustness, recent model-based VO systems have begun combining standard and event-based cameras. While event cameras excel in low-light and high-speed motion, standard cameras provide dense and easier-to-track features. However, the field of image- and event-based VO still predominantly relies on model-based methods and is yet to fully integrate recent image-only advancements leveraging end-to-end learning-based architectures. Seamlessly integrating the two modalities remains challenging due to their different nature, one asynchronous, the other not, limiting the potential for a more effective image- and event-based VO. We introduce RAMP-VO, the first end-to-end learned image- and event-based VO system. It leverages novel Recurrent, Asynchronous, and Massively Parallel (RAMP) encoders capable of fusing asynchronous events with image data, providing 8x faster inference and 33% more accurate predictions than existing solutions. Despite being trained only in simulation, RAMP-VO outperforms previous methods on the newly introduced Apollo and Malapert datasets, and on existing benchmarks, where it improves image- and event-based methods by 58.8% and 30.6%, paving the way for robust and asynchronous VO in space.

Translated Abstract:
비주얼 오도메트리(VO)는 자율 로봇 내비게이션에 정말 중요해, 특히 GPS가 안 되는 곳에서 더 그렇고. 최근에는 기존 카메라와 이벤트 카메라를 결합해서 더 강력한 VO 시스템을 만들려고 하고 있어. 이벤트 카메라는 어두운 곳이나 빠른 움직임에서 잘 작동하고, 기존 카메라는 더 밀집된 특징을 제공해서 추적하기 쉬워. 

하지만 이미지 기반과 이벤트 기반 VO는 여전히 모델 기반 방법에 크게 의존하고 있고, 최근의 이미지 전용 기술을 완전히 통합하지 못했어. 두 가지 모달리티를 매끄럽게 통합하는 게 어려운데, 하나는 비동기적이고 다른 하나는 그렇지 않아서 더 효과적인 이미지-이벤트 기반 VO의 가능성이 제한되고 있어. 

우리는 RAMP-VO라는 첫 번째 엔드 투 엔드 학습 기반 이미지-이벤트 VO 시스템을 소개해. 이 시스템은 비동기 이벤트와 이미지 데이터를 융합할 수 있는 새로운 순환형(RAMP) 인코더를 활용해서, 기존 솔루션보다 8배 빠른 추론 속도와 33% 더 정확한 예측을 제공해. RAMP-VO는 시뮬레이션에서만 훈련됐지만, 새로운 아폴로와 말라페르트 데이터셋에서도 이전 방법들을 뛰어넘었고, 기존 벤치마크에서도 이미지-이벤트 기반 방법을 각각 58.8%와 30.6% 개선했어. 이로써 우주에서 강력하고 비동기적인 VO의 길을 열었어.

================================================================================

URL:
https://arxiv.org/pdf/2310.00158.pdf

Title: Feedback-guided Data Synthesis for Imbalanced Classification

Original Abstract:
Current status quo in machine learning is to use static datasets of real images for training, which often come from long-tailed distributions. With the recent advances in generative models, researchers have started augmenting these static datasets with synthetic data, reporting moderate performance improvements on classification tasks. We hypothesize that these performance gains are limited by the lack of feedback from the classifier to the generative model, which would promote the usefulness of the generated samples to improve the classifier's performance. In this work, we introduce a framework for augmenting static datasets with useful synthetic samples, which leverages one-shot feedback from the classifier to drive the sampling of the generative model. In order for the framework to be effective, we find that the samples must be close to the support of the real data of the task at hand, and be sufficiently diverse. We validate three feedback criteria on a long-tailed dataset (ImageNet-LT) as well as a group-imbalanced dataset (NICO++). On ImageNet-LT, we achieve state-of-the-art results, with over 4 percent improvement on underrepresented classes while being twice efficient in terms of the number of generated synthetic samples. NICO++ also enjoys marked boosts of over 5 percent in worst group accuracy. With these results, our framework paves the path towards effectively leveraging state-of-the-art text-to-image models as data sources that can be queried to improve downstream applications.

Translated Abstract:
현재 머신러닝에서는 실제 이미지로 구성된 고정된 데이터셋을 사용해 훈련하는 게 일반적이야. 이 데이터셋은 종종 긴 꼬리 분포에서 나오는 경우가 많아. 최근 생성 모델이 발전하면서 연구자들은 이런 고정된 데이터셋에 합성 데이터를 추가하기 시작했어. 그리고 분류 작업에서 성능이 중간 정도로 개선되었다고 보고하고 있어.

우리는 이런 성능 향상이 분류기에서 생성 모델로의 피드백 부족 때문에 제한된다고 생각해. 피드백이 있으면 생성된 샘플이 분류기의 성능을 높이는 데 더 유용할 수 있을 거야. 이 연구에서는 유용한 합성 샘플로 고정된 데이터셋을 증강할 수 있는 프레임워크를 소개해. 이 프레임워크는 분류기에서 한 번의 피드백을 받아서 생성 모델의 샘플링을 유도해.

프레임워크가 효과적이려면, 샘플들이 실제 데이터의 지원에 가까워야 하고 다양성도 충분해야 한다고 알아냈어. 우리는 긴 꼬리 데이터셋(ImageNet-LT)과 그룹 불균형 데이터셋(NICO++)에서 세 가지 피드백 기준을 검증했어. ImageNet-LT에서는, 과소 표현된 클래스에서 4% 이상의 개선을 이루며 최신 기술 수준의 결과를 달성했어. 생성된 합성 샘플 수에 비해 두 배 효율적이기도 했고. NICO++에서도 최악의 그룹 정확도에서 5% 이상 개선된 효과를 봤어.

이런 결과로 우리 프레임워크는 최신 텍스트-투-이미지 모델을 데이터 소스로 활용해 다운스트림 응용 프로그램을 개선하는 데 기여할 수 있는 길을 열어줘.

================================================================================

URL:
https://arxiv.org/pdf/2310.07511.pdf

Title: Learning a Cross-modality Anomaly Detector for Remote Sensing Imagery

Original Abstract:
Remote sensing anomaly detector can find the objects deviating from the background as potential targets for Earth monitoring. Given the diversity in earth anomaly types, designing a transferring model with cross-modality detection ability should be cost-effective and flexible to new earth observation sources and anomaly types. However, the current anomaly detectors aim to learn the certain background distribution, the trained model cannot be transferred to unseen images. Inspired by the fact that the deviation metric for score ranking is consistent and independent from the image distribution, this study exploits the learning target conversion from the varying background distribution to the consistent deviation metric. We theoretically prove that the large-margin condition in labeled samples ensures the transferring ability of learned deviation metric. To satisfy this condition, two large margin losses for pixel-level and feature-level deviation ranking are proposed respectively. Since the real anomalies are difficult to acquire, anomaly simulation strategies are designed to compute the model loss. With the large-margin learning for deviation metric, the trained model achieves cross-modality detection ability in five modalities including hyperspectral, visible light, synthetic aperture radar (SAR), infrared and low-light in zero-shot manner.

Translated Abstract:
원거리 감지 이상 탐지기는 배경과 다른 물체를 찾아서 지구 모니터링의 잠재적 목표로 삼을 수 있어. 지구에서 발생하는 이상 현상은 다양하기 때문에, 다양한 관측 자료와 이상 유형에 유연하게 대응할 수 있는 전이 모델을 만드는 게 중요해.

하지만 현재의 이상 탐지기는 특정 배경 분포를 학습하는 데 초점을 맞추고 있어서, 학습된 모델은 보지 못한 이미지에 적용하기 어려워. 이 연구는 점수 순위 매기기 위한 편차 메트릭이 이미지 분포와는 관계없이 일관성이 있다는 점에서 착안했어. 그래서 배경 분포가 달라도 일관된 편차 메트릭으로 학습 목표를 변환하는 방법을 탐구했어.

이론적으로, 레이블이 붙은 샘플에서 큰 여유 조건이 학습된 편차 메트릭의 전이 능력을 보장한다는 걸 증명했어. 이 조건을 만족하기 위해서, 픽셀 수준과 특징 수준의 편차 순위를 위한 두 가지 큰 여유 손실 함수를 제안했어. 실제 이상 현상을 얻기는 어렵기 때문에, 모델 손실을 계산하기 위한 이상 시뮬레이션 전략도 설계했어.

큰 여유 학습을 통해 편차 메트릭을 학습한 이 모델은 하이퍼스펙트럴, 가시광선, 합성 개구 레이더(SAR), 적외선, 저조도 등 다섯 가지 모드에서 제로샷 방식으로 크로스 모달리티 탐지 능력을 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2310.11239.pdf

Title: LiDAR-based 4D Occupancy Completion and Forecasting

Original Abstract:
Scene completion and forecasting are two popular perception problems in research for mobile agents like autonomous vehicles. Existing approaches treat the two problems in isolation, resulting in a separate perception of the two aspects. In this paper, we introduce a novel LiDAR perception task of Occupancy Completion and Forecasting (OCF) in the context of autonomous driving to unify these aspects into a cohesive framework. This task requires new algorithms to address three challenges altogether: (1) sparse-to-dense reconstruction, (2) partial-to-complete hallucination, and (3) 3D-to-4D prediction. To enable supervision and evaluation, we curate a large-scale dataset termed OCFBench from public autonomous driving datasets. We analyze the performance of closely related existing baseline models and our own ones on our dataset. We envision that this research will inspire and call for further investigation in this evolving and crucial area of 4D perception. Our code for data curation and baseline implementation is available at this https URL.

Translated Abstract:
장면 완성과 예측은 자율주행차 같은 모바일 에이전트의 연구에서 인기 있는 인식 문제야. 기존의 접근 방식은 이 두 문제를 따로 다루어서 각각을 따로 인식하게 만들어. 

이 논문에서는 자율주행의 맥락에서 Occupancy Completion and Forecasting (OCF)라는 새로운 LiDAR 인식 과제를 소개해. 이 과제는 두 가지 측면을 통합할 수 있는 일관된 프레임워크를 만드는 거야. 이 작업은 세 가지 도전을 모두 해결할 새로운 알고리즘이 필요해: (1) 희소에서 밀집으로의 재구성, (2) 부분에서 전체로의 환상 생성, (3) 3D에서 4D로의 예측이야. 

감독과 평가를 가능하게 하기 위해, 우리는 공공 자율주행 데이터셋에서 OCFBench라는 대규모 데이터셋을 만들었어. 우리는 이 데이터셋에서 기존의 관련 기준 모델들과 우리 모델의 성능을 분석했어. 이 연구가 4D 인식이라는 발전하고 중요한 분야에 대한 추가 연구를 자극할 것이라고 기대해. 데이터 수집과 기준 구현을 위한 코드는 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2312.00739.pdf

Title: Adversarial Score Distillation: When score distillation meets GAN

Original Abstract:
Existing score distillation methods are sensitive to classifier-free guidance (CFG) scale: manifested as over-smoothness or instability at small CFG scales, while over-saturation at large ones. To explain and analyze these issues, we revisit the derivation of Score Distillation Sampling (SDS) and decipher existing score distillation with the Wasserstein Generative Adversarial Network (WGAN) paradigm. With the WGAN paradigm, we find that existing score distillation either employs a fixed sub-optimal discriminator or conducts incomplete discriminator optimization, resulting in the scale-sensitive issue. We propose the Adversarial Score Distillation (ASD), which maintains an optimizable discriminator and updates it using the complete optimization objective. Experiments show that the proposed ASD performs favorably in 2D distillation and text-to-3D tasks against existing methods. Furthermore, to explore the generalization ability of our WGAN paradigm, we extend ASD to the image editing task, which achieves competitive results. The project page and code are at this https URL.

Translated Abstract:
기존의 스코어 증류 방법들은 분류기 없는 가이드(scale)에 민감해. 작은 scale에서는 너무 부드럽거나 불안정하고, 큰 scale에서는 과도하게 포화되는 문제가 있어. 이 문제를 설명하고 분석하기 위해, 스코어 증류 샘플링(SDS)의 유도 과정을 다시 살펴보고, 기존의 스코어 증류를 워서스틴 생성 적대 신경망(WGAN) 패러다임으로 해석해봤어.

WGAN 패러다임을 통해, 기존의 스코어 증류가 고정된 비최적 판별기를 사용하거나 불완전한 판별기 최적화를 수행하고 있다는 걸 알게 됐어. 이게 바로 scale에 민감한 문제로 이어지는 거지. 그래서 우리는 적대적 스코어 증류(ASD)를 제안했어. 이 방법은 최적화 가능한 판별기를 유지하고, 완전한 최적화 목표를 사용해 업데이트해.

실험 결과, ASD가 2D 증류와 텍스트-투-3D 작업에서 기존 방법들에 비해 좋은 성과를 내는 걸 확인했어. 더 나아가 WGAN 패러다임의 일반화 능력을 탐색하기 위해, ASD를 이미지 편집 작업으로 확장했더니 경쟁력 있는 결과를 얻었어. 프로젝트 페이지와 코드는 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2401.07951.pdf

Title: Image Similarity using An Ensemble of Context-Sensitive Models

Original Abstract:
Image similarity has been extensively studied in computer vision. In recent years, machine-learned models have shown their ability to encode more semantics than traditional multivariate metrics. However, in labelling semantic similarity, assigning a numerical score to a pair of images is impractical, making the improvement and comparisons on the task difficult. In this work, we present a more intuitive approach to build and compare image similarity models based on labelled data in the form of A:R vs B:R, i.e., determining if an image A is closer to a reference image R than another image B. We address the challenges of sparse sampling in the image space (R, A, B) and biases in the models trained with context-based data by using an ensemble model. Our testing results show that the ensemble model constructed performs ~5% better than the best individual context-sensitive models. They also performed better than the models that were directly fine-tuned using mixed imagery data as well as existing deep embeddings, e.g., CLIP and DINO. This work demonstrates that context-based labelling and model training can be effective when an appropriate ensemble approach is used to alleviate the limitation due to sparse sampling.

Translated Abstract:
이미지 유사성은 컴퓨터 비전에서 많이 연구되어 왔어. 최근 몇 년 동안, 머신러닝 모델들이 전통적인 다변량 지표보다 더 많은 의미를 인코딩할 수 있다는 걸 보여줬지. 하지만 의미 유사성을 라벨링하는 데 있어서 두 이미지 쌍에 점수를 매기는 건 현실적으로 힘들어서, 작업의 개선이나 비교가 어려워.

이번 연구에서는 A:R 대 B:R 형태의 라벨이 있는 데이터를 기반으로 이미지 유사성 모델을 만들고 비교하는 좀 더 직관적인 접근 방식을 제안해. 여기서 A라는 이미지가 참조 이미지 R에 얼마나 가까운지, 다른 이미지 B보다 판단하는 방식이야. 우리는 이미지 공간(R, A, B)에서의 희소 샘플링 문제와 컨텍스트 기반 데이터로 훈련된 모델의 편향 문제를 앙상블 모델을 사용해 해결했어.

테스트 결과, 우리가 만든 앙상블 모델이 가장 좋은 개별 컨텍스트 민감 모델보다 약 5% 더 좋은 성능을 보였어. 또한 혼합 이미지 데이터를 사용해 직접 미세 조정한 모델이나 기존의 딥 임베딩, 예를 들어 CLIP와 DINO보다도 성능이 더 좋았어. 이 연구는 적절한 앙상블 접근 방식을 사용하면 희소 샘플링으로 인한 한계를 완화하면서 컨텍스트 기반의 라벨링과 모델 훈련이 효과적일 수 있다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2401.11067.pdf

Title: Make-A-Shape: a Ten-Million-scale 3D Shape Model

Original Abstract:
Significant progress has been made in training large generative models for natural language and images. Yet, the advancement of 3D generative models is hindered by their substantial resource demands for training, along with inefficient, non-compact, and less expressive representations. This paper introduces Make-A-Shape, a new 3D generative model designed for efficient training on a vast scale, capable of utilizing 10 millions publicly-available shapes. Technical-wise, we first innovate a wavelet-tree representation to compactly encode shapes by formulating the subband coefficient filtering scheme to efficiently exploit coefficient relations. We then make the representation generatable by a diffusion model by devising the subband coefficients packing scheme to layout the representation in a low-resolution grid. Further, we derive the subband adaptive training strategy to train our model to effectively learn to generate coarse and detail wavelet coefficients. Last, we extend our framework to be controlled by additional input conditions to enable it to generate shapes from assorted modalities, e.g., single/multi-view images, point clouds, and low-resolution voxels. In our extensive set of experiments, we demonstrate various applications, such as unconditional generation, shape completion, and conditional generation on a wide range of modalities. Our approach not only surpasses the state of the art in delivering high-quality results but also efficiently generates shapes within a few seconds, often achieving this in just 2 seconds for most conditions. Our source code is available at this https URL.

Translated Abstract:
자연어와 이미지에 대한 대규모 생성 모델 훈련에서 큰 발전이 있었어. 하지만 3D 생성 모델은 훈련에 필요한 자원이 많고, 비효율적이며, 표현력이 떨어지는 문제가 있어서 발전이 더디고 있어. 

이 논문에서는 Make-A-Shape라는 새로운 3D 생성 모델을 소개해. 이 모델은 1천만 개의 공개된 형태를 이용해 효율적으로 대규모 훈련을 할 수 있어. 기술적으로, 우리는 먼저 웨이브렛 트리 표현 방식을 혁신했어. 이 방법으로 형태를 간결하게 인코딩할 수 있고, 서브밴드 계수 필터링 방식을 통해 계수 간의 관계를 효과적으로 활용할 수 있어. 

그 다음에는 이 표현을 생성 가능하게 만들기 위해 확산 모델을 사용해서 서브밴드 계수를 저해상도 그리드에 배치하는 방식을 개발했어. 또, 서브밴드 적응 훈련 전략을 도출해서 모델이 거친 웨이브렛 계수와 세부 웨이브렛 계수를 효과적으로 학습할 수 있도록 훈련했어. 마지막으로, 추가 입력 조건으로 제어할 수 있도록 프레임워크를 확장해서 다양한 방식으로부터 형태를 생성할 수 있게 했어. 예를 들어, 단일/다중 뷰 이미지, 포인트 클라우드, 저해상도 복셀 등이 있어. 

우리는 다양한 실험을 통해 무조건 생성, 형태 완성, 조건부 생성 등 여러 응용을 보여줬어. 우리의 접근 방식은 고품질 결과를 제공하는 데 있어 최신 기술을 능가할 뿐만 아니라, 몇 초 안에 형태를 효율적으로 생성할 수 있어. 대부분의 조건에서는 보통 2초 만에 이걸 달성하곤 해. 우리의 소스 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2402.05610.pdf

Title: Extending 6D Object Pose Estimators for Stereo Vision

Original Abstract:
Estimating the 6D pose of objects accurately, quickly, and robustly remains a difficult task. However, recent methods for directly regressing poses from RGB images using dense features have achieved state-of-the-art results. Stereo vision, which provides an additional perspective on the object, can help reduce pose ambiguity and occlusion. Moreover, stereo can directly infer the distance of an object, while mono-vision requires internalized knowledge of the object's size. To extend the state-of-the-art in 6D object pose estimation to stereo, we created a BOP compatible stereo version of the YCB-V dataset. Our method outperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo vision and can easily be adopted for other dense feature-based algorithms.

Translated Abstract:
물체의 6D 포즈를 정확하고 빠르며 강력하게 추정하는 건 여전히 어려운 일이야. 하지만 최근의 방법들은 RGB 이미지에서 밀집된 특징을 사용해서 포즈를 직접 회귀하는 방식으로 최고의 결과를 달성했어. 

스테레오 비전은 물체에 대한 추가적인 시점을 제공해서 포즈의 애매함과 가림 현상을 줄이는 데 도움을 줄 수 있어. 게다가 스테레오는 물체의 거리를 직접 추정할 수 있지만, 모노 비전은 물체의 크기에 대한 내부 지식이 필요해. 

우리는 스테레오에서 6D 물체 포즈 추정의 최신 기술을 확장하기 위해 BOP와 호환되는 스테레오 버전의 YCB-V 데이터셋을 만들었어. 우리의 방법은 스테레오 비전을 활용해서 최신 6D 포즈 추정 알고리즘보다 더 뛰어난 성능을 보이고, 다른 밀집 특징 기반 알고리즘에도 쉽게 적용될 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2402.18307.pdf

Title: Multi-Scale Denoising in the Feature Space for Low-Light Instance Segmentation

Original Abstract:
Instance segmentation for low-light imagery remains largely unexplored due to the challenges imposed by such conditions, for example shot noise due to low photon count, color distortions and reduced contrast. In this paper, we propose an end-to-end solution to address this challenging task. Our proposed method implements weighted non-local blocks (wNLB) in the feature extractor. This integration enables an inherent denoising process at the feature level. As a result, our method eliminates the need for aligned ground truth images during training, thus supporting training on real-world low-light datasets. We introduce additional learnable weights at each layer in order to enhance the network's adaptability to real-world noise characteristics, which affect different feature scales in different ways. Experimental results on several object detectors show that the proposed method outperforms the pretrained networks with an Average Precision (AP) improvement of at least +7.6, with the introduction of wNLB further enhancing AP by upto +1.3.

Translated Abstract:
저조도 이미지에서의 인스턴스 분할은 여러 가지 어려움 때문에 많이 연구되지 않았어. 예를 들어, 저조도에서 발생하는 샷 노이즈, 색 왜곡, 그리고 대비 감소 같은 문제들이 있지. 이 논문에서는 이런 어려운 과제를 해결하기 위한 종단 간(end-to-end) 솔루션을 제안해.

우리의 방법은 특징 추출기에서 가중치 비국소 블록(weighted non-local blocks, wNLB)을 사용해. 이 통합 덕분에, 특징 수준에서 자연스럽게 노이즈 제거가 이루어져. 그래서 우리 방법은 훈련할 때 정렬된 진실 이미지가 필요 없어져서, 실제 저조도 데이터셋을 이용한 훈련이 가능해졌어. 

각 레이어에 추가적인 학습 가능한 가중치를 도입해서, 네트워크가 실제 노이즈 특성에 더 잘 적응할 수 있도록 했어. 이 노이즈는 서로 다른 특징 스케일에 다르게 영향을 미치거든. 여러 객체 탐지기에서 실험한 결과, 제안한 방법이 사전 훈련된 네트워크들보다 평균 정확도(Average Precision, AP)가 최소 +7.6 향상된 걸 보여줬어. wNLB를 도입하면 AP가 최대 +1.3 더 향상되기도 했어.

================================================================================

URL:
https://arxiv.org/pdf/2403.10094.pdf

Title: RangeLDM: Fast Realistic LiDAR Point Cloud Generation

Original Abstract:
Autonomous driving demands high-quality LiDAR data, yet the cost of physical LiDAR sensors presents a significant scaling-up challenge. While recent efforts have explored deep generative models to address this issue, they often consume substantial computational resources with slow generation speeds while suffering from a lack of realism. To address these limitations, we introduce RangeLDM, a novel approach for rapidly generating high-quality range-view LiDAR point clouds via latent diffusion models. We achieve this by correcting range-view data distribution for accurate projection from point clouds to range images via Hough voting, which has a critical impact on generative learning. We then compress the range images into a latent space with a variational autoencoder, and leverage a diffusion model to enhance expressivity. Additionally, we instruct the model to preserve 3D structural fidelity by devising a range-guided discriminator. Experimental results on KITTI-360 and nuScenes datasets demonstrate both the robust expressiveness and fast speed of our LiDAR point cloud generation.

Translated Abstract:
자율주행은 고품질의 LiDAR 데이터가 필요하지만, 실제 LiDAR 센서의 비용이 문제를 일으켜서 확장이 어렵다. 최근에는 이 문제를 해결하기 위해 딥 생성 모델을 사용하는 시도가 있었지만, 많은 계산 자원을 소모하고 생성 속도가 느리며, 사실감이 부족한 경우가 많다.

이런 한계를 극복하기 위해 우리는 RangeLDM이라는 새로운 접근 방식을 소개한다. 이 방법은 잠재 확산 모델을 이용해 고품질의 범위 뷰 LiDAR 포인트 클라우드를 빠르게 생성하는 것이다. 우리는 Hough 투표를 통해 포인트 클라우드에서 범위 이미지로의 정확한 투사를 위해 범위 뷰 데이터 분포를 수정한다. 이 과정은 생성 학습에 매우 중요한 영향을 미친다.

그 다음, 우리는 범위 이미지를 변량 오토인코더로 잠재 공간에 압축하고, 확산 모델을 활용해 표현력을 높인다. 또한, 3D 구조의 신뢰성을 유지하기 위해 범위 유도 판별기를 설계하여 모델을 훈련시킨다. KITTI-360과 nuScenes 데이터셋에서의 실험 결과는 우리의 LiDAR 포인트 클라우드 생성이 강력한 표현력과 빠른 속도를 가지고 있음을 보여준다.

================================================================================

URL:
https://arxiv.org/pdf/2404.00785.pdf

Title: Disentangling Hippocampal Shape Variations: A Study of Neurological Disorders Using Mesh Variational Autoencoder with Contrastive Learning

Original Abstract:
This paper presents a comprehensive study focused on disentangling hippocampal shape variations from diffusion tensor imaging (DTI) datasets within the context of neurological disorders. Leveraging a Graph Variational Autoencoder (VAE) enhanced with Supervised Contrastive Learning, our approach aims to improve interpretability by disentangling two distinct latent variables corresponding to age and the presence of diseases. In our ablation study, we investigate a range of VAE architectures and contrastive loss functions, showcasing the enhanced disentanglement capabilities of our approach. This evaluation uses synthetic 3D torus mesh data and real 3D hippocampal mesh datasets derived from the DTI hippocampal dataset. Our supervised disentanglement model outperforms several state-of-the-art (SOTA) methods like attribute and guided VAEs in terms of disentanglement scores. Our model distinguishes between age groups and disease status in patients with Multiple Sclerosis (MS) using the hippocampus data. Our Graph VAE with Supervised Contrastive Learning shows the volume changes of the hippocampus of MS populations at different ages, and the result is consistent with the current neuroimaging literature. This research provides valuable insights into the relationship between neurological disorder and hippocampal shape changes in different age groups of MS populations using a Graph VAE with Supervised Contrastive loss.

Translated Abstract:
이 논문은 신경 장애의 맥락에서 확산 텐서 이미징(DTI) 데이터셋을 사용해 해마의 형태 변화를 분리하는 포괄적인 연구를 다뤄. 우리는 감독된 대조 학습으로 강화된 그래프 변분 오토인코더(Graph VAE)를 활용해 나이와 질병 존재에 해당하는 두 가지 잠재 변수를 분리함으로써 해석 가능성을 높이는 걸 목표로 해. 

우리는 다양한 VAE 구조와 대조 손실 함수를 조사하는 절제 연구를 진행했어. 그 결과, 우리 접근 방식의 뛰어난 분리 능력을 보여줬지. 이 평가는 합성 3D 토러스 메쉬 데이터와 DTI 해마 데이터셋에서 얻은 실제 3D 해마 메쉬 데이터셋을 사용했어. 

우리의 감독된 분리 모델은 속성 및 가이드 VAE와 같은 여러 최신 방법들보다 분리 점수에서 더 나은 성능을 보였어. 이 모델은 다발성 경화증(MS) 환자들 사이에서 나이 그룹과 질병 상태를 구분할 수 있어. 우리 그래프 VAE는 다양한 나이에서 MS 집단의 해마 볼륨 변화를 보여주었고, 그 결과는 현재 신경 이미징 문헌과 일치해. 

이 연구는 감독된 대조 손실을 가진 그래프 VAE를 사용해 MS 집단의 다양한 나이 그룹에서 신경 장애와 해마 형태 변화 간의 관계에 대한 귀중한 통찰을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2404.09654.pdf

Title: Do LLMs Understand Visual Anomalies? Uncovering LLM's Capabilities in Zero-shot Anomaly Detection

Original Abstract:
Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language. Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts. However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization. In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model. We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM). This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation. We further introduce a novel fine-grained aligner to fuse local pixel-level semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces. Extensive evaluations on MVTec and VisA datasets confirm ALFA's effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec and 8.9% on VisA compared to state-of-the-art approaches.

Translated Abstract:
대형 비전-언어 모델(LVLMs)은 자연어에 의해 안내된 시각적 표현을 잘 이해하는 능력이 뛰어나. 최근 연구에서는 LVLM을 활용해 제로샷 시각 이상 탐지(VAD) 문제를 해결하려고 했어. 이때, 정상 및 비정상 상태를 나타내는 텍스트 설명과 이미지를 쌍으로 사용했는데, 이걸 이상 프롬프트라고 해. 

하지만 기존 방법들은 정적인 이상 프롬프트에 의존하고 있어서 의미가 겹치는 문제가 있고, 중요한 로컬 픽셀 수준의 이미지-텍스트 정렬보다는 전반적인 이미지 수준의 표현에 더 집중하고 있어. 그래서 정확한 이상 위치를 찾기 어렵지. 

이번 논문에서는 ALFA라는 훈련이 필요 없는 접근 방식을 제안해. 이 방법은 통합 모델을 통해 이러한 문제들을 해결할 수 있도록 설계되었어. 우리는 런타임 프롬프트 적응 방식을 제안하는데, 이 방식은 먼저 유용한 이상 프롬프트를 생성해서 대형 언어 모델(LLM)의 능력을 활용해. 

이 전략은 각 이미지에 대한 이상 프롬프트 적응과 의미 겹침 문제를 완화하기 위해 맥락 기반 점수 매기기 메커니즘으로 강화돼. 또한, 우리는 정밀한 이상 위치 지정을 위해 로컬 픽셀 수준의 의미를 융합하는 새로운 세분화 정렬기를 도입했어. 이 정렬기는 이미지-텍스트 정렬을 전반적인 의미 공간에서 로컬 의미 공간으로 투사하는 방식이야. 

MVTec와 VisA 데이터셋에 대한 광범위한 평가 결과, ALFA가 제로샷 VAD에서 언어의 잠재력을 잘 활용한다는 걸 확인했어. MVTec에서는 기존의 최신 방법보다 12.1%, VisA에서는 8.9% 더 나은 성능을 보였어.

================================================================================

URL:
https://arxiv.org/pdf/2404.17503.pdf

Title: Inhomogeneous illumination image enhancement under ex-tremely low visibility condition

Original Abstract:
Imaging through dense fog presents unique challenges, with essential visual information crucial for applications like object detection and recognition obscured, thereby hindering conventional image processing methods. Despite improvements through neural network-based approaches, these techniques falter under extremely low visibility conditions exacerbated by inhomogeneous illumination, which degrades deep learning performance due to inconsistent signal intensities. We introduce in this paper a novel method that adaptively filters background illumination based on Structural Differential and Integral Filtering (SDIF) to enhance only vital signal information. The grayscale banding is eliminated by incorporating a visual optimization strategy based on image gradients. Maximum Histogram Equalization (MHE) is used to achieve high contrast while maintaining fidelity to the original content. We evaluated our algorithm using data collected from both a fog chamber and outdoor environments, and performed comparative analyses with existing methods. Our findings demonstrate that our proposed method significantly enhances signal clarity under extremely low visibility conditions and out-performs existing techniques, offering substantial improvements for deep fog imaging applications.

Translated Abstract:
안개가 짙게 낀 상황에서 이미지를 찍는 건 정말 어려운 일이에요. 객체 감지나 인식 같은 중요한 시각 정보가 가려지기 때문에 기존의 이미지 처리 방법들이 잘 작동하지 않죠. 신경망 기반의 접근법으로 개선이 있었지만, 극히 낮은 가시성 조건에서는 조명이 고르지 않아 성능이 떨어져요. 이로 인해 신호 강도가 일관되지 않아서 딥러닝 성능이 저하되죠.

이 논문에서는 중요한 신호 정보만을 향상시키기 위해 구조적 미분 적분 필터링(SDIF)을 기반으로 배경 조명을 적응적으로 필터링하는 새로운 방법을 제안해요. 이미지 그래디언트에 기반한 시각 최적화 전략을 통해 그레이스케일 밴딩도 없애고요. 최대 히스토그램 평활화(MHE)를 사용해서 원래 내용의 충실도를 유지하면서 높은 대비를 얻어요.

우리는 안개 챔버와 야외에서 수집한 데이터를 사용해서 알고리즘을 평가했어요. 기존 방법들과 비교 분석도 했고요. 연구 결과, 제안한 방법이 극도로 낮은 가시성 조건에서 신호의 선명함을 크게 향상시키고, 기존 기술들보다 성능이 뛰어나다는 걸 보여줬어요. 이는 깊은 안개 이미지 처리에 상당한 개선을 제공해요.

================================================================================

URL:
https://arxiv.org/pdf/2405.06241.pdf

Title: MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth Smooth Regularization

Original Abstract:
This letter introduces a novel framework for dense Visual Simultaneous Localization and Mapping (VSLAM) based on Gaussian Splatting. Recently, SLAM based on Gaussian Splatting has shown promising results. However, in monocular scenarios, the Gaussian maps reconstructed lack geometric accuracy and exhibit weaker tracking capability. To address these limitations, we jointly optimize sparse visual odometry tracking and 3D Gaussian Splatting scene representation for the first time. We obtain depth maps on visual odometry keyframe windows using a fast Multi-View Stereo (MVS) network for the geometric supervision of Gaussian maps. Furthermore, we propose a depth smooth loss and Sparse-Dense Adjustment Ring (SDAR) to reduce the negative effect of estimated depth maps and preserve the consistency in scale between the visual odometry and Gaussian maps. We have evaluated our system across various synthetic and real-world datasets. The accuracy of our pose estimation surpasses existing methods and achieves state-of-the-art. Additionally, it outperforms previous monocular methods in terms of novel view synthesis and geometric reconstruction fidelities.

Translated Abstract:
이 편지는 가우시안 스플래팅에 기반한 새로운 밀집 시각적 동시 위치 추정 및 지도 작성(VSLAM) 프레임워크를 소개해. 최근 가우시안 스플래팅을 활용한 SLAM이 좋은 결과를 보여줬어. 하지만 단안 카메라를 사용할 때, 재구성된 가우시안 맵은 기하학적 정확성이 떨어지고 추적 능력이 약해. 

이런 한계를 해결하기 위해, 우리는 처음으로 희소 비주얼 오도메트리 추적과 3D 가우시안 스플래팅 장면 표현을 함께 최적화했어. 비주얼 오도메트리의 키프레임 윈도우에서 빠른 다중 시점 스테레오(MVS) 네트워크를 사용해 깊이 맵을 얻고, 이걸 통해 가우시안 맵의 기하학적 감독을 해. 또한, 우리는 깊이 스무스 손실과 희소-밀집 조정 링(SDAR)을 제안해서 추정된 깊이 맵의 부정적인 영향을 줄이고 비주얼 오도메트리와 가우시안 맵 간의 스케일 일관성을 유지해.

우리는 이 시스템을 다양한 합성 및 실제 데이터셋에서 평가했어. 우리의 자세 추정 정확도가 기존 방법들을 초월하고 최첨단 성능을 달성했어. 게다가, 새로운 시점 합성과 기하학적 재구성에서 이전 단안 방법들보다 성능이 더 뛰어나.

================================================================================

URL:
https://arxiv.org/pdf/2405.17872.pdf

Title: HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction

Original Abstract:
Robot-assisted minimally invasive surgery benefits from enhancing dynamic scene reconstruction, as it improves surgical outcomes. While Neural Radiance Fields (NeRF) have been effective in scene reconstruction, their slow inference speeds and lengthy training durations limit their applicability. To overcome these limitations, 3D Gaussian Splatting (3D-GS) based methods have emerged as a recent trend, offering rapid inference capabilities and superior 3D quality. However, these methods still struggle with under-reconstruction in both static and dynamic scenes. In this paper, we propose HFGS, a novel approach for deformable endoscopic reconstruction that addresses these challenges from spatial and temporal frequency perspectives. Our approach incorporates deformation fields to better handle dynamic scenes and introduces Spatial High-Frequency Emphasis Reconstruction (SHF) to minimize discrepancies in spatial frequency spectra between the rendered image and its ground truth. Additionally, we introduce Temporal High-Frequency Emphasis Reconstruction (THF) to enhance dynamic awareness in neural rendering by leveraging flow priors, focusing optimization on motion-intensive parts. Extensive experiments on two widely used benchmarks demonstrate that HFGS achieves superior rendering quality.

Translated Abstract:
로봇 보조 최소 침습 수술은 동적 장면 재구성을 향상시켜 수술 결과를 개선하는 데 도움이 돼. NeRF(신경 방사장)는 장면 재구성에 효과적이지만, 느린 추론 속도와 긴 훈련 시간이 문제야. 이 문제를 해결하기 위해 3D Gaussian Splatting(3D-GS) 기반 방법들이 최근에 많이 사용되고 있는데, 이 방법은 빠른 추론 능력과 뛰어난 3D 품질을 제공해. 하지만 여전히 정적 및 동적 장면에서 재구성이 부족한 문제가 있어. 

이 논문에서는 HFGS라는 새로운 접근 방식을 제안해. 이 방법은 변형 가능한 내시경 재구성을 위해 공간적 및 시간적 주파수 관점에서 이 문제를 해결해. 우리의 방법은 동적 장면을 더 잘 처리하기 위해 변형 필드를 포함하고, 렌더링된 이미지와 실제 이미지 간의 공간 주파수 스펙트럼 차이를 최소화하기 위해 Spatial High-Frequency Emphasis Reconstruction(SHF)를 도입해. 또한, Temporal High-Frequency Emphasis Reconstruction(THF)를 도입해 흐름 정보를 활용해 동적 인식을 향상시키고, 움직임이 많은 부분에 최적화를 집중해. 

두 가지 널리 사용되는 벤치마크에서 진행한 실험 결과, HFGS가 뛰어난 렌더링 품질을 달성하는 것을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2406.05786.pdf

Title: CAMS: Convolution and Attention-Free Mamba-based Cardiac Image Segmentation

Original Abstract:
Convolutional Neural Networks (CNNs) and Transformer-based self-attention models have become the standard for medical image segmentation. This paper demonstrates that convolution and self-attention, while widely used, are not the only effective methods for segmentation. Breaking with convention, we present a Convolution and self-Attention-free Mamba-based semantic Segmentation Network named CAMS-Net. Specifically, we design Mamba-based Channel Aggregator and Spatial Aggregator, which are applied independently in each encoder-decoder stage. The Channel Aggregator extracts information across different channels, and the Spatial Aggregator learns features across different spatial locations. We also propose a Linearly Interconnected Factorized Mamba (LIFM) block to reduce the computational complexity of a Mamba block and to enhance its decision function by introducing a non-linearity between two factorized Mamba blocks. Our model outperforms the existing state-of-the-art CNN, self-attention, and Mamba-based methods on CMR and M&Ms-2 Cardiac segmentation datasets, showing how this innovative, convolution, and self-attention-free method can inspire further research beyond CNN and Transformer paradigms, achieving linear complexity and reducing the number of parameters. Source code and pre-trained models will be publicly available upon acceptance.

Translated Abstract:
의료 이미지 분할에서 합성곱 신경망(CNN)과 트랜스포머 기반의 자기 주의 모델이 표준이 되었어. 이 논문에서는 합성곱과 자기 주의가 널리 사용되긴 하지만, 분할에 효과적인 방법이 아닐 수도 있다는 걸 보여줘. 우리는 전통적인 방식에서 벗어나서, 합성곱과 자기 주의가 없는 ‘Mamba 기반의 의미 분할 네트워크(CAMS-Net)’를 제안해.

특히, 우리는 Mamba 기반의 채널 집합기와 공간 집합기를 설계했어. 이 두 가지는 인코더-디코더 단계에서 독립적으로 적용돼. 채널 집합기는 서로 다른 채널에서 정보를 추출하고, 공간 집합기는 서로 다른 공간 위치에서 특징을 학습해. 또한, Mamba 블록의 계산 복잡성을 줄이고, 두 개의 분리된 Mamba 블록 사이에 비선형성을 도입해 결정 기능을 향상시키는 ‘선형적으로 연결된 인수 분해 Mamba(LIFM) 블록’을 제안해.

우리 모델은 CMR과 M&Ms-2 심장 분할 데이터셋에서 기존의 최첨단 CNN, 자기 주의, Mamba 기반 방법보다 성능이 뛰어나. 이 혁신적인 방법이 CNN과 트랜스포머 패러다임을 넘어서는 연구를 자극할 수 있다는 것을 보여줘. 이 방법은 선형 복잡성을 달성하고 파라미터 수를 줄여줘. 소스 코드와 사전 학습된 모델은 논문이 수락되면 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2406.08113.pdf

Title: Valeo4Cast: A Modular Approach to End-to-End Forecasting

Original Abstract:
Motion forecasting is crucial in autonomous driving systems to anticipate the future trajectories of surrounding agents such as pedestrians, vehicles, and traffic signals. In end-to-end forecasting, the model must jointly detect and track from sensor data (cameras or LiDARs) the past trajectories of the different elements of the scene and predict their future locations. We depart from the current trend of tackling this task via end-to-end training from perception to forecasting, and instead use a modular approach. We individually build and train detection, tracking and forecasting modules. We then only use consecutive finetuning steps to integrate the modules better and alleviate compounding errors. We conduct an in-depth study on the finetuning strategies and it reveals that our simple yet effective approach significantly improves performance on the end-to-end forecasting benchmark. Consequently, our solution ranks first in the Argoverse 2 End-to-end Forecasting Challenge, with 63.82 mAPf. We surpass forecasting results by +17.1 points over last year's winner and by +13.3 points over this year's runner-up. This remarkable performance in forecasting can be explained by our modular paradigm, which integrates finetuning strategies and significantly outperforms the end-to-end-trained counterparts.

Translated Abstract:
모션 예측은 자율 주행 시스템에서 보행자, 차량 및 교통 신호 등 주변 개체의 미래 경로를 예측하는 데 매우 중요해. 엔드 투 엔드 예측에서는 모델이 센서 데이터(카메라나 LiDAR)에서 장면의 다양한 요소들의 과거 경로를 감지하고 추적하면서 미래 위치를 예측해야 해. 

우리는 현재 이러한 작업을 인식에서 예측까지 엔드 투 엔드 훈련으로 해결하는 경향에서 벗어나서 모듈 방식으로 접근했어. 감지, 추적, 예측 모듈을 각각 독립적으로 만들어서 훈련했어. 그런 다음 모듈을 더 잘 통합하고 누적 오류를 줄이기 위해 연속적인 미세 조정 단계를 사용했어. 미세 조정 전략에 대한 깊이 있는 연구를 진행했는데, 이 간단하지만 효과적인 접근 방식이 엔드 투 엔드 예측 벤치마크에서 성능을 크게 향상시킨다는 것을 보여줬어.

결과적으로, 우리의 솔루션은 Argoverse 2 엔드 투 엔드 예측 챌린지에서 63.82 mAPf로 1위를 차지했어. 우리는 작년 우승자보다 +17.1 포인트, 올해 준우승자보다 +13.3 포인트 더 높은 예측 결과를 기록했어. 이렇게 뛰어난 예측 성능은 우리의 모듈 방식 덕분인데, 이 방식은 미세 조정 전략을 통합하고 엔드 투 엔드 훈련된 모델들보다 훨씬 더 뛰어난 결과를 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2406.09394.pdf

Title: WonderWorld: Interactive 3D Scene Generation from a Single Image

Original Abstract:
We present WonderWorld, a novel framework for interactive 3D scene generation that enables users to interactively specify scene contents and layout and see the created scenes in low latency. The major challenge lies in achieving fast generation of 3D scenes. Existing scene generation approaches fall short of speed as they often require (1) progressively generating many views and depth maps, and (2) time-consuming optimization of the scene geometry representations. We introduce the Fast Layered Gaussian Surfels (FLAGS) as our scene representation and an algorithm to generate it from a single view. Our approach does not need multiple views, and it leverages a geometry-based initialization that significantly reduces optimization time. Another challenge is generating coherent geometry that allows all scenes to be connected. We introduce the guided depth diffusion that allows partial conditioning of depth estimation. WonderWorld generates connected and diverse 3D scenes in less than 10 seconds on a single A6000 GPU, enabling real-time user interaction and exploration. We demonstrate the potential of WonderWorld for user-driven content creation and exploration in virtual environments. We will release full code and software for reproducibility. Project website: this https URL.

Translated Abstract:
우리는 WonderWorld라는 새로운 프레임워크를 소개해. 이건 사용자들이 장면의 내용과 배치를 직접 지정하고, 만든 장면을 빠르게 볼 수 있게 해줘. 

가장 큰 도전 과제는 3D 장면을 빠르게 생성하는 거야. 기존의 장면 생성 방법들은 속도가 느린데, 보통 (1) 여러 시점과 깊이 맵을 점진적으로 생성해야 하고, (2) 장면 기하학 표현을 최적화하는 데 시간이 많이 걸리거든. 우리는 Fast Layered Gaussian Surfels (FLAGS)라는 장면 표현 방식을 도입하고, 단일 시점에서 이를 생성하는 알고리즘을 개발했어. 이 방법은 여러 시점이 필요 없고, 기하학 기반 초기화를 활용해서 최적화 시간을 크게 줄일 수 있어.

또 다른 도전 과제는 모든 장면이 연결될 수 있도록 일관된 기하학을 생성하는 거야. 우리는 깊이 추정을 부분적으로 조절할 수 있는 guided depth diffusion를 도입했어. WonderWorld는 단일 A6000 GPU에서 10초도 안 걸려서 연결되고 다양한 3D 장면을 생성할 수 있어서, 실시간으로 사용자와 상호작용하고 탐색할 수 있게 해줘.

우리는 WonderWorld가 사용자 주도의 콘텐츠 생성과 가상 환경 탐색에 어떤 가능성이 있는지를 보여줄 거야. 전체 코드와 소프트웨어는 reproducibility를 위해 공개할 예정이야. 프로젝트 웹사이트는 이 URL이야.

================================================================================

URL:
https://arxiv.org/pdf/2406.12044.pdf

Title: ARTIST: Improving the Generation of Text-rich Images with Disentangled Diffusion Models

Original Abstract:
Diffusion models have demonstrated exceptional capabilities in generating a broad spectrum of visual content, yet their proficiency in rendering text is still limited: they often generate inaccurate characters or words that fail to blend well with the underlying image. To address these shortcomings, we introduce a new framework named ARTIST. This framework incorporates a dedicated textual diffusion model to specifically focus on the learning of text structures. Initially, we pretrain this textual model to capture the intricacies of text representation. Subsequently, we finetune a visual diffusion model, enabling it to assimilate textual structure information from the pretrained textual model. This disentangled architecture design and the training strategy significantly enhance the text rendering ability of the diffusion models for text-rich image generation. Additionally, we leverage the capabilities of pretrained large language models to better interpret user intentions, contributing to improved generation quality. Empirical results on the MARIO-Eval benchmark underscore the effectiveness of the proposed method, showing an improvement of up to 15\% in various metrics.

Translated Abstract:
확산 모델은 다양한 시각적 콘텐츠를 생성하는 데 뛰어난 능력을 보였지만, 텍스트를 생성하는 데는 아직 한계가 있어. 종종 잘못된 문자나 단어를 생성해서 이미지와 잘 어우러지지 않거든. 이런 문제를 해결하기 위해 우리는 ARTIST라는 새로운 프레임워크를 소개해. 이 프레임워크는 텍스트 구조 학습에 중점을 둔 전용 텍스트 확산 모델을 포함하고 있어.

먼저, 우리는 이 텍스트 모델을 사전 훈련해서 텍스트 표현의 복잡성을 이해하게 해. 그 다음에는 시각 확산 모델을 미세 조정해서, 사전 훈련된 텍스트 모델의 텍스트 구조 정보를 흡수할 수 있도록 해. 이런 구조 설계와 훈련 전략 덕분에 텍스트가 풍부한 이미지 생성에서 확산 모델의 텍스트 렌더링 능력이 크게 향상돼.

또한 우리는 사전 훈련된 대형 언어 모델의 능력을 활용해서 사용자의 의도를 더 잘 해석하고, 결과적으로 생성 품질을 높이는 데 기여했어. MARIO-Eval 벤치마크에서의 실험 결과는 제안된 방법의 효과를 강조하며, 다양한 메트릭에서 최대 15%의 개선을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2406.18197.pdf

Title: Human-Free Automated Prompting for Vision-Language Anomaly Detection: Prompt Optimization with Meta-guiding Prompt Scheme

Original Abstract:
Pre-trained vision-language models (VLMs) are highly adaptable to various downstream tasks through few-shot learning, making prompt-based anomaly detection a promising approach. Traditional methods depend on human-crafted prompts that require prior knowledge of specific anomaly types. Our goal is to develop a human-free prompt-based anomaly detection framework that optimally learns prompts through data-driven methods, eliminating the need for human intervention. The primary challenge in this approach is the lack of anomalous samples during the training phase. Additionally, the Vision Transformer (ViT)-based image encoder in VLMs is not ideal for pixel-wise anomaly segmentation due to a locality feature mismatch between the original image and the output feature map. To tackle the first challenge, we have developed the Object-Attention Anomaly Generation Module (OAGM) to synthesize anomaly samples for training. Furthermore, our Meta-Guiding Prompt-Tuning Scheme (MPTS) iteratively adjusts the gradient-based optimization direction of learnable prompts to avoid overfitting to the synthesized anomalies. For the second challenge, we propose Locality-Aware Attention, which ensures that each local patch feature attends only to nearby patch features, preserving the locality features corresponding to their original locations. This framework allows for the optimal prompt embeddings by searching in the continuous latent space via backpropagation, free from human semantic constraints. Additionally, the modified locality-aware attention improves the precision of pixel-wise anomaly segmentation.

Translated Abstract:
사전 훈련된 비전-언어 모델(VLM)은 몇 가지 샷 학습을 통해 다양한 다운스트림 작업에 잘 적응할 수 있어, 프롬프트 기반의 이상 탐지가 유망한 접근 방식이 되고 있어. 전통적인 방법은 특정 이상 유형에 대한 사전 지식을 요구하는 사람이 만든 프롬프트에 의존해. 우리의 목표는 데이터 기반 방법을 통해 최적으로 프롬프트를 학습하는 인간이 필요 없는 프롬프트 기반 이상 탐지 프레임워크를 개발하는 거야. 이렇게 하면 인간의 개입 없이도 가능해.

이 접근 방식에서 가장 큰 도전은 훈련 단계에서 이상 샘플이 부족하다는 거야. 그리고 VLM에서 사용하는 비전 변환기(ViT) 기반의 이미지 인코더는 원본 이미지와 출력 피처 맵 간에 지역적 특성 불일치 때문에 픽셀 단위의 이상 세분화에는 적합하지 않아. 첫 번째 도전을 해결하기 위해 우리는 훈련을 위한 이상 샘플을 합성하는 객체 주의 이상 생성 모듈(OAGM)을 개발했어.

또한, 우리의 메타 가이딩 프롬프트 튜닝 방식(MPTS)은 학습 가능한 프롬프트의 기울기 기반 최적화 방향을 반복적으로 조정해 합성된 이상에 과적합되는 것을 피해. 두 번째 도전에는 지역 인식 주의를 제안하는데, 이 방법은 각 지역 패치 특성이 근처 패치 특성만을 주의 깊게 살펴보도록 해서 원래 위치에 해당하는 지역적 특성을 보존해. 

이 프레임워크는 인간의 의미적 제약 없이 역전파를 통해 연속 잠재 공간에서 최적의 프롬프트 임베딩을 검색할 수 있도록 해. 게다가 수정된 지역 인식 주의는 픽셀 단위의 이상 세분화 정확도를 향상시켜.

================================================================================

URL:
https://arxiv.org/pdf/2406.19006.pdf

Title: VideoMambaPro: A Leap Forward for Mamba in Video Understanding

Original Abstract:
Video understanding requires the extraction of rich spatio-temporal representations, which transformer models achieve through self-attention. Unfortunately, self-attention poses a computational burden. In NLP, Mamba has surfaced as an efficient alternative for transformers. However, Mamba's successes do not trivially extend to computer vision tasks, including those in video analysis. In this paper, we theoretically analyze the differences between self-attention and Mamba. We identify two limitations in Mamba's token processing: historical decay and element contradiction. We propose VideoMambaPro (VMP) that solves the identified limitations by adding masked backward computation and elemental residual connections to a VideoMamba backbone. VideoMambaPro shows state-of-the-art video action recognition performance compared to transformer models, and surpasses VideoMamba by clear margins: 7.9% and 8.1% top-1 on Kinetics-400 and Something-Something V2, respectively. Our VideoMambaPro-M model achieves 91.9% top-1 on Kinetics-400, only 0.2% below InternVideo2-6B but with only 1.2% of its parameters. The combination of high performance and efficiency makes VideoMambaPro an interesting alternative for transformer models.

Translated Abstract:
비디오 이해는 풍부한 시공간 표현을 추출해야 하는데, 트랜스포머 모델이 자기 주의(self-attention)를 통해 이를 달성해. 하지만 자기 주의는 계산 부담이 커. 자연어 처리(NLP)에서는 Mamba가 트랜스포머에 대한 효율적인 대안으로 떠올랐어. 하지만 Mamba의 성공이 비디오 분석 같은 컴퓨터 비전 작업에 쉽게 적용되진 않아.

이 논문에서는 자기 주의와 Mamba의 차이를 이론적으로 분석해. Mamba의 토큰 처리에서 두 가지 한계를 찾아냈어: 역사적 감소(historical decay)와 요소 모순(element contradiction). 우리는 VideoMambaPro(VMP)를 제안하는데, 이 모델은 마스크된 역방향 계산(masked backward computation)과 요소 잔여 연결(elemental residual connections)을 VideoMamba 백본에 추가해서 이 한계를 해결해.

VideoMambaPro는 트랜스포머 모델에 비해 최첨단 비디오 행동 인식 성능을 보여주고, VideoMamba보다 Kinetics-400과 Something-Something V2에서 각각 7.9%와 8.1% 더 높은 top-1 성능을 기록했어. 우리의 VideoMambaPro-M 모델은 Kinetics-400에서 91.9%의 top-1 성능을 달성했는데, InternVideo2-6B보다 0.2% 낮지만 그 모델의 파라미터 수는 단 1.2%야. 높은 성능과 효율성을 결합한 VideoMambaPro는 트랜스포머 모델의 흥미로운 대안이 될 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2407.06315.pdf

Title: Shedding More Light on Robust Classifiers under the lens of Energy-based Models

Original Abstract:
By reinterpreting a robust discriminative classifier as Energy-based Model (EBM), we offer a new take on the dynamics of adversarial training (AT). Our analysis of the energy landscape during AT reveals that untargeted attacks generate adversarial images much more in-distribution (lower energy) than the original data from the point of view of the model. Conversely, we observe the opposite for targeted attacks. On the ground of our thorough analysis, we present new theoretical and practical results that show how interpreting AT energy dynamics unlocks a better understanding: (1) AT dynamic is governed by three phases and robust overfitting occurs in the third phase with a drastic divergence between natural and adversarial energies (2) by rewriting the loss of TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization (TRADES) in terms of energies, we show that TRADES implicitly alleviates overfitting by means of aligning the natural energy with the adversarial one (3) we empirically show that all recent state-of-the-art robust classifiers are smoothing the energy landscape and we reconcile a variety of studies about understanding AT and weighting the loss function under the umbrella of EBMs. Motivated by rigorous evidence, we propose Weighted Energy Adversarial Training (WEAT), a novel sample weighting scheme that yields robust accuracy matching the state-of-the-art on multiple benchmarks such as CIFAR-10 and SVHN and going beyond in CIFAR-100 and Tiny-ImageNet. We further show that robust classifiers vary in the intensity and quality of their generative capabilities, and offer a simple method to push this capability, reaching a remarkable Inception Score (IS) and FID using a robust classifier without training for generative modeling. The code to reproduce our results is available at this http URL .

Translated Abstract:
강력한 구분 분류기를 에너지 기반 모델(EBM)로 재해석함으로써, 우리는 적대적 훈련(AT)의 동태에 대한 새로운 관점을 제시합니다. AT 중 에너지 경관을 분석해보니, 비타겟 공격이 원래 데이터보다 모델 관점에서 더 낮은 에너지를 가진 적대적 이미지를 생성한다는 것을 알게 되었습니다. 반면에, 타겟 공격에 대해서는 그 반대 결과를 관찰했습니다.

우리의 철저한 분석을 바탕으로, AT 에너지 동태를 해석함으로써 더 나은 이해를 열어주는 새로운 이론적 및 실용적 결과를 제시합니다: 

(1) AT 동태는 세 단계로 진행되며, 세 번째 단계에서 자연 에너지와 적대적 에너지 간의 극단적인 차이를 보이며 강력한 과적합이 발생합니다. 

(2) TRADES의 손실을 에너지 관점에서 다시 작성함으로써, TRADES가 자연 에너지를 적대적 에너지와 정렬시켜 과적합을 완화한다는 것을 보여줍니다. 

(3) 우리는 최근의 최첨단 강력 분류기들이 에너지 경관을 매끄럽게 하고 있다는 것을 경험적으로 보여주며, AT 이해와 손실 함수 가중치에 대한 다양한 연구를 EBM의 틀 안에서 조화롭게 연결합니다.

철저한 증거에 힘입어, 우리는 가중 에너지 적대적 훈련(WEAT)이라는 새로운 샘플 가중치 방식을 제안합니다. 이 방식은 CIFAR-10과 SVHN 같은 여러 벤치마크에서 최첨단의 강력한 정확도를 달성하며, CIFAR-100과 Tiny-ImageNet에서는 더 나아갑니다. 우리는 또한 강력 분류기들이 생성 능력의 강도와 질에서 차이를 보이며, 이 능력을 향상시키기 위한 간단한 방법을 제시합니다. 이를 통해 훈련 없이도 강력 분류기를 사용하여 놀라운 Inception Score(IS)와 FID를 달성할 수 있습니다. 우리의 결과를 재현할 수 있는 코드는 이 http URL에서 확인할 수 있습니다.

================================================================================

URL:
https://arxiv.org/pdf/2407.07805.pdf

Title: SUMix: Mixup with Semantic and Uncertain Information

Original Abstract:
Mixup data augmentation approaches have been applied for various tasks of deep learning to improve the generalization ability of deep neural networks. Some existing approaches CutMix, SaliencyMix, etc. randomly replace a patch in one image with patches from another to generate the mixed image. Similarly, the corresponding labels are linearly combined by a fixed ratio $\lambda$ by l. The objects in two images may be overlapped during the mixing process, so some semantic information is corrupted in the mixed samples. In this case, the mixed image does not match the mixed label information. Besides, such a label may mislead the deep learning model training, which results in poor performance. To solve this problem, we proposed a novel approach named SUMix to learn the mixing ratio as well as the uncertainty for the mixed samples during the training process. First, we design a learnable similarity function to compute an accurate mix ratio. Second, an approach is investigated as a regularized term to model the uncertainty of the mixed samples. We conduct experiments on five image benchmarks, and extensive experimental results imply that our method is capable of improving the performance of classifiers with different cutting-based mixup approaches. The source code is available at this https URL.

Translated Abstract:
Mixup 데이터 증강 기법은 딥러닝의 다양한 작업에 적용되어 딥 뉴럴 네트워크의 일반화 능력을 향상시키는 데 도움을 줘. 기존의 CutMix나 SaliencyMix 같은 방법들은 한 이미지의 패치를 다른 이미지의 패치로 임의로 교체해서 혼합 이미지를 만들어. 이때, 관련된 레이블은 고정 비율 $\lambda$로 선형 결합돼.

하지만 두 이미지의 객체들이 혼합 과정에서 겹칠 수 있어서, 이로 인해 일부 의미 정보가 손상될 수 있어. 그래서 혼합된 이미지가 혼합 레이블 정보와 맞지 않게 되는 일이 발생해. 이런 잘못된 레이블은 딥러닝 모델 훈련에 혼란을 줄 수 있어서 성능이 떨어지는 문제를 일으킬 수 있어.

이 문제를 해결하기 위해 우리는 SUMix라는 새로운 방법을 제안했어. 이 방법은 훈련 과정에서 혼합 비율과 혼합 샘플의 불확실성을 학습해. 첫째, 정확한 혼합 비율을 계산하기 위한 학습 가능한 유사성 함수를 설계했어. 둘째, 혼합 샘플의 불확실성을 모델링하기 위한 정규화 항을 조사했어.

우리는 다섯 개의 이미지 벤치마크에서 실험을 했고, 많은 실험 결과가 우리의 방법이 다양한 컷팅 기반 믹스업 접근법으로 분류기 성능을 향상시킬 수 있음을 보여줘. 소스 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2407.09498.pdf

Title: OT-VP: Optimal Transport-guided Visual Prompting for Test-Time Adaptation

Original Abstract:
Vision Transformers (ViTs) have demonstrated remarkable capabilities in learning representations, but their performance is compromised when applied to unseen domains. Previous methods either engage in prompt learning during the training phase or modify model parameters at test time through entropy minimization. The former often overlooks unlabeled target data, while the latter doesn't fully address domain shifts. In this work, our approach, Optimal Transport-guided Test-Time Visual Prompting (OT-VP), handles these problems by leveraging prompt learning at test time to align the target and source domains without accessing the training process or altering pre-trained model parameters. This method involves learning a universal visual prompt for the target domain by optimizing the Optimal Transport distance.OT-VP, with only four learned prompt tokens, exceeds state-of-the-art performance across three stylistic datasets-PACS, VLCS, OfficeHome, and one corrupted dataset ImageNet-C. Additionally, OT-VP operates efficiently, both in terms of memory and computation, and is adaptable for extension to online settings.

Translated Abstract:
비전 트랜스포머(ViTs)는 표현 학습에서 뛰어난 능력을 보여주지만, 보지 못한 도메인에 적용할 때 성능이 떨어져. 기존 방법들은 훈련 단계에서 프롬프트 학습을 하거나 테스트 시 모델 매개변수를 수정하는 방식으로 엔트로피 최소화를 사용해. 첫 번째 방법은 라벨이 없는 목표 데이터를 종종 무시하고, 두 번째는 도메인 변화에 완전히 대응하지 못해.

이 연구에서는 최적 수송 기반 테스트 시간 비주얼 프롬프트(OT-VP)라는 방법을 제안해. 이 방법은 훈련 과정에 접근하지 않고, 사전 훈련된 모델 매개변수를 변경하지 않으면서 목표 도메인과 소스 도메인을 정렬하기 위해 테스트 시간에 프롬프트 학습을 활용해. OT-VP는 최적 수송 거리를 최적화해서 목표 도메인을 위한 범용 비주얼 프롬프트를 학습해.

OT-VP는 단 4개의 학습된 프롬프트 토큰만으로도 세 가지 스타일 데이터셋인 PACS, VLCS, OfficeHome과 한 개의 손상된 데이터셋인 ImageNet-C에서 최신 성능을 초과해. 게다가 OT-VP는 메모리와 계산 측면에서도 효율적으로 작동하고, 온라인 환경으로 확장하기에도 적합해.

================================================================================

URL:
https://arxiv.org/pdf/2407.09733.pdf

Title: Textured-GS: Gaussian Splatting with Spatially Defined Color and Opacity

Original Abstract:
In this paper, we introduce Textured-GS, an innovative method for rendering Gaussian splatting that incorporates spatially defined color and opacity variations using Spherical Harmonics (SH). This approach enables each Gaussian to exhibit a richer representation by accommodating varying colors and opacities across its surface, significantly enhancing rendering quality compared to traditional methods. To demonstrate the merits of our approach, we have adapted the Mini-Splatting architecture to integrate textured Gaussians without increasing the number of Gaussians. Our experiments across multiple real-world datasets show that Textured-GS consistently outperforms both the baseline Mini-Splatting and standard 3DGS in terms of visual fidelity. The results highlight the potential of Textured-GS to advance Gaussian-based rendering technologies, promising more efficient and high-quality scene reconstructions.

Translated Abstract:
이 논문에서는 Textured-GS라는 새로운 방법을 소개해. 이 방법은 구형 조화 함수(Spherical Harmonics)를 이용해서 색상과 투명도를 공간적으로 정의해서 가우시안 스플래팅을 렌더링하는 거야. 이 접근법 덕분에 각 가우시안이 표면 전체에서 다양한 색상과 투명도를 가질 수 있어서, 전통적인 방법보다 렌더링 품질이 훨씬 좋아져.

우리 방법의 장점을 보여주기 위해서, Mini-Splatting 구조를 수정해서 텍스처가 있는 가우시안을 통합했는데, 가우시안의 수는 늘리지 않았어. 여러 실제 데이터셋을 가지고 실험해본 결과, Textured-GS가 기본 Mini-Splatting과 일반 3DGS보다 시각적 충실도에서 일관되게 더 나은 성능을 보였어.

이 결과는 Textured-GS가 가우시안 기반 렌더링 기술을 발전시키는 데 큰 가능성이 있음을 보여줘. 더 효율적이고 고품질의 장면 재구성을 약속하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2407.13342.pdf

Title: Implicit Filtering for Learning Neural Signed Distance Functions from 3D Point Clouds

Original Abstract:
Neural signed distance functions (SDFs) have shown powerful ability in fitting the shape geometry. However, inferring continuous signed distance fields from discrete unoriented point clouds still remains a challenge. The neural network typically fits the shape with a rough surface and omits fine-grained geometric details such as shape edges and corners. In this paper, we propose a novel non-linear implicit filter to smooth the implicit field while preserving high-frequency geometry details. Our novelty lies in that we can filter the surface (zero level set) by the neighbor input points with gradients of the signed distance field. By moving the input raw point clouds along the gradient, our proposed implicit filtering can be extended to non-zero level sets to keep the promise consistency between different level sets, which consequently results in a better regularization of the zero level set. We conduct comprehensive experiments in surface reconstruction from objects and complex scene point clouds, the numerical and visual comparisons demonstrate our improvements over the state-of-the-art methods under the widely used benchmarks.

Translated Abstract:
신경 서명 거리 함수(Neural Signed Distance Functions, SDFs)는 형태의 기하학을 맞추는 데 강력한 능력을 보여줬어. 하지만 이산 비방향 포인트 클라우드로부터 연속적인 서명 거리 필드를 추론하는 건 여전히 어려운 일이지. 일반적으로 신경망은 형태를 대충 맞추기 때문에, 형태의 가장자리나 모서리 같은 세밀한 기하학적 디테일은 놓치는 경우가 많아.

이 논문에서는 고주파 기하학적 디테일을 유지하면서 암묵적 필드를 부드럽게 하는 새로운 비선형 필터를 제안해. 우리의 혁신은 서명 거리 필드의 기울기를 가진 이웃 입력 포인트로 표면(제로 레벨 세트)을 필터링할 수 있다는 거야. 입력 원시 포인트 클라우드를 기울기에 따라 이동시킴으로써, 우리가 제안한 암묵적 필터링은 비제로 레벨 세트로도 확장할 수 있어서 서로 다른 레벨 세트 간의 일관성을 유지할 수 있어. 이로 인해 제로 레벨 세트의 규제가 더 잘 이루어지게 돼.

우리는 물체와 복잡한 장면 포인트 클라우드에서 표면 재구성에 대한 포괄적인 실험을 수행했어. 수치적이고 시각적인 비교를 통해, 우리의 방법이 널리 사용되는 벤치마크에서 최신 방법들보다 개선된다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2407.18038.pdf

Title: TiCoSS: Tightening the Coupling between Semantic Segmentation and Stereo Matching within A Joint Learning Framework

Original Abstract:
Semantic segmentation and stereo matching, respectively analogous to the ventral and dorsal streams in our human brain, are two key components of autonomous driving perception systems. Addressing these two tasks with separate networks is no longer the mainstream direction in developing computer vision algorithms, particularly with the recent advances in large vision models and embodied artificial intelligence. The trend is shifting towards combining them within a joint learning framework, especially emphasizing feature sharing between the two tasks. The major contributions of this study lie in comprehensively tightening the coupling between semantic segmentation and stereo matching. Specifically, this study introduces three novelties: (1) a tightly coupled, gated feature fusion strategy, (2) a hierarchical deep supervision strategy, and (3) a coupling tightening loss function. The combined use of these technical contributions results in TiCoSS, a state-of-the-art joint learning framework that simultaneously tackles semantic segmentation and stereo matching. Through extensive experiments on the KITTI and vKITTI2 datasets, along with qualitative and quantitative analyses, we validate the effectiveness of our developed strategies and loss function, and demonstrate its superior performance compared to prior arts, with a notable increase in mIoU by over 9%. Our source code will be publicly available at mias.group/TiCoSS upon publication.

Translated Abstract:
의미 세분화(semantic segmentation)와 스테레오 매칭(stereo matching)은 자율주행 인식 시스템의 두 가지 중요한 요소야. 이 두 작업을 각각 따로 처리하는 게 더 이상 주류 방법이 아니고, 특히 최근의 대형 비전 모델과 구현된 인공지능 덕분에 그런 경향이 변하고 있어. 이제는 두 작업을 함께 학습하는 방향으로 가고 있고, 이때 두 작업 간의 특징을 공유하는 게 중요해.

이 연구의 주요 기여는 의미 세분화와 스테레오 매칭 간의 결합을 더 강하게 만드는 데 있어. 구체적으로, 이 연구는 세 가지 새로운 방법을 소개해: (1) tightly coupled, gated feature fusion 전략, (2) 계층적 깊이 감독(hierarchical deep supervision) 전략, (3) 결합 강화 손실 함수(coupling tightening loss function). 이 기술들을 결합해서 TiCoSS라는 최첨단 공동 학습 프레임워크를 만들었고, 이 프레임워크는 의미 세분화와 스테레오 매칭을 동시에 처리할 수 있어.

KITTI와 vKITTI2 데이터셋을 이용한 여러 실험과 질적, 양적 분석을 통해 우리가 개발한 전략과 손실 함수의 효과를 검증했고, 이전 연구들에 비해 성능이 확실히 뛰어나며, mIoU가 9% 이상 증가한 걸 보여줬어. 우리의 소스 코드는 발표 후에 mias.group/TiCoSS에서 공개할 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2407.19605.pdf

Title: Look Hear: Gaze Prediction for Speech-directed Human Attention

Original Abstract:
For computer systems to effectively interact with humans using spoken language, they need to understand how the words being generated affect the users' moment-by-moment attention. Our study focuses on the incremental prediction of attention as a person is seeing an image and hearing a referring expression defining the object in the scene that should be fixated by gaze. To predict the gaze scanpaths in this incremental object referral task, we developed the Attention in Referral Transformer model or ART, which predicts the human fixations spurred by each word in a referring expression. ART uses a multimodal transformer encoder to jointly learn gaze behavior and its underlying grounding tasks, and an autoregressive transformer decoder to predict, for each word, a variable number of fixations based on fixation history. To train ART, we created RefCOCO-Gaze, a large-scale dataset of 19,738 human gaze scanpaths, corresponding to 2,094 unique image-expression pairs, from 220 participants performing our referral task. In our quantitative and qualitative analyses, ART not only outperforms existing methods in scanpath prediction, but also appears to capture several human attention patterns, such as waiting, scanning, and verification.

Translated Abstract:
컴퓨터 시스템이 사람과 효과적으로 대화하려면, 생성되는 단어가 사용자 주의에 어떻게 영향을 미치는지 이해해야 해. 우리 연구는 사람이 이미지를 보고 특정 사물을 설명하는 말을 들을 때 주의력이 어떻게 변화하는지를 예측하는 거야. 이 과정에서 시선이 어디로 가는지를 예측하기 위해 'Attention in Referral Transformer' 모델, 즉 ART를 개발했어. 이 모델은 각 단어에 따라 인간의 시선이 어디로 가는지를 예측해.

ART는 여러 정보를 함께 처리할 수 있는 멀티모달 변환기 인코더를 사용해서 시선 행동과 그에 따른 기본 작업을 함께 학습해. 그리고 시선의 역사에 기반해서 각 단어마다 시선이 몇 번 갈지를 예측하는 자가 회귀 변환기 디코더도 사용해. ART를 훈련하기 위해, 우리는 19,738개의 인간 시선 경로로 구성된 RefCOCO-Gaze라는 대규모 데이터셋을 만들었어. 이 데이터셋은 220명의 참가자가 수행한 2,094개의 고유한 이미지-표현 쌍에 해당해.

정량적 및 정성적 분석 결과, ART는 기존 방법들보다 시선 경로 예측에서 더 좋은 성능을 보였고, 기다림, 스캔, 확인 같은 여러 인간의 주의 패턴도 잘 잡아내는 것 같아.

================================================================================

URL:
https://arxiv.org/pdf/2408.05211.pdf

Title: VITA: Towards Open-Source Interactive Omni Multimodal LLM

Original Abstract:
The remarkable multimodal capabilities and interactive experience of GPT-4o underscore their necessity in practical applications, yet open-source models rarely excel in both areas. In this paper, we introduce VITA, the first-ever open-source Multimodal Large Language Model (MLLM) adept at simultaneous processing and analysis of Video, Image, Text, and Audio modalities, and meanwhile has an advanced multimodal interactive experience. Starting from Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary followed by bilingual instruction tuning. We further endow the language model with visual and audio capabilities through two-stage multi-task learning of multimodal alignment and instruction tuning. VITA demonstrates robust foundational capabilities of multilingual, vision, and audio understanding, as evidenced by its strong performance across a range of both unimodal and multimodal benchmarks. Beyond foundational capabilities, we have made considerable progress in enhancing the natural multimodal human-computer interaction experience. VITA is the first step for the open-source community to explore the seamless integration of multimodal understanding and interaction. While there is still lots of work to be done on VITA to get close to close-source counterparts, we hope that its role as a pioneer can serve as a cornerstone for subsequent research. Project Page: this https URL.

Translated Abstract:
GPT-4o의 뛰어난 멀티모달 기능과 인터랙티브한 경험은 실제 응용에서 꼭 필요하다는 걸 보여줘. 하지만 오픈소스 모델은 이 두 가지를 동시에 잘하는 경우가 드물어. 이 논문에서는 VITA라는 첫 번째 오픈소스 멀티모달 대형 언어 모델(MLLM)을 소개할게. VITA는 비디오, 이미지, 텍스트, 오디오를 동시에 처리하고 분석할 수 있고, 고급 멀티모달 인터랙티브 경험도 제공해.

Mixtral 8x7B를 언어 기반으로 삼아 중국어 어휘를 확장하고, 이중 언어 지침 튜닝을 진행했어. 그리고 멀티모달 정렬과 지침 튜닝의 두 단계 다중 작업 학습을 통해 언어 모델에 시각 및 청각 기능을 추가했어. VITA는 다국어, 비전, 오디오 이해의 강력한 기본 능력을 보여주며, 다양한 단일 모드 및 멀티모달 벤치마크에서 뛰어난 성과를 보여줬어.

기본 능력 외에도 자연스러운 멀티모달 인간-컴퓨터 상호작용 경험을 개선하는 데도 많은 발전을 이뤘어. VITA는 오픈소스 커뮤니티가 멀티모달 이해와 상호작용을 매끄럽게 통합하는 첫걸음이야. VITA가 클로즈드 소스 모델에 가까워지려면 아직 할 일이 많지만, 이 모델이 선구자로서 이후 연구의 기초가 되길 바래.

================================================================================

URL:
https://arxiv.org/pdf/2408.05743.pdf

Title: Neural Architecture Search based Global-local Vision Mamba for Palm-Vein Recognition

Original Abstract:
Due to the advantages such as high security, high privacy, and liveness recognition, vein recognition has been received more and more attention in past years. Recently, deep learning models, e.g., Mamba has shown robust feature representation with linear computational complexity and successfully applied for visual tasks. However, vision Manba can capture long-distance feature dependencies but unfortunately deteriorate local feature details. Besides, manually designing a Mamba architecture based on human priori knowledge is very time-consuming and error-prone. In this paper, first, we propose a hybrid network structure named Global-local Vision Mamba (GLVM), to learn the local correlations in images explicitly and global dependencies among tokens for vein feature representation. Secondly, we design a Multi-head Mamba to learn the dependencies along different directions, so as to improve the feature representation ability of vision Mamba. Thirdly, to learn the complementary features, we propose a ConvMamba block consisting of three branches, named Multi-head Mamba branch (MHMamba), Feature Iteration Unit branch (FIU), and Convolutional Neural Network (CNN) branch, where the Feature Iteration Unit branch aims to fuse convolutional local features with Mamba-based global representations. Finally, a Globallocal Alternate Neural Architecture Search (GLNAS) method is proposed to search the optimal architecture of GLVM alternately with the evolutionary algorithm, thereby improving the recognition performance for vein recognition tasks. We conduct rigorous experiments on three public palm-vein databases to estimate the performance. The experimental results demonstrate that the proposed method outperforms the representative approaches and achieves state-of-the-art recognition accuracy.

Translated Abstract:
정맥 인식은 높은 보안, 개인 정보 보호, 그리고 생체 인식의 장점 덕분에 최근 몇 년 동안 많은 주목을 받고 있어. 최근에는 Mamba 같은 딥러닝 모델이 선형 계산 복잡도로 강력한 특징 표현을 보여주고, 시각적 작업에 성공적으로 적용되고 있어. 하지만, Vision Mamba는 먼 거리의 특징 의존성은 잘 잡아내지만, 안타깝게도 지역적인 특징 세부사항이 떨어지는 문제가 있어. 게다가, 사람의 사전 지식을 바탕으로 Mamba 아키텍처를 수동으로 설계하는 건 정말 시간도 많이 걸리고 오류도 발생하기 쉬워.

이 논문에서는 먼저, Global-local Vision Mamba (GLVM)라는 하이브리드 네트워크 구조를 제안해. 이 구조는 이미지에서 지역적 상관관계를 명확히 학습하고, 정맥 특징 표현을 위해 토큰 간의 글로벌 의존성도 학습해. 두 번째로, Multi-head Mamba를 설계해서 다양한 방향으로의 의존성을 학습해, Vision Mamba의 특징 표현 능력을 향상시키려 해. 세 번째로, 상호 보완적인 특징을 학습하기 위해 세 가지 브랜치로 구성된 ConvMamba 블록을 제안해. 이 블록은 Multi-head Mamba 브랜치(MHMamba), Feature Iteration Unit 브랜치(FIU), 그리고 Convolutional Neural Network(CNN) 브랜치로 이루어져 있어. 여기서 Feature Iteration Unit 브랜치는 컨볼루션 기반의 지역 특징과 Mamba 기반의 글로벌 표현을 융합하는 게 목표야.

마지막으로, GLVM의 최적 아키텍처를 진화 알고리즘으로 교대로 검색하는 Globallocal Alternate Neural Architecture Search (GLNAS) 방법을 제안해. 이를 통해 정맥 인식 작업의 인식 성능을 향상시킬 수 있어. 우리는 성능을 추정하기 위해 세 개의 공개 손바닥 정맥 데이터베이스에서 엄격한 실험을 수행했어. 실험 결과는 제안한 방법이 대표적인 접근법보다 뛰어나고, 최첨단 인식 정확도를 달성했다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2408.11795.pdf

Title: EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model

Original Abstract:
In the realm of multimodal research, numerous studies leverage substantial image-text pairs to conduct modal alignment learning, transforming Large Language Models (LLMs) into Multimodal LLMs and excelling in a variety of visual-language tasks. The prevailing methodologies primarily fall into two categories: self-attention-based and cross-attention-based methods. While self-attention-based methods offer superior data efficiency due to their simple MLP architecture, they often suffer from lower computational efficiency due to concatenating visual and textual tokens as input for LLM. Conversely, cross-attention-based methods, although less data-efficient due to additional learnable parameters, exhibit higher computational efficiency by avoiding long sequence input for LLM. To address these trade-offs, we introduce the Data-Efficient and Compute-Efficient Multimodal Large Language Model (EE-MLLM). Without introducing additional modules or learnable parameters, EE-MLLM achieves both data and compute efficiency. Specifically, we modify the original self-attention mechanism in MLLM to a composite attention mechanism. This mechanism has two key characteristics: 1) Eliminating the computational overhead of self-attention within visual tokens to achieve compute efficiency, and 2) Reusing the weights on each layer of LLM to facilitate effective modality alignment between vision and language for data efficiency. Experimental results demonstrate the effectiveness of EE-MLLM across a range of benchmarks, including general-purpose datasets like MMBench and SeedBench, as well as fine-grained tasks such as TextVQA and DocVQA.

Translated Abstract:
멀티모달 연구 분야에서는 많은 연구들이 이미지와 텍스트의 쌍을 이용해 모달 정렬 학습을 진행하고 있어. 이렇게 해서 대형 언어 모델(LLM)을 멀티모달 LLM으로 바꾸고, 다양한 시각-언어 작업에서 뛰어난 성능을 보여주고 있어. 

주요 방법론은 크게 두 가지로 나눌 수 있어: 자기 주의 기반 방법과 교차 주의 기반 방법. 자기 주의 기반 방법은 간단한 MLP 아키텍처 덕분에 데이터 효율성은 좋지만, LLM에 시각적 토큰과 텍스트 토큰을 함께 입력해야 해서 계산 효율성은 떨어지는 경향이 있어. 반면, 교차 주의 기반 방법은 추가적인 학습 가능한 파라미터 덕분에 데이터 효율성은 낮지만, LLM에 긴 시퀀스 입력을 피해서 계산 효율성은 더 높아.

이런 장단점을 해결하기 위해, 우리는 데이터 효율성과 계산 효율성을 모두 갖춘 멀티모달 대형 언어 모델(EE-MLLM)을 제안해. 추가적인 모듈이나 학습 가능한 파라미터를 도입하지 않고도 데이터와 계산 효율성을 달성했어. 구체적으로, 우리는 원래의 자기 주의 메커니즘을 복합 주의 메커니즘으로 수정했어. 이 메커니즘은 두 가지 주요 특징이 있어: 1) 시각적 토큰 내에서 자기 주의의 계산 오버헤드를 없애서 계산 효율성을 높이고, 2) LLM의 각 레이어에서 가중치를 재사용해 시각과 언어 간의 효과적인 모달 정렬을 도와 데이터 효율성을 증가시켜.

실험 결과, EE-MLLM은 MMBench와 SeedBench 같은 일반 목적 데이터셋뿐만 아니라 TextVQA와 DocVQA 같은 세부 작업에서도 효과적임을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2408.11836.pdf

Title: Analysis of Unstructured High-Density Crowded Scenes for Crowd Monitoring

Original Abstract:
We are interested in developing an automated system for detection of organized movements in human crowds. Computer vision algorithms can extract information from videos of crowded scenes and automatically detect and track groups of individuals undergoing organized motion that represents an anomalous behavior in the context of conflict aversion. Our system can detect organized cohorts against the background of randomly moving objects and we can estimate the number of participants in an organized cohort, the speed and direction of motion in real time, within three to four video frames, which is less than one second from the onset of motion captured on a CCTV. We have performed preliminary analysis in this context in biological cell data containing up to four thousand objects per frame and will extend this numerically to a hundred-fold for public safety applications.
We envisage using the existing infrastructure of video cameras for acquiring image datasets on-the-fly and deploying an easy-to-use data-driven software system for parsing of significant events by analyzing image sequences taken inside and outside of sports stadiums or other public venues. Other prospective users are organizers of political rallies, civic and wildlife organizations, security firms, and the military. We will optimize the performance of the software by implementing a classification method able to distinguish between activities posing a threat and those not posing a threat.

Translated Abstract:
우리는 사람들의 군중에서 조직적인 움직임을 자동으로 감지하는 시스템을 개발하는 데 관심이 있어. 컴퓨터 비전 알고리즘을 사용하면 혼잡한 장면의 동영상에서 정보를 추출하고, 갈등 회피 상황에서 비정상적인 행동을 나타내는 조직적인 움직임을 하는 그룹을 자동으로 감지하고 추적할 수 있어.

우리 시스템은 무작위로 움직이는 물체 배경 속에서 조직된 그룹을 감지할 수 있고, CCTV에서 포착된 움직임 시작 후 3~4개의 비디오 프레임 안에, 즉 1초도 안 되는 시간 안에 조직된 그룹의 참가자 수, 속도, 방향을 실시간으로 추정할 수 있어. 우리는 이와 관련하여 최대 4,000개의 물체가 있는 생물학적 세포 데이터를 대상으로 초기 분석을 했고, 공공 안전 응용을 위해 이 수치를 100배로 늘릴 계획이야.

우리는 기존의 비디오 카메라 인프라를 사용해서 이미지 데이터셋을 실시간으로 수집하고, 스포츠 경기장이나 다른 공공 장소에서 촬영된 이미지 시퀀스를 분석해 중요한 사건을 파악할 수 있는 사용하기 쉬운 데이터 기반 소프트웨어 시스템을 배포할 생각이야. 정치 집회 주최자, 시민 및 야생 동물 단체, 보안 회사, 군대 등이 이 시스템의 잠재적인 사용자일 거야. 소프트웨어 성능을 최적화하기 위해 위협이 되는 활동과 그렇지 않은 활동을 구분할 수 있는 분류 방법을 구현할 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2408.16662.pdf

Title: Space3D-Bench: Spatial 3D Question Answering Benchmark

Original Abstract:
Answering questions about the spatial properties of the environment poses challenges for existing language and vision foundation models due to a lack of understanding of the 3D world notably in terms of relationships between objects. To push the field forward, multiple 3D Q&A datasets were proposed which, overall, provide a variety of questions, but they individually focus on particular aspects of 3D reasoning or are limited in terms of data modalities. To address this, we present Space3D-Bench - a collection of 1000 general spatial questions and answers related to scenes of the Replica dataset which offers a variety of data modalities: point clouds, posed RGB-D images, navigation meshes and 3D object detections. To ensure that the questions cover a wide range of 3D objectives, we propose an indoor spatial questions taxonomy inspired by geographic information systems and use it to balance the dataset accordingly. Moreover, we provide an assessment system that grades natural language responses based on predefined ground-truth answers by leveraging a Vision Language Model's comprehension of both text and images to compare the responses with ground-truth textual information or relevant visual data. Finally, we introduce a baseline called RAG3D-Chat integrating the world understanding of foundation models with rich context retrieval, achieving an accuracy of 67% on the proposed dataset.

Translated Abstract:
환경의 공간적 특성에 대한 질문에 답하는 것은 기존의 언어 및 비전 모델에게 어려움이 있어. 특히 물체 간의 관계를 이해하는 데 3D 세계에 대한 이해가 부족해서 그렇지. 이 문제를 해결하기 위해 여러 개의 3D Q&A 데이터셋이 제안되었는데, 각 데이터셋은 다양한 질문을 제공하지만 특정 3D 추론의 측면에만 집중하거나 데이터 양이 제한적이야.

그래서 우리는 Space3D-Bench라는 것을 만들었어. 이건 Replica 데이터셋의 장면과 관련된 1000개의 일반적인 공간 질문과 답변 모음이야. 다양한 데이터 양식도 제공하는데, 포인트 클라우드, RGB-D 이미지, 내비게이션 메시, 3D 물체 탐지 등이 포함돼. 질문들이 다양한 3D 목표를 포괄하도록 하기 위해 지리정보 시스템에서 영감을 받은 실내 공간 질문 분류 체계를 제안하고, 이를 이용해 데이터셋의 균형을 맞췄어.

또한, 사전 정의된 정답에 기반해 자연어 응답을 평가하는 시스템도 제공해. 이 시스템은 비전 언어 모델을 활용해 텍스트와 이미지를 이해하고, 응답을 정답 텍스트 정보나 관련된 시각 데이터와 비교해서 점수를 매겨. 마지막으로, RAG3D-Chat이라는 기준 모델을 소개하는데, 이건 기초 모델의 세계 이해를 풍부한 컨텍스트 검색과 통합해 제안된 데이터셋에서 67%의 정확도를 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.00487.pdf

Title: TrackSSM: A General Motion Predictor by State-Space Model

Original Abstract:
Temporal motion modeling has always been a key component in multiple object tracking (MOT) which can ensure smooth trajectory movement and provide accurate positional information to enhance association precision. However, current motion models struggle to be both efficient and effective across different application scenarios. To this end, we propose TrackSSM inspired by the recently popular state space models (SSM), a unified encoder-decoder motion framework that uses data-dependent state space model to perform temporal motion of trajectories. Specifically, we propose Flow-SSM, a module that utilizes the position and motion information from historical trajectories to guide the temporal state transition of object bounding boxes. Based on Flow-SSM, we design a flow decoder. It is composed of a cascaded motion decoding module employing Flow-SSM, which can use the encoded flow information to complete the temporal position prediction of trajectories. Additionally, we propose a Step-by-Step Linear (S$^2$L) training strategy. By performing linear interpolation between the positions of the object in the previous frame and the current frame, we construct the pseudo labels of step-by-step linear training, ensuring that the trajectory flow information can better guide the object bounding box in completing temporal transitions. TrackSSM utilizes a simple Mamba-Block to build a motion encoder for historical trajectories, forming a temporal motion model with an encoder-decoder structure in conjunction with the flow decoder. TrackSSM is applicable to various tracking scenarios and achieves excellent tracking performance across multiple benchmarks, further extending the potential of SSM-like temporal motion models in multi-object tracking tasks. Code and models are publicly available at \url{this https URL}.

Translated Abstract:
시간적인 움직임 모델링은 여러 물체 추적(MOT)에서 항상 중요한 요소였어. 이게 부드러운 궤적 움직임을 보장하고 정확한 위치 정보를 제공해서 연관성을 높이는데 도움을 주거든. 하지만 현재의 움직임 모델들은 다양한 상황에서 효율성과 효과성을 동시에 갖추기가 힘들어.

그래서 우리는 TrackSSM을 제안했어. 이건 최근에 인기 있는 상태 공간 모델(SSM)에서 영감을 받아 만든 통합된 인코더-디코더 움직임 프레임워크야. 데이터에 따라 달라지는 상태 공간 모델을 사용해서 궤적의 시간적인 움직임을 수행해. 구체적으로, 우리는 Flow-SSM이라는 모듈을 제안해. 이 모듈은 과거 궤적의 위치와 움직임 정보를 활용해서 물체의 경계 상자의 시간적 상태 전환을 유도해.

Flow-SSM을 기반으로 플로우 디코더를 설계했어. 이 디코더는 Flow-SSM을 사용하는 연속된 움직임 디코딩 모듈로 구성돼. 이걸 통해 인코딩된 흐름 정보를 사용해서 궤적의 시간적 위치 예측을 완성할 수 있어. 그리고 우리는 단계별 선형(S$^2$L) 훈련 전략도 제안해. 이전 프레임과 현재 프레임의 물체 위치 사이에서 선형 보간을 수행해서 단계별 선형 훈련의 가짜 레이블을 만들어. 이렇게 하면 궤적 흐름 정보가 물체 경계 상자에 시간적 전환을 완료하는데 더 잘 안내할 수 있어.

TrackSSM은 간단한 Mamba-Block을 이용해 과거 궤적의 움직임 인코더를 만들어. 이로써 인코더-디코더 구조와 플로우 디코더를 결합한 시간적 움직임 모델을 형성해. TrackSSM은 다양한 추적 상황에 적용 가능하고 여러 벤치마크에서 뛰어난 추적 성능을 달성해. 이로 인해 SSM과 같은 시간적 움직임 모델의 가능성이 다중 물체 추적 작업에서 더욱 확장되고 있어. 코드와 모델은 공개적으로 이용 가능해.

================================================================================

URL:
https://arxiv.org/pdf/2409.02720.pdf

Title: GET-UP: GEomeTric-aware Depth Estimation with Radar Points UPsampling

Original Abstract:
Depth estimation plays a pivotal role in autonomous driving, facilitating a comprehensive understanding of the vehicle's 3D surroundings. Radar, with its robustness to adverse weather conditions and capability to measure distances, has drawn significant interest for radar-camera depth estimation. However, existing algorithms process the inherently noisy and sparse radar data by projecting 3D points onto the image plane for pixel-level feature extraction, overlooking the valuable geometric information contained within the radar point cloud. To address this gap, we propose GET-UP, leveraging attention-enhanced Graph Neural Networks (GNN) to exchange and aggregate both 2D and 3D information from radar data. This approach effectively enriches the feature representation by incorporating spatial relationships compared to traditional methods that rely only on 2D feature extraction. Furthermore, we incorporate a point cloud upsampling task to densify the radar point cloud, rectify point positions, and derive additional 3D features under the guidance of lidar data. Finally, we fuse radar and camera features during the decoding phase for depth estimation. We benchmark our proposed GET-UP on the nuScenes dataset, achieving state-of-the-art performance with a 15.3% and 14.7% improvement in MAE and RMSE over the previously best-performing model. Code: this https URL

Translated Abstract:
깊이 추정은 자율주행에서 중요한 역할을 해. 차량의 3D 주변 환경을 잘 이해할 수 있게 해주거든. 레이더는 나쁜 날씨에서도 잘 작동하고 거리도 측정할 수 있어서, 레이더-카메라 깊이 추정에 많은 관심을 받고 있어. 

하지만 기존 알고리즘은 본래 소음이 많고 드문 레이더 데이터를 처리할 때 3D 점을 이미지 평면에 투영해서 픽셀 수준의 특징을 추출하는 방식인데, 이 과정에서 레이더 점 구름에 담긴 중요한 기하학적 정보를 놓치고 있어. 이 문제를 해결하기 위해 우리는 GET-UP이라는 방법을 제안해. 이 방법은 주의력을 높인 그래프 신경망(GNN)을 활용해서 레이더 데이터에서 2D와 3D 정보를 서로 교환하고 집계해. 이렇게 하면 전통적인 2D 특징 추출 방식보다 공간적 관계를 포함해 특징 표현이 더 풍부해져.

또한, 레이더 점 구름을 밀집하게 만들고 점 위치를 수정하며 라이다 데이터를 활용해 추가적인 3D 특징을 유도하는 점 구름 업샘플링 작업도 포함했어. 마지막으로, 깊이 추정을 위한 디코딩 단계에서 레이더와 카메라 특징을 융합해. 우리는 nuScenes 데이터셋에서 제안한 GET-UP을 평가했는데, 기존의 최고 성능 모델보다 MAE에서 15.3%, RMSE에서 14.7% 개선된 성과를 냈어. 코드도 있어: 이 URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.03767.pdf

Title: EMCNet : Graph-Nets for Electron Micrographs Classification

Original Abstract:
Characterization of materials via electron micrographs is an important and challenging task in several materials processing industries. Classification of electron micrographs is complex due to the high intra-class dissimilarity, high inter-class similarity, and multi-spatial scales of patterns. However, existing methods are ineffective in learning complex image patterns. We propose an effective end-to-end electron micrograph representation learning-based framework for nanomaterial identification to overcome the challenges. We demonstrate that our framework outperforms the popular baselines on the open-source datasets in nanomaterials-based identification tasks. The ablation studies are reported in great detail to support the efficacy of our approach.

Translated Abstract:
전자 현미경 이미지를 통해 재료를 분석하는 것은 여러 재료 가공 산업에서 중요하면서도 도전적인 작업이야. 전자 현미경 이미지를 분류하는 게 복잡한 이유는 같은 클래스 내에서의 차이가 크고, 서로 다른 클래스 간의 유사성이 높으며, 패턴의 공간적 스케일이 다양하기 때문이야. 하지만 기존의 방법들은 복잡한 이미지 패턴을 배우는 데 효과적이지 않아.

그래서 우리는 나노재료 식별을 위해 효과적인 엔드 투 엔드 전자 현미경 이미지 표현 학습 기반의 프레임워크를 제안해. 이 프레임워크가 나노재료 식별 작업에 있어 인기 있는 기준 모델들보다 성능이 뛰어난 걸 보여줬어. 또한, 우리의 접근 방식의 효과를 지지하기 위해 상세한 분석 연구 결과도 보고했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.04388.pdf

Title: Question-Answering Dense Video Events

Original Abstract:
Multimodal Large Language Models (MLLMs) have shown excellent performance in question-answering of single-event videos. In this paper, we present question-answering dense video events, a novel task that requires answering and grounding the dense-event questions in long videos, thus challenging MLLMs to faithfully comprehend and reason about multiple events occurring over extended time periods. To facilitate the study, we construct DeVE-QA - a dataset featuring 78K questions about 26K events on 10.6K long videos. We then benchmark and show that existing MLLMs excelling at single-event QA struggle to perform well in DeVE-QA. For improvement, we propose DeVi, a novel training-free MLLM approach that highlights a hierarchical captioning module, a temporal event memory module, and a self-consistency checking module to respectively detect, contextualize and memorize, and ground dense-events in long videos for question answering. Extensive experiments show that DeVi is superior at answering dense-event questions and grounding relevant video moments. Compared with existing MLLMs, it achieves a remarkable increase of 4.1 percent and 3.7 percent for G(round)QA accuracy on DeVE-QA and NExT-GQA respectively.

Translated Abstract:
다중 모달 대형 언어 모델(MLLMs)은 단일 이벤트 비디오의 질문-답변에서 뛰어난 성능을 보여줬어. 이 논문에서는 긴 비디오에서의 밀집 이벤트 질문에 답하고, 그 질문을 이해하는 새로운 작업인 질문-답변 밀집 비디오 이벤트를 제안해. 이 작업은 MLLMs가 긴 시간 동안 발생하는 여러 이벤트를 제대로 이해하고 추론해야 하기 때문에 도전적이야.

우리를 위해 DeVE-QA라는 데이터셋을 만들었어. 이 데이터셋은 10,600개의 긴 비디오에서 26,000개의 이벤트에 대한 78,000개의 질문을 포함하고 있어. 그리고 기존의 MLLM들이 단일 이벤트 QA에서는 잘하지만, DeVE-QA에서는 잘 못한다는 것을 보여줬어.

개선을 위해 DeVi라는 새로운 훈련 필요 없는 MLLM 접근법을 제안해. 이 방법은 계층적 캡셔닝 모듈, 시간적 이벤트 메모리 모듈, 자기 일관성 점검 모듈을 강조해. 각각은 밀집 이벤트를 탐지하고, 맥락을 제공하며, 기억하고, 질문에 대한 답변을 위해 긴 비디오에서 밀집 이벤트를 기반으로 해.

광범위한 실험 결과, DeVi가 밀집 이벤트 질문에 답변하고 관련 비디오 순간을 찾는 데 더 뛰어난 성과를 보였어. 기존 MLLM들과 비교했을 때, DeVE-QA와 NExT-GQA에서 각각 4.1%와 3.7%의 놀라운 정확도 향상을 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.04760.pdf

Title: Training-Free Point Cloud Recognition Based on Geometric and Semantic Information Fusion

Original Abstract:
The trend of employing training-free methods for point cloud recognition is becoming increasingly popular due to its significant reduction in computational resources and time costs. However, existing approaches are limited as they typically extract either geometric or semantic features. To address this limitation, we are the first to propose a novel training-free method that integrates both geometric and semantic features. For the geometric branch, we adopt a non-parametric strategy to extract geometric features. In the semantic branch, we leverage a model aligned with text features to obtain semantic features. Additionally, we introduce the GFE module to complement the geometric information of point clouds and the MFF module to improve performance in few-shot settings. Experimental results demonstrate that our method outperforms existing state-of-the-art training-free approaches on mainstream benchmark datasets, including ModelNet and ScanObiectNN.

Translated Abstract:
포인트 클라우드 인식에서 훈련이 필요 없는 방법을 사용하는 트렌드가 점점 인기를 끌고 있어. 이 방법은 계산 자원과 시간 비용을 크게 줄여주거든. 하지만 기존 방법들은 보통 기하학적 특징이나 의미론적 특징 중 하나만 추출하는 한계가 있어. 

이런 한계를 해결하기 위해, 우리는 기하학적 특징과 의미론적 특징을 모두 통합한 새로운 훈련 없는 방법을 처음으로 제안해. 기하학적인 부분에서는 비모수적 전략을 사용해서 기하학적 특징을 추출하고, 의미론적 부분에서는 텍스트 특징과 잘 맞는 모델을 활용해 의미론적 특징을 얻어. 

게다가, 포인트 클라우드의 기하학적 정보를 보완하는 GFE 모듈과, 소수의 샷에서도 성능을 향상시키는 MFF 모듈도 도입했어. 실험 결과, 우리의 방법이 ModelNet과 ScanObjectNN 같은 주요 벤치마크 데이터셋에서 기존의 최첨단 훈련 없는 방법들보다 더 나은 성능을 보였어.

================================================================================

URL:
https://arxiv.org/pdf/2409.04952.pdf

Title: Deep Bayesian Active Learning-to-Rank with Relative Annotation for Estimation of Ulcerative Colitis Severity

Original Abstract:
Automatic image-based severity estimation is an important task in computer-aided diagnosis. Severity estimation by deep learning requires a large amount of training data to achieve a high performance. In general, severity estimation uses training data annotated with discrete (i.e., quantized) severity labels. Annotating discrete labels is often difficult in images with ambiguous severity, and the annotation cost is high. In contrast, relative annotation, in which the severity between a pair of images is compared, can avoid quantizing severity and thus makes it easier. We can estimate relative disease severity using a learning-to-rank framework with relative annotations, but relative annotation has the problem of the enormous number of pairs that can be annotated. Therefore, the selection of appropriate pairs is essential for relative annotation. In this paper, we propose a deep Bayesian active learning-to-rank that automatically selects appropriate pairs for relative annotation. Our method preferentially annotates unlabeled pairs with high learning efficiency from the model uncertainty of the samples. We prove the theoretical basis for adapting Bayesian neural networks to pairwise learning-to-rank and demonstrate the efficiency of our method through experiments on endoscopic images of ulcerative colitis on both private and public datasets. We also show that our method achieves a high performance under conditions of significant class imbalance because it automatically selects samples from the minority classes.

Translated Abstract:
자동 이미지 기반 심각도 추정은 컴퓨터 지원 진단에서 중요한 작업이야. 딥러닝을 이용한 심각도 추정은 높은 성능을 얻으려면 많은 훈련 데이터가 필요해. 일반적으로 심각도 추정은 이산(즉, 정량화된) 심각도 레이블로 주석이 달린 훈련 데이터를 사용해. 그런데 애매한 심각도를 가진 이미지에 주석을 달기는 어렵고, 비용도 비싸.

반면에, 상대 주석은 두 이미지 사이의 심각도를 비교하는 방식인데, 이 방법은 심각도를 정량화하지 않아서 좀 더 쉽게 해. 우리는 상대 주석을 이용해서 학습 순위를 이용한 프레임워크로 상대 질병 심각도를 추정할 수 있지만, 상대 주석의 문제는 주석을 달 수 있는 쌍의 수가 엄청 많다는 거야. 그래서 적절한 쌍을 선택하는 게 상대 주석에선 중요해.

이 논문에서는 상대 주석을 위한 적절한 쌍을 자동으로 선택하는 딥 베이지안 능동 학습 순위 방법을 제안해. 우리 방법은 모델의 불확실성에서 높은 학습 효율을 가진 레이블 없는 쌍을 우선적으로 주석 달아. 우리는 쌍 학습 순위에 베이지안 신경망을 적용하는 이론적 근거를 증명하고, 개인 및 공개 데이터셋에서 궤양성 대장염의 내시경 이미지 실험을 통해 우리 방법의 효율성을 입증해. 또한, 우리 방법이 불균형한 클래스 조건에서도 높은 성능을 달성한다는 걸 보여주는데, 이건 소수 클래스의 샘플을 자동으로 선택하기 때문이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.05005.pdf

Title: Towards Patronizing and Condescending Language in Chinese Videos: A Multimodal Dataset and Detector

Original Abstract:
Patronizing and Condescending Language (PCL) is a form of discriminatory toxic speech targeting vulnerable groups, threatening both online and offline safety. While toxic speech research has mainly focused on overt toxicity, such as hate speech, microaggressions in the form of PCL remain underexplored. Additionally, dominant groups' discriminatory facial expressions and attitudes toward vulnerable communities can be more impactful than verbal cues, yet these frame features are often overlooked. In this paper, we introduce the PCLMM dataset, the first Chinese multimodal dataset for PCL, consisting of 715 annotated videos from Bilibili, with high-quality PCL facial frame spans. We also propose the MultiPCL detector, featuring a facial expression detection module for PCL recognition, demonstrating the effectiveness of modality complementarity in this challenging task. Our work makes an important contribution to advancing microaggression detection within the domain of toxic speech.

Translated Abstract:
패트로나이징 및 경멸적인 언어(PCL)는 취약한 집단을 겨냥한 차별적인 유독 발언의 한 형태로, 온라인과 오프라인에서의 안전을 위협해. 기존의 유독 발언 연구는 주로 증오 발언 같은 명백한 유독성에 초점을 맞췄지만, PCL 같은 미세 공격은 아직 충분히 연구되지 않았어. 게다가, 지배적인 집단의 차별적인 표정과 태도가 취약한 커뮤니티에 미치는 영향은 언어적 신호보다 더 클 수 있는데, 이런 부분은 종종 간과되곤 해.

이 논문에서는 PCL을 위한 첫 번째 중국어 다중 모달 데이터셋인 PCLMM 데이터셋을 소개해. 이 데이터셋은 Bilibili에서 수집한 715개의 주석이 달린 비디오로 구성되어 있고, 고품질 PCL 얼굴 프레임을 포함하고 있어. 또한, PCL 인식을 위한 얼굴 표정 탐지 모듈을 갖춘 MultiPCL 탐지기를 제안해. 이 작업은 이 어려운 작업에서 모달리티 보완의 효과를 보여줘. 우리의 연구는 유독 발언 분야에서 미세 공격 탐지를 발전시키는 데 중요한 기여를 해.

================================================================================

URL:
https://arxiv.org/pdf/2409.05463.pdf

Title: DriveScape: Towards High-Resolution Controllable Multi-View Driving Video Generation

Original Abstract:
Recent advancements in generative models have provided promising solutions for synthesizing realistic driving videos, which are crucial for training autonomous driving perception models. However, existing approaches often struggle with multi-view video generation due to the challenges of integrating 3D information while maintaining spatial-temporal consistency and effectively learning from a unified model. In this paper, we propose an end-to-end framework named DriveScape for multi-view, 3D condition-guided video generation. DriveScape not only streamlines the process by integrating camera data to ensure comprehensive spatial-temporal coverage, but also introduces a Bi-Directional Modulated Transformer module to effectively align 3D road structural information. As a result, our approach enables precise control over video generation, significantly enhancing realism and providing a robust solution for generating multi-view driving videos. Our framework achieves state-of-the-art results on the nuScenes dataset, demonstrating impressive generative quality metrics with an FID score of 8.34 and an FVD score of 76.39, as well as superior performance across various perception tasks. This paves the way for more accurate environmental simulations in autonomous driving. Code will be available at \href{this https URL}{our project homepage}.

Translated Abstract:
최근 생성 모델의 발전 덕분에 현실적인 주행 비디오를 합성하는 데 유망한 해결책이 생겼어. 이런 비디오는 자율주행 인식 모델을 훈련하는 데 중요해. 하지만 기존의 방법들은 3D 정보를 통합하면서 공간-시간 일관성을 유지하고 통합 모델에서 효과적으로 학습하는 데 어려움을 겪고 있어.

이 논문에서는 다중 시점, 3D 조건 기반 비디오 생성을 위한 엔드 투 엔드 프레임워크인 DriveScape를 제안해. DriveScape는 카메라 데이터를 통합해서 포괄적인 공간-시간 범위를 보장하면서 과정도 간소화해. 또한, 3D 도로 구조 정보를 효과적으로 정렬하기 위해 양방향 변조 변환기 모듈을 도입했어.

그 결과, 우리 방법은 비디오 생성에 대한 정밀한 제어를 가능하게 하고, 사실감을 크게 향상시켜서 다중 시점 주행 비디오 생성에 강력한 해결책을 제공해. 우리 프레임워크는 nuScenes 데이터셋에서 최첨단 결과를 달성하며, FID 점수 8.34와 FVD 점수 76.39로 인상적인 생성 품질 지표를 보여줘. 여러 인식 작업에서도 뛰어난 성능을 발휘해. 이 연구는 자율주행에서 더 정확한 환경 시뮬레이션을 가능하게 해. 코드도 우리 프로젝트 홈페이지에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.05494.pdf

Title: An Atmospheric Correction Integrated LULC Segmentation Model for High-Resolution Satellite Imagery

Original Abstract:
The integration of fine-scale multispectral imagery with deep learning models has revolutionized land use and land cover (LULC) classification. However, the atmospheric effects present in Top-of-Atmosphere sensor measured Digital Number values must be corrected to retrieve accurate Bottom-of-Atmosphere surface reflectance for reliable analysis. This study employs look-up-table-based radiative transfer simulations to estimate the atmospheric path reflectance and transmittance for atmospherically correcting high-resolution CARTOSAT-3 Multispectral (MX) imagery for several Indian cities. The corrected surface reflectance data were subsequently used in supervised and semi-supervised segmentation models, demonstrating stability in multi-class (buildings, roads, trees and water bodies) LULC segmentation accuracy, particularly in scenarios with sparsely labelled data.

Translated Abstract:
고해상도의 다중 스펙트럼 이미지를 딥러닝 모델과 결합하는 것이 토지 이용 및 토지 피복(LULC) 분류에 큰 변화를 가져왔어. 하지만, 대기에서 측정된 디지털 숫자 값은 대기 효과로 인해 정확한 바닥 대기 표면 반사율을 얻기 위해 수정이 필요해. 

이 연구에서는 대기 경로의 반사율과 투과율을 추정하기 위해 룩업 테이블 기반의 복사 전달 시뮬레이션을 사용했어. 이 방법을 통해 인도의 여러 도시에서 고해상도 CARTOSAT-3 다중 스펙트럼 이미지의 대기를 수정했어. 수정된 표면 반사율 데이터는 감독 학습 및 반감독 학습 세분화 모델에 사용되었고, 특히 레이블이 부족한 데이터가 있는 상황에서도 다중 클래스(건물, 도로, 나무, 수역) LULC 세분화 정확도가 안정적임을 보여주었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.05862.pdf

Title: Evaluating Multiview Object Consistency in Humans and Image Models

Original Abstract:
We introduce a benchmark to directly evaluate the alignment between human observers and vision models on a 3D shape inference task. We leverage an experimental design from the cognitive sciences which requires zero-shot visual inferences about object shape: given a set of images, participants identify which contain the same/different objects, despite considerable viewpoint variation. We draw from a diverse range of images that include common objects (e.g., chairs) as well as abstract shapes (i.e., procedurally generated `nonsense' objects). After constructing over 2000 unique image sets, we administer these tasks to human participants, collecting 35K trials of behavioral data from over 500 participants. This includes explicit choice behaviors as well as intermediate measures, such as reaction time and gaze data. We then evaluate the performance of common vision models (e.g., DINOv2, MAE, CLIP). We find that humans outperform all models by a wide margin. Using a multi-scale evaluation approach, we identify underlying similarities and differences between models and humans: while human-model performance is correlated, humans allocate more time/processing on challenging trials. All images, data, and code can be accessed via our project page.

Translated Abstract:
우리는 3D 형태 추론 작업에서 인간 관찰자와 비전 모델 간의 정렬을 직접 평가할 수 있는 벤치마크를 소개해. 이 실험 설계는 인지 과학에서 가져온 건데, 여기서는 객체 형태에 대한 제로샷 비주얼 추론이 필요해. 즉, 주어진 이미지 세트에서 참가자들이 동일한 객체와 다른 객체를 구별해야 해, 관점 변화가 많이 있음에도 불구하고.

우리는 일반적인 물체(예: 의자)부터 추상적인 형태(즉, 절차적으로 생성된 ‘헛소리’ 객체)까지 다양한 이미지를 사용했어. 2000개가 넘는 독특한 이미지 세트를 만든 후, 이 작업을 500명 이상의 참가자에게 진행했어. 총 35,000회의 행동 데이터를 수집했지. 여기에는 명시적인 선택 행동뿐만 아니라 반응 시간과 시선 데이터 같은 중간 측정도 포함돼.

그 다음엔 DINOv2, MAE, CLIP 같은 일반적인 비전 모델의 성능을 평가했어. 결과적으로 인간이 모든 모델보다 훨씬 더 잘하는 걸 발견했어. 다중 스케일 평가 접근 방식을 사용해서 모델과 인간 간의 기본적인 유사점과 차이점을 파악했지. 인간과 모델의 성능은 상관관계가 있지만, 인간은 어려운 과제에서 더 많은 시간과 처리를 할애해.

모든 이미지, 데이터, 코드 등은 우리의 프로젝트 페이지에서 접근할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2201.07610.pdf

Title: Nonlinear Unknown Input Observability and Unknown Input Reconstruction: The General Analytical Solution

Original Abstract:
Observability is a fundamental structural property of any dynamic system and describes the possibility of reconstructing the state that characterizes the system from observing its inputs and outputs. Despite the huge effort made to study this property and to introduce analytical criteria able to check whether a dynamic system satisfies this property or not, there is no general analytical criterion to automatically check the state observability when the dynamics are also driven by unknown inputs. Here, we introduce the general analytical solution of this fundamental problem, often called the unknown input observability problem. This paper provides the general analytical solution of this problem, namely, it provides the systematic procedure, based on automatic computation (differentiation and matrix rank determination), that allows us to automatically check the state observability even in the presence of unknown inputs (Algorithm 6.1). A first solution of this problem was presented in the second part of the book: "Observability: A New Theory Based on the Group of Invariance" [45]. The solution presented by this paper completes the previous solution in [45]. In particular, the new solution exhaustively accounts for the systems that do not belong to the category of the systems that are "canonic with respect to their unknown inputs". The analytical derivations largely exploit several new concepts and analytical results introduced in [45]. Finally, as a simple consequence of the results here obtained, we also provide the answer to the problem of unknown input reconstruction which is intimately related to the problem of state observability. We illustrate the implementation of the new algorithm by studying the observability properties of a nonlinear system in the framework of visual-inertial sensor fusion, whose dynamics are driven by two unknown inputs and one known input.

Translated Abstract:
관측 가능성은 동적 시스템의 중요한 구조적 특성으로, 시스템의 입력과 출력을 관찰해서 시스템의 상태를 재구성할 수 있는 가능성을 설명해. 이 특성을 연구하기 위해 많은 노력이 있었고, 동적 시스템이 이 특성을 만족하는지 확인할 수 있는 분석 기준도 도입됐어. 하지만, 동적 시스템의 동작이 알려지지 않은 입력에 의해 발생할 때, 상태 관측 가능성을 자동으로 확인할 수 있는 일반적인 분석 기준은 없었어.

여기서 우리는 이 기본적인 문제, 즉 알려지지 않은 입력 관측 가능성 문제에 대한 일반적인 분석적 해결책을 소개해. 이 논문은 이 문제의 일반적인 분석적 해결책을 제공하는데, 즉 자동 계산(미분과 행렬 계수 결정)을 기반으로 하는 체계적인 절차를 제공해서, 알려지지 않은 입력이 있는 경우에도 상태 관측 가능성을 자동으로 확인할 수 있게 해줘 (알고리즘 6.1). 이 문제의 첫 번째 해결책은 "관측 가능성: 불변 군을 기반으로 한 새로운 이론"이라는 책의 두 번째 부분에서 제시됐어 [45]. 이 논문에서 제시된 해결책은 [45]의 이전 해결책을 보완해. 특히, 새로운 해결책은 "알려지지 않은 입력에 대해 정규화된" 시스템 범주에 속하지 않는 시스템을 철저히 다루고 있어. 이 분석적 유도는 [45]에서 도입된 여러 새로운 개념과 분석 결과를 크게 활용해.

마지막으로, 여기서 얻은 결과의 간단한 결과로, 상태 관측 가능성과 밀접하게 관련된 알려지지 않은 입력 재구성 문제에 대한 답도 제공해. 우리는 두 개의 알려지지 않은 입력과 하나의 알려진 입력에 의해 동작하는 비선형 시스템의 관측 가능성 특성을 연구함으로써 새로운 알고리즘의 구현을 설명할 거야.

================================================================================

URL:
https://arxiv.org/pdf/2212.05376.pdf

Title: What's Wrong with the Absolute Trajectory Error?

Original Abstract:
One of the limitations of the commonly used Absolute Trajectory Error (ATE) is that it is highly sensitive to outliers. As a result, in the presence of just a few outliers, it often fails to reflect the varying accuracy as the inlier trajectory error or the number of outliers varies. In this work, we propose an alternative error metric for evaluating the accuracy of the reconstructed camera trajectory. Our metric, named Discernible Trajectory Error (DTE), is computed in five steps: (1) Shift the ground-truth and estimated trajectories such that both of their geometric medians are located at the origin. (2) Rotate the estimated trajectory such that it minimizes the sum of geodesic distances between the corresponding camera orientations. (3) Scale the estimated trajectory such that the median distance of the cameras to their geometric median is the same as that of the ground truth. (4) Compute, winsorize and normalize the distances between the corresponding cameras. (5) Obtain the DTE by taking the average of the mean and the root-mean-square (RMS) of the resulting distances. This metric is an attractive alternative to the ATE, in that it is capable of discerning the varying trajectory accuracy as the inlier trajectory error or the number of outliers varies. Using the similar idea, we also propose a novel rotation error metric, named Discernible Rotation Error (DRE), which has similar advantages to the DTE. Furthermore, we propose a simple yet effective method for calibrating the camera-to-marker rotation, which is needed for the computation of our metrics. Our methods are verified through extensive simulations.

Translated Abstract:
자주 쓰이는 절대 경로 오차(ATE)의 한계 중 하나는 이상치에 매우 민감하다는 거야. 그래서 몇 개의 이상치만 있어도, 이 오차가 실제 경로 오차나 이상치의 수에 따라 정확성을 잘 반영하지 못해. 

이번 연구에서는 재구성된 카메라 경로의 정확성을 평가하기 위한 대체 오차 지표를 제안해. 이 지표의 이름은 구별 가능한 경로 오차(Discernible Trajectory Error, DTE)야. DTE는 다섯 단계로 계산해: 

1. 실제 경로와 추정된 경로를 이동시켜서 두 경로의 기하학적 중앙이 원점에 오도록 해.
2. 추정된 경로를 회전시켜서 해당 카메라 방향 간의 기하학적 거리 합이 최소가 되도록 해.
3. 추정된 경로를 스케일링해서 카메라들이 기하학적 중앙까지의 중앙 거리가 실제 경로와 같아지도록 해.
4. 해당 카메라들 간의 거리를 계산하고, 윈저화한 다음 정규화해.
5. 최종 거리들의 평균과 RMS의 평균을 구해서 DTE를 얻어.

이 지표는 ATE에 비해 매력적인 대안이야. 왜냐하면 실제 경로 오차나 이상치의 수에 따라 경로 정확성을 잘 구별할 수 있으니까. 비슷한 아이디어로, 구별 가능한 회전 오차(Discernible Rotation Error, DRE)라는 새로운 회전 오차 지표도 제안했어. DRE는 DTE와 비슷한 장점을 가지고 있어. 

그리고 우리의 지표를 계산하는 데 필요한 카메라와 마커 간의 회전을 보정하는 간단하면서도 효과적인 방법도 제안했어. 우리의 방법은 광범위한 시뮬레이션을 통해 검증했어.

================================================================================

URL:
https://arxiv.org/pdf/2310.06488.pdf

Title: SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network

Original Abstract:
Spiking Neural Networks (SNNs) have emerged as a promising alternative to conventional Artificial Neural Networks (ANNs), demonstrating comparable performance in both visual and linguistic tasks while offering the advantage of improved energy efficiency. Despite these advancements, the integration of linguistic and visual features into a unified representation through spike trains poses a significant challenge, and the application of SNNs to multimodal scenarios remains largely unexplored. This paper presents SpikeCLIP, a novel framework designed to bridge the modality gap in spike-based computation. Our approach employs a two-step recipe: an ``alignment pre-training'' to align features across modalities, followed by a ``dual-loss fine-tuning'' to refine the model's performance. Extensive experiments reveal that SNNs achieve results on par with ANNs while substantially reducing energy consumption across various datasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP maintains robust image classification capabilities, even when dealing with classes that fall outside predefined categories. This study marks a significant advancement in the development of energy-efficient and biologically plausible multimodal learning systems.

Translated Abstract:
스파이킹 신경망(SNNs)은 기존의 인공 신경망(ANNs)에 대한 유망한 대안으로 떠오르고 있어. 시각적 작업과 언어적 작업 모두에서 비슷한 성능을 보여주면서 에너지 효율성이 더 좋아지는 장점이 있어. 

하지만 언어적 특징과 시각적 특징을 스파이크 트레인을 통해 하나의 표현으로 통합하는 건 여전히 큰 도전이야. 그래서 SNNs를 다중 모드 상황에 적용하는 건 아직 많이 연구되지 않았어. 

이 논문에서는 SpikeCLIP이라는 새로운 프레임워크를 소개해. 이건 스파이크 기반 계산에서 모드 간의 간극을 메꾸기 위해 설계된 거야. 우리의 접근법은 두 단계로 이루어져 있어: 첫 번째는 "정렬 사전 훈련"으로, 모드 간의 특징을 정렬하는 거고, 두 번째는 "이중 손실 미세 조정"으로 모델 성능을 개선하는 거야. 

많은 실험을 통해 SNNs가 ANNs와 비슷한 결과를 내면서도 다양한 데이터세트에서 에너지 소비를 크게 줄일 수 있다는 걸 보여줬어. 게다가 SpikeCLIP은 미리 정의된 카테고리 밖의 클래스에도 강력한 이미지 분류 능력을 유지해. 이 연구는 에너지 효율적이고 생물학적으로 그럴듯한 다중 모드 학습 시스템 개발에 큰 진전을 이뤘다고 볼 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2310.08116.pdf

Title: Multimodal Active Measurement for Human Mesh Recovery in Close Proximity

Original Abstract:
For physical human-robot interactions (pHRI), a robot needs to estimate the accurate body pose of a target person. However, in these pHRI scenarios, the robot cannot fully observe the target person's body with equipped cameras because the target person must be close to the robot for physical interaction. This close distance leads to severe truncation and occlusions and thus results in poor accuracy of human pose estimation. For better accuracy in this challenging environment, we propose an active measurement and sensor fusion framework of the equipped cameras with touch and ranging sensors such as 2D LiDAR. Touch and ranging sensor measurements are sparse but reliable and informative cues for localizing human body parts. In our active measurement process, camera viewpoints and sensor placements are dynamically optimized to measure body parts with higher estimation uncertainty, which is closely related to truncation or occlusion. In our sensor fusion process, assuming that the measurements of touch and ranging sensors are more reliable than the camera-based estimations, we fuse the sensor measurements to the camera-based estimated pose by aligning the estimated pose towards the measured points. Our proposed method outperformed previous methods on the standard occlusion benchmark with simulated active measurement. Furthermore, our method reliably estimated human poses using a real robot, even with practical constraints such as occlusion by blankets.

Translated Abstract:
물리적 인간-로봇 상호작용(pHRI)에서는 로봇이 목표 인물의 정확한 몸 자세를 추정해야 해. 하지만 이런 pHRI 상황에서는 로봇이 장착된 카메라로 목표 인물의 몸을 완전히 볼 수 없어. 왜냐하면 목표 인물이 로봇과 가까워야 물리적으로 상호작용을 할 수 있기 때문이야. 이 가까운 거리 때문에 신체 일부가 잘리거나 가려지는 문제가 생기고, 그래서 인간 자세 추정의 정확도가 떨어져.

이 어려운 환경에서 더 나은 정확도를 위해, 우리는 터치 및 거리 센서 같은 2D LiDAR를 장착한 카메라의 능동적인 측정 및 센서 융합 프레임워크를 제안해. 터치와 거리 센서의 측정값은 드물지만 신뢰할 수 있고, 인간 신체 부위를 정위치하는 데 도움이 되는 중요한 정보야. 우리의 능동적 측정 과정에서는 카메라의 시점과 센서의 위치를 동적으로 최적화해서, 잘리거나 가려지는 부분과 관련된 신체 부위를 더 잘 측정할 수 있어.

센서 융합 과정에서는 터치와 거리 센서의 측정값이 카메라 기반 추정보다 더 신뢰할 수 있다고 가정하고, 이 센서 측정값을 카메라 기반 추정 자세와 정렬해서 융합해. 우리가 제안한 방법은 시뮬레이션된 능동 측정을 사용한 표준 가림 벤치마크에서 이전 방법들보다 더 좋은 성능을 보여줬어. 게다가, 우리의 방법은 실제 로봇을 사용해서도 신뢰할 수 있는 인간 자세를 추정했어. 이때 담요 같은 실제 제약이 있어도 잘 작동했어.

================================================================================

URL:
https://arxiv.org/pdf/2403.14465.pdf

Title: CathFlow: Self-Supervised Segmentation of Catheters in Interventional Ultrasound Using Optical Flow and Transformers

Original Abstract:
In minimally invasive endovascular procedures, contrast-enhanced angiography remains the most robust imaging technique. However, it is at the expense of the patient and clinician's health due to prolonged radiation exposure. As an alternative, interventional ultrasound has notable benefits such as being radiation-free, fast to deploy, and having a small footprint in the operating room. Yet, ultrasound is hard to interpret, and highly prone to artifacts and noise. Additionally, interventional radiologists must undergo extensive training before they become qualified to diagnose and treat patients effectively, leading to a shortage of staff, and a lack of open-source datasets. In this work, we seek to address both problems by introducing a self-supervised deep learning architecture to segment catheters in longitudinal ultrasound images, without demanding any labeled data. The network architecture builds upon AiAReSeg, a segmentation transformer built with the Attention in Attention mechanism, and is capable of learning feature changes across time and space. To facilitate training, we used synthetic ultrasound data based on physics-driven catheter insertion simulations, and translated the data into a unique CT-Ultrasound common domain, CACTUSS, to improve the segmentation performance. We generated ground truth segmentation masks by computing the optical flow between adjacent frames using FlowNet2, and performed thresholding to obtain a binary map estimate. Finally, we validated our model on a test dataset, consisting of unseen synthetic data and images collected from silicon aorta phantoms, thus demonstrating its potential for applications to clinical data in the future.

Translated Abstract:
최소 침습 혈관 내 시술에서, 조영제 강화 혈관 조영술이 가장 강력한 영상 기법으로 여겨져. 하지만 이 방법은 환자와 의사 모두에게 방사선 노출로 인한 건강 위험이 커. 

대안으로, 중재 초음파가 방사선이 없고 빠르게 사용할 수 있으며 수술실에서 차지하는 공간이 작다는 장점이 있어. 하지만 초음파는 해석하기 어렵고 아티팩트와 잡음에 취약해. 더군다나, 중재 방사선 의사들은 환자를 효과적으로 진단하고 치료하기 위해 많은 교육을 받아야 해서 인력 부족이 발생하고, 공개 데이터셋도 부족해.

이 연구에서는 이러한 두 가지 문제를 해결하기 위해, 라벨이 없는 데이터로 장기간 초음파 이미지에서 카테터를 분할하는 자기 지도 학습 딥러닝 아키텍처를 도입했어. 이 네트워크는 Attention in Attention 메커니즘으로 구축된 분할 변환기인 AiAReSeg를 기반으로 하고, 시간과 공간에 걸쳐 특징 변화를 학습할 수 있어.

훈련을 쉽게 하기 위해 물리 기반 카테터 삽입 시뮬레이션을 바탕으로 한 합성 초음파 데이터를 사용했고, 이 데이터를 CACTUSS라는 독특한 CT-초음파 공통 영역으로 변환해서 분할 성능을 향상시켰어. 우리는 FlowNet2를 사용해 인접 프레임 간의 광학 흐름을 계산해 실제 분할 마스크를 생성하고, 이진 맵 추정을 위해 임계값 처리를 했어.

마지막으로, 우리는 보지 못한 합성 데이터와 실리콘 대동맥 팬텀에서 수집한 이미지를 포함한 테스트 데이터셋에서 모델을 검증했어. 이를 통해 앞으로 임상 데이터에 응용할 수 있는 가능성을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2405.11133.pdf

Title: XCAT-3.0: A Comprehensive Library of Personalized Digital Twins Derived from CT Scans

Original Abstract:
Virtual Imaging Trials (VIT) offer a cost-effective and scalable approach for evaluating medical imaging technologies. Computational phantoms, which mimic real patient anatomy and physiology, play a central role in VITs. However, the current libraries of computational phantoms face limitations, particularly in terms of sample size and diversity. Insufficient representation of the population hampers accurate assessment of imaging technologies across different patient groups. Traditionally, the more realistic computational phantoms were created by manual segmentation, which is a laborious and time-consuming task, impeding the expansion of phantom libraries. This study presents a framework for creating realistic computational phantoms using a suite of automatic segmentation models and performing three forms of automated quality control on the segmented organ masks. The result is the release of over 2500 new computational phantoms, so-named XCAT3.0 after the ubiquitous XCAT computational construct. This new formation embodies 140 structures and represents a comprehensive approach to detailed anatomical modeling. The developed computational phantoms are formatted in both voxelized and surface mesh formats. The framework is combined with an in-house CT scanner simulator to produce realistic CT images. The framework has the potential to advance virtual imaging trials, facilitating comprehensive and reliable evaluations of medical imaging technologies. Phantoms may be requested at this https URL. Code, model weights, and sample CT images are available at this https URL.

Translated Abstract:
가상 이미징 시험(VIT)은 의료 이미징 기술을 평가하는 데 비용 효율적이고 확장 가능한 방법을 제공해. 컴퓨터 팬텀은 실제 환자의 해부학과 생리학을 모방하며 VIT에서 중요한 역할을 해. 하지만 현재의 컴퓨터 팬텀 라이브러리는 샘플 크기와 다양성 측면에서 한계가 있어. 다양한 환자 그룹을 반영하지 못해 이미징 기술의 정확한 평가를 방해하고 있어.

예전에는 더 현실적인 컴퓨터 팬텀을 만들기 위해 수동으로 분할하는 방식이 사용됐는데, 이건 정말 힘들고 시간이 많이 걸리는 작업이야. 그래서 팬텀 라이브러리를 확장하는 데 어려움이 있었어. 이 연구는 자동 분할 모델을 이용해 현실적인 컴퓨터 팬텀을 만드는 프레임워크를 제시하고, 분할된 장기 마스크에 대해 자동 품질 관리를 세 가지 형태로 수행했어.

그 결과로 2500개 이상의 새로운 컴퓨터 팬텀, 이름하여 XCAT3.0이 탄생했어. 이 팬텀은 140개의 구조를 포함하고 있으며, 상세한 해부학 모델링에 대한 포괄적인 접근법을 나타내. 개발된 컴퓨터 팬텀은 보텍셀화된 형식과 표면 메시 형식으로 제공돼. 이 프레임워크는 내부 CT 스캐너 시뮬레이터와 결합되어 현실적인 CT 이미지를 생성할 수 있어.

이 프레임워크는 가상 이미징 시험을 발전시킬 잠재력이 있으며, 의료 이미징 기술에 대한 포괄적이고 신뢰할 수 있는 평가를 돕는 데 기여할 수 있어. 팬텀은 이 https URL에서 요청할 수 있고, 코드, 모델 가중치, 샘플 CT 이미지는 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2408.16340.pdf

Title: Learned Image Transmission with Hierarchical Variational Autoencoder

Original Abstract:
In this paper, we introduce an innovative hierarchical joint source-channel coding (HJSCC) framework for image transmission, utilizing a hierarchical variational autoencoder (VAE). Our approach leverages a combination of bottom-up and top-down paths at the transmitter to autoregressively generate multiple hierarchical representations of the original image. These representations are then directly mapped to channel symbols for transmission by the JSCC encoder. We extend this framework to scenarios with a feedback link, modeling transmission over a noisy channel as a probabilistic sampling process and deriving a novel generative formulation for JSCC with feedback. Compared with existing approaches, our proposed HJSCC provides enhanced adaptability by dynamically adjusting transmission bandwidth, encoding these representations into varying amounts of channel symbols. Extensive experiments on images of varying resolutions demonstrate that our proposed model outperforms existing baselines in rate-distortion performance and maintains robustness against channel noise. The source code will be made available upon acceptance.

Translated Abstract:
이 논문에서는 이미지 전송을 위한 혁신적인 계층형 공동 소스-채널 코딩(HJSCC) 프레임워크를 소개해. 이 프레임워크는 계층형 변분 오토인코더(VAE)를 활용해. 우리의 접근 방식은 송신기에서 하향식과 상향식 경로를 결합하여 원본 이미지의 여러 계층 표현을 자율적으로 생성하는 거야. 이렇게 생성된 표현은 JSCC 인코더에 의해 채널 기호로 직접 매핑되어 전송돼.

우리는 피드백 링크가 있는 상황으로 이 프레임워크를 확장했어. 이 과정에서 잡음이 있는 채널을 확률적 샘플링 프로세스로 모델링하고, 피드백이 있는 JSCC를 위한 새로운 생성 공식을 도출했어. 기존 방법들과 비교했을 때, 우리가 제안한 HJSCC는 전송 대역폭을 동적으로 조정해서 더 나은 적응성을 제공해. 이 표현들을 다양한 양의 채널 기호로 인코딩할 수 있어.

다양한 해상도의 이미지에 대한 실험 결과, 우리의 모델이 기존 기준선들보다 비율-왜곡 성능에서 더 우수하고 채널 잡음에 대해 강인함을 유지한다는 걸 보여줬어. 소스 코드는 논문이 승인되면 제공할 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.01633.pdf

Title: Dreaming is All You Need

Original Abstract:
In classification tasks, achieving a harmonious balance between exploration and precision is of paramount importance. To this end, this research introduces two novel deep learning models, SleepNet and DreamNet, to strike this balance. SleepNet seamlessly integrates supervised learning with unsupervised ``sleep" stages using pre-trained encoder models. Dedicated neurons within SleepNet are embedded in these unsupervised features, forming intermittent ``sleep" blocks that facilitate exploratory learning. Building upon the foundation of SleepNet, DreamNet employs full encoder-decoder frameworks to reconstruct the hidden states, mimicking the human "dreaming" process. This reconstruction process enables further exploration and refinement of the learned representations. Moreover, the principle ideas of our SleepNet and DreamNet are generic and can be applied to both computer vision and natural language processing downstream tasks. Through extensive empirical evaluations on diverse image and text datasets, SleepNet and DreanNet have demonstrated superior performance compared to state-of-the-art models, showcasing the strengths of unsupervised exploration and supervised precision afforded by our innovative approaches.

Translated Abstract:
분류 작업에서는 탐색과 정확성 사이의 조화를 이루는 게 정말 중요해. 이 연구에서는 SleepNet과 DreamNet이라는 두 가지 새로운 딥러닝 모델을 소개하는데, 이 모델들이 그 균형을 맞추는 데 도움을 줘.

SleepNet은 감독 학습과 비감독 "수면" 단계를 매끄럽게 통합해. 여기서 미리 학습된 인코더 모델을 사용하고, SleepNet 안에 있는 전용 뉴런들은 이 비감독 특징에 포함돼서 간헐적인 "수면" 블록을 형성해. 이렇게 해서 탐색 학습이 가능해져.

SleepNet을 기반으로 한 DreamNet은 전체 인코더-디코더 프레임워크를 사용해서 숨겨진 상태를 재구성해. 이 과정은 사람의 "꿈꾸는" 과정을 흉내내는 거야. 이런 재구성 과정을 통해 배운 표현을 더 탐색하고 다듬을 수 있어.

게다가 SleepNet과 DreamNet의 기본 아이디어는 일반적이라 컴퓨터 비전이나 자연어 처리 같은 다양한 후속 작업에도 적용할 수 있어. 여러 이미지와 텍스트 데이터셋에 대한 광범위한 평가를 통해, SleepNet과 DreamNet은 최신 모델들보다 뛰어난 성능을 보여줬어. 이로써 우리의 혁신적인 접근 방식이 비감독 탐색과 감독 정확성의 장점을 잘 활용할 수 있음을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.02453.pdf

Title: FrameCorr: Adaptive, Autoencoder-based Neural Compression for Video Reconstruction in Resource and Timing Constrained Network Settings

Original Abstract:
Despite the growing adoption of video processing via Internet of Things (IoT) devices due to their cost-effectiveness, transmitting captured data to nearby servers poses challenges due to varying timing constraints and scarcity of network bandwidth. Existing video compression methods face difficulties in recovering compressed data when incomplete data is provided. Here, we introduce FrameCorr, a deep-learning based solution that utilizes previously received data to predict the missing segments of a frame, enabling the reconstruction of a frame from partially received data.

Translated Abstract:
비용 효율성 덕분에 IoT 기기를 통한 영상 처리 사용이 늘어나고 있지만, 촬영한 데이터를 가까운 서버로 전송하는 데는 문제가 있어. 타이밍 제약도 다르고, 네트워크 대역폭도 부족하니까.

기존의 영상 압축 방법들은 불완전한 데이터가 제공될 때 압축된 데이터를 복구하는 데 어려움이 있어. 여기서 우리는 FrameCorr라는 딥러닝 기반 솔루션을 소개해. 이 방법은 이전에 받은 데이터를 활용해서 프레임의 누락된 부분을 예측해, 부분적으로 받은 데이터로부터 프레임을 복원할 수 있게 해줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.02813.pdf

Title: MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark

Original Abstract:
This paper introduces MMMU-Pro, a robust version of the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark. MMMU-Pro rigorously assesses multimodal models' true understanding and reasoning capabilities through a three-step process based on MMMU: (1) filtering out questions answerable by text-only models, (2) augmenting candidate options, and (3) introducing a vision-only input setting where questions are embedded within images. This setting challenges AI to truly "see" and "read" simultaneously, testing a fundamental human cognitive skill of seamlessly integrating visual and textual information. Results show that model performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8% to 26.9% across models. We explore the impact of OCR prompts and Chain of Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT generally improves performance. MMMU-Pro provides a more rigorous evaluation tool, closely mimicking real-world scenarios and offering valuable directions for future research in multimodal AI.

Translated Abstract:
이 논문은 MMMU-Pro를 소개하는데, 이건 Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) 벤치마크의 강력한 버전이야. MMMU-Pro는 멀티모달 모델들이 진짜 이해하고 추론하는 능력을 철저히 평가하는데, 세 단계로 진행돼: (1) 텍스트만으로 답할 수 있는 질문을 걸러내고, (2) 후보 선택지를 보강하고, (3) 질문이 이미지 안에 포함된 비전 전용 입력 설정을 도입해.

이 설정은 AI가 진짜로 "보고" "읽는" 능력을 동시에 테스트해. 이는 시각 정보와 텍스트 정보를 매끄럽게 통합하는 인간의 기본 인지 능력을 평가하는 거야. 결과적으로 MMMU-Pro에서 모델 성능이 MMMU보다 훨씬 낮은 걸 보여줬고, 모델에 따라 16.8%에서 26.9%까지 차이가 나.

우리는 OCR 프롬프트와 Chain of Thought (CoT) 추론의 영향을 살펴봤고, OCR 프롬프트는 큰 영향이 없고 CoT는 일반적으로 성능을 향상시킨다는 걸 발견했어. MMMU-Pro는 더 엄격한 평가 도구를 제공하고, 실제 상황을 잘 모방해서 멀티모달 AI의 미래 연구에 중요한 방향을 제시해.

================================================================================

URL:
https://arxiv.org/pdf/2409.04494.pdf

Title: Diff-INR: Generative Regularization for Electrical Impedance Tomography

Original Abstract:
Electrical Impedance Tomography (EIT) is a non-invasive imaging technique that reconstructs conductivity distributions within a body from boundary measurements. However, EIT reconstruction is hindered by its ill-posed nonlinear inverse problem, which complicates accurate results. To tackle this, we propose Diff-INR, a novel method that combines generative regularization with Implicit Neural Representations (INR) through a diffusion model. Diff-INR introduces geometric priors to guide the reconstruction, effectively addressing the shortcomings of traditional regularization methods. By integrating a pre-trained diffusion regularizer with INR, our approach achieves state-of-the-art reconstruction accuracy in both simulation and experimental data. The method demonstrates robust performance across various mesh densities and hyperparameter settings, highlighting its flexibility and efficiency. This advancement represents a significant improvement in managing the ill-posed nature of EIT. Furthermore, the method's principles are applicable to other imaging modalities facing similar challenges with ill-posed inverse problems.

Translated Abstract:
전기 임피던스 단층촬영(EIT)은 신체의 경도 분포를 경계 측정값으로부터 재구성하는 비침습적 이미지 기술이야. 하지만 EIT 재구성은 비정상적이고 비선형적인 역문제로 인해 정확한 결과를 얻는 게 어려워. 

그래서 우리는 Diff-INR이라는 새로운 방법을 제안해. 이 방법은 생성적 정규화와 암묵적 신경 표현(INR)을 확산 모델을 통해 결합한 거야. Diff-INR은 재구성을 안내하기 위해 기하학적 사전 정보를 도입해서 전통적인 정규화 방법의 단점을 잘 해결해. 

사전 훈련된 확산 정규화기를 INR과 통합함으로써, 우리의 접근 방식은 시뮬레이션과 실험 데이터 모두에서 최첨단 재구성 정확도를 달성해. 이 방법은 다양한 메쉬 밀도와 하이퍼파라미터 설정에서 강력한 성능을 보여주며, 유연성과 효율성을 강조해. 

이 발전은 EIT의 비정상적인 특성을 관리하는 데 큰 개선을 나타내. 게다가 이 방법의 원리는 비슷한 비정상 역문제에 직면한 다른 이미지 모달리티에도 적용할 수 있어.

================================================================================

