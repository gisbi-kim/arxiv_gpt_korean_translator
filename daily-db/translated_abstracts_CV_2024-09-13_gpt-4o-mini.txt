URL:
https://arxiv.org/pdf/2409.07463.pdf

Title: Multi-Modal Instruction-Tuning Small-Scale Language-and-Vision Assistant for Semiconductor Electron Micrograph Analysis

Original Abstract:
We present a novel framework for analyzing and interpreting electron microscopy images in semiconductor manufacturing using vision-language instruction tuning. The framework employs a unique teacher-student approach, leveraging pre-trained multimodal large language models such as GPT-4 to generate instruction-following data for zero-shot visual question answering (VQA) and classification tasks, customizing smaller multimodal models (SMMs) for microscopy image analysis, resulting in an instruction-tuned language-and-vision assistant. Our framework merges knowledge engineering with machine learning to integrate domain-specific expertise from larger to smaller multimodal models within this specialized field, greatly reducing the need for extensive human labeling. Our study presents a secure, cost-effective, and customizable approach for analyzing microscopy images, addressing the challenges of adopting proprietary models in semiconductor manufacturing.

Translated Abstract:
우리는 반도체 제조에서 전자 현미경 이미지를 분석하고 해석하기 위한 새로운 프레임워크를 소개해. 이건 비전-언어 지침 튜닝을 사용해. 

이 프레임워크는 독특한 교사-학생 접근 방식을 사용해. 미리 훈련된 다중 모달 대형 언어 모델인 GPT-4를 활용해서 지침을 따르는 데이터를 생성해. 이 데이터는 제로샷 비주얼 질문 응답(VQA)과 분류 작업에 사용되고, 전자 현미경 이미지 분석을 위해 작은 다중 모달 모델(SMM)을 맞춤 설정해. 그래서 결과적으로 지침이 조정된 언어-비전 도우미가 만들어져.

우리 프레임워크는 지식 공학과 머신러닝을 결합해서 이 전문 분야에서 큰 모델에서 작은 모델로 도메인 전문 지식을 통합해. 이 덕분에 광범위한 인간 레이블링의 필요성이 크게 줄어들어. 

우리 연구는 전자 현미경 이미지를 분석하기 위한 안전하고 비용 효율적이며 맞춤형 접근 방식을 제시해. 이 방법은 반도체 제조에서 독점 모델을 도입할 때의 문제를 해결해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07464.pdf

Title: Reflective Human-Machine Co-adaptation for Enhanced Text-to-Image Generation Dialogue System

Original Abstract:
Today's image generation systems are capable of producing realistic and high-quality images. However, user prompts often contain ambiguities, making it difficult for these systems to interpret users' potential intentions. Consequently, machines need to interact with users multiple rounds to better understand users' intents. The unpredictable costs of using or learning image generation models through multiple feedback interactions hinder their widespread adoption and full performance potential, especially for non-expert users. In this research, we aim to enhance the user-friendliness of our image generation system. To achieve this, we propose a reflective human-machine co-adaptation strategy, named RHM-CAS. Externally, the Agent engages in meaningful language interactions with users to reflect on and refine the generated images. Internally, the Agent tries to optimize the policy based on user preferences, ensuring that the final outcomes closely align with user preferences. Various experiments on different tasks demonstrate the effectiveness of the proposed method.

Translated Abstract:
오늘날 이미지 생성 시스템은 사실적이고 고품질의 이미지를 만들어낼 수 있어. 하지만 사용자 요청이 애매한 경우가 많아서, 시스템이 사용자의 의도를 정확히 파악하기 어려워. 그래서 기계는 사용자와 여러 번 상호작용하면서 사용자의 의도를 더 잘 이해할 필요가 있어.

하지만 이런 여러 번의 피드백 상호작용을 통해 이미지 생성 모델을 사용하는 것의 예측 불가능한 비용 때문에, 비전문가 사용자들이 널리 사용하기가 힘들고, 성능도 충분히 발휘하지 못해. 이 연구에서는 이미지 생성 시스템의 사용자 친화성을 높이는 데 초점을 맞추고 있어. 이를 위해 RHM-CAS라는 반사적 인간-기계 공동 적응 전략을 제안해.

외부적으로는 에이전트가 사용자와 의미 있는 언어 상호작용을 하면서 생성된 이미지를 반영하고 다듬어. 내부적으로는 에이전트가 사용자 선호에 맞춰 정책을 최적화하려고 해, 그래서 최종 결과가 사용자 선호와 잘 맞도록 보장해. 다양한 작업에 대한 실험 결과, 제안한 방법의 효과를 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.07469.pdf

Title: Small Object Detection for Indoor Assistance to the Blind using YOLO NAS Small and Super Gradients

Original Abstract:
Advancements in object detection algorithms have opened new avenues for assistive technologies that cater to the needs of visually impaired individuals. This paper presents a novel approach for indoor assistance to the blind by addressing the challenge of small object detection. We propose a technique YOLO NAS Small architecture, a lightweight and efficient object detection model, optimized using the Super Gradients training framework. This combination enables real-time detection of small objects crucial for assisting the blind in navigating indoor environments, such as furniture, appliances, and household items. Proposed method emphasizes low latency and high accuracy, enabling timely and informative voice-based guidance to enhance the user's spatial awareness and interaction with their surroundings. The paper details the implementation, experimental results, and discusses the system's effectiveness in providing a practical solution for indoor assistance to the visually impaired.

Translated Abstract:
물체 감지 알고리즘의 발전이 시각 장애인을 위한 보조 기술에 새로운 길을 열었어. 이 논문에서는 작은 물체 감지 문제를 해결하면서 실내에서 시각 장애인을 도와주는 새로운 방법을 제안해. 우리는 YOLO NAS Small 아키텍처라는 가볍고 효율적인 물체 감지 모델을 제안하고, 이를 Super Gradients 훈련 프레임워크로 최적화했어. 

이 조합 덕분에 가구, 가전제품, 집안 물건 같은 작은 물체를 실시간으로 감지할 수 있어. 이 방법은 낮은 지연 시간과 높은 정확도를 강조해서, 사용자가 주변 환경을 더 잘 인식하고 상호작용할 수 있도록 즉각적이고 유용한 음성 안내를 제공해. 

논문에서는 구현 방법, 실험 결과, 그리고 시각 장애인을 위한 실내 보조 시스템의 효과에 대해 자세히 설명하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07541.pdf

Title: ENACT: Entropy-based Clustering of Attention Input for Improving the Computational Performance of Object Detection Transformers

Original Abstract:
Transformers demonstrate competitive performance in terms of precision on the problem of vision-based object detection. However, they require considerable computational resources due to the quadratic size of the attention weights. In this work, we propose to cluster the transformer input on the basis of its entropy. The reason for this is that the self-information of each pixel (whose sum is the entropy), is likely to be similar among pixels corresponding to the same objects. Clustering reduces the size of data given as input to the transformer and therefore reduces training time and GPU memory usage, while at the same time preserves meaningful information to be passed through the remaining parts of the network. The proposed process is organized in a module called ENACT, that can be plugged-in any transformer architecture that consists of a multi-head self-attention computation in its encoder. We ran extensive experiments using the COCO object detection dataset, and three detection transformers. The obtained results demonstrate that in all tested cases, there is consistent reduction in the required computational resources, while the precision of the detection task is only slightly reduced. The code of the ENACT module will become available at this https URL

Translated Abstract:
트랜스포머는 비전 기반 물체 탐지 문제에서 정확도 면에서 경쟁력 있는 성능을 보여줘. 하지만, 주의 가중치의 크기가 제곱에 비례하기 때문에 많은 계산 자원을 필요로 해. 그래서 우리는 입력 데이터를 엔트로피를 바탕으로 클러스터링하는 방법을 제안해. 

각 픽셀의 자기 정보(그 합이 엔트로피)가 같은 물체에 해당하는 픽셀들 사이에서 비슷할 거라고 생각했기 때문이야. 클러스터링을 통해 트랜스포머에 입력되는 데이터의 크기를 줄일 수 있고, 이렇게 하면 훈련 시간과 GPU 메모리 사용량이 줄어들어. 동시에 네트워크의 나머지 부분에 전달될 중요한 정보는 유지할 수 있어. 이 과정을 ENACT라는 모듈로 구성했는데, 이 모듈은 멀티헤드 자기 주의 계산이 포함된 어떤 트랜스포머 아키텍처에도 추가할 수 있어.

우리는 COCO 물체 탐지 데이터셋과 세 가지 탐지 트랜스포머를 사용해 많은 실험을 했어. 실험 결과, 모든 테스트된 경우에서 필요한 계산 자원이 일관되게 줄어드는 걸 확인했어. 그리고 탐지 작업의 정확도는 약간만 감소했어. ENACT 모듈의 코드는 이 URL에서 사용할 수 있게 될 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07558.pdf

Title: Unsupervised Point Cloud Registration with Self-Distillation

Original Abstract:
Rigid point cloud registration is a fundamental problem and highly relevant in robotics and autonomous driving. Nowadays deep learning methods can be trained to match a pair of point clouds, given the transformation between them. However, this training is often not scalable due to the high cost of collecting ground truth poses. Therefore, we present a self-distillation approach to learn point cloud registration in an unsupervised fashion. Here, each sample is passed to a teacher network and an augmented view is passed to a student network. The teacher includes a trainable feature extractor and a learning-free robust solver such as RANSAC. The solver forces consistency among correspondences and optimizes for the unsupervised inlier ratio, eliminating the need for ground truth labels. Our approach simplifies the training procedure by removing the need for initial hand-crafted features or consecutive point cloud frames as seen in related methods. We show that our method not only surpasses them on the RGB-D benchmark 3DMatch but also generalizes well to automotive radar, where classical features adopted by others fail. The code is available at this https URL .

Translated Abstract:
강체 포인트 클라우드 정합은 로봇 공학과 자율주행에서 아주 중요한 문제야. 요즘 딥러닝 방법을 사용하면 두 개의 포인트 클라우드를 맞추는 훈련을 할 수 있는데, 이때 두 포인트 클라우드 간의 변환이 필요해. 하지만 실제 위치 정보를 모으는 데 드는 비용 때문에 이 훈련은 쉽게 확장할 수 없어.

그래서 우리는 자체 증류(self-distillation) 방법을 제안해. 이 방법은 포인트 클라우드 정합을 비지도 학습 방식으로 배우는 거야. 여기서 각 샘플은 교사 네트워크로 보내지고, 증강된 뷰는 학생 네트워크로 전달돼. 교사 네트워크에는 학습 가능한 특징 추출기와 RANSAC 같은 비학습 강건 솔버가 포함되어 있어. 이 솔버는 대응 관계의 일관성을 강제하고 비지도 내부 비율을 최적화해서 실제 라벨이 필요 없게 만들어.

우리 방법은 초기 수작업 특징이나 연속적인 포인트 클라우드 프레임이 필요 없어서 훈련 과정을 간소화해. 실험 결과, 우리 방법은 RGB-D 벤치마크인 3DMatch에서 기존 방법들을 능가할 뿐만 아니라, 다른 방법들이 실패하는 자동차 레이더에도 잘 일반화돼. 코드도 이 URL에서 사용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07566.pdf

Title: EchoDFKD: Data-Free Knowledge Distillation for Cardiac Ultrasound Segmentation using Synthetic Data

Original Abstract:
The application of machine learning to medical ultrasound videos of the heart, i.e., echocardiography, has recently gained traction with the availability of large public datasets. Traditional supervised tasks, such as ejection fraction regression, are now making way for approaches focusing more on the latent structure of data distributions, as well as generative methods. We propose a model trained exclusively by knowledge distillation, either on real or synthetical data, involving retrieving masks suggested by a teacher model. We achieve state-of-the-art (SOTA) values on the task of identifying end-diastolic and end-systolic frames. By training the model only on synthetic data, it reaches segmentation capabilities close to the performance when trained on real data with a significantly reduced number of weights. A comparison with the 5 main existing methods shows that our method outperforms the others in most cases. We also present a new evaluation method that does not require human annotation and instead relies on a large auxiliary model. We show that this method produces scores consistent with those obtained from human annotations. Relying on the integrated knowledge from a vast amount of records, this method overcomes certain inherent limitations of human annotator labeling. Code: this https URL

Translated Abstract:
최근 공공 데이터셋이 많아지면서, 심장 초음파 영상에 머신러닝을 적용하는 연구가 활발해지고 있어. 전통적인 감독 학습 작업인 박출 분율 회귀 같은 방법 대신, 데이터 분포의 잠재 구조나 생성적 방법에 더 집중하는 접근 방식이 등장하고 있어.

우리는 선생님 모델이 제안한 마스크를 활용해 실제 데이터나 합성 데이터에 대해 지식 증류만으로 훈련된 모델을 제안해. 이 모델은 최첨단 성과를 내며, 심장 이완기와 수축기 프레임을 식별하는 작업에서 좋은 결과를 보여. 합성 데이터만으로 모델을 훈련했지만, 실제 데이터로 훈련한 것과 비슷한 세그멘테이션 능력을 보이고, 필요한 매개변수의 수는 크게 줄어들었어.

기존의 5개 주요 방법과 비교해봤을 때, 우리의 방법이 대부분의 경우에서 더 나은 성과를 내는 걸 확인했어. 또한, 인간 주석이 필요 없는 새로운 평가 방법도 제안했는데, 대신 큰 보조 모델에 의존해. 이 방법이 인간 주석으로 얻은 점수와 일치하는 결과를 만든다는 걸 보여줬어. 방대한 기록에서 통합된 지식을 바탕으로, 이 방법은 인간 주석의 한계를 극복할 수 있어. 

코드: 이 URL 확인해봐.

================================================================================

URL:
https://arxiv.org/pdf/2409.07571.pdf

Title: FaVoR: Features via Voxel Rendering for Camera Relocalization

Original Abstract:
Camera relocalization methods range from dense image alignment to direct camera pose regression from a query image. Among these, sparse feature matching stands out as an efficient, versatile, and generally lightweight approach with numerous applications. However, feature-based methods often struggle with significant viewpoint and appearance changes, leading to matching failures and inaccurate pose estimates. To overcome this limitation, we propose a novel approach that leverages a globally sparse yet locally dense 3D representation of 2D features. By tracking and triangulating landmarks over a sequence of frames, we construct a sparse voxel map optimized to render image patch descriptors observed during tracking. Given an initial pose estimate, we first synthesize descriptors from the voxels using volumetric rendering and then perform feature matching to estimate the camera pose. This methodology enables the generation of descriptors for unseen views, enhancing robustness to view changes. We extensively evaluate our method on the 7-Scenes and Cambridge Landmarks datasets. Our results show that our method significantly outperforms existing state-of-the-art feature representation techniques in indoor environments, achieving up to a 39% improvement in median translation error. Additionally, our approach yields comparable results to other methods for outdoor scenarios while maintaining lower memory and computational costs.

Translated Abstract:
카메라 재위치 추정 방법에는 밀집 이미지 정렬부터 쿼리 이미지에서 카메라 자세를 직접 추정하는 방법까지 다양해. 그중에서도 희소 특징 매칭은 효율적이고 다용도로 쓰일 수 있는 가벼운 접근법으로 주목받고 있어. 하지만 특징 기반 방법은 시점이나 외관이 크게 바뀔 때 종종 실패하고, 자세 추정이 부정확해지는 문제가 있어.

이런 한계를 극복하기 위해 우리는 새로운 접근법을 제안해. 이 방법은 전 세계적으로는 희소하고 지역적으로는 밀집한 3D 표현을 이용해서 2D 특징을 다뤄. 여러 프레임에 걸쳐 랜드마크를 추적하고 삼각 측량을 해서, 추적 중에 관찰된 이미지 패치 설명자를 렌더링하는 데 최적화된 희소 복셀 맵을 만들어. 초기 자세 추정치를 바탕으로 복셀에서 볼륨 렌더링을 통해 설명자를 합성한 후, 특징 매칭을 통해 카메라 자세를 추정해.

이 방법 덕분에 보지 못한 시점에 대한 설명자를 생성할 수 있어서, 시점 변화에 대한 강인성이 향상돼. 우리는 7-Scenes와 Cambridge Landmarks 데이터셋에서 이 방법을 광범위하게 평가했어. 결과적으로, 우리의 방법이 실내 환경에서 기존의 최첨단 특징 표현 기술보다 상당히 뛰어난 성능을 보였고, 중앙 이동 오류에서 최대 39% 개선을 이뤘어. 게다가, 우리의 접근법은 야외 상황에서도 다른 방법들과 유사한 결과를 내면서 메모리와 계산 비용은 더 낮게 유지할 수 있었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07577.pdf

Title: Self-Masking Networks for Unsupervised Adaptation

Original Abstract:
With the advent of billion-parameter foundation models, efficient fine-tuning has become increasingly important for the adaptation of models to downstream tasks. However, especially in computer vision, it can be hard to achieve good performance when access to quality labeled data is lacking. In this work, we propose a method adapting pretrained generalist models in a self-supervised manner by learning binary masks. These self-supervised masking networks (SMNs) are up to 79x more efficient to store and significantly improve performance on label-efficient downstream tasks. We validate the usefulness of learning binary masks as a fine-tuning method on 8 datasets and 3 model architectures, and we demonstrate the effectiveness of SMNs in 3 label-efficient settings.

Translated Abstract:
억 단위 파라미터를 가진 기초 모델들이 등장하면서, 모델을 하위 작업에 맞게 잘 조정하는 게 점점 더 중요해졌어. 하지만 특히 컴퓨터 비전 분야에서는 품질 좋은 레이블 데이터가 부족할 때 좋은 성능을 내기 어려워.

이 연구에서는 미리 학습된 일반 모델을 자기 지도 학습 방식으로 조정하는 방법을 제안해. 이 방법은 이진 마스크를 학습하는 방식이야. 자기 지도 마스킹 네트워크(SMNs)는 저장 공간 면에서 최대 79배 더 효율적이고, 레이블 효율적인 하위 작업에서 성능을 크게 향상시켜.

우리는 8개의 데이터셋과 3개의 모델 아키텍처에서 이진 마스크를 학습하는 방법이 유용하다는 걸 검증했고, 3개의 레이블 효율적인 설정에서 SMNs의 효과를 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07581.pdf

Title: Violence detection in videos using deep recurrent and convolutional neural networks

Original Abstract:
Violence and abnormal behavior detection research have known an increase of interest in recent years, due mainly to a rise in crimes in large cities worldwide. In this work, we propose a deep learning architecture for violence detection which combines both recurrent neural networks (RNNs) and 2-dimensional convolutional neural networks (2D CNN). In addition to video frames, we use optical flow computed using the captured sequences. CNN extracts spatial characteristics in each frame, while RNN extracts temporal characteristics. The use of optical flow allows to encode the movements in the scenes. The proposed approaches reach the same level as the state-of-the-art techniques and sometime surpass them. It was validated on 3 databases achieving good results.

Translated Abstract:
최근 몇 년 동안 폭력과 비정상 행동 감지 연구에 대한 관심이 증가했어. 주로 전 세계 대도시에서 범죄가 늘어난 덕분이야. 

이번 연구에서는 폭력 감지를 위한 딥러닝 구조를 제안하는데, 이 구조는 순환 신경망(RNN)과 2차원 합성곱 신경망(2D CNN)을 결합해. 우리는 비디오 프레임뿐만 아니라 촬영된 시퀀스를 이용해 계산한 광학 흐름도 사용해. 

CNN은 각 프레임에서 공간적인 특징을 추출하고, RNN은 시간적인 특징을 추출해. 광학 흐름을 사용하면 장면의 움직임을 인코딩할 수 있어. 

제안한 방법들은 최신 기술들과 비슷한 성능을 내고, 때때로 그것들을 초월하기도 해. 3개의 데이터베이스에서 검증했는데, 좋은 결과를 얻었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07582.pdf

Title: Minimizing Embedding Distortion for Robust Out-of-Distribution Performance

Original Abstract:
Foundational models, trained on vast and diverse datasets, have demonstrated remarkable capabilities in generalizing across different domains and distributions for various zero-shot tasks. Our work addresses the challenge of retaining these powerful generalization capabilities when adapting foundational models to specific downstream tasks through fine-tuning. To this end, we introduce a novel approach we call "similarity loss", which can be incorporated into the fine-tuning process of any task. By minimizing the distortion of fine-tuned embeddings from the pre-trained embeddings, our method strikes a balance between task-specific adaptation and preserving broad generalization abilities. We evaluate our approach on two diverse tasks: image classification on satellite imagery and face recognition, focusing on open-class and domain shift scenarios to assess out-of-distribution (OOD) performance. We demonstrate that this approach significantly improves OOD performance while maintaining strong in-distribution (ID) performance.

Translated Abstract:
기초 모델은 방대한 데이터셋을 바탕으로 훈련돼 다양한 분야와 분포에서 제로샷 작업을 잘 수행할 수 있는 능력을 보여줬어. 우리의 연구는 이러한 강력한 일반화 능력을 특정 작업에 맞게 조정할 때 어떻게 유지할 수 있을지를 다뤘어.

우리는 "유사성 손실"이라고 부르는 새로운 접근 방식을 도입했어. 이건 어떤 작업의 파인튜닝 과정에도 적용할 수 있어. 우리가 제안하는 방법은 미리 훈련된 임베딩과 파인튜닝된 임베딩 간의 왜곡을 최소화함으로써, 특정 작업에 맞게 조정하는 것과 넓은 일반화 능력을 유지하는 것의 균형을 잡아줘.

우리는 이 방법을 두 가지 다양한 작업에서 평가했어: 위성 이미지의 이미지 분류와 얼굴 인식이야. 여기서 우리는 열린 클래스와 도메인 변화 시나리오에 초점을 맞춰서 분포 밖(OOD) 성능을 평가했어. 결과적으로, 이 접근 방식이 OOD 성능을 크게 향상시키면서도 강력한 분포 내(ID) 성능을 유지한다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07588.pdf

Title: 2D bidirectional gated recurrent unit convolutional Neural networks for end-to-end violence detection In videos

Original Abstract:
Abnormal behavior detection, action recognition, fight and violence detection in videos is an area that has attracted a lot of interest in recent years. In this work, we propose an architecture that combines a Bidirectional Gated Recurrent Unit (BiGRU) and a 2D Convolutional Neural Network (CNN) to detect violence in video sequences. A CNN is used to extract spatial characteristics from each frame, while the BiGRU extracts temporal and local motion characteristics using CNN extracted features from multiple frames. The proposed end-to-end deep learning network is tested in three public datasets with varying scene complexities. The proposed network achieves accuracies up to 98%. The obtained results are promising and show the performance of the proposed end-to-end approach.

Translated Abstract:
비정상 행동 탐지와 액션 인식, 싸움 및 폭력 탐지가 비디오에서 요즘 많이 연구되고 있어. 이번 연구에서는 Bidirectional Gated Recurrent Unit (BiGRU)와 2D Convolutional Neural Network (CNN)을 결합한 아키텍처를 제안해. 이걸로 비디오 시퀀스에서 폭력을 탐지할 수 있어.

CNN은 각 프레임에서 공간적 특성을 추출하고, BiGRU는 여러 프레임에서 CNN이 추출한 특징을 사용해 시간적이고 지역적인 움직임 특성을 뽑아내. 제안한 이 딥러닝 네트워크는 다양한 장면 복잡성을 가진 세 가지 공개 데이터셋에서 테스트했어. 결과적으로, 이 네트워크는 최대 98%의 정확도를 보여줬어. 얻은 결과는 매우 유망하고, 제안한 엔드 투 엔드 방식의 성능을 잘 나타내고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07613.pdf

Title: Token Turing Machines are Efficient Vision Models

Original Abstract:
We propose Vision Token Turing Machines (ViTTM), an efficient, low-latency, memory-augmented Vision Transformer (ViT). Our approach builds on Neural Turing Machines and Token Turing Machines, which were applied to NLP and sequential visual understanding tasks. ViTTMs are designed for non-sequential computer vision tasks such as image classification and segmentation. Our model creates two sets of tokens: process tokens and memory tokens; process tokens pass through encoder blocks and read-write from memory tokens at each encoder block in the network, allowing them to store and retrieve information from memory. By ensuring that there are fewer process tokens than memory tokens, we are able to reduce the inference time of the network while maintaining its accuracy. On ImageNet-1K, the state-of-the-art ViT-B has median latency of 529.5ms and 81.0% accuracy, while our ViTTM-B is 56% faster (234.1ms), with 2.4 times fewer FLOPs, with an accuracy of 82.9%. On ADE20K semantic segmentation, ViT-B achieves 45.65mIoU at 13.8 frame-per-second (FPS) whereas our ViTTM-B model acheives a 45.17 mIoU with 26.8 FPS (+94%).

Translated Abstract:
우리는 Vision Token Turing Machines (ViTTM)을 제안해. 이건 효율적이고 지연 시간이 짧은 메모리 보강 비전 트랜스포머(ViT)야. 우리의 접근 방식은 Neural Turing Machines와 Token Turing Machines를 기반으로 하고, 이들은 NLP와 순차적인 시각 이해 작업에 사용됐어. 

ViTTM은 이미지 분류나 분할 같은 비순차적인 컴퓨터 비전 작업을 위해 설계됐어. 우리 모델은 두 종류의 토큰을 만들어: 프로세스 토큰과 메모리 토큰. 프로세스 토큰은 인코더 블록을 통과하면서 각 인코더 블록에서 메모리 토큰과 읽고 쓰기를 하면서 정보를 저장하고 불러올 수 있어. 프로세스 토큰의 수를 메모리 토큰보다 적게 유지함으로써, 네트워크의 추론 시간을 줄이면서도 정확도를 유지할 수 있어. 

ImageNet-1K에서, 최첨단 ViT-B는 중간 지연 시간이 529.5ms이고 81.0%의 정확도를 기록해. 반면 우리 ViTTM-B는 56% 더 빠른 234.1ms로, FLOP 수는 2.4배 적으면서도 정확도는 82.9%야. ADE20K 의미 분할에서는 ViT-B가 13.8 프레임-퍼-세컨드(FPS)에서 45.65mIoU를 달성한 반면, 우리의 ViTTM-B 모델은 26.8 FPS에서 45.17 mIoU를 달성했어(+94%).

================================================================================

URL:
https://arxiv.org/pdf/2409.07645.pdf

Title: Feature Importance in Pedestrian Intention Prediction: A Context-Aware Review

Original Abstract:
Recent advancements in predicting pedestrian crossing intentions for Autonomous Vehicles using Computer Vision and Deep Neural Networks are promising. However, the black-box nature of DNNs poses challenges in understanding how the model works and how input features contribute to final predictions. This lack of interpretability delimits the trust in model performance and hinders informed decisions on feature selection, representation, and model optimisation; thereby affecting the efficacy of future research in the field. To address this, we introduce Context-aware Permutation Feature Importance (CAPFI), a novel approach tailored for pedestrian intention prediction. CAPFI enables more interpretability and reliable assessments of feature importance by leveraging subdivided scenario contexts, mitigating the randomness of feature values through targeted shuffling. This aims to reduce variance and prevent biased estimations in importance scores during permutations. We divide the Pedestrian Intention Estimation (PIE) dataset into 16 comparable context sets, measure the baseline performance of five distinct neural network architectures for intention prediction in each context, and assess input feature importance using CAPFI. We observed nuanced differences among models across various contextual characteristics. The research reveals the critical role of pedestrian bounding boxes and ego-vehicle speed in predicting pedestrian intentions, and potential prediction biases due to the speed feature through cross-context permutation evaluation. We propose an alternative feature representation by considering proximity change rate for rendering dynamic pedestrian-vehicle locomotion, thereby enhancing the contributions of input features to intention prediction. These findings underscore the importance of contextual features and their diversity to develop accurate and robust intent-predictive models.

Translated Abstract:
최근 자율주행차의 보행자 횡단 의도를 예측하는 데 있어 컴퓨터 비전과 딥 뉴럴 네트워크(DNN)의 발전이 기대를 모으고 있어. 하지만 DNN의 블랙박스 특성 때문에 모델이 어떻게 작동하는지, 입력 특성이 최종 예측에 어떻게 기여하는지 이해하기가 어려워. 이런 해석 가능성 부족은 모델 성능에 대한 신뢰를 제한하고, 특성 선택, 표현, 모델 최적화에 대한 정보에 기반한 결정을 방해해서 앞으로의 연구 효율성에 영향을 미칠 수 있어.

이 문제를 해결하기 위해 우리는 Context-aware Permutation Feature Importance (CAPFI)라는 새로운 접근 방식을 소개해. CAPFI는 세분화된 상황 맥락을 활용해서 특성 중요성에 대한 해석 가능성과 신뢰성을 높이는데 도움을 줘. 이를 통해 특성 값의 무작위성을 줄이고, 중요도 점수의 편향된 추정을 방지할 수 있어. 우리는 보행자 의도 추정을 위한 Pedestrian Intention Estimation (PIE) 데이터셋을 16개의 비교 가능한 맥락 세트로 나누고, 각 맥락에서 의도 예측을 위한 다섯 가지 다른 신경망 아키텍처의 기본 성능을 측정한 후, CAPFI를 사용해 입력 특성의 중요성을 평가했어.

모델 간에 다양한 맥락적 특성에서 미세한 차이를 관찰했어. 이 연구는 보행자 바운딩 박스와 자차 속도가 보행자 의도를 예측하는 데 중요한 역할을 한다는 것을 보여주고, 속도 특성으로 인한 잠재적인 예측 편향도 발견했어. 그래서 우리는 동적인 보행자-차량 운동을 표현하기 위해 근접 변화율을 고려한 대안적 특성 표현을 제안해, 입력 특성이 의도 예측에 기여하는 방식을 강화하고자 해. 이러한 발견은 맥락적 특성과 그 다양성이 정확하고 견고한 의도 예측 모델을 개발하는 데 얼마나 중요한지를 강조해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07649.pdf

Title: DiffTED: One-shot Audio-driven TED Talk Video Generation with Diffusion-based Co-speech Gestures

Original Abstract:
Audio-driven talking video generation has advanced significantly, but existing methods often depend on video-to-video translation techniques and traditional generative networks like GANs and they typically generate taking heads and co-speech gestures separately, leading to less coherent outputs. Furthermore, the gestures produced by these methods often appear overly smooth or subdued, lacking in diversity, and many gesture-centric approaches do not integrate talking head generation. To address these limitations, we introduce DiffTED, a new approach for one-shot audio-driven TED-style talking video generation from a single image. Specifically, we leverage a diffusion model to generate sequences of keypoints for a Thin-Plate Spline motion model, precisely controlling the avatar's animation while ensuring temporally coherent and diverse gestures. This innovative approach utilizes classifier-free guidance, empowering the gestures to flow naturally with the audio input without relying on pre-trained classifiers. Experiments demonstrate that DiffTED generates temporally coherent talking videos with diverse co-speech gestures.

Translated Abstract:
오디오 기반의 말하는 비디오 생성 기술이 많이 발전했지만, 기존 방법들은 주로 비디오-비디오 변환 기술이나 GAN 같은 전통적인 생성 네트워크에 의존해. 이로 인해 말하는 얼굴과 함께하는 제스처가 따로 생성되면서 출력물이 덜 일관되게 나와. 게다가 이런 방법들이 만들어내는 제스처는 너무 부드럽거나 억제된 느낌이 들어서 다양성이 부족해. 많은 제스처 중심 접근법은 말하는 얼굴 생성과 통합되지 않기도 해.

이런 한계를 해결하기 위해 우리는 DiffTED라는 새로운 접근 방식을 소개해. 이건 단일 이미지에서 오디오 기반의 TED 스타일 말하는 비디오를 한 번의 시도로 생성하는 방법이야. 구체적으로, 우리는 확산 모델을 활용해서 Thin-Plate Spline 모션 모델의 주요 포인트 시퀀스를 생성해. 이렇게 하면 아바타의 애니메이션을 정확하게 제어하면서도 일관되고 다양한 제스처를 만들어낼 수 있어. 이 혁신적인 접근 방식은 분류기 없는 가이드를 활용해 제스처가 오디오 입력과 자연스럽게 흐르도록 해, 미리 훈련된 분류기에 의존하지 않거든.

실험 결과, DiffTED는 시간적으로 일관된 말하는 비디오를 생성하면서 다양한 말하는 제스처를 만들어낸다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07650.pdf

Title: Foundation Models Boost Low-Level Perceptual Similarity Metrics

Original Abstract:
For full-reference image quality assessment (FR-IQA) using deep-learning approaches, the perceptual similarity score between a distorted image and a reference image is typically computed as a distance measure between features extracted from a pretrained CNN or more recently, a Transformer network. Often, these intermediate features require further fine-tuning or processing with additional neural network layers to align the final similarity scores with human judgments. So far, most IQA models based on foundation models have primarily relied on the final layer or the embedding for the quality score estimation. In contrast, this work explores the potential of utilizing the intermediate features of these foundation models, which have largely been unexplored so far in the design of low-level perceptual similarity metrics. We demonstrate that the intermediate features are comparatively more effective. Moreover, without requiring any training, these metrics can outperform both traditional and state-of-the-art learned metrics by utilizing distance measures between the features.

Translated Abstract:
딥러닝 접근법을 이용한 전체 참조 이미지 품질 평가(FR-IQA)에서는 왜곡된 이미지와 참조 이미지 간의 지각적 유사성 점수를 보통 사전 훈련된 CNN이나 최근에는 Transformer 네트워크에서 추출한 특징 간의 거리로 계산해. 대개 이러한 중간 특징들은 최종 유사성 점수를 사람의 판단과 맞추기 위해 추가적인 신경망 층으로 미세 조정하거나 처리해야 해.

지금까지 대부분의 IQA 모델은 품질 점수를 추정할 때 주로 최종 레이어나 임베딩에 의존했어. 반면, 이 연구는 이러한 기초 모델의 중간 특징을 활용하는 가능성을 탐구해. 사실, 중간 특징은 아직 저수준 지각 유사성 메트릭 설계에서 크게 탐구되지 않았어. 우리는 중간 특징이 상대적으로 더 효과적이라는 걸 보여줘.

게다가, 훈련 없이도 이러한 메트릭은 특징 간의 거리 측정을 활용해서 전통적인 방법이나 최신 학습된 메트릭보다 더 나은 성능을 낼 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07683.pdf

Title: Open-Vocabulary Remote Sensing Image Semantic Segmentation

Original Abstract:
Open-vocabulary image semantic segmentation (OVS) seeks to segment images into semantic regions across an open set of categories. Existing OVS methods commonly depend on foundational vision-language models and utilize similarity computation to tackle OVS tasks. However, these approaches are predominantly tailored to natural images and struggle with the unique characteristics of remote sensing images, such as rapidly changing orientations and significant scale variations. These challenges complicate OVS tasks in earth vision, requiring specialized approaches. To tackle this dilemma, we propose the first OVS framework specifically designed for remote sensing imagery, drawing inspiration from the distinct remote sensing traits. Particularly, to address the varying orientations, we introduce a rotation-aggregative similarity computation module that generates orientation-adaptive similarity maps as initial semantic maps. These maps are subsequently refined at both spatial and categorical levels to produce more accurate semantic maps. Additionally, to manage significant scale changes, we integrate multi-scale image features into the upsampling process, resulting in the final scale-aware semantic masks. To advance OVS in earth vision and encourage reproducible research, we establish the first open-sourced OVS benchmark for remote sensing imagery, including four public remote sensing datasets. Extensive experiments on this benchmark demonstrate our proposed method achieves state-of-the-art performance. All codes and datasets are available at this https URL.

Translated Abstract:
오픈 어휘 이미지 의미 분할(OVS)은 이미지를 열린 카테고리 세트에 따라 의미 있는 영역으로 나누는 걸 목표로 해. 기존의 OVS 방법들은 주로 기본적인 비전-언어 모델에 의존하고, 유사도 계산을 사용해서 OVS 작업을 처리해. 하지만 이런 접근법은 자연 이미지에 맞춰져 있어서, 원거리 감지 이미지의 독특한 특성, 예를 들어 빠르게 변하는 방향이나 큰 크기 변화에선 잘 작동하지 않아. 이런 문제로 인해 지구 비전에서 OVS 작업이 복잡해지고, 특별한 접근이 필요해.

이 문제를 해결하기 위해, 우리는 원거리 감지 이미지를 위해 특별히 설계된 첫 번째 OVS 프레임워크를 제안해. 이 프레임워크는 원거리 감지의 독특한 특성에서 영감을 얻었어. 특히 방향이 다르게 변하는 문제를 해결하기 위해, 우리는 회전 집계 유사도 계산 모듈을 도입했어. 이 모듈은 방향에 적응한 유사도 맵을 생성해서 초기 의미 맵으로 사용해. 그런 다음 이 맵들은 공간적이고 범주적인 수준에서 정제돼서 더 정확한 의미 맵으로 만들어져.

또한, 큰 크기 변화를 관리하기 위해, 우리는 업샘플링 과정에 다중 스케일 이미지 특징을 통합했어. 이렇게 해서 최종적으로 크기 인식이 가능한 의미 마스크를 얻게 돼. 지구 비전에서 OVS를 발전시키고 재현 가능한 연구를 장려하기 위해, 우리는 네 개의 공개 원거리 감지 데이터셋을 포함한 첫 번째 오픈 소스 OVS 벤치마크를 구축했어. 이 벤치마크에 대한 광범위한 실험 결과, 제안한 방법이 최첨단 성능을 달성한 걸 보여줬어. 모든 코드와 데이터셋은 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07694.pdf

Title: Learn from Balance: Rectifying Knowledge Transfer for Long-Tailed Scenarios

Original Abstract:
Knowledge Distillation (KD) transfers knowledge from a large pre-trained teacher network to a compact and efficient student network, making it suitable for deployment on resource-limited media terminals. However, traditional KD methods require balanced data to ensure robust training, which is often unavailable in practical applications. In such scenarios, a few head categories occupy a substantial proportion of examples. This imbalance biases the trained teacher network towards the head categories, resulting in severe performance degradation on the less represented tail categories for both the teacher and student networks. In this paper, we propose a novel framework called Knowledge Rectification Distillation (KRDistill) to address the imbalanced knowledge inherited in the teacher network through the incorporation of the balanced category priors. Furthermore, we rectify the biased predictions produced by the teacher network, particularly focusing on the tail categories. Consequently, the teacher network can provide balanced and accurate knowledge to train a reliable student network. Intensive experiments conducted on various long-tailed datasets demonstrate that our KRDistill can effectively train reliable student networks in realistic scenarios of data imbalance.

Translated Abstract:
지식 증류(Knowledge Distillation, KD)는 큰 사전 훈련된 교사 네트워크의 지식을 작고 효율적인 학생 네트워크로 옮기는 방법이야. 이 방식은 자원이 제한된 미디어 단말기에 배포하기에 적합해. 하지만 전통적인 KD 방법은 강력한 훈련을 위해 균형 잡힌 데이터가 필요해. 하지만 실제 상황에서는 이런 데이터가 잘 안 나와. 

이런 경우, 몇몇 주요 카테고리가 많은 비율을 차지하게 돼. 이 균형이 맞지 않으면, 훈련된 교사 네트워크가 주요 카테고리에 치우치게 되고, 덜 대표되는 꼬리 카테고리에서는 성능이 많이 떨어져. 이 논문에서는 이러한 문제를 해결하기 위해 지식 수정 증류(Knowledge Rectification Distillation, KRDistill)라는 새로운 프레임워크를 제안해. 이 방법은 균형 잡힌 카테고리 우선순위를 도입해서 교사 네트워크에서 물려받은 불균형한 지식을 해결해.

또한, 우리는 교사 네트워크가 내는 편향된 예측을 수정하는데 집중해, 특히 꼬리 카테고리에 중점을 두고 있어. 그래서 교사 네트워크는 신뢰할 수 있는 학생 네트워크를 훈련하기 위해 균형 잡힌 정확한 지식을 제공할 수 있어. 다양한 긴 꼬리 데이터셋에 대한 집중적인 실험을 통해, 우리의 KRDistill이 데이터 불균형의 현실적인 상황에서도 신뢰할 수 있는 학생 네트워크를 효과적으로 훈련할 수 있음을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07701.pdf

Title: TMFNet: Two-Stream Multi-Channels Fusion Networks for Color Image Operation Chain Detection

Original Abstract:
Image operation chain detection techniques have gained increasing attention recently in the field of multimedia forensics. However, existing detection methods suffer from the generalization problem. Moreover, the channel correlation of color images that provides additional forensic evidence is often ignored. To solve these issues, in this article, we propose a novel two-stream multi-channels fusion networks for color image operation chain detection in which the spatial artifact stream and the noise residual stream are explored in a complementary manner. Specifically, we first propose a novel deep residual architecture without pooling in the spatial artifact stream for learning the global features representation of multi-channel correlation. Then, a set of filters is designed to aggregate the correlation information of multi-channels while capturing the low-level features in the noise residual stream. Subsequently, the high-level features are extracted by the deep residual model. Finally, features from the two streams are fed into a fusion module, to effectively learn richer discriminative representations of the operation chain. Extensive experiments show that the proposed method achieves state-of-the-art generalization ability while maintaining robustness to JPEG compression. The source code used in these experiments will be released at this https URL.

Translated Abstract:
최근 멀티미디어 포렌식 분야에서 이미지 작업 체인 탐지 기술이 주목받고 있어. 하지만 기존의 탐지 방법은 일반화 문제를 겪고 있어. 게다가 색상 이미지의 채널 상관관계는 추가적인 포렌식 증거를 제공하지만, 대개 무시되고 있어.

이 문제를 해결하기 위해, 이 논문에서는 색상 이미지 작업 체인 탐지를 위한 새로운 이중 스트림 다중 채널 융합 네트워크를 제안해. 여기서 공간 아티팩트 스트림과 노이즈 잔여 스트림이 보완적으로 탐색돼. 구체적으로, 우리는 먼저 공간 아티팩트 스트림에서 풀링 없이 새로운 딥 잔여 아키텍처를 제안해 다중 채널 상관관계의 글로벌 특징을 학습해.

그 다음, 노이즈 잔여 스트림에서 저수준 특징을 포착하면서 다중 채널의 상관관계 정보를 집합할 필터 세트를 설계했어. 이후, 딥 잔여 모델을 통해 고수준 특징을 추출해. 마지막으로, 두 스트림에서 추출한 특징을 융합 모듈에 넣어서 작업 체인의 더 풍부한 구별 표현을 효과적으로 학습해.

광범위한 실험 결과, 제안한 방법이 최신 일반화 능력을 달성하면서 JPEG 압축에 대해 강인성을 유지하는 것으로 나타났어. 이 실험에 사용된 소스 코드는 이 URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07714.pdf

Title: CollaMamba: Efficient Collaborative Perception with Cross-Agent Spatial-Temporal State Space Model

Original Abstract:
By sharing complementary perceptual information, multi-agent collaborative perception fosters a deeper understanding of the environment. Recent studies on collaborative perception mostly utilize CNNs or Transformers to learn feature representation and fusion in the spatial dimension, which struggle to handle long-range spatial-temporal features under limited computing and communication resources. Holistically modeling the dependencies over extensive spatial areas and extended temporal frames is crucial to enhancing feature quality. To this end, we propose a resource efficient cross-agent spatial-temporal collaborative state space model (SSM), named CollaMamba. Initially, we construct a foundational backbone network based on spatial SSM. This backbone adeptly captures positional causal dependencies from both single-agent and cross-agent views, yielding compact and comprehensive intermediate features while maintaining linear complexity. Furthermore, we devise a history-aware feature boosting module based on temporal SSM, extracting contextual cues from extended historical frames to refine vague features while preserving low overhead. Extensive experiments across several datasets demonstrate that CollaMamba outperforms state-of-the-art methods, achieving higher model accuracy while reducing computational and communication overhead by up to 71.9% and 1/64, respectively. This work pioneers the exploration of the Mamba's potential in collaborative perception. The source code will be made available.

Translated Abstract:
상호 보완적인 지각 정보를 공유함으로써, 다중 에이전트 협업 지각은 환경에 대한 더 깊은 이해를 촉진해. 최근 협업 지각에 대한 연구들은 주로 CNN이나 트랜스포머를 사용해서 공간 차원에서 특징을 학습하고 융합하는데, 이 방법들은 제한된 컴퓨팅 및 통신 자원 아래에서 장거리 공간-시간 특징을 처리하는 데 어려움을 겪고 있어. 넓은 공간 영역과 긴 시간 프레임에 걸쳐 의존성을 전체적으로 모델링하는 것이 특징 품질을 향상시키는 데 중요해.

이를 위해, 우리는 자원 효율적인 교차 에이전트 공간-시간 협업 상태 공간 모델(SSM)인 CollaMamba를 제안해. 처음에, 우리는 공간 SSM을 기반으로 하는 기본 백본 네트워크를 구축해. 이 백본은 단일 에이전트와 교차 에이전트의 관점에서 위치적 인과 의존성을 잘 포착해서, 간결하고 포괄적인 중간 특징을 생성하면서 선형 복잡성을 유지해.

또한, 우리는 시간 SSM을 기반으로 한 역사 인식 특징 향상 모듈을 개발해. 이 모듈은 확장된 역사적 프레임에서 맥락적인 단서를 추출해 애매한 특징을 정제하면서도 낮은 오버헤드를 유지해. 여러 데이터셋에 대한 광범위한 실험을 통해 CollaMamba가 최신 방법들보다 뛰어난 성능을 보이며, 모델 정확도를 높이면서 계산 및 통신 오버헤드를 각각 71.9%와 1/64까지 줄인다는 것을 보여줬어.

이 연구는 협업 지각에서 Mamba의 잠재력을 탐구하는 첫걸음이야. 소스 코드는 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07715.pdf

Title: FIReStereo: Forest InfraRed Stereo Dataset for UAS Depth Perception in Visually Degraded Environments

Original Abstract:
Robust depth perception in visually-degraded environments is crucial for autonomous aerial systems. Thermal imaging cameras, which capture infrared radiation, are robust to visual degradation. However, due to lack of a large-scale dataset, the use of thermal cameras for unmanned aerial system (UAS) depth perception has remained largely unexplored. This paper presents a stereo thermal depth perception dataset for autonomous aerial perception applications. The dataset consists of stereo thermal images, LiDAR, IMU and ground truth depth maps captured in urban and forest settings under diverse conditions like day, night, rain, and smoke. We benchmark representative stereo depth estimation algorithms, offering insights into their performance in degraded conditions. Models trained on our dataset generalize well to unseen smoky conditions, highlighting the robustness of stereo thermal imaging for depth perception. We aim for this work to enhance robotic perception in disaster scenarios, allowing for exploration and operations in previously unreachable areas. The dataset and source code are available at this https URL.

Translated Abstract:
비주얼 환경이 좋지 않은 상황에서 깊이를 정확하게 인식하는 건 자율 비행 시스템에 정말 중요해. 열화상 카메라는 적외선 방사선을 잡아내서 이런 환경에서도 잘 작동해. 하지만 대규모 데이터셋이 부족해서 무인 항공 시스템(UAS)에서 열화상 카메라를 깊이 인식하는 데 사용하는 건 거의 연구되지 않았어.

이 논문에서는 자율 비행 인식 애플리케이션을 위한 스테레오 열 깊이 인식 데이터셋을 소개해. 이 데이터셋은 도심과 숲에서 다양한 조건(낮, 밤, 비, 연기 등)에서 촬영한 스테레오 열 이미지, LiDAR, IMU, 그리고 실제 깊이 맵으로 구성되어 있어.

우리는 대표적인 스테레오 깊이 추정 알고리즘을 평가해서, 안 좋은 조건에서의 성능을 보여줄 거야. 우리 데이터셋으로 훈련된 모델은 보지 못한 연기 나는 상황에서도 잘 작동해, 스테레오 열화상이 깊이 인식에 정말 강력하다는 걸 보여줘. 

이 연구가 재난 상황에서 로봇 인식을 개선해서, 이전에는 접근할 수 없었던 지역을 탐색하고 작동할 수 있도록 도와주길 바래. 데이터셋과 소스 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07723.pdf

Title: Advancing Depth Anything Model for Unsupervised Monocular Depth Estimation in Endoscopy

Original Abstract:
Depth estimation is a cornerstone of 3D reconstruction and plays a vital role in minimally invasive endoscopic surgeries. However, most current depth estimation networks rely on traditional convolutional neural networks, which are limited in their ability to capture global information. Foundation models offer a promising avenue for enhancing depth estimation, but those currently available are primarily trained on natural images, leading to suboptimal performance when applied to endoscopic images. In this work, we introduce a novel fine-tuning strategy for the Depth Anything Model and integrate it with an intrinsic-based unsupervised monocular depth estimation framework. Our approach includes a low-rank adaptation technique based on random vectors, which improves the model's adaptability to different scales. Additionally, we propose a residual block built on depthwise separable convolution to compensate for the transformer's limited ability to capture high-frequency details, such as edges and textures. Our experimental results on the SCARED dataset show that our method achieves state-of-the-art performance while minimizing the number of trainable parameters. Applying this method in minimally invasive endoscopic surgery could significantly enhance both the precision and safety of these procedures.

Translated Abstract:
깊이 추정은 3D 재구성의 핵심이고 최소 침습 내시경 수술에서 중요한 역할을 해. 하지만 현재 대부분의 깊이 추정 네트워크는 전통적인 합성곱 신경망에 의존하고 있어. 이런 네트워크들은 전역 정보를 잘 포착하는 데 한계가 있어. 

기초 모델들은 깊이 추정을 개선할 수 있는 좋은 방법이지만, 지금까지 나온 모델들은 주로 자연 이미지로 학습되어서 내시경 이미지에 적용할 때 성능이 떨어져. 이번 연구에서는 Depth Anything Model을 위한 새로운 미세 조정 전략을 소개하고, 이를 내재 기반 비지도 단안 깊이 추정 프레임워크와 통합했어. 

우리의 접근법은 랜덤 벡터를 기반으로 한 저순위 적응 기법을 포함하고 있어서 모델이 다양한 스케일에 잘 적응할 수 있게 해. 그리고 깊이별 분리형 합성곱을 기반으로 한 잔여 블록을 제안해서 변환기가 고주파 세부사항, 예를 들어 가장자리나 질감을 포착하는 데 한계를 보완하고 있어. 

SCARED 데이터셋에서 실험한 결과, 우리의 방법이 최신 기술을 뛰어넘는 성능을 보여주면서 학습 가능한 매개변수의 수를 최소화했어. 이 방법을 최소 침습 내시경 수술에 적용하면 이 절차의 정확성과 안전성을 크게 향상시킬 수 있을 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07736.pdf

Title: Transfer Learning Applied to Computer Vision Problems: Survey on Current Progress, Limitations, and Opportunities

Original Abstract:
The field of Computer Vision (CV) has faced challenges. Initially, it relied on handcrafted features and rule-based algorithms, resulting in limited accuracy. The introduction of machine learning (ML) has brought progress, particularly Transfer Learning (TL), which addresses various CV problems by reusing pre-trained models. TL requires less data and computing while delivering nearly equal accuracy, making it a prominent technique in the CV landscape. Our research focuses on TL development and how CV applications use it to solve real-world problems. We discuss recent developments, limitations, and opportunities.

Translated Abstract:
컴퓨터 비전(CV) 분야는 몇 가지 어려움을 겪어왔어. 처음에는 수작업으로 만든 특징과 규칙 기반 알고리즘에 의존했기 때문에 정확도가 제한적이었어. 

하지만 머신러닝(ML)의 도입으로 많은 발전이 있었고, 특히 전이 학습(TL)이 다양한 CV 문제를 해결하는 데 큰 역할을 하고 있어. TL은 미리 학습된 모델을 재사용함으로써 적은 데이터와 컴퓨팅 자원으로 거의 같은 정확도를 제공해. 그래서 CV 분야에서 아주 중요한 기술이 되었지.

우리 연구는 TL의 개발과 CV 애플리케이션이 이를 활용해 실제 문제를 어떻게 해결하는지에 초점을 맞추고 있어. 최근 발전, 한계, 기회에 대해서도 이야기할 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07746.pdf

Title: Learning Brain Tumor Representation in 3D High-Resolution MR Images via Interpretable State Space Models

Original Abstract:
Learning meaningful and interpretable representations from high-dimensional volumetric magnetic resonance (MR) images is essential for advancing personalized medicine. While Vision Transformers (ViTs) have shown promise in handling image data, their application to 3D multi-contrast MR images faces challenges due to computational complexity and interpretability. To address this, we propose a novel state-space-model (SSM)-based masked autoencoder which scales ViT-like models to handle high-resolution data effectively while also enhancing the interpretability of learned representations. We propose a latent-to-spatial mapping technique that enables direct visualization of how latent features correspond to specific regions in the input volumes in the context of SSM. We validate our method on two key neuro-oncology tasks: identification of isocitrate dehydrogenase mutation status and 1p/19q co-deletion classification, achieving state-of-the-art accuracy. Our results highlight the potential of SSM-based self-supervised learning to transform radiomics analysis by combining efficiency and interpretability.

Translated Abstract:
고차원 볼류메트릭 자기공명(MR) 이미지에서 의미 있고 해석 가능한 표현을 배우는 것은 개인화된 의학을 발전시키는 데 중요해. 비전 트랜스포머(ViT)는 이미지 데이터 처리에서 좋은 성과를 보였지만, 3D 멀티-콘트라스트 MR 이미지에 적용할 때 계산 복잡성과 해석 가능성 때문에 어려움이 있어. 

이 문제를 해결하기 위해, 우리는 새로운 상태공간모델(SSM) 기반의 마스크 오토인코더를 제안해. 이 모델은 ViT와 비슷한 구조를 사용하면서도 고해상도 데이터를 효과적으로 처리할 수 있게 해. 또, 배운 표현을 더 잘 해석할 수 있도록 도와줘. 우리는 잠재 공간을 공간으로 매핑하는 기법을 제안했는데, 이게 SSM의 맥락에서 잠재 특징이 입력 볼륨의 특정 영역과 어떻게 연결되는지를 직접 시각화할 수 있게 해.

우리는 이 방법을 두 가지 주요 신경종양 작업에 적용해봤어: 이소시트르산 탈수소효소 변이 상태 식별과 1p/19q 공동 결실 분류. 그 결과, 최첨단 정확도를 달성했어. 우리의 결과는 SSM 기반의 자기 지도 학습이 효율성과 해석 가능성을 결합해 방사선학 분석을 변화시킬 수 있는 가능성을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.07747.pdf

Title: Multi-object event graph representation learning for Video Question Answering

Original Abstract:
Video question answering (VideoQA) is a task to predict the correct answer to questions posed about a given video. The system must comprehend spatial and temporal relationships among objects extracted from videos to perform causal and temporal reasoning. While prior works have focused on modeling individual object movements using transformer-based methods, they falter when capturing complex scenarios involving multiple objects (e.g., "a boy is throwing a ball in a hoop"). We propose a contrastive language event graph representation learning method called CLanG to address this limitation. Aiming to capture event representations associated with multiple objects, our method employs a multi-layer GNN-cluster module for adversarial graph representation learning, enabling contrastive learning between the question text and its relevant multi-object event graph. Our method outperforms a strong baseline, achieving up to 2.2% higher accuracy on two challenging VideoQA datasets, NExT-QA and TGIF-QA-R. In particular, it is 2.8% better than baselines in handling causal and temporal questions, highlighting its strength in reasoning multiple object-based events.

Translated Abstract:
비디오 질문 응답(VideoQA)은 주어진 비디오에 대해 제기된 질문에 대한 올바른 답을 예측하는 작업이야. 이 시스템은 비디오에서 추출된 객체들 간의 공간적이고 시간적인 관계를 이해해야 원인과 시간 추론을 할 수 있어. 이전 연구들은 주로 개별 객체의 움직임을 모델링하는 데 초점을 맞췄는데, 여러 객체가 관련된 복잡한 상황에서는 잘 작동하지 않아. 예를 들어 "소년이 공을 후프에 던지고 있다" 같은 상황이 그렇지.

우리는 이 한계를 해결하기 위해 CLanG라는 대조적 언어 사건 그래프 표현 학습 방법을 제안해. 이 방법은 여러 객체와 관련된 사건 표현을 포착하는 데 중점을 두고, 적대적 그래프 표현 학습을 위해 다층 GNN 클러스터 모듈을 사용해. 질문 텍스트와 관련된 다중 객체 사건 그래프 간의 대조 학습이 가능하게 해.

우리 방법은 강력한 기준선보다 성능이 뛰어나서, NExT-QA와 TGIF-QA-R 같은 두 개의 도전적인 VideoQA 데이터셋에서 최대 2.2% 더 높은 정확도를 기록했어. 특히, 원인과 시간 관련 질문을 처리하는 데 있어서는 기준선보다 2.8% 더 나은 성능을 보여 줘. 이건 여러 객체 기반 사건에 대한 추론에서의 강점을 잘 나타내.

================================================================================

URL:
https://arxiv.org/pdf/2409.07748.pdf

Title: Top-down Activity Representation Learning for Video Question Answering

Original Abstract:
Capturing complex hierarchical human activities, from atomic actions (e.g., picking up one present, moving to the sofa, unwrapping the present) to contextual events (e.g., celebrating Christmas) is crucial for achieving high-performance video question answering (VideoQA). Recent works have expanded multimodal models (e.g., CLIP, LLaVA) to process continuous video sequences, enhancing the model's temporal reasoning capabilities. However, these approaches often fail to capture contextual events that can be decomposed into multiple atomic actions non-continuously distributed over relatively long-term sequences. In this paper, to leverage the spatial visual context representation capability of the CLIP model for obtaining non-continuous visual representations in terms of contextual events in videos, we convert long-term video sequences into a spatial image domain and finetune the multimodal model LLaVA for the VideoQA task. Our approach achieves competitive performance on the STAR task, in particular, with a 78.4% accuracy score, exceeding the current state-of-the-art score by 2.8 points on the NExTQA task.

Translated Abstract:
복잡한 계층적 인간 활동을 잡아내는 건 중요해. 여기서 원자 행동(예: 선물 하나 집기, 소파로 이동하기, 선물 포장 뜯기)부터 맥락적 사건(예: 크리스마스 축하하기)까지 포함돼. 이런 걸 잘 분석해야 비디오 질문 응답(VideoQA)에서 좋은 성과를 낼 수 있어.

최근 연구들은 CLIP이나 LLaVA 같은 멀티모달 모델을 사용해서 연속적인 비디오 시퀀스를 처리하는 방법을 발전시켰어. 이로 인해 모델의 시간적 추론 능력이 좋아졌지. 하지만 이런 접근법은 상대적으로 긴 시퀀스에 걸쳐 여러 원자 행동으로 나눌 수 있는 맥락적 사건을 잘 포착하지 못하는 경우가 많아.

이 논문에서는 CLIP 모델의 공간적 시각 맥락 표현 능력을 활용해서, 비디오에서 맥락적 사건에 대한 비연속적인 시각 표현을 얻으려고 해. 그래서 긴 비디오 시퀀스를 공간 이미지 도메인으로 변환하고, 비디오 질문 응답 작업을 위해 멀티모달 모델 LLaVA를 미세 조정했어. 우리 방법은 STAR 작업에서 경쟁력 있는 성과를 내고, 특히 NExTQA 작업에서 78.4%의 정확도를 기록해 현재 최고 성과보다 2.8 포인트 높은 결과를 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07752.pdf

Title: GatedUniPose: A Novel Approach for Pose Estimation Combining UniRepLKNet and Gated Convolution

Original Abstract:
Pose estimation is a crucial task in computer vision, with wide applications in autonomous driving, human motion capture, and virtual reality. However, existing methods still face challenges in achieving high accuracy, particularly in complex scenes. This paper proposes a novel pose estimation method, GatedUniPose, which combines UniRepLKNet and Gated Convolution and introduces the GLACE module for embedding. Additionally, we enhance the feature map concatenation method in the head layer by using DySample upsampling. Compared to existing methods, GatedUniPose excels in handling complex scenes and occlusion challenges. Experimental results on the COCO, MPII, and CrowdPose datasets demonstrate that GatedUniPose achieves significant performance improvements with a relatively small number of parameters, yielding better or comparable results to models with similar or larger parameter sizes.

Translated Abstract:
포즈 추정은 컴퓨터 비전에서 중요한 작업인데, 자율주행, 인간 동작 캡처, 가상 현실 등 다양한 분야에 쓰여. 하지만 기존 방법들은 복잡한 장면에서 높은 정확도를 얻는 데 여전히 어려움이 있어. 

이 논문에서는 GatedUniPose라는 새로운 포즈 추정 방법을 제안해. 이 방법은 UniRepLKNet과 Gated Convolution을 결합하고 GLACE 모듈을 도입해서 임베딩을 개선해. 또한, DySample 업샘플링을 사용해서 헤드 레이어의 특징 맵 결합 방식을 강화했어. 

GatedUniPose는 기존 방법들보다 복잡한 장면과 가림 문제를 잘 처리해. COCO, MPII, CrowdPose 데이터셋에서 실험한 결과, GatedUniPose는 상대적으로 적은 수의 파라미터로도 상당한 성능 향상을 보여주었고, 비슷하거나 더 많은 파라미터를 가진 모델들과 비교했을 때 더 나은 성능이나 비슷한 성능을 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07756.pdf

Title: DiTAS: Quantizing Diffusion Transformers via Enhanced Activation Smoothing

Original Abstract:
Diffusion Transformers (DiTs) have recently attracted significant interest from both industry and academia due to their enhanced capabilities in visual generation, surpassing the performance of traditional diffusion models that employ U-Net. However, the improved performance of DiTs comes at the expense of higher parameter counts and implementation costs, which significantly limits their deployment on resource-constrained devices like mobile phones. We propose DiTAS, a data-free post-training quantization (PTQ) method for efficient DiT inference. DiTAS relies on the proposed temporal-aggregated smoothing techniques to mitigate the impact of the channel-wise outliers within the input activations, leading to much lower quantization error under extremely low bitwidth. To further enhance the performance of the quantized DiT, we adopt the layer-wise grid search strategy to optimize the smoothing factor. Experimental results demonstrate that our approach enables 4-bit weight, 8-bit activation (W4A8) quantization for DiTs while maintaining comparable performance as the full-precision model.

Translated Abstract:
Diffusion Transformers (DiTs)는 최근 산업과 학계에서 큰 관심을 받고 있어. 이 모델들은 시각 생성에서 뛰어난 성능을 보여주고, 전통적인 U-Net을 사용하는 확산 모델들을 능가하고 있어. 하지만 DiTs의 성능이 좋아진 대신, 더 많은 파라미터와 높은 구현 비용이 필요해. 이 때문에 자원이 제한된 기기, 예를 들어 스마트폰 같은 곳에 배포하기가 어려워.

우리는 DiTAS라는 데이터가 필요 없는 후처리 양자화(PTQ) 방법을 제안해. DiTAS는 입력 활성화에서 채널별 이상치의 영향을 줄이기 위해 제안된 시간 집계 스무딩 기법을 사용해. 이 덕분에 매우 낮은 비트 너비에서도 양자화 오류를 많이 줄일 수 있어. 

또한, 양자화된 DiT의 성능을 더 높이기 위해 레이어별 그리드 탐색 전략을 사용해서 스무딩 팩터를 최적화해. 실험 결과, 우리의 방법은 4비트 가중치와 8비트 활성화(W4A8) 양자화를 가능하게 하면서도 전체 정밀 모델과 비슷한 성능을 유지할 수 있다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.07757.pdf

Title: From Uncertainty to Clarity: Uncertainty-Guided Class-Incremental Learning for Limited Biomedical Samples via Semantic Expansion

Original Abstract:
In real-world clinical settings, data distributions evolve over time, with a continuous influx of new, limited disease cases. Therefore, class incremental learning is of great significance, i.e., deep learning models are required to learn new class knowledge while maintaining accurate recognition of previous diseases. However, traditional deep neural networks often suffer from severe forgetting of prior knowledge when adapting to new data unless trained from scratch, which undesirably costs much time and computational burden. Additionally, the sample sizes for different diseases can be highly imbalanced, with newly emerging diseases typically having much fewer instances, consequently causing the classification bias. To tackle these challenges, we are the first to propose a class-incremental learning method under limited samples in the biomedical field. First, we propose a novel cumulative entropy prediction module to measure the uncertainty of the samples, of which the most uncertain samples are stored in a memory bank as exemplars for the model's later review. Furthermore, we theoretically demonstrate its effectiveness in measuring uncertainty. Second, we developed a fine-grained semantic expansion module through various augmentations, leading to more compact distributions within the feature space and creating sufficient room for generalization to new classes. Besides, a cosine classifier is utilized to mitigate classification bias caused by imbalanced datasets. Across four imbalanced data distributions over two datasets, our method achieves optimal performance, surpassing state-of-the-art methods by as much as 53.54% in accuracy.

Translated Abstract:
실제 임상 환경에서 데이터 분포는 시간이 지남에 따라 변화하고, 새로운 제한된 질병 사례가 지속적으로 발생해. 그래서 클래스 증분 학습이 정말 중요해. 즉, 딥러닝 모델은 새로운 클래스 지식을 배우면서 이전 질병에 대한 정확한 인식을 유지해야 해. 하지만 전통적인 딥 뉴럴 네트워크는 새로운 데이터에 적응할 때 이전 지식을 심하게 잊어버리는 문제가 있어. 새로 시작하지 않는 한, 이 과정은 시간과 계산 자원을 많이 소모해.

게다가, 다양한 질병의 샘플 크기가 매우 불균형할 수 있어. 새로 발생하는 질병은 보통 사례가 훨씬 적어서 분류 편향이 생기게 되지. 이런 문제들을 해결하기 위해, 우리는 생물 의학 분야에서 제한된 샘플로 클래스 증분 학습 방법을 처음으로 제안해.

먼저, 새로운 샘플의 불확실성을 측정하기 위해 새로운 누적 엔트로피 예측 모듈을 제안해. 가장 불확실한 샘플들을 메모리 뱅크에 저장해서 나중에 모델이 다시 검토할 수 있게 해. 그리고 이 불확실성을 측정하는 데 효과적이라는 걸 이론적으로 입증했어.

두 번째로, 다양한 증강 기법을 통해 세분화된 의미 확장 모듈을 개발했어. 이 덕분에 특징 공간 내에서 더 밀집된 분포를 만들고 새로운 클래스에 대한 일반화를 위한 충분한 공간을 만들어 줘. 또한, 불균형 데이터셋으로 인한 분류 편향을 줄이기 위해 코사인 분류기를 사용했어.

두 개의 데이터셋에서 네 가지 불균형 데이터 분포를 대상으로 시험해본 결과, 우리의 방법이 최적의 성능을 보여줬고, 최신 방법들보다 정확도가 최대 53.54% 더 높았어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07762.pdf

Title: Exploring Kolmogorov-Arnold networks for realistic image sharpness assessment

Original Abstract:
Score prediction is crucial in realistic image sharpness assessment after informative features are collected. Recently, Kolmogorov-Arnold networks (KANs) have been developed and witnessed remarkable success in data fitting. This study presents Taylor series based KAN (TaylorKAN). Then, different KANs are explored on four realistic image databases (BID2011, CID2013, CLIVE, and KonIQ-10k) for score prediction by using 15 mid-level features and 2048 high-level features. When setting support vector regression as the baseline, experimental results indicate KANs are generally better or competitive, TaylorKAN is the best on three databases using mid-level feature input, while KANs are inferior on CLIVE when high-level features are used. This is the first study that explores KANs for image quality assessment. It sheds lights on how to select and improve KANs on related tasks.

Translated Abstract:
점수 예측은 유용한 특징들이 수집된 후에 현실적인 이미지 선명도 평가에서 매우 중요해. 최근에 Kolmogorov-Arnold 네트워크(KANs)가 개발되었고, 데이터 적합성에서 놀라운 성공을 거두었어. 이 연구에서는 Taylor 급수를 기반으로 한 KAN(TaylorKAN)을 소개해. 

그리고 15개의 중간 수준 특징과 2048개의 고수준 특징을 사용해 네 가지 현실적인 이미지 데이터베이스(BID2011, CID2013, CLIVE, KonIQ-10k)에서 다양한 KAN들을 점수 예측을 위해 탐색했어. 서포트 벡터 회귀를 기준선으로 설정했을 때, 실험 결과 KAN들이 일반적으로 더 좋거나 경쟁력이 있다는 걸 보여줘. TaylorKAN은 중간 수준 특징 입력을 사용할 때 세 개의 데이터베이스에서 가장 성능이 좋았고, 반면에 KAN들은 CLIVE 데이터베이스에서 고수준 특징을 사용할 때는 성능이 떨어졌어. 

이 연구는 이미지 품질 평가를 위해 KAN을 탐색한 최초의 연구야. KAN을 관련 작업에서 선택하고 개선하는 방법에 대한 통찰을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07779.pdf

Title: ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation

Original Abstract:
Medical image segmentation, a crucial task in computer vision, facilitates the automated delineation of anatomical structures and pathologies, supporting clinicians in diagnosis, treatment planning, and disease monitoring. Notably, transformers employing shifted window-based self-attention have demonstrated exceptional performance. However, their reliance on local window attention limits the fusion of local and global contextual information, crucial for segmenting microtumors and miniature organs. To address this limitation, we propose the Adaptive Semantic Segmentation Network (ASSNet), a transformer architecture that effectively integrates local and global features for precise medical image segmentation. ASSNet comprises a transformer-based U-shaped encoder-decoder network. The encoder utilizes shifted window self-attention across five resolutions to extract multi-scale features, which are then propagated to the decoder through skip connections. We introduce an augmented multi-layer perceptron within the encoder to explicitly model long-range dependencies during feature extraction. Recognizing the constraints of conventional symmetrical encoder-decoder designs, we propose an Adaptive Feature Fusion (AFF) decoder to complement our encoder. This decoder incorporates three key components: the Long Range Dependencies (LRD) block, the Multi-Scale Feature Fusion (MFF) block, and the Adaptive Semantic Center (ASC) block. These components synergistically facilitate the effective fusion of multi-scale features extracted by the decoder while capturing long-range dependencies and refining object boundaries. Comprehensive experiments on diverse medical image segmentation tasks, including multi-organ, liver tumor, and bladder tumor segmentation, demonstrate that ASSNet achieves state-of-the-art results. Code and models are available at: \url{this https URL}.

Translated Abstract:
의료 이미지 분할은 컴퓨터 비전에서 중요한 작업으로, 해부학적 구조와 병리학을 자동으로 구분해 의사들이 진단, 치료 계획, 질병 모니터링에 도움을 줘. 특히, 이동 윈도우 기반의 셀프 어텐션을 사용하는 트랜스포머가 뛰어난 성능을 보였어. 하지만, 로컬 윈도우 어텐션에 의존하다 보니 지역적 정보와 전체적 맥락 정보를 잘 결합하지 못하는 문제가 있어. 이건 미세 종양이나 작은 장기를 분할하는 데 중요한 부분이야.

이 문제를 해결하기 위해 우리는 Adaptive Semantic Segmentation Network (ASSNet)를 제안해. ASSNet은 로컬과 글로벌 특징을 효과적으로 통합해서 정확한 의료 이미지 분할을 할 수 있는 트랜스포머 아키텍처야. ASSNet은 U자 형태의 인코더-디코더 네트워크로 구성되어 있어. 인코더는 다섯 개의 해상도에서 이동 윈도우 셀프 어텐션을 사용해 다중 스케일 특징을 추출하고, 이 특징은 스킵 연결을 통해 디코더로 전달돼.

우리는 인코더 내에 증강된 다층 퍼셉트론을 추가해서 특징 추출 시 장거리 의존성을 명확히 모델링해. 전통적인 대칭 인코더-디코더 설계의 한계를 인식하고, 인코더를 보완하기 위해 Adaptive Feature Fusion (AFF) 디코더를 제안해. 이 디코더는 세 가지 주요 컴포넌트를 포함해: Long Range Dependencies (LRD) 블록, Multi-Scale Feature Fusion (MFF) 블록, 그리고 Adaptive Semantic Center (ASC) 블록. 이 컴포넌트들은 디코더가 추출한 다중 스케일 특징을 효과적으로 결합하면서 장거리 의존성을 포착하고 객체 경계를 정제하는 데 도움을 줘.

다양한 의료 이미지 분할 작업, 즉 다중 장기, 간 종양, 방광 종양 분할에 대한 포괄적인 실험 결과, ASSNet이 최첨단 결과를 달성했음을 보여줘. 코드와 모델은 여기에서 확인할 수 있어: \url{this https URL}.

================================================================================

URL:
https://arxiv.org/pdf/2409.07793.pdf

Title: Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation

Original Abstract:
Medical image segmentation, a critical application of semantic segmentation in healthcare, has seen significant advancements through specialized computer vision techniques. While deep learning-based medical image segmentation is essential for assisting in medical diagnosis, the lack of diverse training data causes the long-tail problem. Moreover, most previous hybrid CNN-ViT architectures have limited ability to combine various attentions in different layers of the Convolutional Neural Network. To address these issues, we propose a Lagrange Duality Consistency (LDC) Loss, integrated with Boundary-Aware Contrastive Loss, as the overall training objective for semi-supervised learning to mitigate the long-tail problem. Additionally, we introduce CMAformer, a novel network that synergizes the strengths of ResUNet and Transformer. The cross-attention block in CMAformer effectively integrates spatial attention and channel attention for multi-scale feature fusion. Overall, our results indicate that CMAformer, combined with the feature fusion framework and the new consistency loss, demonstrates strong complementarity in semi-supervised learning ensembles. We achieve state-of-the-art results on multiple public medical image datasets. Example code are available at: \url{this https URL}.

Translated Abstract:
의료 이미지 세분화는 헬스케어에서 의미론적 세분화의 중요한 응용 분야로, 특별한 컴퓨터 비전 기술 덕분에 많이 발전했어. 딥러닝 기반의 의료 이미지 세분화는 의료 진단에 도움을 주는 데 필수적이지만, 다양한 훈련 데이터가 부족해서 긴 꼬리 문제(long-tail problem)가 생기고 있어. 게다가, 이전의 하이브리드 CNN-ViT 아키텍처는 컨볼루션 신경망의 다양한 층에서 다양한 주의(attention)를 결합하는 데 한계가 있어.

이 문제를 해결하기 위해, 우리는 긴 꼬리 문제를 완화하기 위한 반지도 학습의 전체 훈련 목표로 경계 인식 대조 손실(Boundary-Aware Contrastive Loss)과 통합된 라그랑주 이중성 일관성(Lagrange Duality Consistency, LDC) 손실을 제안해. 그리고 ResUNet과 Transformer의 장점을 합친 새로운 네트워크, CMAformer를 소개해. CMAformer의 크로스-어텐션 블록은 다중 스케일 특징 융합을 위해 공간적 주의와 채널 주의를 효과적으로 통합해.

전반적으로, 우리의 결과는 CMAformer가 특징 융합 프레임워크와 새로운 일관성 손실과 결합했을 때 반지도 학습 앙상블에서 강한 상호 보완성을 보인다는 것을 보여줘. 우리는 여러 공개 의료 이미지 데이터셋에서 최첨단 결과를 달성했어. 예제 코드는 이 링크에서 확인할 수 있어: \url{this https URL}.

================================================================================

URL:
https://arxiv.org/pdf/2409.07796.pdf

Title: In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for Efficient Adaptation

Original Abstract:
Wildlife monitoring via camera traps has become an essential tool in ecology, but the deployment of machine learning models for on-device animal classification faces significant challenges due to domain shifts and resource constraints. This paper introduces WildFit, a novel approach that reconciles the conflicting goals of achieving high domain generalization performance and ensuring efficient inference for camera trap applications. WildFit leverages continuous background-aware model fine-tuning to deploy ML models tailored to the current location and time window, allowing it to maintain robust classification accuracy in the new environment without requiring significant computational resources. This is achieved by background-aware data synthesis, which generates training images representing the new domain by blending background images with animal images from the source domain. We further enhance fine-tuning effectiveness through background drift detection and class distribution drift detection, which optimize the quality of synthesized data and improve generalization performance. Our extensive evaluation across multiple camera trap datasets demonstrates that WildFit achieves significant improvements in classification accuracy and computational efficiency compared to traditional approaches.

Translated Abstract:
카메라 트랩을 이용한 야생동물 모니터링은 생태학에서 중요한 도구가 되고 있지만, 기계 학습 모델을 동물 분류에 적용하는 데는 여러 가지 어려움이 있어. 특히 도메인 변화와 자원 제약 때문에 문제야. 

이 논문에서는 WildFit이라는 새로운 접근 방식을 소개해. WildFit은 높은 도메인 일반화 성능을 달성하면서 카메라 트랩에 효율적으로 적용할 수 있도록 해. 이 방법은 현재의 위치와 시간에 맞춰 ML 모델을 조정하는 지속적인 배경 인식 모델 미세 조정을 이용해. 이렇게 하면 새로운 환경에서도 분류 정확도를 유지하면서 많은 계산 자원을 필요로 하지 않아.

이건 배경 인식 데이터 합성을 통해 이루어져. 즉, 새로운 도메인을 나타내는 훈련 이미지를 생성하기 위해 배경 이미지와 원본 도메인의 동물 이미지를 섞어. 또한 배경 변화 감지와 클래스 분포 변화 감지를 통해 미세 조정 효과를 더욱 향상시켜. 이렇게 해서 합성된 데이터의 품질을 최적화하고 일반화 성능을 높일 수 있어.

우리가 여러 카메라 트랩 데이터셋에서 광범위하게 평가한 결과, WildFit이 전통적인 접근 방식에 비해 분류 정확도와 계산 효율성에서 큰 개선을 이뤘다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.07797.pdf

Title: Quaternion Nuclear Norm minus Frobenius Norm Minimization for color image reconstruction

Original Abstract:
Color image restoration methods typically represent images as vectors in Euclidean space or combinations of three monochrome channels. However, they often overlook the correlation between these channels, leading to color distortion and artifacts in the reconstructed image. To address this, we present Quaternion Nuclear Norm Minus Frobenius Norm Minimization (QNMF), a novel approach for color image reconstruction. QNMF utilizes quaternion algebra to capture the relationships among RGB channels comprehensively. By employing a regularization technique that involves nuclear norm minus Frobenius norm, QNMF approximates the underlying low-rank structure of quaternion-encoded color images. Theoretical proofs are provided to ensure the method's mathematical integrity. Demonstrating versatility and efficacy, the QNMF regularizer excels in various color low-level vision tasks, including denoising, deblurring, inpainting, and random impulse noise removal, achieving state-of-the-art results.

Translated Abstract:
색상 이미지 복원 방법은 보통 이미지를 유클리드 공간의 벡터나 세 개의 단색 채널의 조합으로 표현해. 하지만 이 방법들은 종종 채널 간의 상관관계를 무시해서 복원된 이미지에 색상 왜곡이나 아티팩트가 생기곤 해. 

이를 해결하기 위해 우리는 쿼터니언 핵 노름에서 프로베니우스 노름을 뺀 최소화(QNMF)라는 새로운 접근 방식을 제안해. QNMF는 쿼터니언 대수를 활용해서 RGB 채널 간의 관계를 종합적으로 파악해. 핵 노름에서 프로베니우스 노름을 빼는 정규화 기법을 사용해서, QNMF는 쿼터니언으로 인코딩된 색상 이미지의 기본적인 저랭크 구조를 근사하게 해. 이 방법의 수학적 정합성을 보장하기 위해 이론적인 증명도 제공해.

QNMF 정규화기는 다양한 색상 저수준 비전 작업에서 뛰어난 성능을 보여줘. 노이즈 제거, 블러 제거, 결함 보완, 랜덤 임펄스 노이즈 제거 같은 작업에서 최첨단 결과를 달성하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07798.pdf

Title: GateAttentionPose: Enhancing Pose Estimation with Agent Attention and Improved Gated Convolutions

Original Abstract:
This paper introduces GateAttentionPose, an innovative approach that enhances the UniRepLKNet architecture for pose estimation tasks. We present two key contributions: the Agent Attention module and the Gate-Enhanced Feedforward Block (GEFB). The Agent Attention module replaces large kernel convolutions, significantly improving computational efficiency while preserving global context modeling. The GEFB augments feature extraction and processing capabilities, particularly in complex scenes. Extensive evaluations on COCO and MPII datasets demonstrate that GateAttentionPose outperforms existing state-of-the-art methods, including the original UniRepLKNet, achieving superior or comparable results with improved efficiency. Our approach offers a robust solution for pose estimation across diverse applications, including autonomous driving, human motion capture, and virtual reality.

Translated Abstract:
이 논문에서는 포즈 추정 작업을 위해 UniRepLKNet 아키텍처를 개선한 GateAttentionPose라는 새로운 방법을 소개해. 두 가지 주요 기여를 제시하는데, 하나는 Agent Attention 모듈이고 다른 하나는 Gate-Enhanced Feedforward Block (GEFB)이야.

Agent Attention 모듈은 큰 커널 합성을 대체해서 계산 효율성을 크게 높이면서도 전반적인 맥락 모델링을 유지해. GEFB는 복잡한 장면에서 특징 추출과 처리 능력을 강화해. COCO와 MPII 데이터셋에서의 광범위한 평가 결과, GateAttentionPose는 기존의 최신 방법들, 특히 원래의 UniRepLKNet보다 더 나은 성능을 보여줬어. 효율성도 개선되면서 우수하거나 비슷한 결과를 달성했지.

우리의 접근 방식은 자율 주행, 인간 동작 캡처, 가상 현실 등 다양한 응용 분야에서 포즈 추정에 강력한 해결책을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07801.pdf

Title: SURGIVID: Annotation-Efficient Surgical Video Object Discovery

Original Abstract:
Surgical scenes convey crucial information about the quality of surgery. Pixel-wise localization of tools and anatomical structures is the first task towards deeper surgical analysis for microscopic or endoscopic surgical views. This is typically done via fully-supervised methods which are annotation greedy and in several cases, demanding medical expertise. Considering the profusion of surgical videos obtained through standardized surgical workflows, we propose an annotation-efficient framework for the semantic segmentation of surgical scenes. We employ image-based self-supervised object discovery to identify the most salient tools and anatomical structures in surgical videos. These proposals are further refined within a minimally supervised fine-tuning step. Our unsupervised setup reinforced with only 36 annotation labels indicates comparable localization performance with fully-supervised segmentation models. Further, leveraging surgical phase labels as weak labels can better guide model attention towards surgical tools, leading to $\sim 2\%$ improvement in tool localization. Extensive ablation studies on the CaDIS dataset validate the effectiveness of our proposed solution in discovering relevant surgical objects with minimal or no supervision.

Translated Abstract:
수술 장면은 수술의 질에 대한 중요한 정보를 전달해. 도구와 해부 구조를 픽셀 단위로 위치 파악하는 게 미세 수술이나 내시경 수술을 더 깊이 분석하는 첫 번째 단계야. 보통 이런 작업은 완전 감독 방법을 통해 이루어지는데, 이건 주석을 많이 필요로 하고 의학 전문 지식도 요구돼.

표준화된 수술 흐름을 통해 얻은 많은 수술 비디오를 고려해서, 우리는 수술 장면의 의미 구분을 위한 주석 효율적인 프레임워크를 제안해. 우리는 이미지 기반의 자기 감독 객체 발견을 사용해서 수술 비디오에서 가장 눈에 띄는 도구와 해부 구조를 찾아내.

이 제안들은 최소한의 감독을 통한 미세 조정 단계에서 더 다듬어져. 우리가 사용하는 비감독 설정은 단 36개의 주석 레이블만으로도 완전 감독 모델과 비슷한 위치 파악 성능을 보여줘. 게다가 수술 단계 레이블을 약한 레이블로 활용하면 모델이 수술 도구에 더 집중할 수 있도록 도와줘서 도구 위치 파악에서 약 2% 개선이 있어.

CaDIS 데이터셋에 대한 광범위한 분석 연구는 최소한 또는 아예 감독 없이도 관련 수술 객체를 발견하는 데 있어 우리 제안의 효과를 검증해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07813.pdf

Title: What is YOLOv9: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector

Original Abstract:
This study provides a comprehensive analysis of the YOLOv9 object detection model, focusing on its architectural innovations, training methodologies, and performance improvements over its predecessors. Key advancements, such as the Generalized Efficient Layer Aggregation Network GELAN and Programmable Gradient Information PGI, significantly enhance feature extraction and gradient flow, leading to improved accuracy and efficiency. By incorporating Depthwise Convolutions and the lightweight C3Ghost architecture, YOLOv9 reduces computational complexity while maintaining high precision. Benchmark tests on Microsoft COCO demonstrate its superior mean Average Precision mAP and faster inference times, outperforming YOLOv8 across multiple metrics. The model versatility is highlighted by its seamless deployment across various hardware platforms, from edge devices to high performance GPUs, with built in support for PyTorch and TensorRT integration. This paper provides the first in depth exploration of YOLOv9s internal features and their real world applicability, establishing it as a state of the art solution for real time object detection across industries, from IoT devices to large scale industrial applications.

Translated Abstract:
이 연구는 YOLOv9 객체 탐지 모델에 대해 자세히 분석했어. 주로 아키텍처 혁신, 훈련 방법, 그리고 이전 모델들에 비해 성능이 어떻게 향상됐는지를 다루고 있어.

주요 발전 사항으로는 일반화된 효율적 레이어 집합 네트워크(GELAN)와 프로그래머블 그래디언트 정보(PGI)가 있어. 이 두 가지는 특징 추출과 그래디언트 흐름을 크게 개선시켜서 정확성과 효율성을 높여줘. 깊이별 컨볼루션과 가벼운 C3Ghost 아키텍처를 사용해서 YOLOv9는 계산 복잡성을 줄이면서도 높은 정밀도를 유지해.

마이크로소프트 COCO에서 실시한 벤치마크 테스트 결과, YOLOv9는 평균 정밀도(mAP)와 추론 속도에서 YOLOv8보다 뛰어난 성능을 보여줬어. 이 모델은 엣지 디바이스부터 고성능 GPU까지 다양한 하드웨어 플랫폼에서 간편하게 배포할 수 있어서 유연성이 높아. 게다가 PyTorch와 TensorRT 통합을 내장 지원하고 있어.

이 논문은 YOLOv9의 내부 기능과 그것들이 실제 세계에서 어떻게 활용될 수 있는지를 처음으로 깊이 있게 탐구하고 있어. 이 모델은 IoT 기기부터 대규모 산업 응용까지 다양한 분야에서 실시간 객체 탐지에 최적화된 최신 솔루션으로 자리 잡고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07825.pdf

Title: A Comprehensive Survey on Deep Multimodal Learning with Missing Modality

Original Abstract:
During multimodal model training and reasoning, data samples may miss certain modalities and lead to compromised model performance due to sensor limitations, cost constraints, privacy concerns, data loss, and temporal and spatial factors. This survey provides an overview of recent progress in Multimodal Learning with Missing Modality (MLMM), focusing on deep learning techniques. It is the first comprehensive survey that covers the historical background and the distinction between MLMM and standard multimodal learning setups, followed by a detailed analysis of current MLMM methods, applications, and datasets, concluding with a discussion about challenges and potential future directions in the field.

Translated Abstract:
다중 모달 모델 훈련과 추론 중에 데이터 샘플이 특정 모달리티를 놓치는 경우가 있어. 이런 상황은 센서의 한계, 비용 문제, 개인정보 보호, 데이터 손실, 그리고 시간과 공간적인 요소들 때문에 모델 성능이 저하될 수 있어. 

이 서베이는 누락된 모달리티가 있는 다중 모달 학습(MLMM)에서 최근 진행된 연구들을 간단히 정리한 거야. 특히 딥러닝 기법에 초점을 맞추고 있어. 이 서베이는 MLMM의 역사적 배경과 표준 다중 모달 학습과의 차이점을 다룬 첫 번째 포괄적인 조사야.  

그 다음에는 현재의 MLMM 방법, 응용 분야, 데이터셋에 대한 자세한 분석이 이어지고, 마지막으로 이 분야에서의 도전 과제와 미래 방향에 대한 논의로 마무리돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.07834.pdf

Title: Structured Pruning for Efficient Visual Place Recognition

Original Abstract:
Visual Place Recognition (VPR) is fundamental for the global re-localization of robots and devices, enabling them to recognize previously visited locations based on visual inputs. This capability is crucial for maintaining accurate mapping and localization over large areas. Given that VPR methods need to operate in real-time on embedded systems, it is critical to optimize these systems for minimal resource consumption. While the most efficient VPR approaches employ standard convolutional backbones with fixed descriptor dimensions, these often lead to redundancy in the embedding space as well as in the network architecture. Our work introduces a novel structured pruning method, to not only streamline common VPR architectures but also to strategically remove redundancies within the feature embedding space. This dual focus significantly enhances the efficiency of the system, reducing both map and model memory requirements and decreasing feature extraction and retrieval latencies. Our approach has reduced memory usage and latency by 21% and 16%, respectively, across models, while minimally impacting recall@1 accuracy by less than 1%. This significant improvement enhances real-time applications on edge devices with negligible accuracy loss.

Translated Abstract:
비주얼 장소 인식(Visual Place Recognition, VPR)은 로봇이나 기기가 이전에 방문했던 장소를 시각적 입력을 기반으로 인식할 수 있게 해주는 기술이야. 이 기능은 넓은 지역에서 정확한 맵핑과 위치 인식을 유지하는 데 매우 중요해. VPR 방법은 임베디드 시스템에서 실시간으로 작동해야 하기 때문에, 자원 소비를 최소화하도록 최적화하는 게 필수적이야.

가장 효율적인 VPR 접근 방식은 고정된 디스크립터 차원을 가진 표준 컨볼루션 백본을 사용하는데, 이로 인해 임베딩 공간과 네트워크 구조에서 중복이 발생해. 우리 연구는 새로운 구조적 프루닝 방법을 도입해서, 일반적인 VPR 아키텍처를 간소화할 뿐만 아니라 특징 임베딩 공간의 중복도 전략적으로 제거해. 이렇게 두 가지에 집중함으로써 시스템의 효율성을 크게 향상시켰고, 맵과 모델 메모리 요구사항을 줄이며, 특징 추출과 검색 지연 시간을 감소시켰어.

우리의 접근 방식은 모델 전반에 걸쳐 메모리 사용량을 21% 줄였고, 지연 시간을 16% 감소시켰어. 그리고 recall@1 정확도에는 1%도 안 되는 미세한 영향을 줬어. 이러한 중요한 개선은 엣지 기기에서 실시간 애플리케이션을 지원하면서도 정확도 손실을 거의 없애줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.07843.pdf

Title: Real-time Multi-view Omnidirectional Depth Estimation System for Robots and Autonomous Driving on Real Scenes

Original Abstract:
Omnidirectional Depth Estimation has broad application prospects in fields such as robotic navigation and autonomous driving. In this paper, we propose a robotic prototype system and corresponding algorithm designed to validate omnidirectional depth estimation for navigation and obstacle avoidance in real-world scenarios for both robots and vehicles. The proposed HexaMODE system captures 360$^\circ$ depth maps using six surrounding arranged fisheye cameras. We introduce a combined spherical sweeping method and optimize the model architecture for proposed RtHexa-OmniMVS algorithm to achieve real-time omnidirectional depth estimation. To ensure high accuracy, robustness, and generalization in real-world environments, we employ a teacher-student self-training strategy, utilizing large-scale unlabeled real-world data for model training. The proposed algorithm demonstrates high accuracy in various complex real-world scenarios, both indoors and outdoors, achieving an inference speed of 15 fps on edge computing platforms.

Translated Abstract:
전방위 깊이 추정은 로봇 내비게이션과 자율주행 같은 분야에서 많은 가능성을 가지고 있어. 이 논문에서는 로봇과 차량이 실제 환경에서 내비게이션과 장애물 회피를 위해 전방위 깊이 추정을 검증할 수 있는 로봇 프로토타입 시스템과 알고리즘을 제안해. 

우리가 제안하는 HexaMODE 시스템은 여섯 개의 어안 렌즈 카메라를 사용해서 360도 깊이 맵을 촬영해. 우리는 결합된 구형 스위핑 방법을 도입하고, RtHexa-OmniMVS 알고리즘을 위해 모델 구조를 최적화해서 실시간 전방위 깊이 추정을 가능하게 했어. 

정확도와 견고성, 일반화 능력을 높이기 위해, 우리는 교사-학생 자기 훈련 전략을 사용하고, 대규모 레이블 없는 실제 데이터를 모델 훈련에 활용했어. 제안된 알고리즘은 실내외의 다양한 복잡한 실제 상황에서 높은 정확도를 보여주며, 엣지 컴퓨팅 플랫폼에서 15fps의 추론 속도를 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07887.pdf

Title: UNIT: Unsupervised Online Instance Segmentation through Time

Original Abstract:
Online object segmentation and tracking in Lidar point clouds enables autonomous agents to understand their surroundings and make safe decisions. Unfortunately, manual annotations for these tasks are prohibitively costly. We tackle this problem with the task of class-agnostic unsupervised online instance segmentation and tracking. To that end, we leverage an instance segmentation backbone and propose a new training recipe that enables the online tracking of objects. Our network is trained on pseudo-labels, eliminating the need for manual annotations. We conduct an evaluation using metrics adapted for temporal instance segmentation. Computing these metrics requires temporally-consistent instance labels. When unavailable, we construct these labels using the available 3D bounding boxes and semantic labels in the dataset. We compare our method against strong baselines and demonstrate its superiority across two different outdoor Lidar datasets.

Translated Abstract:
온라인 객체 분할과 추적은 라이다 포인트 클라우드를 이용해 자율 시스템이 주변을 이해하고 안전한 결정을 내릴 수 있게 해줘. 하지만 이런 작업을 위해 수동으로 주석을 다는 건 너무 비싸. 그래서 우리는 클래스에 구애받지 않는 비지도 온라인 인스턴스 분할과 추적 작업을 통해 이 문제를 해결하려고 해.

이를 위해 인스턴스 분할 백본을 활용하고, 객체의 온라인 추적을 가능하게 하는 새로운 훈련 방법을 제안해. 우리 네트워크는 수동 주석 없이도 사용할 수 있는 가짜 레이블로 훈련돼. 우리는 시간적 인스턴스 분할에 맞춰 조정된 지표를 사용해 평가를 진행해. 이 지표를 계산하려면 시간적으로 일관된 인스턴스 레이블이 필요해. 만약 이 레이블이 없으면, 데이터셋에 있는 3D 바운딩 박스와 의미 레이블을 사용해 이 레이블을 만들어.

우리는 강력한 기준선과 비교해서 우리 방법의 우수성을 두 개의 서로 다른 야외 라이다 데이터셋에서 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07896.pdf

Title: Microscopic-Mamba: Revealing the Secrets of Microscopic Images with Just 4M Parameters

Original Abstract:
In the field of medical microscopic image classification (MIC), CNN-based and Transformer-based models have been extensively studied. However, CNNs struggle with modeling long-range dependencies, limiting their ability to fully utilize semantic information in images. Conversely, Transformers are hampered by the complexity of quadratic computations. To address these challenges, we propose a model based on the Mamba architecture: Microscopic-Mamba. Specifically, we designed the Partially Selected Feed-Forward Network (PSFFN) to replace the last linear layer of the Visual State Space Module (VSSM), enhancing Mamba's local feature extraction capabilities. Additionally, we introduced the Modulation Interaction Feature Aggregation (MIFA) module to effectively modulate and dynamically aggregate global and local features. We also incorporated a parallel VSSM mechanism to improve inter-channel information interaction while reducing the number of parameters. Extensive experiments have demonstrated that our method achieves state-of-the-art performance on five public datasets. Code is available at this https URL

Translated Abstract:
의료 미세 이미지 분류(MIC) 분야에서는 CNN 기반 모델과 Transformer 기반 모델이 많이 연구되어 왔어. 하지만 CNN은 긴 거리의 의존성을 모델링하는 데 어려움을 겪어서 이미지의 의미 정보를 제대로 활용하지 못해. 반면에 Transformers는 복잡한 제곱 계산 때문에 문제가 있어. 

이런 문제를 해결하기 위해 우리는 Mamba 아키텍처를 기반으로 한 Microscopic-Mamba 모델을 제안해. 특히, Visual State Space Module(VSSM)의 마지막 선형 레이어를 대체할 수 있도록 Partially Selected Feed-Forward Network(PSFFN)를 설계했어. 이렇게 해서 Mamba의 지역 특성 추출 능력을 강화했지. 

또한, 전역 및 지역 특성을 효과적으로 조절하고 동적으로 집계할 수 있도록 Modulation Interaction Feature Aggregation(MIFA) 모듈도 도입했어. 그리고 매개변수 수를 줄이면서 채널 간 정보 상호작용을 개선하기 위해 병렬 VSSM 메커니즘도 넣었어.

광범위한 실험 결과, 우리의 방법이 다섯 개의 공개 데이터셋에서 최첨단 성능을 달성했다는 걸 보여줬어. 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07904.pdf

Title: FACT: Feature Adaptive Continual-learning Tracker for Multiple Object Tracking

Original Abstract:
Multiple object tracking (MOT) involves identifying multiple targets and assigning them corresponding IDs within a video sequence, where occlusions are often encountered. Recent methods address occlusions using appearance cues through online learning techniques to improve adaptivity or offline learning techniques to utilize temporal information from videos. However, most existing online learning-based MOT methods are unable to learn from all past tracking information to improve adaptivity on long-term occlusions while maintaining real-time tracking speed. On the other hand, temporal information-based offline learning methods maintain a long-term memory to store past tracking information, but this approach restricts them to use only local past information during tracking. To address these challenges, we propose a new MOT framework called the Feature Adaptive Continual-learning Tracker (FACT), which enables real-time tracking and feature learning for targets by utilizing all past tracking information. We demonstrate that the framework can be integrated with various state-of-the-art feature-based trackers, thereby improving their tracking ability. Specifically, we develop the feature adaptive continual-learning (FAC) module, a neural network that can be trained online to learn features adaptively using all past tracking information during tracking. Moreover, we also introduce a two-stage association module specifically designed for the proposed continual learning-based tracking. Extensive experiment results demonstrate that the proposed method achieves state-of-the-art online tracking performance on MOT17 and MOT20 benchmarks. The code will be released upon acceptance.

Translated Abstract:
다중 객체 추적(MOT)은 비디오 시퀀스에서 여러 대상을 식별하고 각 대상에 해당하는 ID를 부여하는 과정인데, 이 과정에서 가림 현상이 자주 발생해. 최근 방법들은 온라인 학습 기술을 통해 외관 정보를 활용해서 가림 현상을 처리하거나, 오프라인 학습 기술로 비디오의 시간 정보를 이용해 적응성을 향상시키고 있어. 하지만 대부분의 기존 온라인 학습 기반 MOT 방법들은 긴 가림 현상에서 적응성을 높이기 위해 모든 과거 추적 정보를 학습할 수 없으면서도 실시간 추적 속도를 유지하는 데 어려움을 겪고 있어.

반면, 시간 정보 기반의 오프라인 학습 방법들은 과거 추적 정보를 저장할 수 있는 장기 기억을 유지하지만, 이 방식은 추적 중에 오직 지역 과거 정보만 사용할 수 있도록 제한해. 이런 문제를 해결하기 위해 우리는 FACT라는 새로운 MOT 프레임워크를 제안해. 이 프레임워크는 모든 과거 추적 정보를 활용해서 실시간으로 대상을 추적하고 특징을 학습할 수 있게 해.

우리는 이 프레임워크가 다양한 최신 특징 기반 추적기와 통합될 수 있어 추적 능력을 향상시킬 수 있다는 걸 보여줬어. 특히, 우리는 모든 과거 추적 정보를 활용해 적응적으로 특징을 학습할 수 있는 온라인 학습 가능한 신경망인 FEATURE ADAPTIVE CONTINUAL-LEARNING (FAC) 모듈을 개발했어. 게다가, 제안된 지속적 학습 기반 추적을 위해 특별히 설계된 2단계 연관 모듈도 소개했어. 

광범위한 실험 결과는 우리가 제안한 방법이 MOT17과 MOT20 벤치마크에서 최신 온라인 추적 성능을 달성한다는 걸 보여줘. 코드는 승인이 나면 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07907.pdf

Title: From COCO to COCO-FP: A Deep Dive into Background False Positives for COCO Detectors

Original Abstract:
Reducing false positives is essential for enhancing object detector performance, as reflected in the mean Average Precision (mAP) metric. Although object detectors have achieved notable improvements and high mAP scores on the COCO dataset, analysis reveals limited progress in addressing false positives caused by non-target visual clutter-background objects not included in the annotated categories. This issue is particularly critical in real-world applications, such as fire and smoke detection, where minimizing false alarms is crucial. In this study, we introduce COCO-FP, a new evaluation dataset derived from the ImageNet-1K dataset, designed to address this issue. By extending the original COCO validation dataset, COCO-FP specifically assesses object detectors' performance in mitigating background false positives. Our evaluation of both standard and advanced object detectors shows a significant number of false positives in both closed-set and open-set scenarios. For example, the AP50 metric for YOLOv9-E decreases from 72.8 to 65.7 when shifting from COCO to COCO-FP. The dataset is available at this https URL.

Translated Abstract:
거짓 양성을 줄이는 건 객체 탐지기의 성능을 높이는 데 정말 중요해. 이건 평균 평균 정밀도(mAP) 지표에 잘 나타나 있어. 객체 탐지기들이 COCO 데이터셋에서 꽤 좋은 성과와 높은 mAP 점수를 기록했지만, 분석해보면 비목표 시각적 혼잡물, 즉 주석에 포함되지 않은 배경 객체들로 인해 발생하는 거짓 양성 문제는 잘 해결되지 않았어. 

이 문제는 실제 상황에서는 특히 중요해. 예를 들어, 화재나 연기 탐지 같은 경우에는 거짓 경고를 최소화하는 게 필수적이거든. 그래서 이번 연구에서는 COCO-FP라는 새로운 평가 데이터셋을 소개할 거야. COCO-FP는 ImageNet-1K 데이터셋에서 파생된 것으로, 이 문제를 해결하기 위해 만들어졌어. 원래의 COCO 검증 데이터셋을 확장해서, COCO-FP는 특히 객체 탐지기가 배경 거짓 양성을 줄이는 성능을 평가하는 데 중점을 두고 있어.

우리가 표준 및 고급 객체 탐지기를 평가해본 결과, 닫힌 집합과 열린 집합 모두에서 많은 거짓 양성이 발생한다는 걸 확인했어. 예를 들어, YOLOv9-E의 AP50 지표는 COCO에서 COCO-FP로 바뀔 때 72.8에서 65.7로 감소해. 이 데이터셋은 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07913.pdf

Title: UGAD: Universal Generative AI Detector utilizing Frequency Fingerprints

Original Abstract:
In the wake of a fabricated explosion image at the Pentagon, an ability to discern real images from fake counterparts has never been more critical. Our study introduces a novel multi-modal approach to detect AI-generated images amidst the proliferation of new generation methods such as Diffusion models. Our method, UGAD, encompasses three key detection steps: First, we transform the RGB images into YCbCr channels and apply an Integral Radial Operation to emphasize salient radial features. Secondly, the Spatial Fourier Extraction operation is used for a spatial shift, utilizing a pre-trained deep learning network for optimal feature extraction. Finally, the deep neural network classification stage processes the data through dense layers using softmax for classification. Our approach significantly enhances the accuracy of differentiating between real and AI-generated images, as evidenced by a 12.64% increase in accuracy and 28.43% increase in AUC compared to existing state-of-the-art methods.

Translated Abstract:
펜타곤에서 조작된 폭발 이미지가 나왔던 사건 이후, 진짜 이미지와 가짜 이미지를 구별하는 능력이 그 어느 때보다 중요해졌어. 우리 연구는 AI가 생성한 이미지를 탐지하기 위한 새로운 다중 모달 접근 방식을 소개해. 요즘 유행하는 확산 모델 같은 신세대 방법들이 많아지면서 이 방법이 필요해졌지.

우리의 방법, UGAD는 세 가지 주요 탐지 단계를 포함해. 첫째, RGB 이미지를 YCbCr 채널로 변환하고, 중요한 방사형 특징을 강조하기 위해 적분 방사 작업을 적용해. 둘째, 공간 이동을 위해 Spatial Fourier Extraction 작업을 사용하는데, 여기서는 최적의 특징 추출을 위해 미리 학습된 딥러닝 네트워크를 활용해. 마지막으로, 심층 신경망 분류 단계에서 데이터가 밀집 레이어를 통해 처리되며, 소프트맥스를 사용해 분류를 해.

우리 접근 방식은 진짜 이미지와 AI가 생성한 이미지를 구별하는 정확도를 크게 향상시켜. 기존의 최첨단 방법들과 비교했을 때 정확도가 12.64% 상승하고, AUC가 28.43% 증가하는 결과를 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07931.pdf

Title: Task-Augmented Cross-View Imputation Network for Partial Multi-View Incomplete Multi-Label Classification

Original Abstract:
In real-world scenarios, multi-view multi-label learning often encounters the challenge of incomplete training data due to limitations in data collection and unreliable annotation processes. The absence of multi-view features impairs the comprehensive understanding of samples, omitting crucial details essential for classification. To address this issue, we present a task-augmented cross-view imputation network (TACVI-Net) for the purpose of handling partial multi-view incomplete multi-label classification. Specifically, we employ a two-stage network to derive highly task-relevant features to recover the missing views. In the first stage, we leverage the information bottleneck theory to obtain a discriminative representation of each view by extracting task-relevant information through a view-specific encoder-classifier architecture. In the second stage, an autoencoder based multi-view reconstruction network is utilized to extract high-level semantic representation of the augmented features and recover the missing data, thereby aiding the final classification task. Extensive experiments on five datasets demonstrate that our TACVI-Net outperforms other state-of-the-art methods.

Translated Abstract:
현실 세계에서는 다중 뷰 다중 레이블 학습이 데이터 수집의 한계나 신뢰할 수 없는 주석 과정 때문에 불완전한 훈련 데이터와 마주하는 경우가 많아. 다중 뷰 특성이 없으면 샘플을 제대로 이해하기 어려워지고, 분류에 필요한 중요한 세부 정보가 빠지게 돼.

이 문제를 해결하기 위해 우리는 부분적인 다중 뷰 불완전 다중 레이블 분류를 처리할 수 있는 작업 증가형 크로스 뷰 보간 네트워크(TACVI-Net)를 제안해. 구체적으로, 우리는 두 단계 네트워크를 사용해서 누락된 뷰를 복구하는 데 필요한 작업 관련 특성을 뽑아내.

첫 번째 단계에서는 정보 병목 이론을 활용해 각 뷰의 판별 가능한 표현을 얻어. 이 과정에서 뷰에 특화된 인코더-분류기 구조를 통해 작업 관련 정보를 추출해. 두 번째 단계에서는 오토인코더 기반의 다중 뷰 재구성 네트워크를 사용해 증강된 특성의 고수준 의미 표현을 추출하고 누락된 데이터를 복구해, 최종 분류 작업을 도와.

다섯 개의 데이터셋에 대한 광범위한 실험 결과, 우리 TACVI-Net이 다른 최신 방법들보다 더 뛰어난 성능을 보인다는 걸 증명했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07940.pdf

Title: Control+Shift: Generating Controllable Distribution Shifts

Original Abstract:
We propose a new method for generating realistic datasets with distribution shifts using any decoder-based generative model. Our approach systematically creates datasets with varying intensities of distribution shifts, facilitating a comprehensive analysis of model performance degradation. We then use these generated datasets to evaluate the performance of various commonly used networks and observe a consistent decline in performance with increasing shift intensity, even when the effect is almost perceptually unnoticeable to the human eye. We see this degradation even when using data augmentations. We also find that enlarging the training dataset beyond a certain point has no effect on the robustness and that stronger inductive biases increase robustness.

Translated Abstract:
우리는 디코더 기반 생성 모델을 사용해서 현실적인 데이터셋을 만드는 새로운 방법을 제안해. 이 방법은 다양한 강도의 분포 변화가 있는 데이터셋을 체계적으로 만들어서 모델 성능 저하를 자세히 분석할 수 있게 해.

이렇게 생성한 데이터셋을 사용해서 여러 일반적으로 쓰이는 네트워크의 성능을 평가해봤는데, 분포 변화의 강도가 높아질수록 성능이 일관되게 떨어지는 걸 보았어. 심지어 그 변화가 사람의 눈으로는 거의 눈에 띄지 않을 때도 그렇더라구. 데이터 증강을 사용해도 이런 저하가 나타났어.

또한, 훈련 데이터셋을 특정 지점 이상으로 키워도 강인성에는 영향을 주지 않는다는 걸 발견했어. 반면에 더 강한 유도 편향이 있으면 강인성이 증가하는 걸 알 수 있었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07960.pdf

Title: Do Vision Foundation Models Enhance Domain Generalization in Medical Image Segmentation?

Original Abstract:
Neural networks achieve state-of-the-art performance in many supervised learning tasks when the training data distribution matches the test data distribution. However, their performance drops significantly under domain (covariate) shift, a prevalent issue in medical image segmentation due to varying acquisition settings across different scanner models and protocols. Recently, foundational models (FMs) trained on large datasets have gained attention for their ability to be adapted for downstream tasks and achieve state-of-the-art performance with excellent generalization capabilities on natural images. However, their effectiveness in medical image segmentation remains underexplored. In this paper, we investigate the domain generalization performance of various FMs, including DinoV2, SAM, MedSAM, and MAE, when fine-tuned using various parameter-efficient fine-tuning (PEFT) techniques such as Ladder and Rein (+LoRA) and decoder heads. We introduce a novel decode head architecture, HQHSAM, which simply integrates elements from two state-of-the-art decoder heads, HSAM and HQSAM, to enhance segmentation performance. Our extensive experiments on multiple datasets, encompassing various anatomies and modalities, reveal that FMs, particularly with the HQHSAM decode head, improve domain generalization for medical image segmentation. Moreover, we found that the effectiveness of PEFT techniques varies across different FMs. These findings underscore the potential of FMs to enhance the domain generalization performance of neural networks in medical image segmentation across diverse clinical settings, providing a solid foundation for future research. Code and models are available for research purposes at \url{this https URL}.

Translated Abstract:
신경망은 훈련 데이터 분포와 테스트 데이터 분포가 일치할 때 많은 감독 학습 작업에서 최첨단 성능을 보여. 하지만 도메인(공변량) 변화가 있을 때 성능이 크게 떨어지는데, 이 문제는 다양한 스캐너 모델과 프로토콜로 인해 의료 이미지 분할에서 자주 발생해.

최근에는 대규모 데이터셋으로 훈련된 기초 모델(FM)이 주목받고 있어. 이 모델들은 하위 작업에 적응할 수 있는 능력이 뛰어나고 자연 이미지에서 최첨단 성능을 보여줘. 하지만 의료 이미지 분할에서의 효과는 아직 잘 연구되지 않았어.

이 논문에서는 다양한 FM, 예를 들어 DinoV2, SAM, MedSAM, MAE의 도메인 일반화 성능을 살펴봐. 우리는 Ladder와 Rein(+LoRA) 같은 다양한 파라미터 효율적인 미세 조정(PEFT) 기법을 사용해 조정했어. 또한, 두 가지 최첨단 디코더 헤드인 HSAM과 HQSAM의 요소를 통합한 새로운 디코드 헤드 아키텍처인 HQHSAM을 소개해. 이 아키텍처는 분할 성능을 향상시키는 데 도움을 줘.

여러 데이터셋에서 광범위한 실험을 진행한 결과, 특히 HQHSAM 디코드 헤드를 사용할 때 FM이 의료 이미지 분할의 도메인 일반화를 개선한다는 것을 발견했어. 그리고 PEFT 기법의 효과는 다른 FM에 따라 다르게 나타났어. 이 연구 결과는 다양한 임상 환경에서 신경망의 도메인 일반화 성능을 향상시킬 수 있는 FM의 잠재력을 강조해. 앞으로의 연구에 대한 기초를 제공해. 코드와 모델은 연구 목적으로 \url{this https URL}에서 사용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07961.pdf

Title: Estimating atmospheric variables from Digital Typhoon Satellite Images via Conditional Denoising Diffusion Models

Original Abstract:
This study explores the application of diffusion models in the field of typhoons, predicting multiple ERA5 meteorological variables simultaneously from Digital Typhoon satellite images. The focus of this study is taken to be Taiwan, an area very vulnerable to typhoons. By comparing the performance of Conditional Denoising Diffusion Probability Model (CDDPM) with Convolutional Neural Networks (CNN) and Squeeze-and-Excitation Networks (SENet), results suggest that the CDDPM performs best in generating accurate and realistic meteorological data. Specifically, CDDPM achieved a PSNR of 32.807, which is approximately 7.9% higher than CNN and 5.5% higher than SENet. Furthermore, CDDPM recorded an RMSE of 0.032, showing a 11.1% improvement over CNN and 8.6% improvement over SENet. A key application of this research can be for imputation purposes in missing meteorological datasets and generate additional high-quality meteorological data using satellite images. It is hoped that the results of this analysis will enable more robust and detailed forecasting, reducing the impact of severe weather events on vulnerable regions. Code accessible at this https URL.

Translated Abstract:
이 연구는 태풍 분야에서 확산 모델의 적용을 살펴보고, 디지털 태풍 위성 이미지로부터 여러 가지 ERA5 기상 변수를 동시에 예측하는 내용을 다루고 있어. 연구의 초점은 태풍에 매우 취약한 대만 지역이야.

조건부 노이즈 제거 확산 확률 모델(CDDPM)과 합성곱 신경망(CNN), 스퀴즈-앤-익사이테이션 네트워크(SENet)의 성능을 비교한 결과, CDDPM이 정확하고 현실적인 기상 데이터를 생성하는 데 가장 우수한 성능을 보였어. 특히 CDDPM은 PSNR이 32.807로, CNN보다 약 7.9% 높고 SENet보다 5.5% 높은 수치를 기록했어. 게다가 CDDPM은 RMSE를 0.032로 기록해, CNN보다 11.1% 개선되었고 SENet보다 8.6% 개선된 결과를 보여줬어.

이 연구의 주요 응용은 결측 기상 데이터의 채우기와 위성 이미지를 이용해 추가적인 고품질 기상 데이터를 생성하는 데 있어. 이번 분석 결과가 보다 견고하고 자세한 예측을 가능하게 해서, 취약한 지역에서의 심각한 기상 사건의 영향을 줄이는 데 도움이 되길 바래. 코드 접근은 이 URL에서 가능해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07966.pdf

Title: ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE

Original Abstract:
Audio-driven 3D facial animation synthesis has been an active field of research with attention from both academia and industry. While there are promising results in this area, recent approaches largely focus on lip-sync and identity control, neglecting the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize speech animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output motion. We argue that emotions and non-determinism are crucial to generate diverse and emotionally-rich facial animations. In this paper, we propose ProbTalk3D a non-deterministic neural network approach for emotion controllable speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and an emotionally rich facial animation dataset 3DMEAD. We provide an extensive comparative analysis of our model against the recent 3D facial animation synthesis approaches, by evaluating the results objectively, qualitatively, and with a perceptual user study. We highlight several objective metrics that are more suitable for evaluating stochastic outputs and use both in-the-wild and ground truth data for subjective evaluation. To our knowledge, that is the first non-deterministic 3D facial animation synthesis method incorporating a rich emotion dataset and emotion control with emotion labels and intensity levels. Our evaluation demonstrates that the proposed model achieves superior performance compared to state-of-the-art emotion-controlled, deterministic and non-deterministic models. We recommend watching the supplementary video for quality judgement. The entire codebase is publicly available (this https URL).

Translated Abstract:
오디오 기반 3D 얼굴 애니메이션 합성은 학계와 산업계에서 활발히 연구되고 있는 분야야. 이 분야에서는 좋은 결과들이 나오고 있지만, 최근 방법들은 주로 입 모양 맞추기와 정체성 제어에 집중하고 있어. 감정이나 감정 제어의 역할은 잘 다루어지지 않고 있어. 주된 이유는 감정이 풍부한 얼굴 애니메이션 데이터와 감정 표현과 함께 음성을 합성할 수 있는 알고리즘이 부족하기 때문이야. 

또한 대부분의 모델은 결정론적이어서 같은 오디오 입력이 주어지면 항상 같은 결과를 내. 우리는 감정과 비결정성이 다양한 감정이 풍부한 얼굴 애니메이션을 생성하는 데 필수적이라고 주장해. 이 논문에서는 ProbTalk3D라는 비결정적 신경망 접근 방식을 제안해. 이 방식은 감정 제어가 가능한 음성 기반 3D 얼굴 애니메이션 합성을 위해 두 단계의 VQ-VAE 모델과 감정이 풍부한 얼굴 애니메이션 데이터셋인 3DMEAD를 사용해.

우리는 모델을 최근 3D 얼굴 애니메이션 합성 방법들과 비교 분석했어. 결과를 객관적으로, 질적으로, 그리고 사용자 연구를 통해 평가했어. 우리는 확률적 결과를 평가하는 데 더 적합한 여러 객관적 지표를 강조하고, 주관적 평가를 위해 실제 환경 데이터와 기준 데이터를 사용했어. 아마도 이 연구는 감정 레이블과 강도 수준을 포함하여 풍부한 감정 데이터셋을 통합한 최초의 비결정적 3D 얼굴 애니메이션 합성 방법이야. 

우리 평가 결과, 제안된 모델이 최신 감정 제어, 결정론적 및 비결정적 모델들보다 우수한 성능을 보인다는 것을 보여줘. 품질 판단을 위해 보조 영상을 시청하는 걸 추천해. 전체 코드베이스는 공개되어 있어 (이 https URL).

================================================================================

URL:
https://arxiv.org/pdf/2409.07967.pdf

Title: Locality-aware Cross-modal Correspondence Learning for Dense Audio-Visual Events Localization

Original Abstract:
Dense-localization Audio-Visual Events (DAVE) aims to identify time boundaries and corresponding categories for events that can be heard and seen concurrently in an untrimmed video. Existing methods typically encode audio and visual representation separately without any explicit cross-modal alignment constraint. Then they adopt dense cross-modal attention to integrate multimodal information for DAVE. Thus these methods inevitably aggregate irrelevant noise and events, especially in complex and long videos, leading to imprecise detection. In this paper, we present LOCO, a Locality-aware cross-modal Correspondence learning framework for DAVE. The core idea is to explore local temporal continuity nature of audio-visual events, which serves as informative yet free supervision signals to guide the filtering of irrelevant information and inspire the extraction of complementary multimodal information during both unimodal and cross-modal learning stages. i) Specifically, LOCO applies Locality-aware Correspondence Correction (LCC) to uni-modal features via leveraging cross-modal local-correlated properties without any extra annotations. This enforces uni-modal encoders to highlight similar semantics shared by audio and visual features. ii) To better aggregate such audio and visual features, we further customize Cross-modal Dynamic Perception layer (CDP) in cross-modal feature pyramid to understand local temporal patterns of audio-visual events by imposing local consistency within multimodal features in a data-driven manner. By incorporating LCC and CDP, LOCO provides solid performance gains and outperforms existing methods for DAVE. The source code will be released.

Translated Abstract:
Dense-localization Audio-Visual Events (DAVE)는 잘라지지 않은 비디오에서 동시에 들리고 보이는 이벤트의 시간 경계와 해당 카테고리를 식별하는 것을 목표로 해. 기존 방법들은 보통 오디오와 비주얼 표현을 따로 인코딩하고, 명확한 교차 모드 정렬 제약 없이 진행해. 그런 다음, 밀집 교차 모드 어텐션을 사용해서 멀티모달 정보를 통합하는데, 이러면 특히 복잡하고 긴 비디오에서 관련 없는 노이즈와 이벤트가 섞여서 정확한 탐지가 어려워져.

이 논문에서는 LOCO라는 프레임워크를 소개해. LOCO의 핵심 아이디어는 오디오-비주얼 이벤트의 지역적 시간 연속성을 탐구하는 거야. 이건 불필요한 정보를 걸러내고, 단일 모드와 교차 모드 학습 단계에서 보완적인 멀티모달 정보를 추출하는 데 도움을 주는 유용한 감독 신호가 돼.

i) 구체적으로, LOCO는 추가적인 주석 없이 교차 모드 지역 상관 특성을 활용해서 단일 모드 특징에 Locality-aware Correspondence Correction (LCC)를 적용해. 이렇게 하면 단일 모드 인코더가 오디오와 비주얼 특징이 공유하는 유사한 의미를 강조하도록 강제돼.

ii) 오디오와 비주얼 특징을 더 잘 통합하기 위해, 우리는 Cross-modal Dynamic Perception layer (CDP)를 교차 모드 특징 피라미드에서 맞춤 설정해. 이건 데이터 기반 방식으로 멀티모달 특징 내에서 지역적 일관성을 부여하면서 오디오-비주얼 이벤트의 지역적 시간 패턴을 이해하도록 해.

LCC와 CDP를 결합함으로써, LOCO는 확실한 성능 향상을 제공하고 기존 방법들을 능가해. 소스 코드는 곧 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.07972.pdf

Title: Deep Height Decoupling for Precise Vision-based 3D Occupancy Prediction

Original Abstract:
The task of vision-based 3D occupancy prediction aims to reconstruct 3D geometry and estimate its semantic classes from 2D color images, where the 2D-to-3D view transformation is an indispensable step. Most previous methods conduct forward projection, such as BEVPooling and VoxelPooling, both of which map the 2D image features into 3D grids. However, the current grid representing features within a certain height range usually introduces many confusing features that belong to other height ranges. To address this challenge, we present Deep Height Decoupling (DHD), a novel framework that incorporates explicit height prior to filter out the confusing features. Specifically, DHD first predicts height maps via explicit supervision. Based on the height distribution statistics, DHD designs Mask Guided Height Sampling (MGHS) to adaptively decoupled the height map into multiple binary masks. MGHS projects the 2D image features into multiple subspaces, where each grid contains features within reasonable height ranges. Finally, a Synergistic Feature Aggregation (SFA) module is deployed to enhance the feature representation through channel and spatial affinities, enabling further occupancy refinement. On the popular Occ3D-nuScenes benchmark, our method achieves state-of-the-art performance even with minimal input frames. Code is available at this https URL.

Translated Abstract:
비전 기반 3D 점유 예측 작업은 2D 컬러 이미지에서 3D 기하학을 재구성하고 그에 대한 의미적 클래스를 추정하는 걸 목표로 해. 여기서 2D에서 3D로 변환하는 과정은 필수적이야. 대부분의 이전 방법들은 BEVPooling이나 VoxelPooling 같은 전방 투사를 사용해서 2D 이미지 특징을 3D 그리드로 변환했어. 하지만 현재의 그리드는 특정 높이 범위 안에 있는 특징을 표현하는데, 다른 높이 범위에 속하는 혼란스러운 특징들이 많이 포함되는 경우가 많아.

이 문제를 해결하기 위해 우리는 Deep Height Decoupling (DHD)이라는 새로운 프레임워크를 제안해. DHD는 혼란스러운 특징을 걸러내기 위해 명시적인 높이 정보를 사용하는 방식이야. 구체적으로, DHD는 먼저 명시적인 감독을 통해 높이 맵을 예측해. 높이 분포 통계에 기반해서 DHD는 Mask Guided Height Sampling (MGHS)를 설계해, 높이 맵을 여러 개의 이진 마스크로 적응적으로 나누는 거야. MGHS는 2D 이미지 특징을 여러 개의 서브스페이스로 투사해서 각 그리드가 합리적인 높이 범위 안에 있는 특징을 포함하게 해.

마지막으로 Synergistic Feature Aggregation (SFA) 모듈을 통해 채널과 공간적 유사성을 이용해 특징 표현을 강화해. 이를 통해 점유 예측을 더 정교하게 할 수 있어. 인기 있는 Occ3D-nuScenes 벤치마크에서, 우리의 방법은 최소한의 입력 프레임으로도 최첨단 성능을 달성했어. 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07973.pdf

Title: Sparse R-CNN OBB: Ship Target Detection in SAR Images Based on Oriented Sparse Proposals

Original Abstract:
We present Sparse R-CNN OBB, a novel framework for the detection of oriented objects in SAR images leveraging sparse learnable proposals. The Sparse R-CNN OBB has streamlined architecture and ease of training as it utilizes a sparse set of 300 proposals instead of training a proposals generator on hundreds of thousands of anchors. To the best of our knowledge, Sparse R-CNN OBB is the first to adopt the concept of sparse learnable proposals for the detection of oriented objects, as well as for the detection of ships in Synthetic Aperture Radar (SAR) images. The detection head of the baseline model, Sparse R-CNN, is re-designed to enable the model to capture object orientation. We also fine-tune the model on RSDD-SAR dataset and provide a performance comparison to state-of-the-art models. Experimental results shows that Sparse R-CNN OBB achieves outstanding performance, surpassing other models on both inshore and offshore scenarios. The code is available at: this http URL.

Translated Abstract:
우리는 Sparse R-CNN OBB라는 새로운 프레임워크를 소개해. 이건 SAR 이미지에서 방향이 있는 객체를 탐지하는 데 사용되는데, 희소하게 학습 가능한 프로포절을 활용해. Sparse R-CNN OBB는 구조가 간단하고 훈련이 쉬워. 왜냐하면 수십만 개의 앵커 대신에 300개의 희소 프로포절 세트를 사용하기 때문이야.

내가 알기로는, Sparse R-CNN OBB가 방향이 있는 객체 탐지에 희소 학습 가능한 프로포절 개념을 처음으로 도입한 모델이야. 또, 합성 개구 레이더(SAR) 이미지에서 배를 탐지하는 데도 처음 적용된 거고. 기본 모델인 Sparse R-CNN의 탐지 헤드를 다시 설계해서 객체의 방향을 잡을 수 있게 했어.

우린 RSDD-SAR 데이터셋에서 모델을 미세 조정하고, 최신 모델들과 성능 비교를 했어. 실험 결과, Sparse R-CNN OBB가 뛰어난 성능을 보여주는데, 내륙과 해양 상황 모두에서 다른 모델들을 능가했어. 코드도 여기에서 확인할 수 있어: 이 URL.

================================================================================

URL:
https://arxiv.org/pdf/2409.07984.pdf

Title: SPARK: Self-supervised Personalized Real-time Monocular Face Capture

Original Abstract:
Feedforward monocular face capture methods seek to reconstruct posed faces from a single image of a person. Current state of the art approaches have the ability to regress parametric 3D face models in real-time across a wide range of identities, lighting conditions and poses by leveraging large image datasets of human faces. These methods however suffer from clear limitations in that the underlying parametric face model only provides a coarse estimation of the face shape, thereby limiting their practical applicability in tasks that require precise 3D reconstruction (aging, face swapping, digital make-up, ...). In this paper, we propose a method for high-precision 3D face capture taking advantage of a collection of unconstrained videos of a subject as prior information. Our proposal builds on a two stage approach. We start with the reconstruction of a detailed 3D face avatar of the person, capturing both precise geometry and appearance from a collection of videos. We then use the encoder from a pre-trained monocular face reconstruction method, substituting its decoder with our personalized model, and proceed with transfer learning on the video collection. Using our pre-estimated image formation model, we obtain a more precise self-supervision objective, enabling improved expression and pose alignment. This results in a trained encoder capable of efficiently regressing pose and expression parameters in real-time from previously unseen images, which combined with our personalized geometry model yields more accurate and high fidelity mesh inference. Through extensive qualitative and quantitative evaluation, we showcase the superiority of our final model as compared to state-of-the-art baselines, and demonstrate its generalization ability to unseen pose, expression and lighting.

Translated Abstract:
피드포워드 단안 얼굴 캡처 방법은 한 사람의 사진 한 장으로 자세한 얼굴을 재구성하려고 해. 현재 기술로는 다양한 사람, 조명 조건, 자세에서 실시간으로 파라메트릭 3D 얼굴 모델을 추정할 수 있어. 하지만 이 방법들은 파라메트릭 얼굴 모델이 얼굴 모양을 대충 추정하는 데 그치기 때문에 정밀한 3D 재구성이 필요한 작업(예: 노화, 얼굴 바꾸기, 디지털 메이크업 등)에서는 한계가 있어.

이 논문에서는 제약 없는 여러 비디오를 활용해서 고정밀 3D 얼굴 캡처 방법을 제안해. 우리의 방법은 두 단계로 이루어져 있어. 먼저, 여러 비디오에서 정확한 기하학과 외형을 캡처해서 그 사람의 상세한 3D 얼굴 아바타를 재구성해. 그 다음, 사전 학습된 단안 얼굴 재구성 방법의 인코더를 사용하고, 디코더는 우리 개인화된 모델로 바꿔. 이렇게 해서 비디오 컬렉션에 대해 전이 학습을 진행해.

우리의 사전 추정된 이미지 형성 모델을 사용하면 더 정확한 자기 지도 목표를 얻을 수 있어서 표정과 자세 정렬이 개선돼. 결과적으로, 훈련된 인코더는 이전에 본 적 없는 이미지에서 실시간으로 자세와 표정 파라미터를 효율적으로 추정할 수 있게 돼. 이렇게 개인화된 기하 모델과 결합하면 더 정확하고 높은 충실도의 메쉬 추론이 가능해져.

광범위한 정성적 및 정량적 평가를 통해, 우리의 최종 모델이 최신 기술 기준에 비해 우수함을 보여주고, 새로운 자세, 표정 및 조명에서도 잘 일반화된다는 걸 증명해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07989.pdf

Title: Enhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms

Original Abstract:
In the context of few-shot classification, the goal is to train a classifier using a limited number of samples while maintaining satisfactory performance. However, traditional metric-based methods exhibit certain limitations in achieving this objective. These methods typically rely on a single distance value between the query feature and support feature, thereby overlooking the contribution of shallow features. To overcome this challenge, we propose a novel approach in this paper. Our approach involves utilizing multi-output embedding network that maps samples into distinct feature spaces. The proposed method extract feature vectors at different stages, enabling the model to capture both global and abstract features. By utilizing these diverse feature spaces, our model enhances its performance. Moreover, employing a self-attention mechanism improves the refinement of features at each stage, leading to even more robust representations and improved overall performance. Furthermore, assigning learnable weights to each stage significantly improved performance and results. We conducted comprehensive evaluations on the MiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way 5-shot scenarios. Additionally, we performed a cross-domain task from MiniImageNet to the CUB dataset, achieving high accuracy in the testing domain. These evaluations demonstrate the efficacy of our proposed method in comparison to state-of-the-art approaches. this https URL

Translated Abstract:
몇 장의 샘플만으로 분류기를 훈련시키는 몇 장 분류(few-shot classification)에서는 제한된 샘플로도 괜찮은 성능을 내는 게 목표야. 하지만 전통적인 거리 기반 방법들은 이 목표를 달성하는 데 한계가 있어. 이 방법들은 일반적으로 쿼리 특징(query feature)과 지원 특징(support feature) 간의 단일 거리 값에 의존하거든. 그래서 얕은 특징의 기여를 간과하게 돼. 

이 문제를 해결하기 위해, 우리는 새로운 접근 방식을 제안해. 우리의 방법은 샘플을 서로 다른 특징 공간으로 매핑하는 다중 출력 임베딩 네트워크(multi-output embedding network)를 활용해. 이 방식은 다양한 단계에서 특징 벡터를 추출할 수 있어서 모델이 전역적(global)이고 추상적인 특징을 모두 포착할 수 있게 돼. 이렇게 다양한 특징 공간을 활용함으로써 모델의 성능을 높일 수 있어. 

또한, 각 단계에서 특징을 개선하는 데 자기 주의 메커니즘(self-attention mechanism)을 사용하면 더 강력한 표현과 향상된 전반적인 성능을 얻을 수 있어. 각 단계에 학습 가능한 가중치(learnable weights)를 할당하는 것 또한 성능과 결과를 크게 개선했어. 우리는 MiniImageNet과 FC100 데이터셋에서 5-way 1-shot과 5-way 5-shot 시나리오에 대해 종합적인 평가를 진행했어. 게다가 MiniImageNet에서 CUB 데이터셋으로의 교차 도메인 작업도 수행했는데, 테스트 도메인에서 높은 정확도를 달성했어. 

이 평가들은 우리가 제안한 방법이 최신 기술들과 비교했을 때 효과적이라는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.07995.pdf

Title: Depth Matters: Exploring Deep Interactions of RGB-D for Semantic Segmentation in Traffic Scenes

Original Abstract:
RGB-D has gradually become a crucial data source for understanding complex scenes in assisted driving. However, existing studies have paid insufficient attention to the intrinsic spatial properties of depth maps. This oversight significantly impacts the attention representation, leading to prediction errors caused by attention shift issues. To this end, we propose a novel learnable Depth interaction Pyramid Transformer (DiPFormer) to explore the effectiveness of depth. Firstly, we introduce Depth Spatial-Aware Optimization (Depth SAO) as offset to represent real-world spatial relationships. Secondly, the similarity in the feature space of RGB-D is learned by Depth Linear Cross-Attention (Depth LCA) to clarify spatial differences at the pixel level. Finally, an MLP Decoder is utilized to effectively fuse multi-scale features for meeting real-time requirements. Comprehensive experiments demonstrate that the proposed DiPFormer significantly addresses the issue of attention misalignment in both road detection (+7.5%) and semantic segmentation (+4.9% / +1.5%) tasks. DiPFormer achieves state-of-the-art performance on the KITTI (97.57% F-score on KITTI road and 68.74% mIoU on KITTI-360) and Cityscapes (83.4% mIoU) datasets.

Translated Abstract:
RGB-D는 보조 운전에서 복잡한 장면을 이해하는 데 점점 더 중요한 데이터 소스가 되고 있어. 그런데 기존 연구들은 깊이 맵의 고유한 공간 특성에 충분히 주목하지 않았어. 이걸 간과하면 주의 표현에 큰 영향을 미쳐서 주의 이동 문제로 인해 예측 오류가 발생하게 돼. 그래서 우리는 깊이를 탐구하는 새로운 학습 가능한 Depth interaction Pyramid Transformer (DiPFormer)를 제안해.

먼저, Depth Spatial-Aware Optimization (Depth SAO)를 도입해서 실제 세계의 공간 관계를 나타내는 오프셋을 만들어. 다음으로, RGB-D의 특징 공간에서 유사성을 Depth Linear Cross-Attention (Depth LCA)를 통해 학습해서 픽셀 수준에서 공간 차이를 명확히 해. 마지막으로, MLP Decoder를 사용해서 다양한 규모의 특징을 효과적으로 결합해 실시간 요구 사항을 충족시켜.

종합적인 실험 결과, 제안한 DiPFormer는 도로 탐지에서는 +7.5%, 의미 분할에서는 +4.9% / +1.5%의 개선을 보여주며 주의 불일치 문제를 크게 해결했어. DiPFormer는 KITTI(도로에서 97.57% F-score, KITTI-360에서 68.74% mIoU)와 Cityscapes(83.4% mIoU) 데이터셋에서 최첨단 성능을 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08026.pdf

Title: Scribble-Guided Diffusion for Training-free Text-to-Image Generation

Original Abstract:
Recent advancements in text-to-image diffusion models have demonstrated remarkable success, yet they often struggle to fully capture the user's intent. Existing approaches using textual inputs combined with bounding boxes or region masks fall short in providing precise spatial guidance, often leading to misaligned or unintended object orientation. To address these limitations, we propose Scribble-Guided Diffusion (ScribbleDiff), a training-free approach that utilizes simple user-provided scribbles as visual prompts to guide image generation. However, incorporating scribbles into diffusion models presents challenges due to their sparse and thin nature, making it difficult to ensure accurate orientation alignment. To overcome these challenges, we introduce moment alignment and scribble propagation, which allow for more effective and flexible alignment between generated images and scribble inputs. Experimental results on the PASCAL-Scribble dataset demonstrate significant improvements in spatial control and consistency, showcasing the effectiveness of scribble-based guidance in diffusion models. Our code is available at this https URL.

Translated Abstract:
최근 텍스트-이미지 확산 모델이 큰 성공을 거두고 있지만, 사용자 의도를 완전히 반영하는 데는 한계가 있어. 기존의 방법들은 텍스트 입력과 함께 바운딩 박스나 영역 마스크를 사용하는데, 이게 정확한 공간 지침을 제공하지 못해서 종종 물체의 방향이 맞지 않거나 의도와 다르게 생성되는 경우가 많아. 

이런 문제를 해결하기 위해 우리는 Scribble-Guided Diffusion (ScribbleDiff)라는 방법을 제안해. 이건 사용자 제공의 간단한 낙서를 시각적 프롬프트로 활용해서 이미지 생성을 안내하는 훈련이 필요 없는 접근 방식이야. 하지만 낙서를 확산 모델에 포함시키는 건 도전이 있어. 왜냐하면 낙서가 드문드문하고 얇아서 정확한 방향 정렬을 보장하기가 어려워.

이런 문제를 극복하기 위해 우리는 순간 정렬(moment alignment)과 낙서 전파(scribble propagation)를 도입했어. 이 방법들은 생성된 이미지와 낙서 입력 간의 정렬을 더 효과적이고 유연하게 할 수 있게 도와줘. PASCAL-Scribble 데이터셋에서의 실험 결과는 공간 제어와 일관성에서 큰 향상을 보여줬고, 확산 모델에서 낙서 기반 지침의 효과iveness를 입증했어. 우리 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08031.pdf

Title: LED: Light Enhanced Depth Estimation at Night

Original Abstract:
Nighttime camera-based depth estimation is a highly challenging task, especially for autonomous driving applications, where accurate depth perception is essential for ensuring safe navigation. We aim to improve the reliability of perception systems at night time, where models trained on daytime data often fail in the absence of precise but costly LiDAR sensors. In this work, we introduce Light Enhanced Depth (LED), a novel cost-effective approach that significantly improves depth estimation in low-light environments by harnessing a pattern projected by high definition headlights available in modern vehicles. LED leads to significant performance boosts across multiple depth-estimation architectures (encoder-decoder, Adabins, DepthFormer) both on synthetic and real datasets. Furthermore, increased performances beyond illuminated areas reveal a holistic enhancement in scene understanding. Finally, we release the Nighttime Synthetic Drive Dataset, a new synthetic and photo-realistic nighttime dataset, which comprises 49,990 comprehensively annotated images.

Translated Abstract:
야간 카메라 기반 깊이 추정은 특히 자율주행 차량에선 정말 어려운 작업이야. 안전한 네비게이션을 위해선 정확한 깊이 인지가 필수인데, 주간 데이터로 훈련된 모델들이 밤에는 잘 작동하지 않고, 비싼 LiDAR 센서 없이는 정확한 결과를 내기 힘들어.

이 연구에서는 Light Enhanced Depth (LED)라는 새로운 방법을 소개해. 이 방법은 현대 차량의 고해상도 헤드라이트가 비추는 패턴을 활용해서 저조도 환경에서 깊이 추정을 크게 개선해. LED는 여러 깊이 추정 아키텍처(인코더-디코더, Adabins, DepthFormer)에서 합성 데이터와 실제 데이터 모두에서 성능을 크게 향상시켜.

게다가, 조명이 있는 지역을 넘어선 성능 향상은 장면 이해의 전반적인 개선을 보여줘. 마지막으로, 우리는 49,990개의 자세히 주석이 달린 이미지로 구성된 새로운 합성 야간 데이터셋인 Nighttime Synthetic Drive Dataset를 공개해.

================================================================================

URL:
https://arxiv.org/pdf/2409.08042.pdf

Title: Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared Novel-view Synthesis

Original Abstract:
Novel-view synthesis based on visible light has been extensively studied. In comparison to visible light imaging, thermal infrared imaging offers the advantage of all-weather imaging and strong penetration, providing increased possibilities for reconstruction in nighttime and adverse weather scenarios. However, thermal infrared imaging is influenced by physical characteristics such as atmospheric transmission effects and thermal conduction, hindering the precise reconstruction of intricate details in thermal infrared scenes, manifesting as issues of floaters and indistinct edge features in synthesized images. To address these limitations, this paper introduces a physics-induced 3D Gaussian splatting method named Thermal3D-GS. Thermal3D-GS begins by modeling atmospheric transmission effects and thermal conduction in three-dimensional media using neural networks. Additionally, a temperature consistency constraint is incorporated into the optimization objective to enhance the reconstruction accuracy of thermal infrared images. Furthermore, to validate the effectiveness of our method, the first large-scale benchmark dataset for this field named Thermal Infrared Novel-view Synthesis Dataset (TI-NSD) is created. This dataset comprises 20 authentic thermal infrared video scenes, covering indoor, outdoor, and UAV(Unmanned Aerial Vehicle) scenarios, totaling 6,664 frames of thermal infrared image data. Based on this dataset, this paper experimentally verifies the effectiveness of Thermal3D-GS. The results indicate that our method outperforms the baseline method with a 3.03 dB improvement in PSNR and significantly addresses the issues of floaters and indistinct edge features present in the baseline method. Our dataset and codebase will be released in \href{this https URL}{\textcolor{red}{Thermal3DGS}}.

Translated Abstract:
가시광 기반의 새로운 시점 합성에 대한 연구는 많이 진행됐어. 가시광 이미징과 비교했을 때, 열 적외선 이미징은 모든 날씨에서 이미지를 얻을 수 있고, 강한 침투력을 제공하는 장점이 있어. 그래서 밤이나 악천후에서도 재구성이 더 가능해. 

하지만 열 적외선 이미지는 대기 전파 효과나 열 전도 같은 물리적 특성의 영향을 받기 때문에, 복잡한 세부 사항을 정확히 재구성하는 데 방해가 돼. 이로 인해 생성된 이미지에서는 부유물이나 불명확한 경계 특징 같은 문제가 발생해. 

이런 한계를 해결하기 위해, 이 논문에서는 Thermal3D-GS라는 물리 기반 3D 가우시안 스플래팅 방법을 소개해. Thermal3D-GS는 먼저 신경망을 활용해 3D 매체에서 대기 전파 효과와 열 전도를 모델링해. 그리고 재구성 정확도를 높이기 위해 온도 일관성 제약 조건을 최적화 목표에 추가했어.

또한, 우리 방법의 효과를 검증하기 위해 Thermal Infrared Novel-view Synthesis Dataset (TI-NSD)라는 대규모 벤치마크 데이터셋을 만들었어. 이 데이터셋은 실내, 실외, UAV(무인 항공기) 시나리오를 포함한 20개의 실제 열 적외선 비디오 장면으로 구성되어 있고, 총 6,664 프레임의 열 적외선 이미지 데이터를 담고 있어.

이 데이터셋을 기반으로 Thermal3D-GS의 효과를 실험적으로 검증했어. 결과는 우리 방법이 기준 방법보다 3.03 dB 더 나은 PSNR을 보여주면서, 기준 방법에서 나타나는 부유물이나 불명확한 경계 특징 문제를 크게 해결한다는 걸 보여줬어. 우리의 데이터셋과 코드베이스는 \href{this https URL}{\textcolor{red}{Thermal3DGS}}에서 공개할 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08056.pdf

Title: Expansive Supervision for Neural Radiance Field

Original Abstract:
Neural Radiance Fields have achieved success in creating powerful 3D media representations with their exceptional reconstruction capabilities. However, the computational demands of volume rendering pose significant challenges during model training. Existing acceleration techniques often involve redesigning the model architecture, leading to limitations in compatibility across different frameworks. Furthermore, these methods tend to overlook the substantial memory costs incurred. In response to these challenges, we introduce an expansive supervision mechanism that efficiently balances computational load, rendering quality and flexibility for neural radiance field training. This mechanism operates by selectively rendering a small but crucial subset of pixels and expanding their values to estimate the error across the entire area for each iteration. Compare to conventional supervision, our method effectively bypasses redundant rendering processes, resulting in notable reductions in both time and memory consumption. Experimental results demonstrate that integrating expansive supervision within existing state-of-the-art acceleration frameworks can achieve 69% memory savings and 42% time savings, with negligible compromise in visual quality.

Translated Abstract:
신경 복사장(Neural Radiance Fields)은 뛰어난 재구축 능력 덕분에 강력한 3D 미디어 표현을 만드는 데 성공했어. 하지만 볼륨 렌더링의 계산 요구가 모델 훈련 중에 큰 도전 과제가 돼. 기존의 가속 기법들은 보통 모델 구조를 다시 설계하는 방식인데, 이러면 다양한 프레임워크에서 호환성에 제한이 생겨. 게다가, 이런 방법들은 상당한 메모리 비용을 간과하는 경향이 있어.

이런 문제들에 대응하기 위해, 우리는 계산 부하, 렌더링 품질, 유연성을 효율적으로 조절하는 확장된 감독 메커니즘을 소개해. 이 메커니즘은 소수의 중요한 픽셀만 선택적으로 렌더링하고, 그 값을 확장해 전체 영역에 대한 오류를 추정하는 방식으로 작동해. 전통적인 감독 방식과 비교했을 때, 우리 방법은 불필요한 렌더링 과정을 효과적으로 피할 수 있어서 시간과 메모리 소모를 크게 줄일 수 있어.

실험 결과에 따르면, 기존의 최첨단 가속 프레임워크에 확장된 감독을 통합하면 메모리를 69% 절약하고, 시간을 42% 줄일 수 있어. 시각적 품질 또한 거의 손해가 없어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08077.pdf

Title: Diffusion-Based Image-to-Image Translation by Noise Correction via Prompt Interpolation

Original Abstract:
We propose a simple but effective training-free approach tailored to diffusion-based image-to-image translation. Our approach revises the original noise prediction network of a pretrained diffusion model by introducing a noise correction term. We formulate the noise correction term as the difference between two noise predictions; one is computed from the denoising network with a progressive interpolation of the source and target prompt embeddings, while the other is the noise prediction with the source prompt embedding. The final noise prediction network is given by a linear combination of the standard denoising term and the noise correction term, where the former is designed to reconstruct must-be-preserved regions while the latter aims to effectively edit regions of interest relevant to the target prompt. Our approach can be easily incorporated into existing image-to-image translation methods based on diffusion models. Extensive experiments verify that the proposed technique achieves outstanding performance with low latency and consistently improves existing frameworks when combined with them.

Translated Abstract:
우리는 확산 기반 이미지-투-이미지 변환을 위해 간단하지만 효과적인 훈련 없는 접근 방식을 제안해. 이 방식은 미리 훈련된 확산 모델의 원래 노이즈 예측 네트워크를 수정해서 노이즈 보정 항을 추가하는 거야.

노이즈 보정 항은 두 개의 노이즈 예측의 차이로 정의돼. 하나는 소스와 타겟 프롬프트 임베딩의 점진적 보간을 사용해서 denoising 네트워크에서 계산된 노이즈고, 다른 하나는 소스 프롬프트 임베딩을 사용한 노이즈 예측이야. 최종 노이즈 예측 네트워크는 일반적인 denoising 항과 노이즈 보정 항의 선형 조합으로 구성돼. 일반적인 denoising 항은 꼭 보존해야 할 영역을 재구성하는 데 초점을 맞추고, 노이즈 보정 항은 타겟 프롬프트와 관련된 관심 영역을 효과적으로 편집하도록 설계됐어.

우리의 접근 방식은 기존의 확산 모델 기반 이미지-투-이미지 변환 방법에 쉽게 통합될 수 있어. 많은 실험을 통해 이 기술이 낮은 지연 시간으로 뛰어난 성능을 보여주고, 기존 프레임워크와 결합했을 때 일관되게 성능을 개선한다는 걸 확인했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08083.pdf

Title: SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality

Original Abstract:
Foundation models like ChatGPT and Sora that are trained on a huge scale of data have made a revolutionary social impact. However, it is extremely challenging for sensors in many different fields to collect similar scales of natural images to train strong foundation models. To this end, this work presents a simple and effective framework SimMAT to study an open problem: the transferability from vision foundation models trained on natural RGB images to other image modalities of different physical properties (e.g., polarization). SimMAT consists of a modality-agnostic transfer layer (MAT) and a pretrained foundation model. We apply SimMAT to a representative vision foundation model Segment Anything Model (SAM) to support any evaluated new image modality. Given the absence of relevant benchmarks, we construct a new benchmark to evaluate the transfer learning performance. Our experiments confirm the intriguing potential of transferring vision foundation models in enhancing other sensors' performance. Specifically, SimMAT can improve the segmentation performance (mIoU) from 22.15% to 53.88% on average for evaluated modalities and consistently outperforms other baselines. We hope that SimMAT can raise awareness of cross-modal transfer learning and benefit various fields for better results with vision foundation models.

Translated Abstract:
기초 모델인 ChatGPT와 Sora 같은 것들이 대규모 데이터로 훈련되면서 혁신적인 사회적 영향을 미쳤어. 하지만, 다양한 분야의 센서들이 비슷한 규모의 자연 이미지를 수집해서 강력한 기초 모델을 훈련하는 것은 정말 어려워. 

그래서 이 연구에서는 SimMAT이라는 간단하고 효과적인 프레임워크를 제안해. 이건 자연 RGB 이미지로 훈련된 비전 기초 모델이 다른 물리적 성질을 가진 이미지 모달리티(예: 편광)로 얼마나 잘 전이될 수 있는지를 연구하는 열린 문제야. SimMAT은 모달리티에 구애받지 않는 전이 레이어(MAT)와 미리 훈련된 기초 모델로 구성돼. 우리는 SimMAT을 대표적인 비전 기초 모델인 Segment Anything Model(SAM)에 적용해서 새로운 이미지 모달리티를 지원해.

관련 벤치마크가 없어서, 우리는 전이 학습 성능을 평가하기 위한 새로운 벤치마크를 만들었어. 실험 결과, 비전 기초 모델을 다른 센서의 성능 향상에 활용할 수 있는 흥미로운 가능성을 확인했어. 특히, SimMAT은 평가된 모달리티에서 평균적으로 세분화 성능(mIoU)을 22.15%에서 53.88%로 향상시킬 수 있었고, 다른 기준선보다 항상 더 나은 성능을 보였어. 우리는 SimMAT이 교차 모달 전이 학습에 대한 인식을 높이고, 비전 기초 모델로 다양한 분야에서 더 나은 결과를 얻는 데 도움이 되길 바래.

================================================================================

URL:
https://arxiv.org/pdf/2409.08091.pdf

Title: EZIGen: Enhancing zero-shot subject-driven image generation with precise subject encoding and decoupled guidance

Original Abstract:
Zero-shot subject-driven image generation aims to produce images that incorporate a subject from a given example image. The challenge lies in preserving the subject's identity while aligning with the text prompt, which often requires modifying certain aspects of the subject's appearance. Despite advancements in diffusion model based methods, existing approaches still struggle to balance identity preservation with text prompt alignment. In this study, we conducted an in-depth investigation into this issue and uncovered key insights for achieving effective identity preservation while maintaining a strong balance. Our key findings include: (1) the design of the subject image encoder significantly impacts identity preservation quality, and (2) generating an initial layout is crucial for both text alignment and identity preservation. Building on these insights, we introduce a new approach called EZIGen, which employs two main strategies: a carefully crafted subject image Encoder based on the UNet architecture of the pretrained Stable Diffusion model to ensure high-quality identity transfer, following a process that decouples the guidance stages and iteratively refines the initial image layout. Through these strategies, EZIGen achieves state-of-the-art results on multiple subject-driven benchmarks with a unified model and 100 times less training data.

Translated Abstract:
제로샷 주제 기반 이미지 생성은 주어진 예시 이미지에서 주제를 포함한 이미지를 만드는 걸 목표로 해. 이 과정에서 주제의 정체성을 유지하면서 텍스트 프롬프트와 맞추는 게 어려운데, 종종 주제의 외형을 일부 수정해야 해. 확산 모델 기반의 방법들이 발전했지만, 기존 방법들은 여전히 정체성 유지와 텍스트 프롬프트 정렬 사이의 균형을 잡는 데 어려움을 겪고 있어.

이번 연구에서는 이 문제를 깊이 있게 조사했고, 효과적으로 정체성을 유지하면서도 균형을 잘 맞출 수 있는 주요 통찰을 발견했어. 우리의 주요 발견은 다음과 같아: (1) 주제 이미지 인코더의 설계가 정체성 유지 품질에 큰 영향을 미친다는 것과, (2) 초기 레이아웃을 생성하는 것이 텍스트 정렬과 정체성 유지 모두에 중요하다는 거야.

이 통찰을 바탕으로, 우리는 EZIGen이라는 새로운 접근 방식을 소개해. EZIGen은 두 가지 주요 전략을 사용해: 하나는 정체성 전이를 높이기 위해 사전 학습된 스테이블 확산 모델의 UNet 아키텍처를 기반으로 한 주제 이미지 인코더를 정교하게 설계하고, 다른 하나는 가이던스 단계를 분리하고 초기 이미지 레이아웃을 반복적으로 정제하는 과정을 따르는 거야. 이러한 전략을 통해 EZIGen은 통합 모델과 100배 적은 훈련 데이터로 여러 주제 기반 벤치마크에서 최첨단 결과를 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08102.pdf

Title: Bayesian Self-Training for Semi-Supervised 3D Segmentation

Original Abstract:
3D segmentation is a core problem in computer vision and, similarly to many other dense prediction tasks, it requires large amounts of annotated data for adequate training. However, densely labeling 3D point clouds to employ fully-supervised training remains too labor intensive and expensive. Semi-supervised training provides a more practical alternative, where only a small set of labeled data is given, accompanied by a larger unlabeled set. This area thus studies the effective use of unlabeled data to reduce the performance gap that arises due to the lack of annotations. In this work, inspired by Bayesian deep learning, we first propose a Bayesian self-training framework for semi-supervised 3D semantic segmentation. Employing stochastic inference, we generate an initial set of pseudo-labels and then filter these based on estimated point-wise uncertainty. By constructing a heuristic $n$-partite matching algorithm, we extend the method to semi-supervised 3D instance segmentation, and finally, with the same building blocks, to dense 3D visual grounding. We demonstrate state-of-the-art results for our semi-supervised method on SemanticKITTI and ScribbleKITTI for 3D semantic segmentation and on ScanNet and S3DIS for 3D instance segmentation. We further achieve substantial improvements in dense 3D visual grounding over supervised-only baselines on ScanRefer. Our project page is available at this http URL.

Translated Abstract:
3D 분할은 컴퓨터 비전에서 핵심 문제 중 하나야. 다른 밀집 예측 작업들처럼, 제대로 훈련하려면 많은 주석 데이터가 필요해. 하지만 3D 포인트 클라우드를 완전하게 라벨링하는 건 너무 힘들고 비싸. 그래서 반지도 학습이 더 실용적인 대안으로 떠오르고 있어. 이 방법은 적은 양의 라벨이 있는 데이터와 더 많은 라벨이 없는 데이터를 사용하는 거야. 이 연구는 이렇게 라벨이 없는 데이터를 효과적으로 활용해서 주석 부족으로 생기는 성능 차이를 줄이는 방법을 살펴봐.

우리는 베이지안 딥러닝에서 영감을 받아서, 반지도 3D 의미 분할을 위한 베이지안 자기 학습 프레임워크를 제안해. 확률적 추론을 사용해서 초기의 의사 라벨 세트를 만들고, 이걸 점별 불확실성을 기반으로 걸러내. 그리고 휴리스틱 n-부분 매칭 알고리즘을 만들면서 이 방법을 반지도 3D 인스턴스 분할에도 확장해. 마지막으로, 같은 구성 요소를 사용해서 밀집 3D 시각 기초 작업에도 적용했어.

우리는 SemanticKITTI와 ScribbleKITTI에서 3D 의미 분할을 위한 반지도 방법으로 최신 기술을 보여줬고, ScanNet과 S3DIS에서 3D 인스턴스 분할에서도 좋은 성과를 냈어. 또한, ScanRefer에서 감독만으로 한 기준에 비해 밀집 3D 시각 기초 작업에서도 큰 개선을 이뤘어. 프로젝트 페이지는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08156.pdf

Title: MagicStyle: Portrait Stylization Based on Reference Image

Original Abstract:
The development of diffusion models has significantly advanced the research on image stylization, particularly in the area of stylizing a content image based on a given style image, which has attracted many scholars. The main challenge in this reference image stylization task lies in how to maintain the details of the content image while incorporating the color and texture features of the style image. This challenge becomes even more pronounced when the content image is a portrait which has complex textural details. To address this challenge, we propose a diffusion model-based reference image stylization method specifically for portraits, called MagicStyle. MagicStyle consists of two phases: Content and Style DDIM Inversion (CSDI) and Feature Fusion Forward (FFF). The CSDI phase involves a reverse denoising process, where DDIM Inversion is performed separately on the content image and the style image, storing the self-attention query, key and value features of both images during the inversion process. The FFF phase executes forward denoising, harmoniously integrating the texture and color information from the pre-stored feature queries, keys and values into the diffusion generation process based on our Well-designed Feature Fusion Attention (FFA). We conducted comprehensive comparative and ablation experiments to validate the effectiveness of our proposed MagicStyle and FFA.

Translated Abstract:
확산 모델의 발전 덕분에 이미지 스타일화 연구가 많이 발전했어. 특히 주어진 스타일 이미지를 바탕으로 콘텐츠 이미지를 스타일화하는 분야에서 많은 연구자들이 관심을 가지고 있어. 이 작업에서 가장 큰 도전은 콘텐츠 이미지의 세부사항을 유지하면서 스타일 이미지의 색상과 질감을 어떻게 잘 녹여낼 것인가야. 특히 인물 사진처럼 복잡한 질감이 있는 콘텐츠 이미지일 때 이 문제가 더 두드러지지.

이 문제를 해결하기 위해 우리는 MagicStyle이라는 인물 사진 전용의 확산 모델 기반 스타일화 방법을 제안해. MagicStyle은 두 단계로 나뉘어져 있어: 콘텐츠 및 스타일 DDIM 역변환(CSDI)과 특징 융합 전방 진행(FFF)이야. CSDI 단계에서는 역잡음 처리를 진행하는데, 콘텐츠 이미지와 스타일 이미지 각각에 대해 DDIM 역변환을 수행해. 이 과정에서 두 이미지의 자기 주의 쿼리, 키, 값 특징을 저장해.

FFF 단계에서는 저장된 특징 쿼리, 키, 값을 활용해서 질감과 색상 정보를 조화롭게 통합해 확산 생성 과정에서 잘 섞이도록 해. 이때 Well-designed Feature Fusion Attention(FFA)을 기반으로 작업해. 우리는 MagicStyle과 FFA의 효과를 검증하기 위해 다양한 비교 실험과 제거 실험을 진행했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08159.pdf

Title: SDformer: Efficient End-to-End Transformer for Depth Completion

Original Abstract:
Depth completion aims to predict dense depth maps with sparse depth measurements from a depth sensor. Currently, Convolutional Neural Network (CNN) based models are the most popular methods applied to depth completion tasks. However, despite the excellent high-end performance, they suffer from a limited representation area. To overcome the drawbacks of CNNs, a more effective and powerful method has been presented: the Transformer, which is an adaptive self-attention setting sequence-to-sequence model. While the standard Transformer quadratically increases the computational cost from the key-query dot-product of input resolution which improperly employs depth completion tasks. In this work, we propose a different window-based Transformer architecture for depth completion tasks named Sparse-to-Dense Transformer (SDformer). The network consists of an input module for the depth map and RGB image features extraction and concatenation, a U-shaped encoder-decoder Transformer for extracting deep features, and a refinement module. Specifically, we first concatenate the depth map features with the RGB image features through the input model. Then, instead of calculating self-attention with the whole feature maps, we apply different window sizes to extract the long-range depth dependencies. Finally, we refine the predicted features from the input module and the U-shaped encoder-decoder Transformer module to get the enriching depth features and employ a convolution layer to obtain the dense depth map. In practice, the SDformer obtains state-of-the-art results against the CNN-based depth completion models with lower computing loads and parameters on the NYU Depth V2 and KITTI DC datasets.

Translated Abstract:
깊이 완성(depth completion)은 깊이 센서에서 얻은 희소한 깊이 측정값을 바탕으로 밀집 깊이 맵을 예측하는 작업이야. 현재 깊이 완성 작업에 가장 많이 쓰이는 방법은 컨볼루션 신경망(CNN) 기반 모델들이야. 하지만 CNN은 성능은 뛰어나지만 표현 영역이 제한되는 단점이 있어. 

이런 CNN의 단점을 극복하기 위해 더 효과적이고 강력한 방법인 트랜스포머(Transformer)가 등장했어. 트랜스포머는 적응형 자기 주의(attention) 설정을 사용하는 시퀀스-투-시퀀스 모델이야. 하지만 표준 트랜스포머는 입력 해상도에 따라 계산 비용이 기하급수적으로 증가해서 깊이 완성 작업에 적합하지 않아. 

이번 연구에서는 깊이 완성 작업을 위해 '스파스-투-덴스 트랜스포머(SDformer)'라는 새로운 윈도우 기반 트랜스포머 구조를 제안해. 이 네트워크는 깊이 맵과 RGB 이미지 특징을 추출하고 결합하는 입력 모듈, 깊은 특징을 추출하는 U자형 인코더-디코더 트랜스포머, 그리고 정제 모듈로 구성돼. 

먼저 입력 모듈을 통해 깊이 맵 특징과 RGB 이미지 특징을 결합해. 그 다음, 전체 특징 맵을 사용해 자기 주의를 계산하는 대신, 다양한 윈도우 크기를 적용해서 긴 거리의 깊이 의존성을 추출해. 마지막으로 입력 모듈과 U자형 인코더-디코더 트랜스포머 모듈에서 예측된 특징을 정제해서 풍부한 깊이 특징을 얻고, 컨볼루션 층을 사용해 밀집 깊이 맵을 만들어. 

실제로 SDformer는 NYU Depth V2와 KITTI DC 데이터셋에서 CNN 기반 깊이 완성 모델에 비해 더 적은 계산 부담과 파라미터로 최첨단 결과를 얻었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08162.pdf

Title: Cross-Attention Based Influence Model for Manual and Nonmanual Sign Language Analysis

Original Abstract:
Both manual (relating to the use of hands) and non-manual markers (NMM), such as facial expressions or mouthing cues, are important for providing the complete meaning of phrases in American Sign Language (ASL). Efforts have been made in advancing sign language to spoken/written language understanding, but most of these have primarily focused on manual features only. In this work, using advanced neural machine translation methods, we examine and report on the extent to which facial expressions contribute to understanding sign language phrases. We present a sign language translation architecture consisting of two-stream encoders, with one encoder handling the face and the other handling the upper body (with hands). We propose a new parallel cross-attention decoding mechanism that is useful for quantifying the influence of each input modality on the output. The two streams from the encoder are directed simultaneously to different attention stacks in the decoder. Examining the properties of the parallel cross-attention weights allows us to analyze the importance of facial markers compared to body and hand features during a translating task.

Translated Abstract:
수동적인 손 동작과 비수동적인 표지, 예를 들어 얼굴 표정이나 입 모양 같은 것들은 미국 수화(ASL)에서 구문이 완전한 의미를 전달하는 데 중요해. 수화를 음성이나 문자 언어로 이해하는 연구가 진행되고 있지만, 대부분은 손 동작에만 집중했어.

이번 연구에서는 최신 신경 기계 번역 방법을 사용해서 얼굴 표정이 수화 구문 이해에 얼마나 기여하는지를 살펴봤어. 우리는 얼굴을 처리하는 인코더와 상체(손 포함)를 처리하는 인코더로 이루어진 두 개의 스트림 인코더 구조를 제안해.

또한, 각 입력 모달리티가 출력에 미치는 영향을 정량화할 수 있는 새로운 병렬 크로스 어텐션 디코딩 메커니즘을 제안했어. 인코더의 두 스트림은 디코더의 서로 다른 어텐션 스택으로 동시에 전달돼. 병렬 크로스 어텐션 가중치의 특성을 분석함으로써, 번역 작업 중 얼굴 표지가 몸이나 손 특징에 비해 얼마나 중요한지를 분석할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08167.pdf

Title: High-Frequency Anti-DreamBooth: Robust Defense Against Image Synthesis

Original Abstract:
Recently, text-to-image generative models have been misused to create unauthorized malicious images of individuals, posing a growing social problem. Previous solutions, such as Anti-DreamBooth, add adversarial noise to images to protect them from being used as training data for malicious generation. However, we found that the adversarial noise can be removed by adversarial purification methods such as DiffPure. Therefore, we propose a new adversarial attack method that adds strong perturbation on the high-frequency areas of images to make it more robust to adversarial purification. Our experiment showed that the adversarial images retained noise even after adversarial purification, hindering malicious image generation.

Translated Abstract:
최근에 텍스트-이미지 생성 모델이 악용돼서 개인의 무단 악성 이미지가 만들어지는 문제가 커지고 있어. 이전의 해결책인 Anti-DreamBooth는 이미지에 적대적 노이즈를 추가해서 악성 생성에 이용되는 걸 막으려고 했어. 

근데 우리가 발견한 건, 이 적대적 노이즈가 DiffPure 같은 적대적 정화 방법으로 제거될 수 있다는 거였어. 그래서 우리는 새로운 적대적 공격 방법을 제안해. 이 방법은 이미지의 고주파 영역에 강한 변화를 주어서 적대적 정화에 강해지도록 하는 거야. 

실험 결과, 적대적 이미지가 적대적 정화 후에도 노이즈를 계속 유지해서 악성 이미지 생성이 방해된다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08169.pdf

Title: Learning to Match 2D Keypoints Across Preoperative MR and Intraoperative Ultrasound

Original Abstract:
We propose in this paper a texture-invariant 2D keypoints descriptor specifically designed for matching preoperative Magnetic Resonance (MR) images with intraoperative Ultrasound (US) images. We introduce a matching-by-synthesis strategy, where intraoperative US images are synthesized from MR images accounting for multiple MR modalities and intraoperative US variability. We build our training set by enforcing keypoints localization over all images then train a patient-specific descriptor network that learns texture-invariant discriminant features in a supervised contrastive manner, leading to robust keypoints descriptors. Our experiments on real cases with ground truth show the effectiveness of the proposed approach, outperforming the state-of-the-art methods and achieving 80.35% matching precision on average.

Translated Abstract:
이 논문에서는 수술 전 자기공명영상(MR)과 수술 중 초음파영상(US)을 매칭하기 위해 특별히 설계된 질감 불변 2D 키포인트 설명자를 제안해. 

여기서 매칭-합성 전략을 도입했어. 이 전략은 여러 MR 모달리티와 수술 중 US의 변동성을 고려해서 MR 이미지로부터 수술 중 US 이미지를 합성하는 방식이야. 

우리는 모든 이미지에서 키포인트 위치를 강제로 지정해서 훈련 세트를 만들고, 이후에는 환자 특화 설명자 네트워크를 훈련해. 이 네트워크는 질감 불변의 판별 특징을 학습하는데, 감독된 대조 방식으로 이루어져서 강력한 키포인트 설명자를 생성해. 

실제 사례에 대한 실험 결과, 제안한 방법이 효과적임을 보여주었고, 최신 기술보다 더 높은 성능을 보였어. 평균적으로 80.35%의 매칭 정확도를 달성했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08171.pdf

Title: Low-Cost Tree Crown Dieback Estimation Using Deep Learning-Based Segmentation

Original Abstract:
The global increase in observed forest dieback, characterised by the death of tree foliage, heralds widespread decline in forest ecosystems. This degradation causes significant changes to ecosystem services and functions, including habitat provision and carbon sequestration, which can be difficult to detect using traditional monitoring techniques, highlighting the need for large-scale and high-frequency monitoring. Contemporary developments in the instruments and methods to gather and process data at large-scales mean this monitoring is now possible. In particular, the advancement of low-cost drone technology and deep learning on consumer-level hardware provide new opportunities. Here, we use an approach based on deep learning and vegetation indices to assess crown dieback from RGB aerial data without the need for expensive instrumentation such as LiDAR. We use an iterative approach to match crown footprints predicted by deep learning with field-based inventory data from a Mediterranean ecosystem exhibiting drought-induced dieback, and compare expert field-based crown dieback estimation with vegetation index-based estimates. We obtain high overall segmentation accuracy (mAP: 0.519) without the need for additional technical development of the underlying Mask R-CNN model, underscoring the potential of these approaches for non-expert use and proving their applicability to real-world conservation. We also find colour-coordinate based estimates of dieback correlate well with expert field-based estimation. Substituting ground truth for Mask R-CNN model predictions showed negligible impact on dieback estimates, indicating robustness. Our findings demonstrate the potential of automated data collection and processing, including the application of deep learning, to improve the coverage, speed and cost of forest dieback monitoring.

Translated Abstract:
전 세계적으로 나무 잎이 죽는 현상인 숲의 고사 현상이 증가하고 있어, 숲 생태계의 광범위한 쇠퇴를 예고하고 있어. 이런 악화는 서식지 제공이나 탄소 격리 같은 생태계 서비스와 기능에 큰 변화를 일으키는데, 전통적인 모니터링 기법으로는 이런 변화를 감지하기 힘들어. 그래서 대규모 고주파 모니터링이 필요해. 

요즘은 대규모로 데이터를 수집하고 처리할 수 있는 도구와 방법이 발전하면서 이런 모니터링이 가능해졌어. 특히, 저렴한 드론 기술과 소비자용 하드웨어에서의 딥러닝 발전이 새로운 기회를 만들어주고 있어. 

우리는 RGB 항공 데이터를 이용해 비싼 장비 없이도 나무의 크라운 고사를 평가하는 딥러닝과 식생 지수를 기반으로 한 접근 방법을 사용했어. 딥러닝으로 예측한 크라운의 형태와 지중해 생태계에서 발생한 가뭄으로 인한 고사 데이터를 비교하는 반복적인 방법을 썼고, 전문가의 현장 기반 크라운 고사 추정과 식생 지수 기반 추정을 비교했어. Mask R-CNN 모델의 추가 기술 개발 없이도 높은 전체 분할 정확도(mAP: 0.519)를 얻었고, 이런 접근 방식이 비전문가도 사용할 수 있는 가능성을 보여주며 실제 보존에 적용 가능하다는 걸 입증했어. 

또한 색상 좌표 기반의 고사 추정치가 전문가의 현장 기반 추정과 잘 일치하는 것도 발견했어. Mask R-CNN 모델 예측을 지상 진실 데이터로 대체해도 고사 추정에 미치는 영향이 미미해서 견고성을 나타냈어. 우리의 연구 결과는 자동화된 데이터 수집과 처리, 특히 딥러닝의 적용이 숲 고사 모니터링의 범위, 속도, 비용을 개선할 수 있는 잠재력을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.08181.pdf

Title: Enhancing Canine Musculoskeletal Diagnoses: Leveraging Synthetic Image Data for Pre-Training AI-Models on Visual Documentations

Original Abstract:
The examination of the musculoskeletal system in dogs is a challenging task in veterinary practice. In this work, a novel method has been developed that enables efficient documentation of a dog's condition through a visual representation. However, since the visual documentation is new, there is no existing training data. The objective of this work is therefore to mitigate the impact of data scarcity in order to develop an AI-based diagnostic support system. To this end, the potential of synthetic data that mimics realistic visual documentations of diseases for pre-training AI models is investigated. We propose a method for generating synthetic image data that mimics realistic visual documentations. Initially, a basic dataset containing three distinct classes is generated, followed by the creation of a more sophisticated dataset containing 36 different classes. Both datasets are used for the pre-training of an AI model. Subsequently, an evaluation dataset is created, consisting of 250 manually created visual documentations for five different diseases. This dataset, along with a subset containing 25 examples. The obtained results on the evaluation dataset containing 25 examples demonstrate a significant enhancement of approximately 10% in diagnosis accuracy when utilizing generated synthetic images that mimic real-world visual documentations. However, these results do not hold true for the larger evaluation dataset containing 250 examples, indicating that the advantages of using synthetic data for pre-training an AI model emerge primarily when dealing with few examples of visual documentations for a given disease. Overall, this work provides valuable insights into mitigating the limitations imposed by limited training data through the strategic use of generated synthetic data, presenting an approach applicable beyond the canine musculoskeletal assessment domain.

Translated Abstract:
개에서 근골격계 시스템을 검사하는 건 수의학에서 어려운 일이야. 이 연구에서는 개의 상태를 시각적으로 나타내서 효율적으로 문서화하는 새로운 방법을 개발했어. 그런데 이 시각적 문서화가 새로워서 기존의 훈련 데이터가 없어. 그래서 이 연구의 목표는 데이터 부족 문제를 해결해서 AI 기반 진단 지원 시스템을 만드는 거야.

이를 위해 질병의 사실적인 시각적 문서화를 모방하는 합성 데이터의 가능성을 조사했어. 우리는 사실적인 시각적 문서화를 모방하는 합성 이미지 데이터를 생성하는 방법을 제안해. 처음에는 세 가지 다른 클래스를 포함하는 기본 데이터셋을 만들고, 그 다음에는 36개의 다양한 클래스를 포함하는 더 복잡한 데이터셋을 만들었어. 이 두 데이터셋은 AI 모델의 사전 훈련에 사용돼.

그 후, 다섯 가지 다른 질병에 대해 250개의 수작업으로 만든 시각적 문서화로 구성된 평가 데이터셋을 만들었어. 이 데이터셋과 25개의 예제를 포함하는 하위 집합도 있어. 25개의 예제를 포함한 평가 데이터셋에서 얻은 결과는 실제 시각적 문서화를 모방한 합성 이미지를 사용할 때 진단 정확도가 약 10% 향상됐다는 걸 보여줘. 하지만 250개의 예제를 포함한 더 큰 평가 데이터셋에서는 이런 결과가 나타나지 않았어. 이는 특정 질병에 대한 시각적 문서화 예제가 적을 때 합성 데이터를 사전 훈련에 사용하는 것의 장점이 주로 나타난다는 걸 의미해.

전반적으로 이 연구는 생성된 합성 데이터를 전략적으로 사용해서 제한된 훈련 데이터의 한계를 완화하는 데 유용한 통찰력을 제공해. 이 접근법은 개의 근골격계 평가 분야를 넘어 적용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08189.pdf

Title: Gaussian Garments: Reconstructing Simulation-Ready Clothing with Photorealistic Appearance from Multi-View Video

Original Abstract:
We introduce Gaussian Garments, a novel approach for reconstructing realistic simulation-ready garment assets from multi-view videos. Our method represents garments with a combination of a 3D mesh and a Gaussian texture that encodes both the color and high-frequency surface details. This representation enables accurate registration of garment geometries to multi-view videos and helps disentangle albedo textures from lighting effects. Furthermore, we demonstrate how a pre-trained graph neural network (GNN) can be fine-tuned to replicate the real behavior of each garment. The reconstructed Gaussian Garments can be automatically combined into multi-garment outfits and animated with the fine-tuned GNN.

Translated Abstract:
우리는 Gaussian Garments라는 새로운 방법을 소개해요. 이 방법은 여러 방향에서 촬영한 비디오를 이용해 실제같은 시뮬레이션 준비가 된 의류 자산을 재구성하는 거예요. 

우리의 방법은 의류를 3D 메쉬와 가우시안 텍스처의 조합으로 표현해요. 이 가우시안 텍스처는 색깔과 고주파 표면 디테일을 모두 담고 있어요. 이렇게 표현하면 의류의 형태를 여러 방향의 비디오에 정확하게 맞출 수 있고, 조명 효과와 알베도 텍스처를 분리할 수 있어요. 

또한, 미리 훈련된 그래프 신경망(GNN)을 어떻게 조정해서 각 의류의 실제 동작을 재현할 수 있는지 보여줄 거예요. 재구성된 Gaussian Garments는 자동으로 여러 의류를 조합할 수 있고, 조정된 GNN으로 애니메이션도 가능해요.

================================================================================

URL:
https://arxiv.org/pdf/2409.08202.pdf

Title: What Makes a Maze Look Like a Maze?

Original Abstract:
A unique aspect of human visual understanding is the ability to flexibly interpret abstract concepts: acquiring lifted rules explaining what they symbolize, grounding them across familiar and unfamiliar contexts, and making predictions or reasoning about them. While off-the-shelf vision-language models excel at making literal interpretations of images (e.g., recognizing object categories such as tree branches), they still struggle to make sense of such visual abstractions (e.g., how an arrangement of tree branches may form the walls of a maze). To address this challenge, we introduce Deep Schema Grounding (DSG), a framework that leverages explicit structured representations of visual abstractions for grounding and reasoning. At the core of DSG are schemas--dependency graph descriptions of abstract concepts that decompose them into more primitive-level symbols. DSG uses large language models to extract schemas, then hierarchically grounds concrete to abstract components of the schema onto images with vision-language models. The grounded schema is used to augment visual abstraction understanding. We systematically evaluate DSG and different methods in reasoning on our new Visual Abstractions Dataset, which consists of diverse, real-world images of abstract concepts and corresponding question-answer pairs labeled by humans. We show that DSG significantly improves the abstract visual reasoning performance of vision-language models, and is a step toward human-aligned understanding of visual abstractions.

Translated Abstract:
인간의 시각적 이해의 독특한 점은 추상적인 개념을 유연하게 해석할 수 있는 능력이야. 즉, 그들이 상징하는 것에 대한 규칙을 배우고, 이를 익숙한 상황과 낯선 상황에 연결시키며, 예측하거나 추론하는 거지. 

기존의 비전-언어 모델들은 이미지의 문자적 해석은 잘하지만 (예: 나뭇가지 같은 객체의 범주를 인식하는 것), 이런 시각적 추상 개념을 이해하는 데는 여전히 어려움이 있어. 예를 들어, 나뭇가지의 배열이 미로의 벽을 형성할 수 있다는 걸 이해하는 건 힘든 거야.

이 문제를 해결하기 위해 우리는 Deep Schema Grounding (DSG)이라는 프레임워크를 도입했어. 이건 시각적 추상 개념을 명확하게 구조화된 표현으로 활용해 grounding과 추론을 하는 거야. DSG의 핵심은 스키마인데, 이건 추상 개념을 더 기본적인 기호로 나누는 의존 그래프 설명이야. DSG는 대형 언어 모델을 사용해 스키마를 추출하고, 그 다음에는 비전-언어 모델을 통해 스키마의 구체적인 요소를 추상적인 요소로 계층적으로 이미지에 연결해.

이렇게 연결된 스키마는 시각적 추상 이해를 향상시키는 데 사용돼. 우리는 DSG와 다양한 추론 방법을 새로운 Visual Abstractions Dataset에서 체계적으로 평가했어. 이 데이터셋은 추상 개념의 다양한 실제 이미지와 인간이 레이블링한 질문-답변 쌍으로 구성되어 있어. 

우리는 DSG가 비전-언어 모델의 추상적 시각 추론 성능을 크게 향상시킨다는 걸 보여줬고, 이는 시각적 추상 개념에 대해 인간과 일치하는 이해로 나아가는 한 걸음이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08206.pdf

Title: ComAlign: Compositional Alignment in Vision-Language Models

Original Abstract:
Vision-language models (VLMs) like CLIP have showcased a remarkable ability to extract transferable features for downstream tasks. Nonetheless, the training process of these models is usually based on a coarse-grained contrastive loss between the global embedding of images and texts which may lose the compositional structure of these modalities. Many recent studies have shown VLMs lack compositional understandings like attribute binding and identifying object relationships. Although some recent methods have tried to achieve finer-level alignments, they either are not based on extracting meaningful components of proper granularity or don't properly utilize the modalities' correspondence (especially in image-text pairs with more ingredients). Addressing these limitations, we introduce Compositional Alignment (ComAlign), a fine-grained approach to discover more exact correspondence of text and image components using only the weak supervision in the form of image-text pairs. Our methodology emphasizes that the compositional structure (including entities and relations) extracted from the text modality must also be retained in the image modality. To enforce correspondence of fine-grained concepts in image and text modalities, we train a lightweight network lying on top of existing visual and language encoders using a small dataset. The network is trained to align nodes and edges of the structure across the modalities. Experimental results on various VLMs and datasets demonstrate significant improvements in retrieval and compositional benchmarks, affirming the effectiveness of our plugin model.

Translated Abstract:
비전-언어 모델(VLM)인 CLIP 같은 모델은 다운스트림 작업에 필요한 전이 가능한 특징을 잘 추출하는 능력을 보여줬어. 하지만 이런 모델의 훈련 과정은 보통 이미지와 텍스트의 전역 임베딩 사이의 거친 대조 손실을 기반으로 하는데, 이 과정에서 이 두 모달리티의 구성 구조가 사라질 수 있어. 최근 연구들에서 VLM이 속성 연결이나 객체 관계 식별 같은 구성적 이해가 부족하다는 걸 보여줬어. 

최근의 몇 가지 방법들이 더 세밀한 정렬을 시도했지만, 의미 있는 구성 요소를 제대로 추출하지 않거나 모달리티의 연관성을 잘 활용하지 못하고 있어(특히 더 많은 요소가 있는 이미지-텍스트 쌍에서). 이런 한계를 해결하기 위해, 우리는 Compositional Alignment(ComAlign)를 도입해. 이건 이미지-텍스트 쌍의 약한 감독을 이용해 텍스트와 이미지 구성 요소의 정확한 상응 관계를 발견하는 세밀한 접근 방식이야. 

우리 방법론은 텍스트 모달리티에서 추출한 구성 구조(엔티티와 관계 포함)가 이미지 모달리티에서도 보존되어야 한다는 점을 강조해. 이미지와 텍스트 모달리티에서 세밀한 개념의 상응 관계를 강화하기 위해, 우리는 기존의 시각적 및 언어 인코더 위에 가벼운 네트워크를 훈련해. 이 네트워크는 두 모달리티 간의 구조의 노드와 엣지를 정렬하도록 훈련돼. 여러 VLM과 데이터셋에서의 실험 결과는 검색 및 구성 기준에서 상당한 향상을 보여주며, 우리의 플러그인 모델의 효과성을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08207.pdf

Title: VI3DRM:Towards meticulous 3D Reconstruction from Sparse Views via Photo-Realistic Novel View Synthesis

Original Abstract:
Recently, methods like Zero-1-2-3 have focused on single-view based 3D reconstruction and have achieved remarkable success. However, their predictions for unseen areas heavily rely on the inductive bias of large-scale pretrained diffusion models. Although subsequent work, such as DreamComposer, attempts to make predictions more controllable by incorporating additional views, the results remain unrealistic due to feature entanglement in the vanilla latent space, including factors such as lighting, material, and structure. To address these issues, we introduce the Visual Isotropy 3D Reconstruction Model (VI3DRM), a diffusion-based sparse views 3D reconstruction model that operates within an ID consistent and perspective-disentangled 3D latent space. By facilitating the disentanglement of semantic information, color, material properties and lighting, VI3DRM is capable of generating highly realistic images that are indistinguishable from real photographs. By leveraging both real and synthesized images, our approach enables the accurate construction of pointmaps, ultimately producing finely textured meshes or point clouds. On the NVS task, tested on the GSO dataset, VI3DRM significantly outperforms state-of-the-art method DreamComposer, achieving a PSNR of 38.61, an SSIM of 0.929, and an LPIPS of 0.027. Code will be made available upon publication.

Translated Abstract:
최근에 Zero-1-2-3 같은 방법들이 단일 뷰 기반의 3D 재구성에 집중하면서 꽤 성공적인 결과를 얻었어. 하지만 보지 못한 영역에 대한 예측은 대규모로 미리 훈련된 확산 모델의 유도 편향에 크게 의존해. DreamComposer 같은 후속 연구는 추가적인 뷰를 포함시켜서 예측을 더 통제 가능하게 하려고 했지만, 조명, 재질, 구조 같은 요소들이 얽혀 있어서 결과가 여전히 비현실적이야.

이런 문제를 해결하기 위해, 우리는 Visual Isotropy 3D Reconstruction Model (VI3DRM)을 소개해. 이 모델은 ID 일관성과 관점 분리가 잘 된 3D 잠재 공간에서 작동하는 확산 기반의 희소 뷰 3D 재구성 모델이야. VI3DRM은 의미 정보, 색상, 재질 속성, 조명을 분리하는 데 도움을 줘서, 실제 사진과 구분이 안 갈 정도로 매우 사실적인 이미지를 생성할 수 있어.

실제 이미지와 합성된 이미지를 모두 활용함으로써, 우리의 접근 방식은 정확한 포인트 맵을 구성할 수 있게 해주고, 결국 정밀한 텍스처 메쉬나 포인트 클라우드를 만들어내. NVS 작업에서는 GSO 데이터셋을 기반으로 테스트했을 때, VI3DRM이 최신 기술인 DreamComposer보다 훨씬 뛰어난 성능을 보여주었어. PSNR 38.61, SSIM 0.929, LPIPS 0.027을 기록했지. 코드도 출판 후에 공개할 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08215.pdf

Title: LT3SD: Latent Trees for 3D Scene Diffusion

Original Abstract:
We present LT3SD, a novel latent diffusion model for large-scale 3D scene generation. Recent advances in diffusion models have shown impressive results in 3D object generation, but are limited in spatial extent and quality when extended to 3D scenes. To generate complex and diverse 3D scene structures, we introduce a latent tree representation to effectively encode both lower-frequency geometry and higher-frequency detail in a coarse-to-fine hierarchy. We can then learn a generative diffusion process in this latent 3D scene space, modeling the latent components of a scene at each resolution level. To synthesize large-scale scenes with varying sizes, we train our diffusion model on scene patches and synthesize arbitrary-sized output 3D scenes through shared diffusion generation across multiple scene patches. Through extensive experiments, we demonstrate the efficacy and benefits of LT3SD for large-scale, high-quality unconditional 3D scene generation and for probabilistic completion for partial scene observations.

Translated Abstract:
우리는 LT3SD라는 새로운 잠재 확산 모델을 소개해. 이 모델은 대규모 3D 장면 생성을 목표로 하고 있어. 최근 확산 모델의 발전 덕분에 3D 객체 생성에서 괜찮은 결과를 보여줬지만, 3D 장면으로 확장할 때 공간적인 범위와 품질이 제한적이었어.

복잡하고 다양한 3D 장면 구조를 만들기 위해, 우리는 낮은 주파수 기하학과 높은 주파수 세부 사항을 효과적으로 인코딩할 수 있는 잠재 트리 표현을 도입했어. 이렇게 하면, 이 잠재 3D 장면 공간에서 생성적 확산 과정을 학습할 수 있어. 각 해상도 수준에서 장면의 잠재 구성 요소를 모델링하는 거지.

다양한 크기의 대규모 장면을 합성하기 위해, 우리는 장면 패치에서 확산 모델을 훈련시켰고, 여러 장면 패치에서 공유된 확산 생성을 통해 임의 크기의 3D 장면을 생성할 수 있어. 여러 실험을 통해, LT3SD가 대규모 고품질 무조건 3D 장면 생성을 위한 효과성과 이점을 보여준다는 것을 입증했어. 또한, 부분 장면 관찰에 대한 확률적 완성에도 유용하다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08240.pdf

Title: IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation

Original Abstract:
While Text-to-Image (T2I) diffusion models excel at generating visually appealing images of individual instances, they struggle to accurately position and control the features generation of multiple instances. The Layout-to-Image (L2I) task was introduced to address the positioning challenges by incorporating bounding boxes as spatial control signals, but it still falls short in generating precise instance features. In response, we propose the Instance Feature Generation (IFG) task, which aims to ensure both positional accuracy and feature fidelity in generated instances. To address the IFG task, we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances feature depiction by incorporating additional appearance tokens and utilizing an Instance Semantic Map to align instance-level features with spatial locations. The IFAdapter guides the diffusion process as a plug-and-play module, making it adaptable to various community models. For evaluation, we contribute an IFG benchmark and develop a verification pipeline to objectively compare models' abilities to generate instances with accurate positioning and features. Experimental results demonstrate that IFAdapter outperforms other models in both quantitative and qualitative evaluations.

Translated Abstract:
텍스트-이미지(T2I) 확산 모델은 개별 이미지 생성에선 뛰어나지만, 여러 개체의 위치와 특징을 정확하게 조절하는 데는 어려움이 있어. 그래서 Layout-to-Image(L2I) 작업이 도입됐는데, 이건 바운딩 박스를 공간 제어 신호로 사용해서 위치 문제를 해결하려고 했어. 하지만 여전히 개체 특징을 정확하게 생성하는 데는 부족해. 

그래서 우리는 Instance Feature Generation(IFG) 작업을 제안해. 이 작업의 목표는 생성된 개체의 위치 정확성과 특징 충실도를 보장하는 거야. IFG 작업을 다루기 위해 Instance Feature Adapter(IFAdapter)를 도입했어. IFAdapter는 추가적인 외형 토큰을 포함하고, Instance Semantic Map을 활용해서 개체 수준의 특징을 공간적 위치와 맞추는 데 도움을 줘. 이 IFAdapter는 확산 과정에서 플러그 앤 플레이 모듈로 작동해서 다양한 커뮤니티 모델에 쉽게 적용할 수 있어.

평가를 위해 우리는 IFG 벤치마크를 만들고, 모델들이 정확한 위치와 특징으로 개체를 생성하는 능력을 객관적으로 비교할 수 있는 검증 파이프라인을 개발했어. 실험 결과, IFAdapter가 정량적, 정성적 평가 모두에서 다른 모델들을 능가하는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08245.pdf

Title: Style Based Clustering of Visual Artworks

Original Abstract:
Clustering artworks based on style has many potential real-world applications like art recommendations, style-based search and retrieval, and the study of artistic style evolution in an artwork corpus. However, clustering artworks based on style is largely an unaddressed problem. A few present methods for clustering artworks principally rely on generic image feature representations derived from deep neural networks and do not specifically deal with the artistic style. In this paper, we introduce and deliberate over the notion of style-based clustering of visual artworks. Our main objective is to explore neural feature representations and architectures that can be used for style-based clustering and observe their impact and effectiveness. We develop different methods and assess their relative efficacy for style-based clustering through qualitative and quantitative analysis by applying them to four artwork corpora and four curated synthetically styled datasets. Our analysis provides some key novel insights on architectures, feature representations, and evaluation methods suitable for style-based clustering.

Translated Abstract:
예술 작품을 스타일에 따라 클러스터링하는 것은 예술 추천, 스타일 기반 검색 및 검색, 그리고 예술 작품 집합에서 예술 스타일의 진화를 연구하는 등 여러 실제 응용 가능성이 있어. 하지만 스타일에 기반한 작품 클러스터링은 아직 해결되지 않은 문제야. 현재 사용되는 몇 가지 방법은 주로 딥 뉴럴 네트워크에서 파생된 일반 이미지 특징 표현에 의존하고, 예술 스타일에 대해 특별히 다루지 않아.

이 논문에서는 시각 예술 작품의 스타일 기반 클러스터링 개념을 소개하고 논의해. 우리의 주요 목표는 스타일 기반 클러스터링에 사용할 수 있는 뉴럴 특징 표현과 아키텍처를 탐구하고, 그 효과와 영향을 관찰하는 거야. 다양한 방법을 개발하고, 네 가지 예술 작품 집합과 네 가지 큐레이션된 합성 스타일 데이터셋에 적용해서 스타일 기반 클러스터링에 대한 상대적 효능을 정성적 및 정량적으로 분석해. 우리의 분석은 스타일 기반 클러스터링에 적합한 아키텍처, 특징 표현, 평가 방법에 대한 몇 가지 중요한 새로운 통찰을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.08248.pdf

Title: TextBoost: Towards One-Shot Personalization of Text-to-Image Models via Fine-tuning Text Encoder

Original Abstract:
Recent breakthroughs in text-to-image models have opened up promising research avenues in personalized image generation, enabling users to create diverse images of a specific subject using natural language prompts. However, existing methods often suffer from performance degradation when given only a single reference image. They tend to overfit the input, producing highly similar outputs regardless of the text prompt. This paper addresses the challenge of one-shot personalization by mitigating overfitting, enabling the creation of controllable images through text prompts. Specifically, we propose a selective fine-tuning strategy that focuses on the text encoder. Furthermore, we introduce three key techniques to enhance personalization performance: (1) augmentation tokens to encourage feature disentanglement and alleviate overfitting, (2) a knowledge-preservation loss to reduce language drift and promote generalizability across diverse prompts, and (3) SNR-weighted sampling for efficient training. Extensive experiments demonstrate that our approach efficiently generates high-quality, diverse images using only a single reference image while significantly reducing memory and storage requirements.

Translated Abstract:
최근 텍스트-이미지 모델에서 큰 발전이 있었고, 이 덕분에 개인화된 이미지 생성 연구가 활발해졌어. 이제 사용자들이 자연어 프롬프트를 사용해서 특정 주제의 다양한 이미지를 만들 수 있게 됐지. 

하지만 기존 방법들은 단일 참조 이미지만 주어졌을 때 성능이 떨어지는 문제가 있어. 입력에 과적합(overfitting)돼서 텍스트 프롬프트와 상관없이 매우 유사한 결과물을 만들어내는 경향이 있어. 이 논문에서는 이런 일회성 개인화의 문제를 해결하기 위해 과적합을 줄이는 방법을 제안해, 텍스트 프롬프트를 통해 조절 가능한 이미지를 만들 수 있게 했어.

특히, 우리는 텍스트 인코더에 집중한 선택적 미세 조정 전략을 제안해. 그리고 개인화 성능을 높이기 위해 세 가지 주요 기술을 도입했어: (1) 특징 분리를 촉진하고 과적합을 줄이기 위한 증강 토큰, (2) 언어 드리프트를 줄이고 다양한 프롬프트에 대한 일반화를 촉진하기 위한 지식 보존 손실, (3) 효율적인 훈련을 위한 SNR 가중 샘플링. 

많은 실험을 통해, 우리의 접근 방식이 단 하나의 참조 이미지만으로도 높은 품질의 다양한 이미지를 효율적으로 생성하면서 메모리와 저장 요구 사항을 크게 줄일 수 있음을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08251.pdf

Title: Dynamic Prompting of Frozen Text-to-Image Diffusion Models for Panoptic Narrative Grounding

Original Abstract:
Panoptic narrative grounding (PNG), whose core target is fine-grained image-text alignment, requires a panoptic segmentation of referred objects given a narrative caption. Previous discriminative methods achieve only weak or coarse-grained alignment by panoptic segmentation pretraining or CLIP model adaptation. Given the recent progress of text-to-image Diffusion models, several works have shown their capability to achieve fine-grained image-text alignment through cross-attention maps and improved general segmentation performance. However, the direct use of phrase features as static prompts to apply frozen Diffusion models to the PNG task still suffers from a large task gap and insufficient vision-language interaction, yielding inferior performance. Therefore, we propose an Extractive-Injective Phrase Adapter (EIPA) bypass within the Diffusion UNet to dynamically update phrase prompts with image features and inject the multimodal cues back, which leverages the fine-grained image-text alignment capability of Diffusion models more sufficiently. In addition, we also design a Multi-Level Mutual Aggregation (MLMA) module to reciprocally fuse multi-level image and phrase features for segmentation refinement. Extensive experiments on the PNG benchmark show that our method achieves new state-of-the-art performance.

Translated Abstract:
파노픽 내러티브 그라운딩(PNG)은 세밀한 이미지-텍스트 정렬을 목표로 하는데, 이는 특정 객체의 파노픽 분할을 필요로 해. 이전의 구별 방법들은 파노픽 분할 사전 훈련이나 CLIP 모델 적응을 통해서만 약한 또는 대충 맞는 정렬을 달성했어.

최근 텍스트-이미지 확산 모델의 발전 덕분에, 몇몇 연구들은 교차 주의 맵을 사용해 세밀한 이미지-텍스트 정렬을 달성하고 더 나은 일반화된 세분화 성능을 보여줬어. 하지만 구문 특징을 정적 프롬프트로 사용해서 고정된 확산 모델을 PNG 작업에 적용하는 건 여전히 큰 격차와 부족한 비전-언어 상호작용으로 인해 성능이 떨어져.

그래서 우리는 확산 UNet 내에서 구문 프롬프트를 이미지 특징으로 동적으로 업데이트하고 다중 모드 신호를 다시 주입하는 추출-주입 구문 어댑터(EIPA)를 제안해. 이 방법은 확산 모델의 세밀한 이미지-텍스트 정렬 능력을 더 충분히 활용할 수 있게 해. 

또한, 우리는 다중 수준 상호 집합(Multi-Level Mutual Aggregation, MLMA) 모듈을 설계해서 다중 수준의 이미지와 구문 특징을 상호 융합해 세분화를 개선해. PNG 벤치마크에서의 광범위한 실험 결과, 우리의 방법이 새로운 최첨단 성능을 달성했다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.08258.pdf

Title: Improving Virtual Try-On with Garment-focused Diffusion Models

Original Abstract:
Diffusion models have led to the revolutionizing of generative modeling in numerous image synthesis tasks. Nevertheless, it is not trivial to directly apply diffusion models for synthesizing an image of a target person wearing a given in-shop garment, i.e., image-based virtual try-on (VTON) task. The difficulty originates from the aspect that the diffusion process should not only produce holistically high-fidelity photorealistic image of the target person, but also locally preserve every appearance and texture detail of the given garment. To address this, we shape a new Diffusion model, namely GarDiff, which triggers the garment-focused diffusion process with amplified guidance of both basic visual appearance and detailed textures (i.e., high-frequency details) derived from the given garment. GarDiff first remoulds a pre-trained latent diffusion model with additional appearance priors derived from the CLIP and VAE encodings of the reference garment. Meanwhile, a novel garment-focused adapter is integrated into the UNet of diffusion model, pursuing local fine-grained alignment with the visual appearance of reference garment and human pose. We specifically design an appearance loss over the synthesized garment to enhance the crucial, high-frequency details. Extensive experiments on VITON-HD and DressCode datasets demonstrate the superiority of our GarDiff when compared to state-of-the-art VTON approaches. Code is publicly available at: \href{this https URL}{this https URL}.

Translated Abstract:
확산 모델은 많은 이미지 합성 작업에서 생성 모델링의 혁신을 이끌어냈어. 하지만, 주어진 매장에서의 의상을 입은 목표 인물의 이미지를 합성하는 데 확산 모델을 직접 적용하는 건 쉬운 일이 아니야. 이 과정은 목표 인물의 전체적인 높은 품질의 포토리얼리스틱 이미지를 만들어야 할 뿐만 아니라, 주어진 의상의 모든 외형과 텍스처 세부사항도 잘 유지해야 하거든.

이 문제를 해결하기 위해, 우리는 GarDiff라는 새로운 확산 모델을 만들었어. 이 모델은 의상에 집중한 확산 과정을 유도하고, 주어진 의상에서 얻은 기본적인 시각적 외형과 세부 텍스처(즉, 고주파 세부사항)의 강력한 가이드를 추가해줘. GarDiff는 먼저, 참조 의상의 CLIP과 VAE 인코딩에서 얻은 추가적인 외형 정보를 사용해서 미리 훈련된 잠재 확산 모델을 재구성해. 그와 동시에, 확산 모델의 UNet에 새로운 의상 중심 어댑터를 통합해서 참조 의상과 사람의 포즈의 시각적 외형을 지역적으로 세밀하게 조정해.

우리는 합성된 의상에 대한 외형 손실을 특별히 설계해서 중요한 고주파 세부사항을 강화해. VITON-HD와 DressCode 데이터셋에서의 광범위한 실험을 통해, 우리의 GarDiff가 최신 VTON 접근 방식들에 비해 뛰어난 성능을 보인다는 걸 입증했어. 코드도 공개되어 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08260.pdf

Title: Improving Text-guided Object Inpainting with Semantic Pre-inpainting

Original Abstract:
Recent years have witnessed the success of large text-to-image diffusion models and their remarkable potential to generate high-quality images. The further pursuit of enhancing the editability of images has sparked significant interest in the downstream task of inpainting a novel object described by a text prompt within a designated region in the image. Nevertheless, the problem is not trivial from two aspects: 1) Solely relying on one single U-Net to align text prompt and visual object across all the denoising timesteps is insufficient to generate desired objects; 2) The controllability of object generation is not guaranteed in the intricate sampling space of diffusion model. In this paper, we propose to decompose the typical single-stage object inpainting into two cascaded processes: 1) semantic pre-inpainting that infers the semantic features of desired objects in a multi-modal feature space; 2) high-fieldity object generation in diffusion latent space that pivots on such inpainted semantic features. To achieve this, we cascade a Transformer-based semantic inpainter and an object inpainting diffusion model, leading to a novel CAscaded Transformer-Diffusion (CAT-Diffusion) framework for text-guided object inpainting. Technically, the semantic inpainter is trained to predict the semantic features of the target object conditioning on unmasked context and text prompt. The outputs of the semantic inpainter then act as the informative visual prompts to guide high-fieldity object generation through a reference adapter layer, leading to controllable object inpainting. Extensive evaluations on OpenImages-V6 and MSCOCO validate the superiority of CAT-Diffusion against the state-of-the-art methods. Code is available at \url{this https URL}.

Translated Abstract:
최근 몇 년 동안 대규모 텍스트-이미지 확산 모델이 성공을 거두었고, 고품질 이미지를 생성할 수 있는 놀라운 잠재력을 보여주고 있어. 이미지의 편집 가능성을 높이려는 노력은 특정 영역에 텍스트 프롬프트로 설명된 새로운 객체를 채우는 작업에 큰 관심을 불러일으켰어. 하지만 이 문제는 두 가지 측면에서 간단하지 않아: 

1) 하나의 U-Net에만 의존해서 모든 디노이징 타임스텝에서 텍스트 프롬프트와 시각적 객체를 맞추는 건 원하는 객체를 생성하기에 부족해.
2) 복잡한 샘플링 공간에서 객체 생성의 제어 가능성이 보장되지 않아.

이 논문에서는 전형적인 단일 단계 객체 채우기를 두 개의 연속된 과정으로 나누는 방법을 제안해: 

1) 다중 모달 특징 공간에서 원하는 객체의 의미적 특징을 추론하는 의미적 사전 채우기.
2) 그런 의미적 특징을 바탕으로 확산 잠재 공간에서 고품질 객체 생성을 하는 단계.

이를 위해, 우리는 Transformer 기반의 의미적 채우기 모델과 객체 채우기 확산 모델을 연결해서 텍스트에 기반한 객체 채우기를 위한 새로운 CAscaded Transformer-Diffusion (CAT-Diffusion) 프레임워크를 만들었어. 기술적으로, 의미적 채우기 모델은 마스킹되지 않은 맥락과 텍스트 프롬프트에 따라 목표 객체의 의미적 특징을 예측하도록 훈련돼. 그런 다음 의미적 채우기 모델의 출력이 고품질 객체 생성을 안내하는 유용한 시각적 프롬프트 역할을 하게 돼. 

OpenImages-V6와 MSCOCO에 대한 광범위한 평가 결과, CAT-Diffusion이 최신 방법들에 비해 우수하다는 걸 확인했어. 코드도 제공돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.08270.pdf

Title: FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally

Original Abstract:
This study addresses the challenge of accurately segmenting 3D Gaussian Splatting from 2D masks. Conventional methods often rely on iterative gradient descent to assign each Gaussian a unique label, leading to lengthy optimization and sub-optimal solutions. Instead, we propose a straightforward yet globally optimal solver for 3D-GS segmentation. The core insight of our method is that, with a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially a linear function with respect to the labels of each Gaussian. As such, the optimal label assignment can be solved via linear programming in closed form. This solution capitalizes on the alpha blending characteristic of the splatting process for single step optimization. By incorporating the background bias in our objective function, our method shows superior robustness in 3D segmentation against noises. Remarkably, our optimization completes within 30 seconds, about 50$\times$ faster than the best existing methods. Extensive experiments demonstrate the efficiency and robustness of our method in segmenting various scenes, and its superior performance in downstream tasks such as object removal and inpainting. Demos and code will be available at this https URL.

Translated Abstract:
이 연구는 2D 마스크에서 3D 가우시안 스플래팅을 정확하게 분할하는 문제를 다루고 있어. 기존 방법들은 보통 각 가우시안에 고유한 레이블을 부여하기 위해 반복적인 경량 하강법을 사용해서, 최적화 과정이 길어지고 결과도 좋지 않게 나오는 경우가 많았어.

그래서 우리는 3D-GS 분할을 위한 간단하면서도 전역 최적의 해결책을 제안해. 우리의 방법의 핵심은 복원된 3D-GS 장면이 있을 때, 2D 마스크의 렌더링이 각 가우시안의 레이블에 대해 기본적으로 선형 함수라는 거야. 그래서 최적의 레이블 할당은 폐쇄형 선형 프로그래밍으로 해결할 수 있어. 이 방법은 한 번의 최적화를 위해 스플래팅 과정의 알파 블렌딩 특성을 활용해.

우리의 목적 함수에 배경 바이어스를 포함했기 때문에, 3D 분할에서 소음에 대한 강력한 내성을 보여줘. 놀랍게도, 우리 최적화는 30초 이내에 완료되며, 기존의 가장 좋은 방법보다 약 50배 빠른 속도를 자랑해. 다양한 장면을 분할하는 데 있어 우리 방법의 효율성과 강력함을 보여주는 광범위한 실험 결과도 있어. 또한, 객체 제거나 인페인팅 같은 다운스트림 작업에서도 뛰어난 성능을 발휘해. 데모와 코드는 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08271.pdf

Title: DreamBeast: Distilling 3D Fantastical Animals with Part-Aware Knowledge Transfer

Original Abstract:
We present DreamBeast, a novel method based on score distillation sampling (SDS) for generating fantastical 3D animal assets composed of distinct parts. Existing SDS methods often struggle with this generation task due to a limited understanding of part-level semantics in text-to-image diffusion models. While recent diffusion models, such as Stable Diffusion 3, demonstrate a better part-level understanding, they are prohibitively slow and exhibit other common problems associated with single-view diffusion models. DreamBeast overcomes this limitation through a novel part-aware knowledge transfer mechanism. For each generated asset, we efficiently extract part-level knowledge from the Stable Diffusion 3 model into a 3D Part-Affinity implicit representation. This enables us to instantly generate Part-Affinity maps from arbitrary camera views, which we then use to modulate the guidance of a multi-view diffusion model during SDS to create 3D assets of fantastical animals. DreamBeast significantly enhances the quality of generated 3D creatures with user-specified part compositions while reducing computational overhead, as demonstrated by extensive quantitative and qualitative evaluations.

Translated Abstract:
우리는 DreamBeast라는 새로운 방법을 소개해. 이건 점수 증류 샘플링(SDS)에 기반한 것으로, 독특한 부품으로 구성된 환상적인 3D 동물 자산을 생성하는 데 사용돼. 기존의 SDS 방법들은 텍스트-이미지 확산 모델에서 부품 수준의 의미를 이해하는 데 한계가 있어서 이 작업을 잘 수행하지 못해.

최근의 확산 모델들, 예를 들어 Stable Diffusion 3은 부품 수준의 이해가 더 좋긴 한데, 너무 느리게 작동하고 단일 뷰 확산 모델과 관련된 다른 문제들도 있어. DreamBeast는 새로운 부품 인식 지식 전이 메커니즘을 통해 이러한 한계를 극복해.

각 자산을 생성할 때, 우리는 Stable Diffusion 3 모델에서 부품 수준의 지식을 효율적으로 추출해서 3D 부품 친화적 암시 표현으로 변환해. 이걸로 우리는 임의의 카메라 뷰에서 부품 친화도 맵을 즉시 생성할 수 있고, 그 맵을 사용해 SDS 중에 다중 뷰 확산 모델의 가이드를 조정해서 환상적인 동물의 3D 자산을 만들어. 

DreamBeast는 사용자 지정 부품 조합으로 생성된 3D 생물의 품질을 크게 향상시키면서도 계산 비용은 줄여. 우리는 여러 가지 정량적 및 정성적 평가를 통해 이걸 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08272.pdf

Title: Click2Mask: Local Editing with Dynamic Mask Generation

Original Abstract:
Recent advancements in generative models have revolutionized image generation and editing, making these tasks accessible to non-experts. This paper focuses on local image editing, particularly the task of adding new content to a loosely specified area. Existing methods often require a precise mask or a detailed description of the location, which can be cumbersome and prone to errors. We propose Click2Mask, a novel approach that simplifies the local editing process by requiring only a single point of reference (in addition to the content description). A mask is dynamically grown around this point during a Blended Latent Diffusion (BLD) process, guided by a masked CLIP-based semantic loss. Click2Mask surpasses the limitations of segmentation-based and fine-tuning dependent methods, offering a more user-friendly and contextually accurate solution. Our experiments demonstrate that Click2Mask not only minimizes user effort but also delivers competitive or superior local image manipulation results compared to SoTA methods, according to both human judgement and automatic metrics. Key contributions include the simplification of user input, the ability to freely add objects unconstrained by existing segments, and the integration potential of our dynamic mask approach within other editing methods.

Translated Abstract:
최근 생성 모델의 발전으로 이미지 생성과 편집이 크게 변화했어. 이제 비전문가도 이런 작업을 쉽게 할 수 있게 되었지. 이 논문은 지역 이미지 편집에 초점을 맞추고, 특히 대충 정해진 영역에 새로운 내용을 추가하는 작업에 대해 이야기해.

기존 방법들은 정확한 마스크나 위치에 대한 자세한 설명이 필요해서 번거롭고 실수할 가능성이 높았어. 그래서 우리는 Click2Mask라는 새로운 접근 방식을 제안해. 이 방법은 콘텐츠 설명 외에 단 하나의 참조 점만 필요로 해서 지역 편집 과정을 간단하게 만들어. 이 점을 중심으로 마스크가 동적으로 성장하는데, 이 과정은 마스크가 있는 CLIP 기반의 의미 손실을 통해 안내돼.

Click2Mask는 세분화 기반이나 미세 조정 의존 방법의 한계를 극복해서, 더 사용자 친화적이고 맥락에 맞는 해결책을 제공해. 우리의 실험 결과에 따르면, Click2Mask는 사용자 노력을 최소화할 뿐만 아니라, 기존의 최신 방법들과 비교했을 때 경쟁력 있거나 더 나은 지역 이미지 조작 결과를 보여줘. 주요 기여점은 사용자 입력의 간소화, 기존 세그먼트에 구애받지 않고 자유롭게 객체를 추가할 수 있는 능력, 그리고 우리의 동적 마스크 접근 방식을 다른 편집 방법들과 통합할 수 있는 가능성이야.

================================================================================

URL:
https://arxiv.org/pdf/2409.08277.pdf

Title: Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active Sensor

Original Abstract:
High frame rate and accurate depth estimation plays an important role in several tasks crucial to robotics and automotive perception. To date, this can be achieved through ToF and LiDAR devices for indoor and outdoor applications, respectively. However, their applicability is limited by low frame rate, energy consumption, and spatial sparsity. Depth on Demand (DoD) allows for accurate temporal and spatial depth densification achieved by exploiting a high frame rate RGB sensor coupled with a potentially lower frame rate and sparse active depth sensor. Our proposal jointly enables lower energy consumption and denser shape reconstruction, by significantly reducing the streaming requirements on the depth sensor thanks to its three core stages: i) multi-modal encoding, ii) iterative multi-modal integration, and iii) depth decoding. We present extended evidence assessing the effectiveness of DoD on indoor and outdoor video datasets, covering both environment scanning and automotive perception use cases.

Translated Abstract:
높은 프레임 속도와 정확한 깊이 추정은 로봇과 자동차 인식에 중요한 역할을 해. 지금까지 실내에서는 ToF 센서를, 실외에서는 LiDAR 장치를 사용해서 이걸 이뤘어. 하지만 이 장치들은 낮은 프레임 속도, 에너지 소비, 그리고 공간적으로 희소한 특성 때문에 한계가 있어.

Depth on Demand (DoD)는 높은 프레임 속도의 RGB 센서와 상대적으로 낮은 프레임 속도와 희소한 활성 깊이 센서를 이용해서 시간적이고 공간적인 깊이 밀도를 정확하게 높일 수 있게 해줘. 우리 제안은 세 가지 주요 단계인 i) 다중 모달 인코딩, ii) 반복적인 다중 모달 통합, iii) 깊이 디코딩 덕분에 깊이 센서에 대한 스트리밍 요구를 크게 줄여서 에너지 소비를 낮추고 더 밀도 있는 형태 재구성을 가능하게 해.

우리는 DoD의 효과를 실내와 실외 비디오 데이터셋을 통해 평가한 결과를 보여줘. 이 데이터셋은 환경 스캔과 자동차 인식 같은 다양한 용도를 포함하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08278.pdf

Title: DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with Diffusion Priors

Original Abstract:
We present DreamHOI, a novel method for zero-shot synthesis of human-object interactions (HOIs), enabling a 3D human model to realistically interact with any given object based on a textual description. This task is complicated by the varying categories and geometries of real-world objects and the scarcity of datasets encompassing diverse HOIs. To circumvent the need for extensive data, we leverage text-to-image diffusion models trained on billions of image-caption pairs. We optimize the articulation of a skinned human mesh using Score Distillation Sampling (SDS) gradients obtained from these models, which predict image-space edits. However, directly backpropagating image-space gradients into complex articulation parameters is ineffective due to the local nature of such gradients. To overcome this, we introduce a dual implicit-explicit representation of a skinned mesh, combining (implicit) neural radiance fields (NeRFs) with (explicit) skeleton-driven mesh articulation. During optimization, we transition between implicit and explicit forms, grounding the NeRF generation while refining the mesh articulation. We validate our approach through extensive experiments, demonstrating its effectiveness in generating realistic HOIs.

Translated Abstract:
우리는 DreamHOI라는 새로운 방법을 소개하는데, 이 방법은 제로샷으로 인간-객체 상호작용(HOI)을 합성할 수 있어. 이 기술은 3D 인간 모델이 주어진 텍스트 설명에 기반해 어떤 객체와도 현실감 있게 상호작용할 수 있도록 해줘. 

이 작업은 현실의 객체들이 다양한 종류와 형태를 가지고 있어서 복잡해지고, 다양한 HOI를 포함한 데이터셋이 부족하기 때문에 더욱 어려워. 그래서 우리는 방대한 데이터 없이도 작업을 수행하기 위해, 수십억 개의 이미지-캡션 쌍으로 훈련된 텍스트-이미지 확산 모델을 활용해.

우리는 이 모델들로부터 얻은 Score Distillation Sampling(SDS) 그래디언트를 사용해 스킨(mesh)된 인간 모델의 움직임을 최적화해. 이 그래디언트는 이미지 공간에서의 수정을 예측해주는데, 복잡한 움직임 파라미터에 직접적으로 그래디언트를 역전파하는 건 효과적이지 않아. 그래서 우리는 스킨(mesh)된 모델의 이중 표현을 도입했어. 이는 (암묵적) 신경 복사 필드(NeRFs)와 (명시적) 뼈대 기반의 메시 움직임을 결합한 거야.

최적화하는 동안 우리는 암묵적 형태와 명시적 형태 사이를 전환하면서 NeRF 생성을 바탕으로 메시의 움직임을 다듬어. 우리는 이 방법을 광범위한 실험을 통해 검증했고, 현실감 있는 HOI를 생성하는 데 효과적임을 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07457.pdf

Title: LSST: Learned Single-Shot Trajectory and Reconstruction Network for MR Imaging

Original Abstract:
Single-shot magnetic resonance (MR) imaging acquires the entire k-space data in a single shot and it has various applications in whole-body imaging. However, the long acquisition time for the entire k-space in single-shot fast spin echo (SSFSE) MR imaging poses a challenge, as it introduces T2-blur in the acquired images. This study aims to enhance the reconstruction quality of SSFSE MR images by (a) optimizing the trajectory for measuring the k-space, (b) acquiring fewer samples to speed up the acquisition process, and (c) reducing the impact of T2-blur. The proposed method adheres to physics constraints due to maximum gradient strength and slew-rate available while optimizing the trajectory within an end-to-end learning framework. Experiments were conducted on publicly available fastMRI multichannel dataset with 8-fold and 16-fold acceleration factors. An experienced radiologist's evaluation on a five-point Likert scale indicates improvements in the reconstruction quality as the ACL fibers are sharper than comparative methods.

Translated Abstract:
단일 촬영 자기공명(MR) 이미지는 전체 k-공간 데이터를 한 번에 수집하는 방식이고, 전신 이미징에 여러 가지로 쓰여. 하지만 단일 촬영 빠른 스핀 에코(SSFSE) MR 이미징에서는 전체 k-공간을 수집하는데 시간이 오래 걸려서 문제가 생겨. 이게 이미지에 T2 블러를 유발하거든.

이 연구는 SSFSE MR 이미지의 재구성 품질을 높이기 위한 거야. 방법은 (a) k-공간을 측정하는 궤적을 최적화하고, (b) 샘플 수를 줄여서 수집 속도를 높이고, (c) T2 블러의 영향을 줄이는 거야. 제안된 방법은 최대 기울기 강도와 슬루율 같은 물리적 제약을 따르면서 궤적을 최적화해. 이 과정은 엔드 투 엔드 학습 프레임워크 안에서 이루어져.

실험은 공개된 fastMRI 다채널 데이터셋을 사용해서 8배와 16배 가속 요인을 적용했어. 경험이 많은 방사선 전문의가 5점 리커트 척도로 평가했는데, ACL 섬유가 다른 방법들보다 더 선명해져서 재구성 품질이 개선되었다고 해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07466.pdf

Title: An Artificial Neural Network for Image Classification Inspired by Aversive Olfactory Learning Circuits in Caenorhabditis Elegans

Original Abstract:
This study introduces an artificial neural network (ANN) for image classification task, inspired by the aversive olfactory learning circuits of the nematode Caenorhabditis elegans (C. elegans). Despite the remarkable performance of ANNs in a variety of tasks, they face challenges such as excessive parameterization, high training costs and limited generalization capabilities. C. elegans, with its simple nervous system comprising only 302 neurons, serves as a paradigm in neurobiological research and is capable of complex behaviors including learning. This research identifies key neural circuits associated with aversive olfactory learning in C. elegans through behavioral experiments and high-throughput gene sequencing, translating them into an image classification ANN architecture. Additionally, two other image classification ANNs with distinct architectures were constructed for comparative performance analysis to highlight the advantages of bio-inspired design. The results indicate that the ANN inspired by the aversive olfactory learning circuits of C. elegans achieves higher accuracy, better consistency and faster convergence rates in image classification task, especially when tackling more complex classification challenges. This study not only showcases the potential of bio-inspired design in enhancing ANN capabilities but also provides a novel perspective and methodology for future ANN design.

Translated Abstract:
이 연구는 선충인 Caenorhabditis elegans (C. elegans)의 혐오성 후각 학습 회로에서 영감을 받아 이미지 분류 작업을 위한 인공 신경망(ANN)을 소개해. ANNs는 여러 작업에서 뛰어난 성능을 보이지만, 파라미터가 많아서 복잡하고, 훈련 비용이 높으며, 일반화 능력이 제한되는 문제들이 있어. 

C. elegans는 302개의 뉴런만으로 이루어진 간단한 신경계를 가지고 있어서 신경 생물학 연구의 모델로 사용돼. 이 선충은 학습을 포함한 복잡한 행동을 할 수 있어. 이 연구에서는 C. elegans의 혐오성 후각 학습과 관련된 주요 신경 회로를 행동 실험과 고속 유전자 서열 분석을 통해 찾아냈고, 이를 이미지 분류 ANN 구조로 변환했어. 

또한, 비교 성능 분석을 위해 다른 구조를 가진 두 가지 이미지 분류 ANN도 만들었어. 이를 통해 생물학적 영감을 받은 디자인의 장점을 강조했지. 연구 결과, C. elegans의 혐오성 후각 학습 회로에서 영감을 받은 ANN이 이미지 분류 작업에서 더 높은 정확도, 더 나은 일관성, 더 빠른 수렴 속도를 보였어. 특히 복잡한 분류 문제를 다룰 때 더욱 두드러졌고. 

이 연구는 ANN의 능력을 향상시키는 데 생물학적 영감 디자인의 가능성을 보여줄 뿐만 아니라, 미래 ANN 디자인을 위한 새로운 관점과 방법론도 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07493.pdf

Title: Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review

Original Abstract:
The Complex Emotion Recognition System (CERS) deciphers complex emotional states by examining combinations of basic emotions expressed, their interconnections, and the dynamic variations. Through the utilization of advanced algorithms, CERS provides profound insights into emotional dynamics, facilitating a nuanced understanding and customized responses. The attainment of such a level of emotional recognition in machines necessitates the knowledge distillation and the comprehension of novel concepts akin to human cognition. The development of AI systems for discerning complex emotions poses a substantial challenge with significant implications for affective computing. Furthermore, obtaining a sizable dataset for CERS proves to be a daunting task due to the intricacies involved in capturing subtle emotions, necessitating specialized methods for data collection and processing. Incorporating physiological signals such as Electrocardiogram (ECG) and Electroencephalogram (EEG) can notably enhance CERS by furnishing valuable insights into the user's emotional state, enhancing the quality of datasets, and fortifying system dependability. A comprehensive literature review was conducted in this study to assess the efficacy of machine learning, deep learning, and meta-learning approaches in both basic and complex emotion recognition utilizing EEG, ECG signals, and facial expression datasets. The chosen research papers offer perspectives on potential applications, clinical implications, and results of CERSs, with the objective of promoting their acceptance and integration into clinical decision-making processes. This study highlights research gaps and challenges in understanding CERSs, encouraging further investigation by relevant studies and organizations. Lastly, the significance of meta-learning approaches in improving CERS performance and guiding future research endeavors is underscored.

Translated Abstract:
복합 감정 인식 시스템(Complex Emotion Recognition System, CERS)은 기본 감정의 조합과 그 상호 연결성, 그리고 동적인 변화를 분석해서 복합적인 감정 상태를 해석해. 고급 알고리즘을 사용해서 CERS는 감정의 동적인 면에 대한 깊은 통찰을 제공하고, 보다 섬세한 이해와 맞춤형 반응을 가능하게 해.

기계에서 이런 수준의 감정 인식을 달성하려면, 인간의 인지와 비슷한 새로운 개념을 이해하고 지식을 정제하는 과정이 필요해. 복합 감정을 인식하는 AI 시스템을 개발하는 건 큰 도전이고, 감정 컴퓨팅에 중요한 의미가 있어. 게다가 CERS에 필요한 대규모 데이터 세트를 얻는 건 미세한 감정을 포착하는 데 복잡성이 있어서 쉽지 않아. 그래서 데이터 수집과 처리에 특별한 방법이 필요해.

심전도(ECG)나 뇌파(EEG) 같은 생리적 신호를 포함하면 CERS가 크게 개선돼. 이런 신호는 사용자의 감정 상태에 대한 귀중한 통찰을 제공하고, 데이터 세트의 품질을 높이며 시스템의 신뢰성을 강화해. 이 연구에서는 EEG, ECG 신호와 얼굴 표정 데이터 세트를 사용한 기본 및 복합 감정 인식에서 기계 학습, 딥 러닝, 메타 학습 접근 방식의 효율성을 평가하기 위해 포괄적인 문헌 조사를 했어.

선택된 연구 논문들은 CERS의 잠재적인 응용, 임상적 의미, 결과에 대한 관점을 제공하며, 이를 통해 임상 결정 과정에서의 수용과 통합을 촉진하는 것이 목표야. 이 연구는 CERS를 이해하는 데 있어 연구의 공백과 도전과제를 강조하며, 관련 연구와 기관의 추가 조사를 권장해. 마지막으로, CERS 성능을 개선하고 미래 연구 방향을 안내하는 데 있어 메타 학습 접근 방식의 중요성을 강조했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07564.pdf

Title: TabMixer: Noninvasive Estimation of the Mean Pulmonary Artery Pressure via Imaging and Tabular Data Mixing

Original Abstract:
Right Heart Catheterization is a gold standard procedure for diagnosing Pulmonary Hypertension by measuring mean Pulmonary Artery Pressure (mPAP). It is invasive, costly, time-consuming and carries risks. In this paper, for the first time, we explore the estimation of mPAP from videos of noninvasive Cardiac Magnetic Resonance Imaging. To enhance the predictive capabilities of Deep Learning models used for this task, we introduce an additional modality in the form of demographic features and clinical measurements. Inspired by all-Multilayer Perceptron architectures, we present TabMixer, a novel module enabling the integration of imaging and tabular data through spatial, temporal and channel mixing. Specifically, we present the first approach that utilizes Multilayer Perceptrons to interchange tabular information with imaging features in vision models. We test TabMixer for mPAP estimation and show that it enhances the performance of Convolutional Neural Networks, 3D-MLP and Vision Transformers while being competitive with previous modules for imaging and tabular data. Our approach has the potential to improve clinical processes involving both modalities, particularly in noninvasive mPAP estimation, thus, significantly enhancing the quality of life for individuals affected by Pulmonary Hypertension. We provide a source code for using TabMixer at this https URL.

Translated Abstract:
우심실 카테터 삽입은 폐 고혈압을 진단하는 데 가장 표준적인 방법으로, 평균 폐동맥압(mPAP)을 측정하는 절차야. 하지만 이 방법은 침습적이고, 비용이 많이 들고, 시간이 오래 걸리며, 위험도 있어. 

이 논문에서는 처음으로 비침습적인 심장 자기공명영상(Cardio MRI) 비디오에서 mPAP를 추정하는 방법을 탐구해. 이 작업에 사용되는 딥러닝 모델의 예측 능력을 높이기 위해, 인구통계학적 특성과 임상 측정값 같은 추가적인 데이터를 도입했어. 

다층 퍼셉트론 아키텍처에서 영감을 받아서, 우리는 TabMixer라는 새로운 모듈을 소개해. 이 모듈은 이미징 데이터와 표 형식 데이터를 공간적, 시간적, 채널 믹싱을 통해 통합할 수 있게 해줘. 구체적으로는, 비전 모델에서 다층 퍼셉트론을 활용해 표 형식 정보와 이미징 특성을 교환하는 첫 번째 접근 방식을 제시해. 

우리는 TabMixer를 mPAP 추정에 테스트했으며, 이 모듈이 합성곱 신경망, 3D-MLP, 비전 트랜스포머의 성능을 향상시킨다는 것을 보여줬어. 그리고 이전의 이미징 및 표 형식 데이터 모듈과 경쟁할 수 있는 성능을 보였어. 

우리의 접근 방식은 두 가지 방식 모두를 포함하는 임상 과정의 개선 가능성이 있어, 특히 비침습적인 mPAP 추정에서 더 나아가서 폐 고혈압 환자의 삶의 질을 크게 향상시킬 수 있어. TabMixer를 사용할 수 있는 소스 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07584.pdf

Title: DS-ViT: Dual-Stream Vision Transformer for Cross-Task Distillation in Alzheimer's Early Diagnosis

Original Abstract:
In the field of Alzheimer's disease diagnosis, segmentation and classification tasks are inherently interconnected. Sharing knowledge between models for these tasks can significantly improve training efficiency, particularly when training data is scarce. However, traditional knowledge distillation techniques often struggle to bridge the gap between segmentation and classification due to the distinct nature of tasks and different model architectures. To address this challenge, we propose a dual-stream pipeline that facilitates cross-task and cross-architecture knowledge sharing. Our approach introduces a dual-stream embedding module that unifies feature representations from segmentation and classification models, enabling dimensional integration of these features to guide the classification model. We validated our method on multiple 3D datasets for Alzheimer's disease diagnosis, demonstrating significant improvements in classification performance, especially on small datasets. Furthermore, we extended our pipeline with a residual temporal attention mechanism for early diagnosis, utilizing images taken before the atrophy of patients' brain mass. This advancement shows promise in enabling diagnosis approximately six months earlier in mild and asymptomatic stages, offering critical time for intervention.

Translated Abstract:
알츠하이머 병 진단 분야에서, 분할(segmentation)과 분류(classification) 작업은 서로 연결되어 있어. 이 작업들을 위해 모델 간에 지식을 공유하면 훈련 효율성이 크게 향상될 수 있어, 특히 훈련 데이터가 부족할 때 더욱 그렇지. 그런데 전통적인 지식 증류 기법은 분할과 분류의 성격이 다르고 모델 구조도 다르기 때문에 이 두 작업의 간극을 메우기 힘들어.

이런 문제를 해결하기 위해 우리는 교차 작업과 교차 구조 간의 지식 공유를 돕는 이중 스트림 파이프라인을 제안해. 우리의 방법은 분할 모델과 분류 모델로부터 나온 특징 표현을 통합하는 이중 스트림 임베딩 모듈을 도입해. 이 모듈은 이 특징들을 차원적으로 통합해서 분류 모델을 안내하는 역할을 해.

우리는 알츠하이머 병 진단을 위한 여러 3D 데이터셋에서 우리의 방법을 검증했어. 그 결과, 특히 작은 데이터셋에서 분류 성능이 크게 향상된 것을 보여줬어. 게다가, 우리는 잔여 시간 주의 메커니즘을 추가해 초기 진단을 위해 파이프라인을 확장했어. 이 메커니즘은 환자의 뇌 용적이 줄어들기 전의 이미지를 활용해. 이 발전은 경미하거나 무증상 단계에서 진단을 약 6개월 정도 더 일찍 할 수 있는 가능성을 보여주고, 중재를 위한 중요한 시간을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.07609.pdf

Title: A Cost-Aware Approach to Adversarial Robustness in Neural Networks

Original Abstract:
Considering the growing prominence of production-level AI and the threat of adversarial attacks that can evade a model at run-time, evaluating the robustness of models to these evasion attacks is of critical importance. Additionally, testing model changes likely means deploying the models to (e.g. a car or a medical imaging device), or a drone to see how it affects performance, making un-tested changes a public problem that reduces development speed, increases cost of development, and makes it difficult (if not impossible) to parse cause from effect. In this work, we used survival analysis as a cloud-native, time-efficient and precise method for predicting model performance in the presence of adversarial noise. For neural networks in particular, the relationships between the learning rate, batch size, training time, convergence time, and deployment cost are highly complex, so researchers generally rely on benchmark datasets to assess the ability of a model to generalize beyond the training data. To address this, we propose using accelerated failure time models to measure the effect of hardware choice, batch size, number of epochs, and test-set accuracy by using adversarial attacks to induce failures on a reference model architecture before deploying the model to the real world. We evaluate several GPU types and use the Tree Parzen Estimator to maximize model robustness and minimize model run-time simultaneously. This provides a way to evaluate the model and optimise it in a single step, while simultaneously allowing us to model the effect of model parameters on training time, prediction time, and accuracy. Using this technique, we demonstrate that newer, more-powerful hardware does decrease the training time, but with a monetary and power cost that far outpaces the marginal gains in accuracy.

Translated Abstract:
생산 수준의 AI가 점점 더 중요해지면서, 모델이 실제 실행 중에 회피 공격을 어떻게 견디는지 평가하는 것이 정말 중요해졌어. 이러한 회피 공격에 대한 모델의 강건성을 테스트하는 것은 필수적이야. 모델을 바꾸는 과정에서 자동차나 의료 영상 장치 같은 실제 환경에 모델을 배포해야 할 경우가 많고, 이 과정에서 검증되지 않은 변경은 개발 속도를 늦추고 비용을 증가시키며 원인과 결과를 구분하기 어렵게 만들 수 있어.

이 연구에서는 적대적 노이즈가 있는 상황에서 모델 성능을 예측하기 위해 생존 분석을 사용했어. 특히 신경망의 경우, 학습률, 배치 크기, 훈련 시간, 수렴 시간, 배포 비용 간의 관계가 복잡해서 연구자들은 보통 벤치마크 데이터셋을 사용해 모델이 훈련 데이터 이상으로 일반화할 수 있는지를 평가해. 이를 해결하기 위해, 우리는 하드웨어 선택, 배치 크기, 에폭 수, 테스트 세트 정확도의 영향을 측정하기 위해 가속 실패 시간 모델을 제안해. 이렇게 해서 실제 환경에 모델을 배포하기 전에 적대적 공격을 사용해 참조 모델 아키텍처에서 실패를 유도할 수 있어.

여러 종류의 GPU를 평가하고, Tree Parzen Estimator를 사용해 모델의 강건성을 극대화하고 모델 실행 시간을 최소화하는 방법을 제시했어. 이 과정은 모델을 평가하고 최적화하는 방법을 제공하며, 동시에 모델 파라미터가 훈련 시간, 예측 시간, 정확도에 미치는 영향을 모델링할 수 있어. 이 기술을 사용해 보면, 새로운 강력한 하드웨어가 훈련 시간을 줄여주긴 하지만, 그에 따른 비용과 전력 소모가 정확도의 한계 이점을 훨씬 초과한다는 것을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2409.07623.pdf

Title: Object Depth and Size Estimation using Stereo-vision and Integration with SLAM

Original Abstract:
Autonomous robots use simultaneous localization and mapping (SLAM) for efficient and safe navigation in various environments. LiDAR sensors are integral in these systems for object identification and localization. However, LiDAR systems though effective in detecting solid objects (e.g., trash bin, bottle, etc.), encounter limitations in identifying semitransparent or non-tangible objects (e.g., fire, smoke, steam, etc.) due to poor reflecting characteristics. Additionally, LiDAR also fails to detect features such as navigation signs and often struggles to detect certain hazardous materials that lack a distinct surface for effective laser reflection. In this paper, we propose a highly accurate stereo-vision approach to complement LiDAR in autonomous robots. The system employs advanced stereo vision-based object detection to detect both tangible and non-tangible objects and then uses simple machine learning to precisely estimate the depth and size of the object. The depth and size information is then integrated into the SLAM process to enhance the robot's navigation capabilities in complex environments. Our evaluation, conducted on an autonomous robot equipped with LiDAR and stereo-vision systems demonstrates high accuracy in the estimation of an object's depth and size. A video illustration of the proposed scheme is available at: \url{this https URL}.

Translated Abstract:
자율 로봇은 다양한 환경에서 효율적이고 안전한 내비게이션을 위해 동시에 위치 추정과 맵핑(SLAM)을 사용해. 이 시스템에서 LiDAR 센서는 물체 식별과 위치 파악에 중요한 역할을 해. 하지만 LiDAR 시스템은 고체 물체(예: 쓰레기통, 병 등)는 잘 감지하지만, 반투명하거나 무형의 물체(예: 불, 연기, 증기 등)는 반사 특성이 좋지 않아서 잘 못 잡아. 그리고 LiDAR는 내비게이션 표지판 같은 특징을 감지하는 데도 실패하고, 효과적인 레aser 반사가 없는 특정 위험 물질을 감지하는 데 어려움을 겪어.

이번 논문에서는 자율 로봇의 LiDAR를 보완할 수 있는 매우 정확한 스테레오 비전 접근 방식을 제안해. 이 시스템은 고급 스테레오 비전 기반 물체 감지를 사용해서 유형 물체와 무형 물체를 모두 감지하고, 간단한 머신러닝을 통해 물체의 깊이와 크기를 정확하게 추정해. 깊이와 크기 정보는 SLAM 과정에 통합되어 로봇의 복잡한 환경 내비게이션 능력을 향상시켜.

LiDAR와 스테레오 비전 시스템을 갖춘 자율 로봇에서 수행한 평가 결과, 물체의 깊이와 크기를 높은 정확도로 추정할 수 있음을 보여줬어. 제안된 방법의 동영상 설명은 이 링크에서 볼 수 있어: \url{this https URL}.

================================================================================

URL:
https://arxiv.org/pdf/2409.07759.pdf

Title: SwinGS: Sliding Window Gaussian Splatting for Volumetric Video Streaming with Arbitrary Length

Original Abstract:
Recent advances in 3D Gaussian Splatting (3DGS) have garnered significant attention in computer vision and computer graphics due to its high rendering speed and remarkable quality. While extant research has endeavored to extend the application of 3DGS from static to dynamic scenes, such efforts have been consistently impeded by excessive model sizes, constraints on video duration, and content deviation. These limitations significantly compromise the streamability of dynamic 3D Gaussian models, thereby restricting their utility in downstream applications, including volumetric video, autonomous vehicle, and immersive technologies such as virtual, augmented, and mixed reality.
This paper introduces SwinGS, a novel framework for training, delivering, and rendering volumetric video in a real-time streaming fashion. To address the aforementioned challenges and enhance streamability, SwinGS integrates spacetime Gaussian with Markov Chain Monte Carlo (MCMC) to adapt the model to fit various 3D scenes across frames, in the meantime employing a sliding window captures Gaussian snapshots for each frame in an accumulative way. We implement a prototype of SwinGS and demonstrate its streamability across various datasets and scenes. Additionally, we develop an interactive WebGL viewer enabling real-time volumetric video playback on most devices with modern browsers, including smartphones and tablets. Experimental results show that SwinGS reduces transmission costs by 83.6% compared to previous work with ignorable compromise in PSNR. Moreover, SwinGS easily scales to long video sequences without compromising quality.

Translated Abstract:
최근 3D 가우시안 스플래팅(3DGS) 기술이 컴퓨터 비전과 컴퓨터 그래픽스 분야에서 주목받고 있어. 이 기술은 빠른 렌더링 속도와 뛰어난 품질 덕분인데, 기존 연구들은 3DGS의 적용을 정적인 장면에서 동적인 장면으로 확장하려고 노력해왔어. 하지만 모델 크기가 너무 크고, 비디오 길이에 제약이 있으며, 내용이 일관되지 않는 문제 때문에 항상 어려움을 겪었어. 이런 제한 때문에 동적인 3D 가우시안 모델의 스트리밍이 힘들어지고, 결국 자율주행차나 가상 현실, 증강 현실 같은 다양한 응용에 활용하기 어려워졌어.

이 논문에서는 SwinGS라는 새로운 프레임워크를 소개해. SwinGS는 실시간으로 볼륨 비디오를 훈련하고, 전달하고, 렌더링할 수 있게 해줘. 앞서 언급한 문제들을 해결하고 스트리밍 성능을 높이기 위해, SwinGS는 시공간 가우시안을 마르코프 체인 몬테카를로(MCMC)와 결합해서 모델이 다양한 3D 장면에 맞춰 조정되도록 해. 또한, 슬라이딩 윈도우를 사용해서 각 프레임에 대한 가우시안 스냅샷을 누적 방식으로 캡처해.

우리는 SwinGS의 프로토타입을 구현했고, 다양한 데이터셋과 장면에서 이 기술이 잘 스트리밍된다는 것을 보여줬어. 또, 대부분의 최신 브라우저가 설치된 스마트폰과 태블릿을 포함한 기기에서 실시간으로 볼륨 비디오를 재생할 수 있는 인터랙티브 WebGL 뷰어도 개발했어. 실험 결과에 따르면, SwinGS는 이전 연구에 비해 전송 비용을 83.6% 줄이면서 PSNR에서는 거의 손실이 없었어. 게다가 SwinGS는 품질을 유지하며 긴 비디오 시퀀스에도 쉽게 확장할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07827.pdf

Title: Bridging Paintings and Music -- Exploring Emotion based Music Generation through Paintings

Original Abstract:
Rapid advancements in artificial intelligence have significantly enhanced generative tasks involving music and images, employing both unimodal and multimodal approaches. This research develops a model capable of generating music that resonates with the emotions depicted in visual arts, integrating emotion labeling, image captioning, and language models to transform visual inputs into musical compositions. Addressing the scarcity of aligned art and music data, we curated the Emotion Painting Music Dataset, pairing paintings with corresponding music for effective training and evaluation. Our dual-stage framework converts images to text descriptions of emotional content and then transforms these descriptions into music, facilitating efficient learning with minimal data. Performance is evaluated using metrics such as Fréchet Audio Distance (FAD), Total Harmonic Distortion (THD), Inception Score (IS), and KL divergence, with audio-emotion text similarity confirmed by the pre-trained CLAP model to demonstrate high alignment between generated music and text. This synthesis tool bridges visual art and music, enhancing accessibility for the visually impaired and opening avenues in educational and therapeutic applications by providing enriched multi-sensory experiences.

Translated Abstract:
인공지능의 빠른 발전 덕분에 음악과 이미지 같은 생성 작업이 크게 향상되었어. 이 연구에서는 시각 예술에서 표현된 감정과 잘 어울리는 음악을 만들어낼 수 있는 모델을 개발했어. 이 모델은 감정 레이블링, 이미지 캡셔닝, 언어 모델을 통합해서 시각 입력을 음악 작곡으로 변환해.

예술과 음악 데이터를 연결할 수 있는 자료가 부족해서, 우리는 '감정 그림 음악 데이터셋'을 만들었어. 이 데이터셋은 그림과 그에 맞는 음악을 짝지어서 훈련과 평가에 효과적으로 사용할 수 있도록 했지. 우리의 두 단계 프레임워크는 이미지를 감정 내용을 설명하는 텍스트로 변환한 다음, 이 설명을 음악으로 바꿔. 이렇게 하면 최소한의 데이터로도 효율적으로 학습할 수 있어.

성능 평가는 Fréchet Audio Distance (FAD), Total Harmonic Distortion (THD), Inception Score (IS), KL divergence 같은 지표를 사용해서 진행해. 생성된 음악과 텍스트 사이의 유사성은 미리 훈련된 CLAP 모델을 통해 확인했어. 이 도구는 시각 예술과 음악을 연결해주고, 시각 장애인에게도 더 나은 접근성을 제공해. 또 교육과 치료 분야에서도 다감각 경험을 제공할 수 있는 새로운 길을 열어주고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07830.pdf

Title: ReGentS: Real-World Safety-Critical Driving Scenario Generation Made Stable

Original Abstract:
Machine learning based autonomous driving systems often face challenges with safety-critical scenarios that are rare in real-world data, hindering their large-scale deployment. While increasing real-world training data coverage could address this issue, it is costly and dangerous. This work explores generating safety-critical driving scenarios by modifying complex real-world regular scenarios through trajectory optimization. We propose ReGentS, which stabilizes generated trajectories and introduces heuristics to avoid obvious collisions and optimization problems. Our approach addresses unrealistic diverging trajectories and unavoidable collision scenarios that are not useful for training robust planner. We also extend the scenario generation framework to handle real-world data with up to 32 agents. Additionally, by using a differentiable simulator, our approach simplifies gradient descent-based optimization involving a simulator, paving the way for future advancements. The code is available at this https URL.

Translated Abstract:
기계 학습 기반 자율 주행 시스템은 실제 데이터에서 드물게 발생하는 안전-critical 상황에 어려움을 겪고 있어 대규모 배포에 방해가 되고 있어. 실제 훈련 데이터의 범위를 늘리면 이 문제를 해결할 수 있지만, 비용이 많이 들고 위험해. 

이번 연구에서는 복잡한 실제 정규 상황을 경로 최적화를 통해 수정해서 안전-critical 주행 시나리오를 생성하는 방법을 탐구했어. 우리는 ReGentS를 제안하는데, 이 시스템은 생성된 경로를 안정화하고 명백한 충돌과 최적화 문제를 피하는 휴리스틱을 도입해. 

우리의 접근법은 비현실적인 분기 경로와 피할 수 없는 충돌 시나리오를 해결하는데, 이런 것들은 강력한 계획자를 훈련하는 데 쓸모가 없어. 또한 시나리오 생성 프레임워크를 확장해서 최대 32개의 에이전트를 처리할 수 있게 했어. 마지막으로, 미분 가능한 시뮬레이터를 사용해서 시뮬레이터와 관련된 경량 하강 최적화를 단순화하여 미래의 발전 가능성을 열어줘. 코드는 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07862.pdf

Title: Context-Aware Optimal Transport Learning for Retinal Fundus Image Enhancement

Original Abstract:
Retinal fundus photography offers a non-invasive way to diagnose and monitor a variety of retinal diseases, but is prone to inherent quality glitches arising from systemic imperfections or operator/patient-related factors. However, high-quality retinal images are crucial for carrying out accurate diagnoses and automated analyses. The fundus image enhancement is typically formulated as a distribution alignment problem, by finding a one-to-one mapping between a low-quality image and its high-quality counterpart. This paper proposes a context-informed optimal transport (OT) learning framework for tackling unpaired fundus image enhancement. In contrast to standard generative image enhancement methods, which struggle with handling contextual information (e.g., over-tampered local structures and unwanted artifacts), the proposed context-aware OT learning paradigm better preserves local structures and minimizes unwanted artifacts. Leveraging deep contextual features, we derive the proposed context-aware OT using the earth mover's distance and show that the proposed context-OT has a solid theoretical guarantee. Experimental results on a large-scale dataset demonstrate the superiority of the proposed method over several state-of-the-art supervised and unsupervised methods in terms of signal-to-noise ratio, structural similarity index, as well as two downstream tasks. The code is available at \url{this https URL}.

Translated Abstract:
망막 사진 촬영은 다양한 망막 질환을 진단하고 모니터링하는 비침습적인 방법이야. 하지만 시스템의 결함이나 조작자/환자 관련 요인 때문에 품질에 문제가 생길 수 있어. 그래서 고품질의 망막 이미지는 정확한 진단과 자동 분석을 위해 정말 중요해. 

망막 이미지 향상은 일반적으로 저품질 이미지와 고품질 이미지 간의 일대일 대응을 찾는 분포 정렬 문제로 다뤄져. 이 논문에서는 쌍이 없는 망막 이미지 향상을 위한 맥락 정보 기반 최적 수송(OT) 학습 프레임워크를 제안해. 전통적인 생성 이미지 향상 방법들은 맥락 정보를 잘 처리하지 못하는데, 예를 들면 과도하게 수정된 지역 구조나 원치 않는 아티팩트 같은 것들이야. 제안하는 맥락 인식 OT 학습 방식은 지역 구조를 잘 유지하고 원치 않는 아티팩트를 최소화하는 데 더 효과적이야. 

우리는 깊은 맥락 특징을 활용해 지구 이동자 거리로 제안하는 맥락 인식 OT를 도출하고, 이 방법이 확실한 이론적 보장을 가진다는 걸 보여줘. 대규모 데이터셋에서의 실험 결과는 신호 대 잡음 비율, 구조적 유사도 지수, 그리고 두 가지 다운스트림 작업에서 제안한 방법이 여러 최신 감독 및 비감독 방법들보다 우수하다는 걸 증명해. 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.07914.pdf

Title: InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation

Original Abstract:
We present InterACT: Inter-dependency aware Action Chunking with Hierarchical Attention Transformers, a novel imitation learning framework for bimanual manipulation that integrates hierarchical attention to capture inter-dependencies between dual-arm joint states and visual inputs. InterACT consists of a Hierarchical Attention Encoder and a Multi-arm Decoder, both designed to enhance information aggregation and coordination. The encoder processes multi-modal inputs through segment-wise and cross-segment attention mechanisms, while the decoder leverages synchronization blocks to refine individual action predictions, providing the counterpart's prediction as context. Our experiments on a variety of simulated and real-world bimanual manipulation tasks demonstrate that InterACT significantly outperforms existing methods. Detailed ablation studies validate the contributions of key components of our work, including the impact of CLS tokens, cross-segment encoders, and synchronization blocks.

Translated Abstract:
우리는 InterACT를 소개해. 이건 임베디드 학습 프레임워크로, 두 손을 사용하는 조작을 위한 거야. 이 프레임워크는 계층적 주의를 사용해서 두 팔의 관절 상태와 시각 입력 간의 상호 의존성을 포착해.

InterACT는 계층적 주의 인코더와 다중 팔 디코더로 구성돼. 이 두 개는 정보 집계와 조정을 더 잘 할 수 있도록 설계됐어. 인코더는 여러 종류의 입력을 세그먼트별 및 세그먼트 간 주의 메커니즘을 통해 처리하고, 디코더는 동기화 블록을 사용해서 각 행동 예측을 더 다듬어. 이때 상대의 예측을 맥락으로 활용해.

우리가 여러 가지 시뮬레이션과 실제 환경에서의 두 손 조작 과제에 대해 실험한 결과, InterACT가 기존 방법들보다 훨씬 뛰어난 성능을 보여줬어. 세부적인 제거 연구를 통해 CLS 토큰, 세그먼트 간 인코더, 동기화 블록 같은 주요 요소들이 기여한 바를 검증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08000.pdf

Title: OCTAMamba: A State-Space Model Approach for Precision OCTA Vasculature Segmentation

Original Abstract:
Optical Coherence Tomography Angiography (OCTA) is a crucial imaging technique for visualizing retinal vasculature and diagnosing eye diseases such as diabetic retinopathy and glaucoma. However, precise segmentation of OCTA vasculature remains challenging due to the multi-scale vessel structures and noise from poor image quality and eye lesions. In this study, we proposed OCTAMamba, a novel U-shaped network based on the Mamba architecture, designed to segment vasculature in OCTA accurately. OCTAMamba integrates a Quad Stream Efficient Mining Embedding Module for local feature extraction, a Multi-Scale Dilated Asymmetric Convolution Module to capture multi-scale vasculature, and a Focused Feature Recalibration Module to filter noise and highlight target areas. Our method achieves efficient global modeling and local feature extraction while maintaining linear complexity, making it suitable for low-computation medical applications. Extensive experiments on the OCTA 3M, OCTA 6M, and ROSSA datasets demonstrated that OCTAMamba outperforms state-of-the-art methods, providing a new reference for efficient OCTA segmentation. Code is available at this https URL

Translated Abstract:
광학 코히어런스 단층 촬영 혈관조영술(OCTA)은 망막 혈관을 시각화하고 당뇨병성 망막병증이나 녹내장 같은 눈 질환을 진단하는 데 중요한 이미징 기술이야. 하지만 OCTA 혈관을 정확하게 분할하는 건 다중 크기의 혈관 구조와 낮은 이미지 품질, 눈 병변에서 오는 노이즈 때문에 여전히 어려워.

이 연구에서는 OCTAMamba라는 새로운 U자형 네트워크를 제안했어. 이건 Mamba 아키텍처를 기반으로 만들어져서 OCTA에서 혈관을 정확하게 분할하는 데 초점을 맞추고 있어. OCTAMamba는 지역적 특징 추출을 위한 쿼드 스트림 효율적 마이닝 임베딩 모듈, 다중 크기 혈관을 포착하는 다중 스케일 팽창 비대칭 컨볼루션 모듈, 그리고 노이즈를 필터링하고 목표 영역을 강조하기 위한 포커스드 피처 리칼리브레이션 모듈을 통합하고 있어.

우리 방법은 효율적인 전역 모델링과 지역적 특징 추출을 동시에 하면서 선형 복잡성을 유지해, 저사양 의료 적용에 적합해. OCTA 3M, OCTA 6M, 그리고 ROSSA 데이터셋에서의 광범위한 실험 결과, OCTAMamba가 최신 방법들을 초월하는 성능을 보였고, 효율적인 OCTA 분할을 위한 새로운 기준을 제공했어. 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08068.pdf

Title: AutoPET Challenge: Tumour Synthesis for Data Augmentation

Original Abstract:
Accurate lesion segmentation in whole-body PET/CT scans is crucial for cancer diagnosis and treatment planning, but limited datasets often hinder the performance of automated segmentation models. In this paper, we explore the potential of leveraging the deep prior from a generative model to serve as a data augmenter for automated lesion segmentation in PET/CT scans. We adapt the DiffTumor method, originally designed for CT images, to generate synthetic PET-CT images with lesions. Our approach trains the generative model on the AutoPET dataset and uses it to expand the training data. We then compare the performance of segmentation models trained on the original and augmented datasets. Our findings show that the model trained on the augmented dataset achieves a higher Dice score, demonstrating the potential of our data augmentation approach. In a nutshell, this work presents a promising direction for improving lesion segmentation in whole-body PET/CT scans with limited datasets, potentially enhancing the accuracy and reliability of cancer diagnostics.

Translated Abstract:
전체 신체 PET/CT 스캔에서 정확한 병변 분할은 암 진단과 치료 계획에 정말 중요해. 하지만 데이터셋이 제한적이면 자동 분할 모델의 성능이 떨어질 수 있어. 그래서 이 논문에서는 생성 모델의 딥 프라이어를 활용해서 자동 병변 분할을 위한 데이터 증강 방법을 탐구해봤어.

우리는 원래 CT 이미지를 위해 설계된 DiffTumor 방법을 조정해서 병변이 있는 합성 PET-CT 이미지를 생성했어. 이 방법은 AutoPET 데이터셋으로 생성 모델을 훈련시키고, 이를 통해 훈련 데이터를 확장하는 방식이야. 그 다음에는 원본 데이터셋과 증강된 데이터셋으로 훈련된 분할 모델의 성능을 비교했어.

결과를 보니까, 증강된 데이터셋으로 훈련된 모델이 더 높은 Dice 점수를 기록했어. 이게 우리 데이터 증강 접근 방식의 가능성을 보여주는 거지. 간단히 말해서, 이 연구는 제한된 데이터셋으로 전체 신체 PET/CT 스캔에서 병변 분할을 개선할 수 있는 유망한 방향을 제시하고 있어. 이게 암 진단의 정확성과 신뢰성을 높일 수 있을 것 같아.

================================================================================

URL:
https://arxiv.org/pdf/2409.08122.pdf

Title: GAZEploit: Remote Keystroke Inference Attack by Gaze Estimation from Avatar Views in VR/MR Devices

Original Abstract:
The advent and growing popularity of Virtual Reality (VR) and Mixed Reality (MR) solutions have revolutionized the way we interact with digital platforms. The cutting-edge gaze-controlled typing methods, now prevalent in high-end models of these devices, e.g., Apple Vision Pro, have not only improved user experience but also mitigated traditional keystroke inference attacks that relied on hand gestures, head movements and acoustic side-channels. However, this advancement has paradoxically given birth to a new, potentially more insidious cyber threat, GAZEploit.
In this paper, we unveil GAZEploit, a novel eye-tracking based attack specifically designed to exploit these eye-tracking information by leveraging the common use of virtual appearances in VR applications. This widespread usage significantly enhances the practicality and feasibility of our attack compared to existing methods. GAZEploit takes advantage of this vulnerability to remotely extract gaze estimations and steal sensitive keystroke information across various typing scenarios-including messages, passwords, URLs, emails, and passcodes. Our research, involving 30 participants, achieved over 80% accuracy in keystroke inference. Alarmingly, our study also identified over 15 top-rated apps in the Apple Store as vulnerable to the GAZEploit attack, emphasizing the urgent need for bolstered security measures for this state-of-the-art VR/MR text entry method.

Translated Abstract:
가상 현실(VR)과 혼합 현실(MR) 기술의 발전과 인기가 높아지면서 우리가 디지털 플랫폼과 상호작용하는 방식이 크게 변화했어. 요즘 고급 모델, 예를 들어 애플 비전 프로 같은 기기에서 사용하는 눈 움직임으로 제어하는 타이핑 방법이 사용자 경험을 개선했을 뿐만 아니라, 기존의 손 제스처나 머리 움직임, 소리 기반 공격을 줄여줬어. 그런데 이 발전이 아이러니하게도 새로운 사이버 위협인 GAZEploit을 만들어냈어.

이 논문에서는 GAZEploit이라는 새로운 눈 추적 기반 공격 방법을 소개해. 이 공격은 VR 애플리케이션에서 가상 모습이 흔히 사용되는 것을 이용해 눈 추적 정보를 악용하도록 설계됐어. 이런 사용이 널리 퍼져 있기 때문에 우리의 공격은 기존 방법들보다 더 실용적이고 실행 가능해. GAZEploit은 이 취약점을 이용해 원거리에서 시선 추정값을 추출하고, 메시지, 비밀번호, URL, 이메일, 패스코드와 같은 민감한 타이핑 정보를 빼낼 수 있어. 

우리 연구는 30명의 참가자를 대상으로 진행했는데, 타이핑 추정 정확도가 80%를 넘었어. 놀랍게도, 연구 결과 애플 스토어에서 GAZEploit 공격에 취약한 15개 이상의 인기 앱도 확인됐어. 이로 인해 최신 VR/MR 텍스트 입력 방식에 대한 보안 조치를 강화할 필요성이 급박하게 대두되고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08130.pdf

Title: The JPEG Pleno Learning-based Point Cloud Coding Standard: Serving Man and Machine

Original Abstract:
Efficient point cloud coding has become increasingly critical for multiple applications such as virtual reality, autonomous driving, and digital twin systems, where rich and interactive 3D data representations may functionally make the difference. Deep learning has emerged as a powerful tool in this domain, offering advanced techniques for compressing point clouds more efficiently than conventional coding methods while also allowing effective computer vision tasks performed in the compressed domain thus, for the first time, making available a common compressed visual representation effective for both man and machine. Taking advantage of this potential, JPEG has recently finalized the JPEG Pleno Learning-based Point Cloud Coding (PCC) standard offering efficient lossy coding of static point clouds, targeting both human visualization and machine processing by leveraging deep learning models for geometry and color coding. The geometry is processed directly in its original 3D form using sparse convolutional neural networks, while the color data is projected onto 2D images and encoded using the also learning-based JPEG AI standard. The goal of this paper is to provide a complete technical description of the JPEG PCC standard, along with a thorough benchmarking of its performance against the state-of-the-art, while highlighting its main strengths and weaknesses. In terms of compression performance, JPEG PCC outperforms the conventional MPEG PCC standards, especially in geometry coding, achieving significant rate reductions. Color compression performance is less competitive but this is overcome by the power of a full learning-based coding framework for both geometry and color and the associated effective compressed domain processing.

Translated Abstract:
효율적인 포인트 클라우드 코딩은 가상 현실, 자율 주행, 디지털 트윈 시스템 같은 다양한 응용 프로그램에서 점점 더 중요해지고 있어. 이런 곳에서는 풍부하고 상호작용할 수 있는 3D 데이터 표현이 큰 차이를 만들 수 있거든. 딥러닝이 이 분야에서 강력한 도구로 떠올랐고, 기존의 코딩 방법보다 포인트 클라우드를 더 효율적으로 압축할 수 있는 고급 기술을 제공해. 이 덕분에 압축된 상태에서도 컴퓨터 비전 작업을 효과적으로 수행할 수 있게 되었고, 인간과 기계 모두에게 유용한 공통의 압축된 시각 표현이 가능해졌어.

이런 가능성을 활용해서 최근에 JPEG에서 JPEG Pleno Learning-based Point Cloud Coding (PCC) 표준을 최종 확정했어. 이 표준은 정적 포인트 클라우드를 효율적으로 손실 압축할 수 있도록 해주고, 인간의 시각화와 기계 처리를 모두 겨냥하고 있어. 딥러닝 모델을 사용해 기하학과 색상을 코딩하는 방식이야. 기하학 데이터는 원래 3D 형태로 희소 컨볼루션 신경망을 통해 직접 처리되고, 색상 데이터는 2D 이미지로 변환되어 학습 기반의 JPEG AI 표준을 사용해 인코딩돼.

이 논문의 목표는 JPEG PCC 표준에 대한 완전한 기술 설명을 제공하고, 최신 기술과의 성능 비교를 통해 장단점을 자세히 보여주는 거야. 압축 성능 면에서 JPEG PCC는 기존의 MPEG PCC 표준보다 뛰어난데, 특히 기하학 코딩에서 큰 비율 감소를 달성했어. 색상 압축 성능은 경쟁력이 떨어지지만, 기하학과 색상 모두에 대해 완전한 학습 기반 코딩 프레임워크의 힘과 효과적인 압축 도메인 처리를 통해 극복할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08143.pdf

Title: Effective Segmentation of Post-Treatment Gliomas Using Simple Approaches: Artificial Sequence Generation and Ensemble Models

Original Abstract:
Segmentation is a crucial task in the medical imaging field and is often an important primary step or even a prerequisite to the analysis of medical volumes. Yet treatments such as surgery complicate the accurate delineation of regions of interest. The BraTS Post-Treatment 2024 Challenge published the first public dataset for post-surgery glioma segmentation and addresses the aforementioned issue by fostering the development of automated segmentation tools for glioma in MRI data. In this effort, we propose two straightforward approaches to enhance the segmentation performances of deep learning-based methodologies. First, we incorporate an additional input based on a simple linear combination of the available MRI sequences input, which highlights enhancing tumors. Second, we employ various ensembling methods to weigh the contribution of a battery of models. Our results demonstrate that these approaches significantly improve segmentation performance compared to baseline models, underscoring the effectiveness of these simple approaches in improving medical image segmentation tasks.

Translated Abstract:
분할(segmentation)은 의료 영상 분야에서 중요한 작업이야. 보통은 의료 데이터 분석의 첫 단계이거나 필수적인 과정이지. 하지만 수술 같은 치료가 있으면 관심 영역을 정확하게 구분하기가 어려워져. 

BraTS Post-Treatment 2024 Challenge에서는 수술 후 신경교종 분할을 위한 첫 번째 공개 데이터셋을 발표했어. 이 대회는 MRI 데이터에서 신경교종을 자동으로 분할할 수 있는 도구 개발을 촉진하는 데 초점을 맞추고 있어. 

우리는 딥러닝 기반 방법의 분할 성능을 향상시키기 위해 두 가지 간단한 접근 방식을 제안해. 첫 번째로, 기존 MRI 시퀀스 데이터를 간단한 선형 조합으로 추가 입력으로 넣어서 종양을 더 잘 강조하는 거야. 두 번째로, 여러 모델의 기여도를 조절하기 위해 다양한 앙상블 방법을 사용해. 

우리의 결과는 이 방법들이 기본 모델에 비해 분할 성능을 크게 향상시킨다는 걸 보여줘. 이런 간단한 접근 방식이 의료 영상 분할 작업을 개선하는 데 효과적임을 강조하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08163.pdf

Title: Open Source Infrastructure for Automatic Cell Segmentation

Original Abstract:
Automated cell segmentation is crucial for various biological and medical applications, facilitating tasks like cell counting, morphology analysis, and drug discovery. However, manual segmentation is time-consuming and prone to subjectivity, necessitating robust automated methods. This paper presents open-source infrastructure, utilizing the UNet model, a deep-learning architecture noted for its effectiveness in image segmentation tasks. This implementation is integrated into the open-source DeepChem package, enhancing accessibility and usability for researchers and practitioners. The resulting tool offers a convenient and user-friendly interface, reducing the barrier to entry for cell segmentation while maintaining high accuracy. Additionally, we benchmark this model against various datasets, demonstrating its robustness and versatility across different imaging conditions and cell types.

Translated Abstract:
자동 세포 세분화는 생물학 및 의학에서 중요해. 세포 수 세기, 형태 분석, 약물 발견 같은 작업을 쉽게 할 수 있게 도와주거든. 하지만 수동 세분화는 시간이 많이 걸리고 주관적일 수 있어서, 강력한 자동화 방법이 필요해.

이 논문에서는 UNet 모델을 이용한 오픈소스 인프라를 소개해. UNet은 이미지 세분화 작업에 효과적인 딥러닝 구조로 유명해. 이 구현은 오픈소스 DeepChem 패키지에 통합되어 연구자와 실무자들이 더 쉽게 접근하고 사용할 수 있도록 해.

이 도구는 사용하기 편한 인터페이스를 제공해서, 세포 세분화에 대한 진입 장벽을 낮추면서도 높은 정확도를 유지해. 게다가, 우리는 이 모델을 다양한 데이터셋과 비교해서 테스트해봤고, 다양한 이미징 조건과 세포 유형에서 강력하고 다재다능함을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08170.pdf

Title: AD-Lite Net: A Lightweight and Concatenated CNN Model for Alzheimer's Detection from MRI Images

Original Abstract:
Alzheimer's Disease (AD) is a non-curable progressive neurodegenerative disorder that affects the human brain, leading to a decline in memory, cognitive abilities, and eventually, the ability to carry out daily tasks. Manual diagnosis of Alzheimer's disease from MRI images is fraught with less sensitivity and it is a very tedious process for neurologists. Therefore, there is a need for an automatic Computer Assisted Diagnosis (CAD) system, which can detect AD at early stages with higher accuracy. In this research, we have proposed a novel AD-Lite Net model (trained from scratch), that could alleviate the aforementioned problem. The novelties we bring here in this research are, (I) We have proposed a very lightweight CNN model by incorporating Depth Wise Separable Convolutional (DWSC) layers and Global Average Pooling (GAP) layers. (II) We have leveraged a ``parallel concatenation block'' (pcb), in the proposed AD-Lite Net model. This pcb consists of a Transformation layer (Tx-layer), followed by two convolutional layers, which are thereby concatenated with the original base model. This Tx-layer converts the features into very distinct kind of features, which are imperative for the Alzheimer's disease. As a consequence, the proposed AD-Lite Net model with ``parallel concatenation'' converges faster and automatically mitigates the class imbalance problem from the MRI datasets in a very generalized way. For the validity of our proposed model, we have implemented it on three different MRI datasets. Furthermore, we have combined the ADNI and AD datasets and subsequently performed a 10-fold cross-validation experiment to verify the model's generalization ability. Extensive experimental results showed that our proposed model has outperformed all the existing CNN models, and one recent trend Vision Transformer (ViT) model by a significant margin.

Translated Abstract:
알츠하이머병(AD)은 치료할 수 없는 진행성 신경퇴행성 질환으로, 인간의 뇌에 영향을 주어 기억력, 인지 능력, 그리고 결국 일상적인 작업 수행 능력이 저하되는 병이야. MRI 이미지를 통해 알츠하이머병을 수동으로 진단하는 건 민감도가 떨어지고 신경과 의사에게는 정말 지루한 과정이야. 그래서 조기 단계에서 더 높은 정확도로 AD를 감지할 수 있는 자동화된 컴퓨터 보조 진단(CAD) 시스템이 필요해.

이 연구에서는 이런 문제를 해결할 수 있는 새로운 AD-Lite Net 모델을 제안했어(처음부터 훈련한 거야). 여기서 우리가 가져온 새로운 점은 두 가지야. 첫째, Depth Wise Separable Convolutional(DWSC) 레이어와 Global Average Pooling(GAP) 레이어를 포함해서 아주 가벼운 CNN 모델을 제안했어. 둘째, 제안한 AD-Lite Net 모델에 “병렬 연결 블록(parallel concatenation block, pcb)”을 활용했어. 이 pcb는 변환 레이어(Tx-layer)와 두 개의 컨볼루션 레이어로 구성되어, 원래 모델과 연결돼. 이 Tx-layer는 알츠하이머병에 중요한 아주 독특한 특징으로 변환해.

결과적으로, “병렬 연결”이 적용된 AD-Lite Net 모델은 더 빠르게 수렴하고, MRI 데이터셋에서 클래스 불균형 문제를 일반적인 방법으로 자동으로 완화해. 제안한 모델의 유효성을 위해 세 가지 다른 MRI 데이터셋에서 구현해봤어. 게다가 ADNI와 AD 데이터셋을 합쳐서 10겹 교차 검증 실험을 수행해 모델의 일반화 능력을 확인했어. 많은 실험 결과를 통해, 우리 모델이 기존의 모든 CNN 모델과 최근 트렌드인 Vision Transformer(ViT) 모델보다 상당히 우수한 성능을 보였다는 걸 알 수 있었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08232.pdf

Title: Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging

Original Abstract:
Segmenting brain tumors in multi-parametric magnetic resonance imaging enables performing quantitative analysis in support of clinical trials and personalized patient care. This analysis provides the potential to impact clinical decision-making processes, including diagnosis and prognosis. In 2023, the well-established Brain Tumor Segmentation (BraTS) challenge presented a substantial expansion with eight tasks and 4,500 brain tumor cases. In this paper, we present a deep learning-based ensemble strategy that is evaluated for newly included tumor cases in three tasks: pediatric brain tumors (PED), intracranial meningioma (MEN), and brain metastases (MET). In particular, we ensemble outputs from state-of-the-art nnU-Net and Swin UNETR models on a region-wise basis. Furthermore, we implemented a targeted post-processing strategy based on a cross-validated threshold search to improve the segmentation results for tumor sub-regions. The evaluation of our proposed method on unseen test cases for the three tasks resulted in lesion-wise Dice scores for PED: 0.653, 0.809, 0.826; MEN: 0.876, 0.867, 0.849; and MET: 0.555, 0.6, 0.58; for the enhancing tumor, tumor core, and whole tumor, respectively. Our method was ranked first for PED, third for MEN, and fourth for MET, respectively.

Translated Abstract:
다중 매개변수 자기공명영상(MRI)에서 뇌종양을 분할하는 것은 임상 시험과 개인 맞춤형 환자 치료를 지원하는 양적 분석을 수행할 수 있게 해줘. 이 분석은 진단과 예후 같은 임상 의사결정 과정에 영향을 미칠 수 있는 잠재력을 제공해. 

2023년에, 잘 알려진 뇌종양 분할(BraTS) 챌린지가 8개의 작업과 4,500개의 뇌종양 사례로 크게 확장되었어. 이 논문에서는 새로 포함된 종양 사례에 대해 세 가지 작업인 소아 뇌종양(PED), 두개내 수막종(MEN), 그리고 뇌 전이종양(MET)에 대해 평가된 딥러닝 기반의 앙상블 전략을 소개해. 특히, 최신 nnU-Net과 Swin UNETR 모델의 출력을 지역별로 앙상블했어. 

또한, 종양 하위 영역의 분할 결과를 개선하기 위해 교차 검증된 임계값 탐색을 기반으로 한 목표 지향적인 후처리 전략을 구현했어. 세 가지 작업에 대한 보지 않은 테스트 사례에서 우리 방법의 평가 결과는 PED: 0.653, 0.809, 0.826; MEN: 0.876, 0.867, 0.849; MET: 0.555, 0.6, 0.58의 병변별 Dice 점수를 기록했어. 우리 방법은 PED에서 1위, MEN에서 3위, MET에서 4위를 차지했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.08273.pdf

Title: Hand-Object Interaction Pretraining from Videos

Original Abstract:
We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories. We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories. We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions. Generative modeling on this data gives us a task-agnostic base policy. This policy captures a general yet flexible manipulation prior. We empirically demonstrate that finetuning this policy, with both reinforcement learning (RL) and behavior cloning (BC), enables sample-efficient adaptation to downstream tasks and simultaneously improves robustness and generalizability compared to prior approaches. Qualitative experiments are available at: \url{this https URL}.

Translated Abstract:
우리는 3D 손-물체 상호작용 궤적에서 일반적인 로봇 조작 사전 정보를 배우는 방법을 제안해. 우리는 실제 환경에서 촬영된 비디오를 사용해서 감각-운동 로봇 궤적을 생성하는 프레임워크를 만들었어. 이 과정에서 인간의 손과 조작되는 물체를 공유된 3D 공간에서 동시에 올리고, 인간의 움직임을 로봇의 행동으로 다시 맞추는 방식이야. 

이 데이터를 바탕으로 생성 모델링을 하면 작업에 구애받지 않는 기본 정책을 얻을 수 있어. 이 정책은 일반적이면서도 유연한 조작 사전 정보를 포착해. 우리는 이 정책을 강화 학습(RL)과 행동 복제(BC)로 미세 조정하면, 후속 작업에 대한 샘플 효율적인 적응이 가능하고, 이전 방법들과 비교했을 때 강인함과 일반화 능력이 동시에 향상된다는 것을 경험적으로 보여줬어. 

자세한 실험 결과는 여기에 있어: \url{this https URL}.

================================================================================

URL:
https://arxiv.org/pdf/2302.10763.pdf

Title: Contrastive Learning and the Emergence of Attributes Associations

Original Abstract:
In response to an object presentation, supervised learning schemes generally respond with a parsimonious label. Upon a similar presentation we humans respond again with a label, but are flooded, in addition, by a myriad of associations. A significant portion of these consist of the presented object attributes. Contrastive learning is a semi-supervised learning scheme based on the application of identity preserving transformations on the object input representations. It is conjectured in this work that these same applied transformations preserve, in addition to the identity of the presented object, also the identity of its semantically meaningful attributes. The corollary of this is that the output representations of such a contrastive learning scheme contain valuable information not only for the classification of the presented object, but also for the presence or absence decision of any attribute of interest. Simulation results which demonstrate this idea and the feasibility of this conjecture are presented.

Translated Abstract:
물체가 제시되면, 감독 학습 방식에서는 일반적으로 간단한 레이블로 반응해. 비슷한 물체가 제시되면, 우리 인간은 다시 레이블로 반응하지만, 그와 함께 많은 연관 정보가 떠오르지. 이들 중 상당 부분은 제시된 물체의 속성으로 구성돼.

대조 학습은 물체 입력 표현에 대해 정체성을 유지하는 변환을 적용하는 반감독 학습 방식이야. 이 연구에서는 이러한 변환이 제시된 물체의 정체성뿐만 아니라, 의미 있는 속성의 정체성도 유지한다고 추측하고 있어. 이로 인해 대조 학습 방식의 출력 표현은 제시된 물체의 분류뿐만 아니라, 관심 있는 속성의 존재 여부를 판단하는 데도 유용한 정보를 포함하고 있어.

이 아이디어와 이 추측이 가능하다는 것을 보여주는 시뮬레이션 결과도 제시될 거야.

================================================================================

URL:
https://arxiv.org/pdf/2303.14703.pdf

Title: Exploring the Interplay Between Colorectal Cancer Subtypes Genomic Variants and Cellular Morphology: A Deep-Learning Approach

Original Abstract:
Molecular subtypes of colorectal cancer (CRC) significantly influence treatment decisions. While convolutional neural networks (CNNs) have recently been introduced for automated CRC subtype identification using H&E stained histopathological images, the correlation between CRC subtype genomic variants and their corresponding cellular morphology expressed by their imaging phenotypes is yet to be fully explored. The goal of this study was to determine such correlations by incorporating genomic variants in CNN models for CRC subtype classification from H&E images. We utilized the publicly available TCGA-CRC-DX dataset, which comprises whole slide images from 360 CRC-diagnosed patients (260 for training and 100 for testing). This dataset also provides information on CRC subtype classifications and genomic variations. We trained CNN models for CRC subtype classification that account for potential correlation between genomic variations within CRC subtypes and their corresponding cellular morphology patterns. We assessed the interplay between CRC subtypes' genomic variations and cellular morphology patterns by evaluating the CRC subtype classification accuracy of the different models in a stratified 5-fold cross-validation experimental setup using the area under the ROC curve (AUROC) and average precision (AP) as the performance metrics. Combining the CNN models account for variations in CIMP and SNP further improved classification accuracy (AUROC: 0.847$\pm$0.01 vs. 0.787$\pm$0.03, p$=$0.01, AP: 0.68$\pm$0.02 vs. 0.64$\pm$0.05).

Translated Abstract:
대장암(CRC)의 분자 아형은 치료 결정에 큰 영향을 미쳐. 최근에 합성곱 신경망(CNN)이 H&E 염색된 조직병리 이미지를 사용해서 CRC 아형을 자동으로 식별하는 데 도입됐지만, CRC 아형의 유전자 변이와 그에 해당하는 세포 형태 간의 관계는 아직 완전히 밝혀지지 않았어. 이 연구의 목표는 H&E 이미지에서 CRC 아형 분류를 위해 유전자 변이를 CNN 모델에 포함시켜서 이런 관계를 확인하는 거야.

우리는 TCGA-CRC-DX 데이터셋을 사용했어. 이 데이터셋은 360명의 CRC 진단 환자에서 얻은 전체 슬라이드 이미지를 포함하고 있어(260개는 훈련용, 100개는 테스트용이야). 이 데이터셋은 CRC 아형 분류와 유전자 변이에 대한 정보도 제공해. 우리는 CRC 아형 분류를 위한 CNN 모델을 훈련시켰고, CRC 아형 내에서 유전자 변이와 그에 해당하는 세포 형태 패턴 간의 잠재적 상관관계를 고려했어.

우리는 다양한 모델의 CRC 아형 분류 정확도를 평가해서 CRC 아형의 유전자 변이와 세포 형태 패턴 간의 상호작용을 확인했어. 5겹 교차 검증 실험을 통해 ROC 곡선 아래 면적(AUROC)과 평균 정밀도(AP)를 성능 지표로 사용했어. CIMP와 SNP의 변이를 고려한 CNN 모델을 결합하니까 분류 정확도가 더 좋아졌어(AUROC: 0.847±0.01 vs. 0.787±0.03, p=0.01, AP: 0.68±0.02 vs. 0.64±0.05).

================================================================================

URL:
https://arxiv.org/pdf/2303.15288.pdf

Title: Memory-Efficient 3D Denoising Diffusion Models for Medical Image Processing

Original Abstract:
Denoising diffusion models have recently achieved state-of-the-art performance in many image-generation tasks. They do, however, require a large amount of computational resources. This limits their application to medical tasks, where we often deal with large 3D volumes, like high-resolution three-dimensional data. In this work, we present a number of different ways to reduce the resource consumption for 3D diffusion models and apply them to a dataset of 3D images. The main contribution of this paper is the memory-efficient patch-based diffusion model \textit{PatchDDM}, which can be applied to the total volume during inference while the training is performed only on patches. While the proposed diffusion model can be applied to any image generation tasks, we evaluate the method on the tumor segmentation task of the BraTS2020 dataset and demonstrate that we can generate meaningful three-dimensional segmentations.

Translated Abstract:
최근에 노이즈 제거 확산 모델이 여러 이미지 생성 작업에서 최첨단 성능을 달성했어. 하지만 이 모델들은 많은 컴퓨터 자원을 필요로 해. 그래서 큰 3D 볼륨을 다루는 의료 작업에서는 사용하기 어려운 경우가 많아.

이번 연구에서는 3D 확산 모델의 자원 소비를 줄이는 여러 가지 방법을 제안하고, 이를 3D 이미지 데이터셋에 적용했어. 이 논문의 주요 기여는 메모리 효율적인 패치 기반 확산 모델인 \textit{PatchDDM}이야. 이 모델은 추론할 때 전체 볼륨에 적용할 수 있지만, 훈련은 패치 단위로만 진행돼.

제안된 확산 모델은 어떤 이미지 생성 작업에도 적용할 수 있지만, 우리는 BraTS2020 데이터셋의 종양 분할 작업에서 방법을 평가해봤어. 그 결과, 의미 있는 3차원 분할을 생성할 수 있다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2308.03717.pdf

Title: Nerve Block Target Localization and Needle Guidance for Autonomous Robotic Ultrasound Guided Regional Anesthesia

Original Abstract:
Visual servoing for the development of autonomous robotic systems capable of administering UltraSound (US) guided regional anesthesia requires real-time segmentation of nerves, needle tip localization and needle trajectory extrapolation. First, we recruited 227 patients to build a large dataset of 41,000 anesthesiologist annotated images from US videos of brachial plexus nerves and developed models to localize nerves in the US images. Generalizability of the best suited model was tested on the datasets constructed from separate US scanners. Using these nerve segmentation predictions, we define automated anesthesia needle targets by fitting an ellipse to the nerve contours. Next, we developed an image analysis tool to guide the needle toward their targets. For the segmentation of the needle, a natural RGB pre-trained neural network was first fine-tuned on a large US dataset for domain transfer and then adapted for the needle using a small dataset. The segmented needle trajectory angle is calculated using Radon transformation and the trajectory is extrapolated from the needle tip. The intersection of the extrapolated trajectory with the needle target guides the needle navigation for drug delivery. The needle trajectory average error was within acceptable range of 5 mm as per experienced anesthesiologists. The entire dataset has been released publicly for further study by the research community at this https URL

Translated Abstract:
자율 로봇 시스템이 초음파(US) 유도 하부 마취를 수행하려면, 신경 실시간 분할, 바늘 끝 위치 찾기, 바늘 경로 예측이 필요해. 먼저, 227명의 환자를 모집해서 41,000개의 마취과 의사가 주석을 단 이미지를 포함한 대규모 데이터셋을 만들었고, 그걸 바탕으로 US 이미지에서 신경을 찾는 모델을 개발했어. 가장 적합한 모델의 일반화 능력은 다른 US 스캐너로 만든 데이터셋에서 테스트했어.

이 신경 분할 예측을 사용해서, 신경 윤곽에 타원을 맞춰 자동화된 마취 바늘 목표를 정의했어. 다음으로, 바늘을 목표로 안내하는 이미지 분석 도구를 개발했지. 바늘 분할을 위해, 자연 RGB로 미리 훈련된 신경망을 대규모 US 데이터셋에 맞춰 세밀 조정하고, 그 후 작은 데이터셋으로 바늘에 맞춰 수정했어. 바늘 경로 각도는 라돈 변환을 사용해서 계산하고, 바늘 끝에서 경로를 예측해. 예측된 경로와 바늘 목표의 교차점이 약물 전달을 위한 바늘 내비게이션을 안내해줘.

바늘 경로 평균 오차는 경험이 많은 마취과 의사 기준으로 5mm 이내로 허용 가능한 범위였어. 전체 데이터셋은 연구 커뮤니티의 추가 연구를 위해 공개되었고, 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2308.16819.pdf

Title: BTSeg: Barlow Twins Regularization for Domain Adaptation in Semantic Segmentation

Original Abstract:
We introduce BTSeg (Barlow Twins regularized Segmentation), an innovative, semi-supervised training approach enhancing semantic segmentation models in order to effectively tackle adverse weather conditions without requiring additional labeled training data. Images captured at similar locations but under varying adverse conditions are regarded as manifold representation of the same scene, thereby enabling the model to conceptualize its understanding of the environment. BTSeg shows cutting-edge performance for the new challenging ACG benchmark and sets a new state-of-the-art for weakly-supervised domain adaptation for the ACDC dataset. To support further research, we have made our code publicly available at this https URL .

Translated Abstract:
BTSeg(Barlow Twins 정규화 세분화)를 소개해. 이건 혁신적인 반지도 학습 방법으로, 추가적인 레이블이 있는 훈련 데이터 없이도 악천후 상황에서 의미론적 세분화 모델을 효과적으로 향상시켜.

비슷한 장소에서 찍은 이미지들이지만 각기 다른 악조건에서 촬영된 것들은 같은 장면의 다양성을 나타내는 것으로 봐. 그래서 모델이 환경을 이해하는 방식을 더 잘 설계할 수 있어. 

BTSeg는 새로운 도전적인 ACG 벤치마크에서 최첨단 성능을 보여주고, ACDC 데이터셋에 대한 약한 지도 학습 도메인 적응에서 새로운 최첨단을 세웠어. 

더 많은 연구를 지원하기 위해, 우리의 코드를 이 https URL에서 공개했어.

================================================================================

URL:
https://arxiv.org/pdf/2309.07808.pdf

Title: What Matters to Enhance Traffic Rule Compliance of Imitation Learning for End-to-End Autonomous Driving

Original Abstract:
End-to-end autonomous driving, where the entire driving pipeline is replaced with a single neural network, has recently gained research attention because of its simpler structure and faster inference time. Despite this appealing approach largely reducing the complexity in the driving pipeline, it also leads to safety issues because the trained policy is not always compliant with the traffic rules. In this paper, we proposed P-CSG, a penalty-based imitation learning approach with contrastive-based cross semantics generation sensor fusion technologies to increase the overall performance of end-to-end autonomous driving. In this method, we introduce three penalties - red light, stop sign, and curvature speed penalty to make the agent more sensitive to traffic rules. The proposed cross semantics generation helps to align the shared information of different input modalities. We assessed our model's performance using the CARLA Leaderboard - Town 05 Long Benchmark and Longest6 Benchmark, achieving 8.5% and 2.0% driving score improvement compared to the baselines. Furthermore, we conducted robustness evaluations against adversarial attacks like FGSM and Dot attacks, revealing a substantial increase in robustness compared to other baseline models. More detailed information can be found at this https URL.

Translated Abstract:
최근 전체 자율주행 프로세스를 하나의 신경망으로 대체하는 "엔드 투 엔드 자율주행"이 연구자들 사이에서 주목받고 있어. 구조가 간단하고 추론 속도가 빠르다는 장점이 있거든. 하지만 이런 방식이 주행 파이프라인의 복잡성을 줄이는 대신, 훈련된 정책이 항상 교통 법규를 준수하지 않아서 안전 문제를 일으킬 수 있어.

이 논문에서는 P-CSG라는 방법을 제안했어. 이건 벌점을 기반으로 한 모방 학습 접근 방식으로, 대조 기반의 교차 의미 생성 센서 융합 기술을 사용해서 엔드 투 엔드 자율주행의 전반적인 성능을 높이는 거야. 이 방법에서는 빨간 신호, 정지 표지, 곡률 속도에 대한 세 가지 벌점을 도입해서 에이전트가 교통 법규에 더 민감하게 반응하도록 해.

제안된 교차 의미 생성 기술은 서로 다른 입력 방식의 공유 정보를 정렬하는 데 도움을 줘. 우리는 CARLA 리더보드의 Town 05 Long Benchmark와 Longest6 Benchmark를 사용해서 모델 성능을 평가했어. 그 결과, 기준 모델에 비해 각각 8.5%와 2.0%의 주행 점수 개선을 달성했지. 게다가 FGSM와 Dot 공격 같은 적대적 공격에 대한 강인성 평가도 진행했는데, 다른 기준 모델들에 비해 상당한 강인성을 보여줬어. 더 자세한 정보는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2309.14660.pdf

Title: CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud Registration

Original Abstract:
Image-to-point cloud (I2P) registration is a fundamental task for robots and autonomous vehicles to achieve cross-modality data fusion and localization. Current I2P registration methods primarily focus on estimating correspondences at the point or pixel level, often neglecting global alignment. As a result, I2P matching can easily converge to a local optimum if it lacks high-level guidance from global constraints. To improve the success rate and general robustness, this paper introduces CoFiI2P, a novel I2P registration network that extracts correspondences in a coarse-to-fine manner. First, the image and point cloud data are processed through a two-stream encoder-decoder network for hierarchical feature extraction. Second, a coarse-to-fine matching module is designed to leverage these features and establish robust feature correspondences. Specifically, In the coarse matching phase, a novel I2P transformer module is employed to capture both homogeneous and heterogeneous global information from the image and point cloud data. This enables the estimation of coarse super-point/super-pixel matching pairs with discriminative descriptors. In the fine matching module, point/pixel pairs are established with the guidance of super-point/super-pixel correspondences. Finally, based on matching pairs, the transform matrix is estimated with the EPnP-RANSAC algorithm. Experiments conducted on the KITTI Odometry dataset demonstrate that CoFiI2P achieves impressive results, with a relative rotation error (RRE) of 1.14 degrees and a relative translation error (RTE) of 0.29 meters, while maintaining real-time speed.Additional experiments on the Nuscenes datasets confirm our method's generalizability. The project page is available at \url{this https URL}.

Translated Abstract:
이미지-포인트 클라우드(I2P) 정합은 로봇과 자율주행차가 서로 다른 데이터 유형을 융합하고 위치를 파악하는 데 중요한 작업이야. 현재 I2P 정합 방법들은 주로 포인트나 픽셀 수준에서 대응 관계를 추정하는 데 집중하고 있는데, 이 과정에서 전체적인 정렬을 간과하는 경우가 많아. 그래서 I2P 매칭은 글로벌 제약이 없는 경우 지역 최적점에 쉽게 수렴할 수 있어. 

이 논문에서는 성공률과 일반적인 강인성을 높이기 위해 CoFiI2P라는 새로운 I2P 정합 네트워크를 소개해. 이 네트워크는 거칠게부터 세밀하게 대응 관계를 추출하는 방식으로 설계되었어. 먼저, 이미지와 포인트 클라우드 데이터를 두 개의 스트림으로 구성된 인코더-디코더 네트워크를 통해 처리해서 계층적 특징을 추출해. 그 다음, 이러한 특징을 활용해서 강력한 특징 대응 관계를 수립하는 거칠게부터 세밀한 매칭 모듈이 디자인되어 있어.

구체적으로 거칠게 매칭하는 단계에서는 새로운 I2P 변환기 모듈을 사용해서 이미지와 포인트 클라우드 데이터에서 동질적이고 이질적인 글로벌 정보를 모두 포착해. 이렇게 해서 구별 가능한 설명자를 가진 거칠게 매칭되는 슈퍼 포인트/슈퍼 픽셀 쌍을 추정할 수 있어. 세밀한 매칭 모듈에서는 슈퍼 포인트/슈퍼 픽셀 대응 관계의 안내를 받아 포인트/픽셀 쌍을 형성해. 마지막으로, 매칭된 쌍을 바탕으로 EPnP-RANSAC 알고리즘을 사용해 변환 행렬을 추정해.

KITTI 오도메트리 데이터셋에서 진행된 실험 결과, CoFiI2P가 놀라운 성과를 달성했어. 상대 회전 오차(RRE)는 1.14도, 상대 변환 오차(RTE)는 0.29미터로, 실시간 속도를 유지하면서 말이야. Nuscenes 데이터셋에서 추가 실험을 통해 우리 방법의 일반화 가능성도 확인했어. 프로젝트 페이지는 \url{this https URL}에서 볼 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2311.13254.pdf

Title: Unified Domain Adaptive Semantic Segmentation

Original Abstract:
Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS) aims to transfer the supervision from a labeled source domain to an unlabeled target domain. The majority of existing UDA-SS works typically consider images whilst recent attempts have extended further to tackle videos by modeling the temporal dimension. Although the two lines of research share the major challenges -- overcoming the underlying domain distribution shift, their studies are largely independent, resulting in fragmented insights, a lack of holistic understanding, and missed opportunities for cross-pollination of ideas. This fragmentation prevents the unification of methods, leading to redundant efforts and suboptimal knowledge transfer across image and video domains. Under this observation, we advocate unifying the study of UDA-SS across video and image scenarios, enabling a more comprehensive understanding, synergistic advancements, and efficient knowledge sharing. To that end, we explore the unified UDA-SS from a general data augmentation perspective, serving as a unifying conceptual framework, enabling improved generalization, and potential for cross-pollination of ideas, ultimately contributing to the overall progress and practical impact of this field of research. Specifically, we propose a Quad-directional Mixup (QuadMix) method, characterized by tackling distinct point attributes and feature inconsistencies through four-directional paths for intra- and inter-domain mixing in a feature space. To deal with temporal shifts with videos, we incorporate optical flow-guided feature aggregation across spatial and temporal dimensions for fine-grained domain alignment. Extensive experiments show that our method outperforms the state-of-the-art works by large margins on four challenging UDA-SS benchmarks. Our source code and models will be released at \url{this https URL}.

Translated Abstract:
비지도 도메인 적응 의미 분할(UDA-SS)은 레이블이 있는 소스 도메인에서 레이블이 없는 타겟 도메인으로 감독을 전이하는 걸 목표로 해. 기존의 UDA-SS 연구들은 주로 이미지에 초점을 맞추고 있었는데, 최근에는 비디오까지 확장하려는 시도가 있었어. 이때 시간적인 요소를 모델링하기도 하지. 

이 두 연구 방향은 공통의 큰 도전 과제를 가지고 있어. 바로 도메인 분포의 변화 문제를 극복하는 건데, 그런데 이 연구들이 서로 독립적으로 진행되다 보니 통합된 통찰력이 부족해지고, 전체적인 이해도 떨어지며, 아이디어 교류의 기회를 놓치고 있어. 이렇게 나뉘어 있는 상황은 방법론의 통합을 방해하고, 이미지와 비디오 도메인 간의 지식 전이도 비효율적으로 만들어. 

이런 문제를 해결하기 위해 우리는 UDA-SS를 비디오와 이미지 모두에서 통합적으로 연구할 필요성을 주장해. 그러면 더 포괄적인 이해와 시너지 있는 발전, 효율적인 지식 공유가 가능해질 거야. 그래서 우리는 일반적인 데이터 증강 관점에서 통합된 UDA-SS를 탐구하고, 이를 통해 더 나은 일반화와 아이디어의 교류 가능성을 높여서 이 연구 분야의 전반적인 발전과 실질적인 영향을 기여하고자 해. 

구체적으로 우리는 Quad-directional Mixup(QuadMix) 방법을 제안하는데, 이는 네 방향의 경로를 통해 특성 공간 내에서 도메인 간 혼합을 수행하면서 서로 다른 포인트 속성과 특성 불일치를 해결하는 데 초점을 맞추고 있어. 비디오의 시간적 변화 문제를 다루기 위해서는 공간적 및 시간적 차원에서 광학 흐름에 기반한 특성 집합을 포함시켜 세밀한 도메인 정렬을 이루고 있어. 

광범위한 실험 결과, 우리의 방법이 네 가지 도전적인 UDA-SS 벤치마크에서 최신 기술보다 훨씬 더 나은 성능을 보인다는 걸 보여줬어. 우리의 소스 코드와 모델은 \url{this https URL}에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2312.00330.pdf

Title: StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter

Original Abstract:
Text-to-video (T2V) models have shown remarkable capabilities in generating diverse videos. However, they struggle to produce user-desired stylized videos due to (i) text's inherent clumsiness in expressing specific styles and (ii) the generally degraded style fidelity. To address these challenges, we introduce StyleCrafter, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image. Considering the scarcity of stylized video datasets, we propose to first train a style control adapter using style-rich image datasets, then transfer the learned stylization ability to video generation through a tailor-made finetuning paradigm. To promote content-style disentanglement, we remove style descriptions from the text prompt and extract style information solely from the reference image using a decoupling learning strategy. Additionally, we design a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features, which helps generalization across various text and style combinations. StyleCrafter efficiently generates high-quality stylized videos that align with the content of the texts and resemble the style of the reference images. Experiments demonstrate that our approach is more flexible and efficient than existing competitors.

Translated Abstract:
텍스트-비디오(T2V) 모델은 다양한 비디오를 생성하는 데 뛰어난 능력을 보여줬어. 하지만 사용자들이 원하는 스타일의 비디오를 만드는 데는 어려움이 있어. 그 이유는 (i) 텍스트가 특정 스타일을 표현하는 데 어색하고 (ii) 스타일의 품질이 일반적으로 떨어지기 때문이야.

이 문제를 해결하기 위해 우리는 StyleCrafter라는 방법을 소개해. 이 방법은 스타일 제어 어댑터를 이용해 미리 훈련된 T2V 모델을 향상시켜주고, 참조 이미지를 제공함으로써 어떤 스타일로든 비디오를 생성할 수 있게 해. 스타일이 풍부한 이미지 데이터셋이 부족한 점을 고려해서, 먼저 스타일 제어 어댑터를 스타일이 풍부한 이미지 데이터셋으로 훈련하고, 그 후에 학습한 스타일링 능력을 비디오 생성에 맞게 조정하는 방법을 사용해.

컨텐츠와 스타일을 분리하는 걸 촉진하기 위해, 텍스트 프롬프트에서 스타일 설명을 제거하고 스타일 정보는 오로지 참조 이미지에서만 추출해. 이 과정에서 디커플링 학습 전략을 사용해. 또한, 텍스트 기반의 컨텐츠 특징과 이미지 기반의 스타일 특징의 영향을 균형있게 조절하기 위해 스케일 적응형 융합 모듈을 설계했어. 이게 다양한 텍스트와 스타일 조합에서 일반화되는 데 도움이 돼.

StyleCrafter는 텍스트의 내용과 참조 이미지의 스타일에 맞는 고품질 스타일 비디오를 효율적으로 생성해. 실험 결과, 우리의 접근 방식이 기존 경쟁자들보다 더 유연하고 효율적임을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2312.14206.pdf

Title: LLM4VG: Large Language Models Evaluation for Video Grounding

Original Abstract:
Recently, researchers have attempted to investigate the capability of LLMs in handling videos and proposed several video LLM models. However, the ability of LLMs to handle video grounding (VG), which is an important time-related video task requiring the model to precisely locate the start and end timestamps of temporal moments in videos that match the given textual queries, still remains unclear and unexplored in literature. To fill the gap, in this paper, we propose the LLM4VG benchmark, which systematically evaluates the performance of different LLMs on video grounding tasks. Based on our proposed LLM4VG, we design extensive experiments to examine two groups of video LLM models on video grounding: (i) the video LLMs trained on the text-video pairs (denoted as VidLLM), and (ii) the LLMs combined with pretrained visual description models such as the video/image captioning model. We propose prompt methods to integrate the instruction of VG and description from different kinds of generators, including caption-based generators for direct visual description and VQA-based generators for information enhancement. We also provide comprehensive comparisons of various VidLLMs and explore the influence of different choices of visual models, LLMs, prompt designs, etc, as well. Our experimental evaluations lead to two conclusions: (i) the existing VidLLMs are still far away from achieving satisfactory video grounding performance, and more time-related video tasks should be included to further fine-tune these models, and (ii) the combination of LLMs and visual models shows preliminary abilities for video grounding with considerable potential for improvement by resorting to more reliable models and further guidance of prompt instructions.

Translated Abstract:
최근 연구자들은 LLM(대규모 언어 모델)이 비디오를 다룰 수 있는 능력을 조사하려고 시도했으며, 여러 비디오 LLM 모델을 제안했습니다. 하지만 LLM이 비디오 그라운딩(VG)을 처리하는 능력은 아직 명확하지 않고 문헌에서도 잘 다뤄지지 않았습니다. VG는 주어진 텍스트 쿼리에 맞춰 비디오에서 특정 순간의 시작과 끝 타임스탬프를 정확히 찾아야 하는 중요한 시간 관련 비디오 작업입니다.

이러한 연구의 빈틈을 메우기 위해, 우리는 LLM4VG 벤치마크를 제안합니다. 이 벤치마크는 다양한 LLM의 비디오 그라운딩 작업 성능을 체계적으로 평가합니다. 제안한 LLM4VG를 기반으로, 우리는 두 그룹의 비디오 LLM 모델을 비디오 그라운딩에 대해 광범위한 실험을 설계했습니다: (i) 텍스트-비디오 쌍으로 훈련된 비디오 LLM(VidLLM), (ii) 비디오/이미지 캡셔닝 모델 같은 미리 학습된 시각적 설명 모델과 결합된 LLM입니다.

우리는 VG 지침과 다양한 생성기에서의 설명을 통합하는 프롬프트 방법을 제안합니다. 여기에는 직접적인 시각적 설명을 위한 캡션 기반 생성기와 정보 강화를 위한 VQA 기반 생성기가 포함됩니다. 또한 다양한 VidLLM에 대한 포괄적인 비교를 제공하고, 시각적 모델, LLM, 프롬프트 디자인 등 다양한 선택이 미치는 영향을 탐구합니다.

우리의 실험 평가 결과는 두 가지 결론을 도출합니다: (i) 기존 VidLLM은 여전히 만족스러운 비디오 그라운딩 성능을 달성하지 못하고 있으며, 이러한 모델을 더욱 미세 조정하기 위해 더 많은 시간 관련 비디오 작업이 포함되어야 합니다. (ii) LLM과 시각적 모델의 조합은 비디오 그라운딩에 대한 초기 능력을 보여주며, 더 신뢰할 수 있는 모델과 추가적인 프롬프트 지침을 통해 개선될 잠재력이 큽니다.

================================================================================

URL:
https://arxiv.org/pdf/2401.11123.pdf

Title: Uncertainty-aware Bridge based Mobile-Former Network for Event-based Pattern Recognition

Original Abstract:
The mainstream human activity recognition (HAR) algorithms are developed based on RGB cameras, which are easily influenced by low-quality images (e.g., low illumination, motion blur). Meanwhile, the privacy protection issue caused by ultra-high definition (HD) RGB cameras aroused more and more people's attention. Inspired by the success of event cameras which perform better on high dynamic range, no motion blur, and low energy consumption, we propose to recognize human actions based on the event stream. We propose a lightweight uncertainty-aware information propagation based Mobile-Former network for efficient pattern recognition, which aggregates the MobileNet and Transformer network effectively. Specifically, we first embed the event images using a stem network into feature representations, then, feed them into uncertainty-aware Mobile-Former blocks for local and global feature learning and fusion. Finally, the features from MobileNet and Transformer branches are concatenated for pattern recognition. Extensive experiments on multiple event-based recognition datasets fully validated the effectiveness of our model. The source code of this work will be released at this https URL.

Translated Abstract:
주류 인간 활동 인식(HAR) 알고리즘은 RGB 카메라를 기반으로 개발되는데, 이 카메라는 저화질 이미지(예: 조명이 낮거나, 움직임이 흐릿한 경우)에 쉽게 영향을 받는 문제가 있어. 게다가 초고화질(HD) RGB 카메라로 인한 개인정보 보호 문제도 점점 더 많은 사람들의 주목을 받고 있어.

우리는 높은 동적 범위에서 성능이 좋은 이벤트 카메라의 성공에 영감을 받아, 이벤트 스트림을 기반으로 인간 행동을 인식하는 방법을 제안해. 효율적인 패턴 인식을 위한 경량의 불확실성 인식 정보 전파 기반 Mobile-Former 네트워크를 제안하는데, 이 네트워크는 MobileNet과 Transformer 네트워크를 효과적으로 결합해.

구체적으로, 우리는 먼저 스템 네트워크를 사용해 이벤트 이미지를 특징 표현으로 변환한 후, 이를 불확실성 인식 Mobile-Former 블록에 넣어 지역적 및 전역적 특징 학습과 융합을 해. 마지막으로, MobileNet과 Transformer 브랜치에서 나온 특징을 연결해 패턴 인식을 수행해. 여러 이벤트 기반 인식 데이터셋에서의 광범위한 실험을 통해 우리의 모델이 효과적임을 충분히 검증했어. 이 연구의 소스 코드는 이 https URL에서 공개될 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2401.14718.pdf

Title: A Survey on Future Frame Synthesis: Bridging Deterministic and Generative Approaches

Original Abstract:
Future Frame Synthesis (FFS) aims to enable models to generate sequences of future frames based on existing content. This task has garnered widespread application across various domains. In this paper, we comprehensively survey both historical and contemporary works in this field, encompassing widely used datasets and algorithms. Our survey scrutinizes the challenges and the evolving landscape of FFS within the realm of computer vision. We propose a novel taxonomy centered on the stochastic nature of related algorithms. This taxonomy emphasizes the gradual transition from deterministic to generative synthesis methodologies, highlighting significant advancements and shifts in approach.

Translated Abstract:
미래 프레임 합성(Future Frame Synthesis, FFS)은 기존 콘텐츠를 바탕으로 미래의 프레임 시퀀스를 생성할 수 있게 하는 기술이야. 이 작업은 여러 분야에서 널리 사용되고 있어.

이 논문에서는 이 분야의 역사적인 작업과 현대적인 연구를 포괄적으로 조사했어. 여기에는 자주 쓰이는 데이터셋과 알고리즘도 포함돼. 우리의 조사는 컴퓨터 비전 영역에서 FFS가 직면한 도전 과제와 변화하는 환경을 분석해.

우리는 관련 알고리즘의 확률적 성격에 초점을 맞춘 새로운 분류 체계를 제안해. 이 분류 체계는 결정론적인 방법에서 생성적 합성 방법으로의 점진적인 전환을 강조하고, 중요한 발전과 접근 방식의 변화를 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2402.02003.pdf

Title: GenFace: A Large-Scale Fine-Grained Face Forgery Benchmark and Cross Appearance-Edge Learning

Original Abstract:
The rapid advancement of photorealistic generators has reached a critical juncture where the discrepancy between authentic and manipulated images is increasingly indistinguishable. Thus, benchmarking and advancing techniques detecting digital manipulation become an urgent issue. Although there have been a number of publicly available face forgery datasets, the forgery faces are mostly generated using GAN-based synthesis technology, which does not involve the most recent technologies like diffusion. The diversity and quality of images generated by diffusion models have been significantly improved and thus a much more challenging face forgery dataset shall be used to evaluate SOTA forgery detection literature. In this paper, we propose a large-scale, diverse, and fine-grained high-fidelity dataset, namely GenFace, to facilitate the advancement of deepfake detection, which contains a large number of forgery faces generated by advanced generators such as the diffusion-based model and more detailed labels about the manipulation approaches and adopted generators. In addition to evaluating SOTA approaches on our benchmark, we design an innovative cross appearance-edge learning (CAEL) detector to capture multi-grained appearance and edge global representations, and detect discriminative and general forgery traces. Moreover, we devise an appearance-edge cross-attention (AECA) module to explore the various integrations across two domains. Extensive experiment results and visualizations show that our detection model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations. Code and datasets will be available at \url{this https URL

Translated Abstract:
포토리얼리스틱 생성기가 빠르게 발전하면서 진짜 이미지와 조작된 이미지의 차이가 점점 구분하기 어려워지고 있어. 그래서 디지털 조작을 탐지하는 기술을 평가하고 발전시키는 것이 급선무야. 공개된 얼굴 위조 데이터셋이 몇 개 있지만, 대부분 GAN 기반의 합성 기술로 생성된 얼굴들이야. 최근의 확산 기술 같은 최신 기술이 포함되어 있지 않아. 

확산 모델로 생성된 이미지의 다양성과 품질이 크게 향상됐고, 이제는 훨씬 더 도전적인 얼굴 위조 데이터셋이 필요해. 이 논문에서는 GenFace라는 대규모, 다양하고 세밀한 고충실도 데이터셋을 제안해. 이 데이터셋은 확산 기반 모델 같은 고급 생성기로 생성된 많은 위조 얼굴을 포함하고, 조작 방식과 사용된 생성기에 대한 더 자세한 라벨도 제공해. 

우리의 벤치마크에서 최신 기술(SOTA) 방법들을 평가하는 것 외에도, 다중 세밀한 형태와 엣지의 전역 표현을 캡처하고 구별적이며 일반적인 위조 흔적을 탐지하는 혁신적인 교차 외관-엣지 학습(CAEL) 탐지기를 설계했어. 또한, 두 가지 도메인 간의 다양한 통합을 탐색하는 외관-엣지 교차 주의(AECA) 모듈도 만들었어. 

광범위한 실험 결과와 시각화를 통해 우리의 탐지 모델이 크로스 생성기, 크로스 위조, 크로스 데이터셋 평가와 같은 다양한 설정에서 최신 기술보다 더 뛰어난 성능을 보였다는 걸 보여줘. 코드와 데이터셋은 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2404.11256.pdf

Title: MMCBE: Multi-modality Dataset for Crop Biomass Prediction and Beyond

Original Abstract:
Crop biomass, a critical indicator of plant growth, health, and productivity, is invaluable for crop breeding programs and agronomic research. However, the accurate and scalable quantification of crop biomass remains inaccessible due to limitations in existing measurement methods. One of the obstacles impeding the advancement of current crop biomass prediction methodologies is the scarcity of publicly available datasets. Addressing this gap, we introduce a new dataset in this domain, i.e. Multi-modality dataset for crop biomass estimation (MMCBE). Comprising 216 sets of multi-view drone images, coupled with LiDAR point clouds, and hand-labelled ground truth, MMCBE represents the first multi-modality one in the field. This dataset aims to establish benchmark methods for crop biomass quantification and foster the development of vision-based approaches. We have rigorously evaluated state-of-the-art crop biomass estimation methods using MMCBE and ventured into additional potential applications, such as 3D crop reconstruction from drone imagery and novel-view rendering. With this publication, we are making our comprehensive dataset available to the broader community.

Translated Abstract:
작물 바이오매스는 식물의 성장, 건강, 생산성을 나타내는 중요한 지표로, 작물 육종 프로그램과 농업 연구에 매우 유용해. 하지만 기존 측정 방법의 한계로 인해 작물 바이오매스를 정확하고 확장성 있게 측정하는 건 아직 어려워. 현재 작물 바이오매스 예측 방법의 발전을 방해하는 장애물 중 하나는 공개된 데이터셋이 부족하다는 거야.

이 문제를 해결하기 위해, 우리는 새로운 데이터셋인 다중 모달리티 작물 바이오매스 추정 데이터셋(MMCBE)을 소개해. 이 데이터셋은 216세트의 다중 시점 드론 이미지와 LiDAR 포인트 클라우드, 그리고 수작업으로 라벨링한 실제 데이터를 포함하고 있어. MMCBE는 이 분야에서 처음으로 다중 모달리티 데이터셋이야.

이 데이터셋은 작물 바이오매스 측정 방법의 기준을 마련하고, 비전 기반 접근 방법의 발전을 촉진하는 데 목적이 있어. 우리는 MMCBE를 사용해 최신 작물 바이오매스 추정 방법을 철저히 평가했고, 드론 이미지를 활용한 3D 작물 재구성 및 새로운 시점 렌더링 같은 추가적인 가능성 있는 응용도 연구했어. 이번 발표를 통해 우리는 우리의 포괄적인 데이터셋을 더 넓은 커뮤니티에 공개하고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2405.14529.pdf

Title: AnomalyDINO: Boosting Patch-based Few-shot Anomaly Detection with DINOv2

Original Abstract:
Recent advances in multimodal foundation models have set new standards in few-shot anomaly detection. This paper explores whether high-quality visual features alone are sufficient to rival existing state-of-the-art vision-language models. We affirm this by adapting DINOv2 for one-shot and few-shot anomaly detection, with a focus on industrial applications. We show that this approach does not only rival existing techniques but can even outmatch them in many settings. Our proposed vision-only approach, AnomalyDINO, is based on patch similarities and enables both image-level anomaly prediction and pixel-level anomaly segmentation. The approach is methodologically simple and training-free and, thus, does not require any additional data for fine-tuning or meta-learning. Despite its simplicity, AnomalyDINO achieves state-of-the-art results in one- and few-shot anomaly detection (e.g., pushing the one-shot performance on MVTec-AD from an AUROC of 93.1% to 96.6%). The reduced overhead, coupled with its outstanding few-shot performance, makes AnomalyDINO a strong candidate for fast deployment, e.g., in industrial contexts.

Translated Abstract:
최근 멀티모달 기초 모델의 발전이 몇 번의 샷으로 이상 탐지에서 새로운 기준을 세웠어. 이 논문은 고품질의 시각적 특징만으로도 기존의 최첨단 비전-언어 모델과 경쟁할 수 있는지를 살펴봐. 우리는 DINOv2를 한 번의 샷과 몇 번의 샷 이상 탐지에 맞게 조정하면서 산업 응용에 초점을 맞췄어.

이 접근 방식이 기존 기술과 경쟁할 수 있을 뿐만 아니라, 여러 설정에서 이를 능가할 수 있다는 걸 보여줘. 우리가 제안한 비전 전용 접근법인 AnomalyDINO는 패치 유사성에 기반하고 있어서 이미지 수준의 이상 예측과 픽셀 수준의 이상 분할을 가능하게 해. 이 방법은 원리가 간단하고 훈련이 필요 없어서 추가적인 데이터 없이도 미세 조정이나 메타 학습이 필요하지 않아.

간단함에도 불구하고, AnomalyDINO는 한 번의 샷과 몇 번의 샷 이상 탐지에서 최첨단 성과를 달성해 (예를 들어, MVTec-AD에서 한 번의 샷 성능을 AUROC 93.1%에서 96.6%로 끌어올렸어). 적은 오버헤드와 뛰어난 몇 번의 샷 성능 덕분에 AnomalyDINO는 산업 환경 같은 곳에서 빠르게 배포할 수 있는 강력한 후보야.

================================================================================

URL:
https://arxiv.org/pdf/2406.08070.pdf

Title: CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models

Original Abstract:
Classifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse. Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves. More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS), we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG. CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales, reduced mode collapse, etc. Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales. Moreover, CFG++ can be easily integrated into high-order diffusion solvers and naturally extends to distilled diffusion models. Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance. Project Page: this https URL.

Translated Abstract:
분류기 없는 가이드(CFG)는 텍스트 기반 생성에서 현대 확산 모델의 중요한 도구야. 효과적이긴 하지만, 몇 가지 단점이 있어. 예를 들어, CFG를 사용하는 DDIM은 역변환이 불가능해서 이미지 편집이 복잡해지고, 고품질 출력을 위해 필요한 높은 가이드 스케일은 자주 모드 붕괴 같은 문제를 일으켜.

많은 사람들이 이런 문제들이 확산 모델의 고유한 한계라고 생각하지만, 이 논문은 문제가 사실 CFG와 관련된 오프 매니폴드 현상에서 온다는 걸 밝혀. 더 구체적으로, 최근 확산 모델 기반의 역문제 해결 방법(DIS)에서 영감을 받아, 텍스트 가이드를 역문제로 재정의하고 텍스트 조건 스코어 매칭 손실을 이용해 CFG++라는 새로운 접근 방식을 개발했어. 

CFG++는 CFG에 대한 간단한 해결책을 제공하면서도, 텍스트-이미지 생성의 샘플 품질 향상, 역변환 가능성, 더 작은 가이드 스케일, 모드 붕괴 감소 등 많은 개선을 보여줘. 게다가 CFG++는 낮은 가이드 스케일에서 무조건 샘플링과 조건부 샘플링 간의 매끄러운 보간을 가능하게 해, 모든 스케일에서 전통적인 CFG보다 일관되게 성능이 좋아.

또한, CFG++는 고차원 확산 솔버에 쉽게 통합될 수 있고, 증류된 확산 모델로 자연스럽게 확장될 수 있어. 실험 결과는 이 방법이 텍스트-이미지 생성, DDIM 역전환, 편집, 역문제 해결에서 성능을 크게 향상시킨다는 것을 보여줘. 이는 텍스트 가이드를 활용하는 다양한 분야에 넓은 영향을 미칠 수 있는 가능성을 시사해.

================================================================================

URL:
https://arxiv.org/pdf/2406.08090.pdf

Title: From Sim-to-Real: Toward General Event-based Low-light Frame Interpolation with Per-scene Optimization

Original Abstract:
Video Frame Interpolation (VFI) is important for video enhancement, frame rate up-conversion, and slow-motion generation. The introduction of event cameras, which capture per-pixel brightness changes asynchronously, has significantly enhanced VFI capabilities, particularly for high-speed, nonlinear motions. However, these event-based methods encounter challenges in low-light conditions, notably trailing artifacts and signal latency, which hinder their direct applicability and generalization. Addressing these issues, we propose a novel per-scene optimization strategy tailored for low-light conditions. This approach utilizes the internal statistics of a sequence to handle degraded event data under low-light conditions, improving the generalizability to different lighting and camera settings. To evaluate its robustness in low-light condition, we further introduce EVFI-LL, a unique RGB+Event dataset captured under low-light conditions. Our results demonstrate state-of-the-art performance in low-light environments. Project page: this https URL.

Translated Abstract:
비디오 프레임 보간(Video Frame Interpolation, VFI)은 비디오 품질을 높이거나 프레임 레이트를 업컨버전 하거나 슬로우 모션을 만드는 데 중요해. 최근에 이벤트 카메라가 등장했는데, 이 카메라는 픽셀 단위로 밝기 변화를 비동기적으로 캡처해. 이 덕분에 VFI의 성능이 크게 향상되었고, 특히 고속 비선형 운동에 강해졌어. 

하지만 이런 이벤트 기반 방법은 저조도 환경에서 문제를 겪어. 예를 들면, 잔상 아티팩트나 신호 지연 같은 문제가 발생해서 직접적으로 적용하기 어렵고 일반화도 잘 안 돼. 그래서 우리는 저조도 조건에 맞춘 새로운 장면별 최적화 전략을 제안해. 이 방법은 시퀀스의 내부 통계를 이용해서 저조도 조건에서 손상된 이벤트 데이터를 처리하고, 다양한 조명과 카메라 설정에 대한 일반화 능력을 향상시켜.

또한 저조도 환경에서의 강인성을 평가하기 위해, 저조도 조건에서 캡처한 독특한 RGB+Event 데이터셋인 EVFI-LL을 소개해. 우리의 결과는 저조도 환경에서 최첨단 성능을 보여줘. 프로젝트 페이지는 이 링크에 있어: this https URL.

================================================================================

URL:
https://arxiv.org/pdf/2406.08344.pdf

Title: Blind Image Deblurring with FFT-ReLU Sparsity Prior

Original Abstract:
Blind image deblurring is the process of recovering a sharp image from a blurred one without prior knowledge about the blur kernel. It is a small data problem, since the key challenge lies in estimating the unknown degrees of blur from a single image or limited data, instead of learning from large datasets. The solution depends heavily on developing algorithms that effectively model the image degradation process. We introduce a method that leverages a prior which targets the blur kernel to achieve effective deblurring across a wide range of image types. In our extensive empirical analysis, our algorithm achieves results that are competitive with the state-of-the-art blind image deblurring algorithms, and it offers up to two times faster inference, making it a highly efficient solution.

Translated Abstract:
블라인드 이미지 디블러링은 흐릿한 이미지에서 선명한 이미지를 복원하는 과정이야. 이때 흐림의 원인인 블러 커널에 대한 사전 정보가 없다는 게 특징이야. 이건 작은 데이터 문제인데, 한 장의 이미지나 제한된 데이터에서 흐림의 정도를 추정하는 게 주된 도전이야. 그래서 대규모 데이터셋에서 배우는 게 아니고, 알고리즘을 잘 개발하는 게 중요해.

우리는 다양한 이미지 유형에 대해 효과적인 디블러링을 달성하기 위해 블러 커널을 목표로 하는 사전 정보를 활용하는 방법을 소개해. 우리의 광범위한 실험 분석에서, 우리 알고리즘은 최신 블라인드 이미지 디블러링 알고리즘과 경쟁할 만한 결과를 보여줬어. 게다가 추론 속도가 최대 두 배 빠르기 때문에 매우 효율적인 솔루션이야.

================================================================================

URL:
https://arxiv.org/pdf/2406.17100.pdf

Title: FaceScore: Benchmarking and Enhancing Face Quality in Human Generation

Original Abstract:
Diffusion models (DMs) have achieved significant success in generating imaginative images given textual descriptions. However, they are likely to fall short when it comes to real-life scenarios with intricate details. The low-quality, unrealistic human faces in text-to-image generation are one of the most prominent issues, hindering the wide application of DMs in practice. Targeting addressing such an issue, we first assess the face quality of generations from popular pre-trained DMs with the aid of human annotators and then evaluate the alignment between existing metrics with human judgments. Observing that existing metrics can be unsatisfactory for quantifying face quality, we develop a novel metric named FaceScore (FS) by fine-tuning the widely used ImageReward on a dataset of (win, loss) face pairs cheaply crafted by an inpainting pipeline of DMs. Extensive studies reveal FS enjoys a superior alignment with humans. On the other hand, FS opens up the door for enhancing DMs for better face generation. With FS offering image ratings, we can easily perform preference learning algorithms to refine DMs like SDXL. Comprehensive experiments verify the efficacy of our approach for improving face quality. The code is released at this https URL.

Translated Abstract:
확산 모델(DM)은 텍스트 설명에 따라 창의적인 이미지를 생성하는 데 큰 성공을 거두었어. 하지만 복잡한 세부사항이 있는 실제 상황에서는 부족할 수 있어. 특히 텍스트에서 이미지를 생성할 때 나오는 낮은 품질의 비현실적인 인간 얼굴은 DM의 실제 적용을 막는 가장 큰 문제 중 하나야.

이 문제를 해결하기 위해, 먼저 인기 있는 사전 훈련된 DM에서 생성된 얼굴 품질을 사람의 평가자와 함께 평가했어. 그리고 기존의 지표들이 사람의 판단과 얼마나 일치하는지 살펴봤지. 그런데 기존 지표는 얼굴 품질을 정량화하는 데 만족스럽지 못하다는 걸 발견했어. 그래서 우리는 FaceScore(FS)라는 새로운 지표를 개발했어. 이건 널리 사용되는 ImageReward를 DM의 인페인팅 파이프라인으로 저렴하게 만든 (승, 패) 얼굴 쌍 데이터셋에 맞춰 조정한 거야.

광범위한 연구 결과, FS는 사람과의 일치도가 훨씬 높다는 걸 알게 됐어. FS는 DM의 얼굴 생성 품질을 향상시킬 수 있는 기회를 열어줘. FS가 이미지 평점을 제공하니까, 우리는 쉽게 선호 학습 알고리즘을 사용해서 SDXL 같은 DM을 개선할 수 있어. 다양한 실험을 통해 우리의 접근 방식이 얼굴 품질 개선에 효과적이라는 걸 검증했어. 코드는 이 https URL에서 공개했어.

================================================================================

URL:
https://arxiv.org/pdf/2406.18544.pdf

Title: GS-ROR: 3D Gaussian Splatting for Reflective Object Relighting via SDF Priors

Original Abstract:
3D Gaussian Splatting (3DGS) has shown a powerful capability for novel view synthesis due to its detailed expressive ability and highly efficient rendering speed. Unfortunately, creating relightable 3D assets with 3DGS is still problematic, particularly for reflective objects, as its discontinuous representation raises difficulties in constraining geometries. Inspired by previous works, the signed distance field (SDF) can serve as an effective way for geometry regularization. However, a direct incorporation between Gaussians and SDF significantly slows training. To this end, we propose GS-ROR for reflective objects relighting with 3DGS aided by SDF priors. At the core of our method is the mutual supervision of the depth and normal between deferred Gaussians and SDF, which avoids the expensive volume rendering of SDF. Thanks to this mutual supervision, the learned deferred Gaussians are well-constrained with a minimal time cost. As the Gaussians are rendered in a deferred shading mode, while the alpha-blended Gaussians are smooth, individual Gaussians may still be outliers, yielding floater artifacts. Therefore, we further introduce an SDF-aware pruning strategy to remove Gaussian outliers, which are located distant from the surface defined by SDF, avoiding the floater issue. Consequently, our method outperforms the existing Gaussian-based inverse rendering methods in terms of relighting quality. Our method also exhibits competitive relighting quality compared to NeRF-based methods with at most 25% of training time and allows rendering at 200+ frames per second on an RTX4090.

Translated Abstract:
3D 가우시안 스플래팅(3DGS)은 새로운 시점 합성에서 강력한 능력을 보여주고 있어. 세밀한 표현력과 효율적인 렌더링 속도가 큰 장점이지. 하지만 3DGS로 재조명 가능한 3D 자산을 만드는 건 여전히 문제야, 특히 반사 물체의 경우에는 더더욱 어려워. 왜냐면 불연속적인 표현이 기하학적 제약을 주기 때문이야.

이전에 연구된 것을 참고해서, 서명 거리 필드(SDF)가 기하학 정규화에 효과적인 방법이 될 수 있다는 걸 알았어. 하지만 가우시안과 SDF를 직접 결합하면 훈련 속도가 많이 느려져. 그래서 우리는 SDF 프라이어를 활용한 반사 물체 재조명을 위한 GS-ROR을 제안해. 우리의 방법의 핵심은 지연된 가우시안과 SDF 간의 깊이와 법선의 상호 감독이야. 이 방법은 SDF의 비싼 볼륨 렌더링을 피할 수 있도록 해줘.

이 상호 감독 덕분에 배운 지연 가우시안이 최소한의 시간 비용으로 잘 제약을 받을 수 있어. 가우시안이 지연 음영 모드에서 렌더링되기 때문에 알파 혼합된 가우시안은 부드럽지만, 개별 가우시안은 여전히 아웃라이어일 수 있어. 그래서 우리는 SDF를 고려한 가지치기 전략을 추가해, SDF가 정의한 표면에서 멀리 떨어진 가우시안 아웃라이어를 제거해줘. 이로 인해 부유물 문제를 피할 수 있어.

결과적으로, 우리의 방법은 재조명 품질 면에서 기존의 가우시안 기반 역 렌더링 방법보다 더 나은 성능을 보여줘. 또한, 우리의 방법은 NeRF 기반 방법과 비교했을 때 훈련 시간의 최대 25%로 경쟁력 있는 재조명 품질을 보여주고, RTX4090에서 200프레임 이상으로 렌더링할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2406.18790.pdf

Title: MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data

Original Abstract:
We train a model to generate images from multimodal prompts of interleaved text and images such as "a <picture of a man> man and his <picture of a dog> dog in an <picture of a cartoon> animated style." We bootstrap a multimodal dataset by extracting semantically meaningful image crops corresponding to words in the image captions of synthetically generated and publicly available text-image data. Our model, MUMU, is composed of a vision-language model encoder with a diffusion decoder and is trained on a single 8xH100 GPU node. Despite being only trained on crops from the same image, MUMU learns to compose inputs from different images into a coherent output. For example, an input of a realistic person and a cartoon will output the same person in the cartoon style, and an input of a standing subject and a scooter will output the subject riding the scooter. As a result, our model generalizes to tasks such as style transfer and character consistency. Our results show the promise of using multimodal models as general purpose controllers for image generation.

Translated Abstract:
우리는 텍스트와 이미지가 섞인 여러 가지 프롬프트를 바탕으로 이미지를 생성하는 모델을 훈련시켰어. 예를 들어, "한 <남자 사진>과 그의 <개 사진>이 <만화 스타일>로 그려진 모습" 같은 거야. 

멀티모달 데이터셋을 만들기 위해, 우리가 생성한 텍스트-이미지 데이터에서 이미지 캡션에 있는 단어에 맞는 의미 있는 이미지 부분을 뽑아냈어. 우리의 모델인 MUMU는 비전-언어 모델 인코더와 디퓨전 디코더로 구성되어 있고, 단일 8xH100 GPU 노드에서 훈련됐어. 

MUMU는 같은 이미지에서 잘라낸 부분만 가지고 훈련됐음에도 불구하고, 서로 다른 이미지의 입력을 조합해서 일관성 있는 출력을 만들어낼 수 있어. 예를 들어, 실제 사람과 만화를 입력하면 만화 스타일로 같은 사람을 출력하고, 서 있는 주체와 스쿠터를 입력하면 그 주체가 스쿠터를 타는 모습을 출력해. 

결과적으로, 우리의 모델은 스타일 전이와 캐릭터 일관성 같은 작업에 잘 일반화돼. 우리의 연구 결과는 멀티모달 모델이 이미지 생성을 위한 범용 제어기로서의 가능성을 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2407.02403.pdf

Title: Face Reconstruction Transfer Attack as Out-of-Distribution Generalization

Original Abstract:
Understanding the vulnerability of face recognition systems to malicious attacks is of critical importance. Previous works have focused on reconstructing face images that can penetrate a targeted verification system. Even in the white-box scenario, however, naively reconstructed images misrepresent the identity information, hence the attacks are easily neutralized once the face system is updated or changed. In this paper, we aim to reconstruct face images which are capable of transferring face attacks on unseen encoders. We term this problem as Face Reconstruction Transfer Attack (FRTA) and show that it can be formulated as an out-of-distribution (OOD) generalization problem. Inspired by its OOD nature, we propose to solve FRTA by Averaged Latent Search and Unsupervised Validation with pseudo target (ALSUV). To strengthen the reconstruction attack on OOD unseen encoders, ALSUV reconstructs the face by searching the latent of amortized generator StyleGAN2 through multiple latent optimization, latent optimization trajectory averaging, and unsupervised validation with a pseudo target. We demonstrate the efficacy and generalization of our method on widely used face datasets, accompanying it with extensive ablation studies and visually, qualitatively, and quantitatively analyses. The source code will be released.

Translated Abstract:
얼굴 인식 시스템이 악의적인 공격에 얼마나 취약한지 이해하는 건 정말 중요해. 이전 연구들은 특정 검증 시스템을 뚫을 수 있는 얼굴 이미지를 재구성하는 데 초점을 맞췄어. 하지만 화이트 박스 환경에서도, 단순히 재구성한 이미지는 정체성 정보를 잘못 표현해서 얼굴 시스템이 업데이트되거나 바뀌면 쉽게 무력화될 수 있어.

이번 논문에서는 보지 못한 인코더에 얼굴 공격을 전이할 수 있는 얼굴 이미지를 재구성하는 걸 목표로 해. 우리는 이 문제를 얼굴 재구성 전이 공격(FRTA)이라고 부르고, 이를 분포 밖 일반화 문제로 정리할 수 있음을 보여줘. OOD 성격에서 영감을 받아서, FRTA를 평균 잠재 검색 및 의사 목표를 통한 비지도 검증(ALSUV)으로 해결할 방법을 제안해.

ALSUV는 OOD 보지 못한 인코더에 대한 재구성 공격을 강화하기 위해, 여러 번의 잠재 최적화, 잠재 최적화 궤적 평균화, 그리고 의사 목표를 통한 비지도 검증으로 StyleGAN2의 잠재를 검색해 얼굴을 재구성해. 우리는 이 방법이 널리 사용되는 얼굴 데이터셋에서 효과적이고 일반화된다는 걸 보여주고, 다양한 분석과 함께 광범위한 ablation 연구도 포함했어. 소스 코드는 나중에 공개할 거야.

================================================================================

URL:
https://arxiv.org/pdf/2407.11820.pdf

Title: Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation

Original Abstract:
Audio-Visual Segmentation (AVS) aims to achieve pixel-level localization of sound sources in videos, while Audio-Visual Semantic Segmentation (AVSS), as an extension of AVS, further pursues semantic understanding of audio-visual scenes. However, since the AVSS task requires the establishment of audio-visual correspondence and semantic understanding simultaneously, we observe that previous methods have struggled to handle this mashup of objectives in end-to-end training, resulting in insufficient learning and sub-optimization. Therefore, we propose a two-stage training strategy called \textit{Stepping Stones}, which decomposes the AVSS task into two simple subtasks from localization to semantic understanding, which are fully optimized in each stage to achieve step-by-step global optimization. This training strategy has also proved its generalization and effectiveness on existing methods. To further improve the performance of AVS tasks, we propose a novel framework Adaptive Audio Visual Segmentation, in which we incorporate an adaptive audio query generator and integrate masked attention into the transformer decoder, facilitating the adaptive fusion of visual and audio features. Extensive experiments demonstrate that our methods achieve state-of-the-art results on all three AVS benchmarks. The project homepage can be accessed at this https URL.

Translated Abstract:
오디오-비주얼 분할(Audio-Visual Segmentation, AVS)은 비디오에서 소리의 위치를 픽셀 단위로 찾는 걸 목표로 해. 오디오-비주얼 의미 분할(Audio-Visual Semantic Segmentation, AVSS)은 AVS의 확장으로, 오디오와 비주얼 장면을 더 깊이 이해하는 걸 추구해. 

하지만 AVSS 작업은 오디오와 비주얼 간의 연결과 의미 이해를 동시에 요구하기 때문에, 이전 방법들은 이 두 가지 목표를 함께 처리하는 데 어려움을 겪었어. 결과적으로 학습이 충분하지 않거나 최적화가 잘 되지 않았어. 그래서 우리는 \textit{Stepping Stones}라는 두 단계 훈련 전략을 제안해. 이 방법은 AVSS 작업을 로컬라이제이션과 의미 이해라는 두 개의 간단한 하위 작업으로 나누고, 각 단계에서 완전히 최적화해서 단계별로 글로벌 최적화를 달성해.

이 훈련 전략은 기존 방법에서도 일반화와 효과성을 입증했어. AVS 작업의 성능을 더 높이기 위해서 우리는 Adaptive Audio Visual Segmentation이라는 새로운 프레임워크를 제안해. 이 프레임워크에서는 적응형 오디오 쿼리 생성기를 도입하고 마스크된 주의(attention)를 트랜스포머 디코더에 통합해 시각적 특징과 오디오 특징의 적응형 융합을 도와줘.

광범위한 실험 결과, 우리의 방법은 모든 세 가지 AVS 벤치마크에서 최첨단 성과를 달성했어. 프로젝트 홈페이지는 이 https URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2407.18100.pdf

Title: DINOv2 Rocks Geological Image Analysis: Classification, Segmentation, and Interpretability

Original Abstract:
Recent advancements in computer vision have significantly improved image analysis tasks. Yet, deep learning models often struggle when applied to domains outside their training distribution, such as in geosciences, where domain-specific data can be scarce. This study investigates the classification, segmentation, and interpretability of CT-scan images of rock samples, focusing on the application of modern computer vision techniques to geoscientific tasks. We compare a range of segmentation methods to assess their efficacy, efficiency, and adaptability in geological image analysis. The methods evaluated include Otsu thresholding, clustering techniques (K-means, fuzzy C-means), a supervised machine learning approach (Random Forest), and deep learning models (UNet, ResNet152, and DINOv2), using ten binary sandstone datasets and three multi-class calcite datasets. DINOv2 was selected for its promising results in feature extraction and its potential applicability in geoscientific tasks, prompting further assessment of its interpretability and effectiveness in processing CT-scanned rock data. For classification, a non-fine-tuned DINOv2 demonstrates strong performance in classifying rock images, even when the CT-scans are outside its original training set. In segmentation tasks, thresholding and clustering techniques, though computationally efficient, produce subpar results despite preprocessing efforts. In contrast, supervised methods achieve better performance. While deep learning methods demand greater computational resources, they require minimal intervention and offer superior generalization. A LoRA fine-tuned DINOv2, in particular, excels in out-of-distribution segmentation and outperforms other methods in multi-class tasks, even with limited data. Notably, the segmentation masks generated by DINOv2 often appear more accurate than the original targets, based on visual inspection.

Translated Abstract:
최근 컴퓨터 비전의 발전 덕분에 이미지 분석 작업이 많이 개선되었어. 하지만 딥러닝 모델은 훈련 데이터와 다른 분야에 적용할 때 어려움을 겪는 경우가 많아. 특히 지구과학 분야에서는 특정 데이터가 부족할 때가 많지. 이 연구는 암석 샘플의 CT 스캔 이미지를 분류하고 세분화하며 해석 가능한 방법을 조사해. 현대 컴퓨터 비전 기술을 지구과학 작업에 적용하는 데 초점을 맞추고 있어.

여러 가지 세분화 방법을 비교해서 그 효과, 효율성, 적응성을 평가했어. 평가한 방법으로는 Otsu 임계값 설정, 클러스터링 기법(K-평균, 퍼지 C-평균), 감독식 머신러닝 접근법(랜덤 포레스트), 그리고 딥러닝 모델(UNet, ResNet152, DINOv2)이 있어. 이 연구에서는 10개의 이진 모래암 데이터세트와 3개의 다중 클래스 방해석 데이터세트를 사용했어. DINOv2는 특징 추출에서 좋은 결과를 보여서 지구과학 작업에 적합할 거라는 기대를 가지고 더 평가해보기로 했어.

분류 작업에서, 비세밀하게 조정된 DINOv2는 CT 스캔 이미지가 원래 훈련 세트와 다르더라도 강력한 성능을 보여줬어. 세분화 작업에서는 임계값 설정과 클러스터링 기법이 계산적으로 효율적이긴 하지만, 전처리에도 불구하고 결과는 그다지 좋지 않았어. 반면 감독식 방법은 더 나은 성능을 보였어. 딥러닝 방법은 더 많은 계산 자원을 필요로 하지만, 개입이 적고 일반화 성능은 뛰어나.

LoRA로 조정된 DINOv2는 특히 분포 외 세분화에서 뛰어난 성능을 보이고, 제한된 데이터로도 다중 클래스 작업에서 다른 방법보다 잘해냈어. 특히 DINOv2가 생성한 세분화 마스크는 시각적으로 확인할 때 원래 목표보다 더 정확해 보이는 경우가 많아.

================================================================================

URL:
https://arxiv.org/pdf/2407.18134.pdf

Title: $\mathbb{X}$-Sample Contrastive Loss: Improving Contrastive Learning with Sample Similarity Graphs

Original Abstract:
Learning good representations involves capturing the diverse ways in which data samples relate. Contrastive loss - an objective matching related samples - underlies methods from self-supervised to multimodal learning. Contrastive losses, however, can be viewed more broadly as modifying a similarity graph to indicate how samples should relate in the embedding space. This view reveals a shortcoming in contrastive learning: the similarity graph is binary, as only one sample is the related positive sample. Crucially, similarities \textit{across} samples are ignored. Based on this observation, we revise the standard contrastive loss to explicitly encode how a sample relates to others. We experiment with this new objective, called $\mathbb{X}$-Sample Contrastive, to train vision models based on similarities in class or text caption descriptions. Our study spans three scales: ImageNet-1k with 1 million, CC3M with 3 million, and CC12M with 12 million samples. The representations learned via our objective outperform both contrastive self-supervised and vision-language models trained on the same data across a range of tasks. When training on CC12M, we outperform CLIP by $0.6\%$ on both ImageNet and ImageNet Real. Our objective appears to work particularly well in lower-data regimes, with gains over CLIP of $16.8\%$ on ImageNet and $18.1\%$ on ImageNet Real when training with CC3M. Finally, our objective seems to encourage the model to learn representations that separate objects from their attributes and backgrounds, with gains of $3.3$-$5.6$\% over CLIP on ImageNet9. We hope the proposed solution takes a small step towards developing richer learning objectives for understanding sample relations in foundation models.

Translated Abstract:
좋은 표현을 배우는 것은 데이터 샘플 간의 다양한 관계를 파악하는 걸 포함해. 대조 손실(contrastive loss)은 관련 샘플을 맞추기 위한 목표로, 자기 지도 학습(self-supervised learning)부터 다중 모달 학습(multimodal learning)까지 다양한 방법의 기초가 돼. 그런데 대조 손실은 좀 더 넓은 관점에서 샘플 간의 유사성 그래프를 수정해 샘플들이 임베딩 공간에서 어떻게 관계를 가져야 하는지를 나타내는 것으로 볼 수 있어. 

이렇게 보면 대조 학습의 한계가 드러나. 유사성 그래프가 이진(binary) 형태라서, 오직 하나의 샘플만 관련된 긍정 샘플로 간주되거든. 그래서 샘플 간의 유사성을 무시하게 돼. 이 점을 바탕으로, 우리는 표준 대조 손실을 수정해 샘플이 다른 샘플과 어떻게 관계가 있는지를 명시적으로 인코딩하도록 했어. 

우리는 $\mathbb{X}$-샘플 대조(X-Sample Contrastive)라고 부르는 이 새로운 목표를 가지고, 클래스나 텍스트 캡션 설명의 유사성에 기반해 비전 모델을 훈련하는 실험을 했어. 우리의 연구는 세 가지 규모에 걸쳐 진행됐어: 100만 개의 샘플을 가진 ImageNet-1k, 300만 개의 CC3M, 1200만 개의 CC12M. 우리가 제안한 목표를 통해 학습한 표현들은 대조 자기 지도 학습과 같은 데이터로 훈련된 비전-언어 모델보다 다양한 작업에서 더 나은 성능을 보였어. CC12M으로 훈련했을 때, 우리는 ImageNet과 ImageNet Real에서 CLIP보다 0.6% 더 나은 성능을 기록했어. 

특히 데이터가 적은 경우에 우리 목표가 잘 작동하는 것 같아. CC3M으로 훈련했을 때, ImageNet에서 CLIP보다 16.8%, ImageNet Real에서 18.1%의 성과 향상을 보였어. 마지막으로, 우리의 목표는 모델이 객체와 그 속성 및 배경을 분리하는 표현을 배우도록 유도하는 것 같아. ImageNet9에서 CLIP보다 3.3~5.6% 더 나은 성과를 기록했어. 우리는 제안한 솔루션이 샘플 관계를 이해하는 더 풍부한 학습 목표를 개발하는 데 작은 걸음이 되기를 희망해.

================================================================================

URL:
https://arxiv.org/pdf/2407.21757.pdf

Title: Learning Video Context as Interleaved Multimodal Sequences

Original Abstract:
Narrative videos, such as movies, pose significant challenges in video understanding due to their rich contexts (characters, dialogues, storylines) and diverse demands (identify who, relationship, and reason). In this paper, we introduce MovieSeq, a multimodal language model developed to address the wide range of challenges in understanding video contexts. Our core idea is to represent videos as interleaved multimodal sequences (including images, plots, videos, and subtitles), either by linking external knowledge databases or using offline models (such as whisper for subtitles). Through instruction-tuning, this approach empowers the language model to interact with videos using interleaved multimodal instructions. For example, instead of solely relying on video as input, we jointly provide character photos alongside their names and dialogues, allowing the model to associate these elements and generate more comprehensive responses. To demonstrate its effectiveness, we validate MovieSeq's performance on six datasets (LVU, MAD, Movienet, CMD, TVC, MovieQA) across five settings (video classification, audio description, video-text retrieval, video captioning, and video question-answering). The code will be public at this https URL.

Translated Abstract:
서사 비디오, 예를 들어 영화는 등장인물, 대화, 스토리라인 같은 풍부한 맥락 때문에 비디오 이해에 큰 도전 과제를 제시해. 그리고 누가 누구인지, 그 관계는 어떤지, 이유는 무엇인지 등 다양한 요구 사항이 있어. 

이 논문에서는 비디오 맥락을 이해하는 데 필요한 여러 가지 도전을 해결하기 위해 MovieSeq라는 다중 모달 언어 모델을 소개해. 우리의 핵심 아이디어는 비디오를 이미지, 줄거리, 비디오, 자막 등 다양한 요소가 섞인 다중 모달 시퀀스로 표현하는 거야. 외부 지식 데이터베이스와 연결하거나 자막을 위한 오프라인 모델(예: Whisper)을 사용할 수 있어.

이런 방식을 통해 언어 모델이 비디오와 함께 다중 모달 지침을 사용해서 상호작용할 수 있도록 해. 예를 들어, 비디오만 입력으로 사용하는 대신 등장인물의 사진과 이름, 대화를 함께 제공하면 모델이 이 요소들을 연결 지어 더 포괄적인 답변을 생성할 수 있어.

우리는 MovieSeq의 효과를 보여주기 위해 여섯 개의 데이터셋(LVU, MAD, Movienet, CMD, TVC, MovieQA)에서 다섯 가지 설정(비디오 분류, 오디오 설명, 비디오-텍스트 검색, 비디오 자막 생성, 비디오 질문-답변)으로 성능을 검증했어. 코드도 이 링크에서 공개할 예정이야.

================================================================================

URL:
https://arxiv.org/pdf/2408.02369.pdf

Title: The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC 2024

Original Abstract:
This paper delineates the visual speech recognition (VSR) system introduced by the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2024), engaging in all four tracks, including the fixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In terms of data processing, we leverage the lip motion extractor from the baseline1 to produce multiscale video data. Besides, various augmentation techniques are applied during training, encompassing speed perturbation, random rotation, horizontal flipping, and color transformation. The VSR model adopts an end-to-end architecture with joint CTC/attention loss, introducing Enhanced ResNet3D visual frontend, E-Branchformer encoder, and Bi-directional Transformer decoder. Our approach yields a 30.47% CER for the Single-Speaker Task and 34.30% CER for the Multi-Speaker Task, securing second place in the open track of the Single-Speaker Task and first place in the other three tracks.

Translated Abstract:
이 논문은 NPU-ASLP(팀 237)가 제2회 중국 연속 시각 음성 인식 챌린지(CNVSRC 2024)에서 선보인 시각 음성 인식(VSR) 시스템에 대해 설명해. 이 시스템은 단일 화자 VSR 작업과 다중 화자 VSR 작업의 고정 및 개방 트랙 등 네 가지 트랙에 참여했어.

데이터 처리 측면에서, 우리는 기본 모델에서 입술 움직임 추출기를 사용해서 다중 스케일 비디오 데이터를 만들었어. 또 훈련 중에 속도 변동, 랜덤 회전, 수평 반전, 색상 변환 같은 다양한 증강 기법도 적용했어.

VSR 모델은 엔드 투 엔드 구조를 채택하고, CTC/어텐션 손실을 함께 사용하는데, 여기에는 Enhanced ResNet3D 시각 프론트엔드, E-Branchformer 인코더, 양방향 변환기 디코더가 포함되어 있어. 우리의 접근 방식은 단일 화자 작업에서 30.47%의 CER을, 다중 화자 작업에서 34.30%의 CER을 달성했어. 단일 화자 작업의 개방 트랙에서는 2위를, 나머지 세 개 트랙에서는 1위를 차지했어.

================================================================================

URL:
https://arxiv.org/pdf/2408.03156.pdf

Title: Iterative CT Reconstruction via Latent Variable Optimization of Shallow Diffusion Models

Original Abstract:
Image-generative artificial intelligence (AI) has garnered significant attention in recent years. In particular, the diffusion model, a core component of generative AI, produces high-quality images with rich diversity. In this study, we proposed a novel computed tomography (CT) reconstruction method by combining the denoising diffusion probabilistic model with iterative CT reconstruction. In sharp contrast to previous studies, we optimized the fidelity loss of CT reconstruction with respect to the latent variable of the diffusion model, instead of the image and model parameters. To suppress the changes in anatomical structures produced by the diffusion model, we shallowed the diffusion and reverse processes and fixed a set of added noises in the reverse process to make it deterministic during the inference. We demonstrated the effectiveness of the proposed method through the sparse-projection CT reconstruction of 1/10 projection data. Despite the simplicity of the implementation, the proposed method has the potential to reconstruct high-quality images while preserving the patient's anatomical structures and was found to outperform existing methods, including iterative reconstruction, iterative reconstruction with total variation, and the diffusion model alone in terms of quantitative indices such as the structural similarity index and peak signal-to-noise ratio. We also explored further sparse-projection CT reconstruction using 1/20 projection data with the same trained diffusion model. As the number of iterations increased, the image quality improved comparable to that of 1/10 sparse-projection CT reconstruction. In principle, this method can be widely applied not only to CT but also to other imaging modalities.

Translated Abstract:
최근 몇 년 동안 이미지 생성 인공지능(AI)이 많은 관심을 받고 있어. 특히, 생성 AI의 핵심인 확산 모델이 다양한 고품질 이미지를 만들어내는 데 큰 역할을 하고 있어. 

이번 연구에서는 노이즈 제거 확산 확률 모델을 이용해 새로운 컴퓨터 단층 촬영(CT) 재구성 방법을 제안했어. 이전 연구들과는 다르게, 우리는 CT 재구성의 신뢰도 손실을 이미지나 모델 파라미터가 아니라 확산 모델의 잠재 변수에 맞춰 최적화했어. 또한, 확산 모델이 만들어내는 해부학적 구조의 변화를 줄이기 위해 확산 과정과 역과정을 얕게 하고, 역과정에서 추가된 노이즈를 고정해서 예측 시 결정적으로 만들었어.

제안한 방법의 효과를 1/10 투영 데이터의 희소 투영 CT 재구성을 통해 보여줬어. 구현이 간단함에도 불구하고, 이 방법은 환자의 해부학적 구조를 잘 유지하면서 고품질 이미지를 재구성할 수 있는 가능성이 있어. 기존의 방법들보다 성능이 더 좋았고, 구조적 유사도 지수와 피크 신호 대 잡음 비율 같은 정량적 지표에서도 우수한 결과를 나타냈어. 

또한, 같은 훈련된 확산 모델을 사용해 1/20 투영 데이터로 희소 투영 CT 재구성을 추가로 탐색했어. 반복 횟수가 증가함에 따라 이미지 품질이 1/10 희소 투영 CT 재구성과 비슷하게 개선되었어. 이 방법은 원칙적으로 CT뿐만 아니라 다른 이미징 방식에도 널리 적용될 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2408.10060.pdf

Title: Facial Wrinkle Segmentation for Cosmetic Dermatology: Pretraining with Texture Map-Based Weak Supervision

Original Abstract:
Facial wrinkle detection plays a crucial role in cosmetic dermatology. Precise manual segmentation of facial wrinkles is challenging and time-consuming, with inherent subjectivity leading to inconsistent results among graders. To address this issue, we propose two solutions. First, we build and release the first public facial wrinkle dataset, 'FFHQ-Wrinkle', an extension of the NVIDIA FFHQ dataset. It includes 1,000 images with human labels and 50,000 images with automatically generated weak labels. This dataset could serve as a foundation for the research community to develop advanced wrinkle detection algorithms. Second, we introduce a simple training strategy utilizing texture maps, applicable to various segmentation models, to detect wrinkles across the face. Our two-stage training strategy first pretrain models on a large dataset with weak labels (N=50k), or masked texture maps generated through computer vision techniques, without human intervention. We then finetune the models using human-labeled data (N=1k), which consists of manually labeled wrinkle masks. The network takes as input a combination of RGB and masked texture map of the image, comprising four channels, in finetuning. We effectively combine labels from multiple annotators to minimize subjectivity in manual labeling. Our strategies demonstrate improved segmentation performance in facial wrinkle segmentation both quantitatively and visually compared to existing pretraining methods. The dataset is available at this https URL.

Translated Abstract:
얼굴 주름 탐지는 화장품 피부과에서 아주 중요한 역할을 해. 얼굴 주름을 정확하게 수동으로 나누는 건 어렵고 시간이 많이 걸려. 게다가 주관적인 요소가 있어서 평가하는 사람마다 결과가 다를 수 있어. 이 문제를 해결하기 위해 두 가지 방법을 제안해.

첫째, 우리는 'FFHQ-Wrinkle'라는 첫 번째 공개 얼굴 주름 데이터세트를 만들고 배포했어. 이건 NVIDIA FFHQ 데이터세트를 확장한 거고, 1,000개의 사람 라벨이 있는 이미지와 50,000개의 자동 생성된 약한 라벨이 있는 이미지를 포함하고 있어. 이 데이터세트는 연구자들이 더 발전된 주름 탐지 알고리즘을 개발하는 데 기초 자료로 활용할 수 있을 거야.

둘째, 우리는 얼굴 전반에 걸쳐 주름을 탐지하기 위해 다양한 분할 모델에 적용할 수 있는 간단한 훈련 전략을 소개해. 우리의 두 단계 훈련 전략은 먼저 약한 라벨(N=50k)이 있는 큰 데이터세트에서 모델을 사전 훈련해. 이 과정에서는 사람의 개입 없이 컴퓨터 비전 기술로 생성된 마스크된 텍스처 맵을 사용해. 그런 다음, 사람 라벨이 있는 데이터(N=1k), 즉 수동으로 라벨링한 주름 마스크를 사용해 모델을 세밀하게 조정해.

네트워크는 RGB와 마스크된 텍스처 맵을 합친 걸 입력으로 받아서 훈련해. 이렇게 하면 여러 평가자의 라벨을 효과적으로 결합해 수동 라벨링의 주관성을 줄일 수 있어. 우리의 전략은 기존의 사전 훈련 방법과 비교했을 때 얼굴 주름 분할에서 정량적이고 시각적으로 향상된 성능을 보여줘. 데이터세트는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2408.10843.pdf

Title: Detecting Wildfires on UAVs with Real-time Segmentation Trained by Larger Teacher Models

Original Abstract:
Early detection of wildfires is essential to prevent large-scale fires resulting in extensive environmental, structural, and societal damage. Uncrewed aerial vehicles (UAVs) can cover large remote areas effectively with quick deployment requiring minimal infrastructure and equipping them with small cameras and computers enables autonomous real-time detection. In remote areas, however, the UAVs are limited to on-board computing for detection due to the lack of high-bandwidth mobile networks. This limits the detection to methods which are light enough for the on-board computer alone. For accurate camera-based localisation, segmentation of the detected smoke is essential but training data for deep learning-based wildfire smoke segmentation is limited. This study shows how small specialised segmentation models can be trained using only bounding box labels, leveraging zero-shot foundation model supervision. The method offers the advantages of needing only fairly easily obtainable bounding box labels and requiring training solely for the smaller student network. The proposed method achieved 63.3% mIoU on a manually annotated and diverse wildfire dataset. The used model can perform in real-time at ~25 fps with a UAV-carried NVIDIA Jetson Orin NX computer while reliably recognising smoke, demonstrated at real-world forest burning events. Code is available at this https URL

Translated Abstract:
야생 화재를 조기에 발견하는 건 큰 화재로 인한 환경, 구조물, 사회적 피해를 막는 데 정말 중요해. 드론(UAV)은 넓고 외진 지역을 효과적으로 커버할 수 있고, 빠르게 배치할 수 있어서 인프라가 거의 필요 없어. 작은 카메라와 컴퓨터를 장착하면 자율적으로 실시간으로 감지가 가능해.

하지만 외진 지역에서는 고속 모바일 네트워크가 없어서 드론이 감지할 때 온보드 컴퓨터에 제한돼. 그래서 감지 방법이 온보드 컴퓨터만으로 충분히 가벼운 것들로 한정돼. 정확한 카메라 기반 위치 파악을 위해서는 감지된 연기의 분할이 필수인데, 딥러닝 기반의 야생 화재 연기 분할을 위한 훈련 데이터는 제한적이야.

이 연구에서는 바운딩 박스 레이블만으로 작은 전문 분할 모델을 훈련할 수 있는 방법을 보여줘. 제로샷 기초 모델 감독을 활용하는 거지. 이 방법은 비교적 쉽게 구할 수 있는 바운딩 박스 레이블만 필요하고, 작은 학생 네트워크만 훈련하면 되니까 장점이 많아. 제안된 방법은 수동으로 주석이 달린 다양한 야생 화재 데이터셋에서 63.3% mIoU를 달성했어. 사용된 모델은 드론에 장착된 NVIDIA Jetson Orin NX 컴퓨터로 약 25fps의 실시간 성능을 보여주고, 실제 숲 불타는 사건에서도 신뢰성 있게 연기를 인식했어. 코드도 이 https URL에서 사용할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2408.12528.pdf

Title: Show-o: One Single Transformer to Unify Multimodal Understanding and Generation

Original Abstract:
We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at this https URL.

Translated Abstract:
우리는 Show-o라는 통합된 트랜스포머 모델을 소개해. 이 모델은 다양한 모달리티의 이해와 생성을 통합해. 완전 자율 회귀 모델과는 달리, Show-o는 자율 회귀와 (이산) 확산 모델링을 결합해서 다양한 입력과 출력을 유연하게 처리할 수 있어.

이 통합 모델은 시각-언어 작업을 다양하게 지원해. 예를 들어, 시각적 질문-응답, 텍스트-이미지 생성, 텍스트 안내에 의한 인페인팅/추출, 혼합 모달리티 생성 같은 것들이야. 여러 벤치마크 테스트에서 기존의 개별 모델들과 비교했을 때, 비슷하거나 더 나은 성능을 보여주고 있어. 이 모델은 이해나 생성을 위해 맞춤화된 동일하거나 더 많은 파라미터를 가진 모델들과 경쟁할 수 있어.

이런 점에서 Show-o는 차세대 기초 모델로서의 가능성을 크게 보여줘. 코드와 모델은 이 링크에서 제공돼.

================================================================================

URL:
https://arxiv.org/pdf/2408.15679.pdf

Title: DEAR: Depth-Enhanced Action Recognition

Original Abstract:
Detecting actions in videos, particularly within cluttered scenes, poses significant challenges due to the limitations of 2D frame analysis from a camera perspective. Unlike human vision, which benefits from 3D understanding, recognizing actions in such environments can be difficult. This research introduces a novel approach integrating 3D features and depth maps alongside RGB features to enhance action recognition accuracy. Our method involves processing estimated depth maps through a separate branch from the RGB feature encoder and fusing the features to understand the scene and actions comprehensively. Using the Side4Video framework and VideoMamba, which employ CLIP and VisionMamba for spatial feature extraction, our approach outperformed our implementation of the Side4Video network on the Something-Something V2 dataset. Our code is available at: this https URL

Translated Abstract:
비디오에서 행동을 감지하는 건 특히 복잡한 장면에서는 상당히 어려워. 카메라 시점에서 2D 프레임 분석의 한계 때문에 그렇지. 사람의 시각은 3D 이해 덕분에 더 잘 인식할 수 있지만, 이런 환경에서는 행동을 알아보기가 힘들어. 

이 연구에서는 3D 특징과 깊이 맵을 RGB 특징과 통합하는 새로운 방법을 제안해. 이 방법은 RGB 특징 인코더와는 별도의 브랜치를 통해 추정된 깊이 맵을 처리하고, 그 특징들을 융합해서 장면과 행동을 더 잘 이해할 수 있도록 해. 

Side4Video 프레임워크와 VideoMamba를 사용해서 공간 특징 추출을 위해 CLIP과 VisionMamba를 활용했고, 우리의 방법은 Something-Something V2 데이터셋에서 Side4Video 네트워크를 구현한 것보다 더 나은 성능을 보였어. 

우리의 코드는 이 링크에서 확인할 수 있어: 이 URL

================================================================================

URL:
https://arxiv.org/pdf/2408.16322.pdf

Title: BEVal: A Cross-dataset Evaluation Study of BEV Segmentation Models for Autonomous Driving

Original Abstract:
Current research in semantic bird's-eye view segmentation for autonomous driving focuses solely on optimizing neural network models using a single dataset, typically nuScenes. This practice leads to the development of highly specialized models that may fail when faced with different environments or sensor setups, a problem known as domain shift. In this paper, we conduct a comprehensive cross-dataset evaluation of state-of-the-art BEV segmentation models to assess their performance across different training and testing datasets and setups, as well as different semantic categories. We investigate the influence of different sensors, such as cameras and LiDAR, on the models' ability to generalize to diverse conditions and scenarios. Additionally, we conduct multi-dataset training experiments that improve models' BEV segmentation performance compared to single-dataset training. Our work addresses the gap in evaluating BEV segmentation models under cross-dataset validation. And our findings underscore the importance of enhancing model generalizability and adaptability to ensure more robust and reliable BEV segmentation approaches for autonomous driving applications. The code for this paper available at this https URL .

Translated Abstract:
현재 자율주행을 위한 의미 기반 새의 눈 뷰(BEV) 세분화 연구는 주로 nuScenes라는 하나의 데이터셋을 사용해 신경망 모델을 최적화하는 데 집중하고 있어. 이런 방식은 특정 환경이나 센서 설정에서 잘 작동하지 않는 전문화된 모델이 개발되는 문제를 일으켜. 이를 도메인 시프트(domain shift)라고 해.

이 논문에서는 최신 BEV 세분화 모델들을 여러 데이터셋을 사용해 평가해봤어. 다양한 훈련 및 테스트 데이터셋과 설정, 그리고 여러 의미 카테고리에서 성능을 비교하는 거야. 카메라나 LiDAR 같은 다양한 센서가 모델의 일반화 능력에 어떤 영향을 미치는지도 조사했어.

또한, 여러 데이터셋을 활용한 훈련 실험을 통해 단일 데이터셋으로 훈련한 것보다 BEV 세분화 성능을 향상시켰어. 우리 연구는 교차 데이터셋 검증에서 BEV 세분화 모델을 평가하는 데 부족한 점을 보완하고 있어. 결과적으로, 모델의 일반화 능력과 적응력을 높이는 것이 자율주행 응용을 위한 더 강력하고 신뢰할 수 있는 BEV 세분화 접근 방식을 확보하는 데 중요하다는 점을 강조했어. 이 논문의 코드는 이 링크에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.00342.pdf

Title: AdaNAT: Exploring Adaptive Policy for Token-Based Image Generation

Original Abstract:
Recent studies have demonstrated the effectiveness of token-based methods for visual content generation. As a representative work, non-autoregressive Transformers (NATs) are able to synthesize images with decent quality in a small number of steps. However, NATs usually necessitate configuring a complicated generation policy comprising multiple manually-designed scheduling rules. These heuristic-driven rules are prone to sub-optimality and come with the requirements of expert knowledge and labor-intensive efforts. Moreover, their one-size-fits-all nature cannot flexibly adapt to the diverse characteristics of each individual sample. To address these issues, we propose AdaNAT, a learnable approach that automatically configures a suitable policy tailored for every sample to be generated. In specific, we formulate the determination of generation policies as a Markov decision process. Under this framework, a lightweight policy network for generation can be learned via reinforcement learning. Importantly, we demonstrate that simple reward designs such as FID or pre-trained reward models, may not reliably guarantee the desired quality or diversity of generated samples. Therefore, we propose an adversarial reward design to guide the training of policy networks effectively. Comprehensive experiments on four benchmark datasets, i.e., ImageNet-256 & 512, MS-COCO, and CC3M, validate the effectiveness of AdaNAT. Code and pre-trained models will be released at this https URL.

Translated Abstract:
최근 연구들은 토큰 기반 방법이 시각적 콘텐츠 생성에 효과적이라는 것을 보여줬어. 대표적인 예로, 비자기회귀 트랜스포머(NATs)는 적은 단계로 괜찮은 품질의 이미지를 합성할 수 있어. 하지만 NATs는 보통 여러 가지 수동으로 설계된 스케줄링 규칙을 포함하는 복잡한 생성 정책을 설정해야 해. 이런 규칙들은 최적이 아닐 수 있고, 전문가의 지식과 많은 노력이 필요해. 게다가, 모든 샘플에 똑같이 적용할 수 있는 방식이라 각 샘플의 다양한 특성에 유연하게 적응하기 어려워.

이런 문제를 해결하기 위해, 우리는 AdaNAT이라는 학습 가능한 접근 방식을 제안해. 이 방법은 생성될 샘플마다 적절한 정책을 자동으로 설정해. 구체적으로, 생성 정책을 결정하는 과정을 마르코프 결정 프로세스로 모델링했어. 이 틀 안에서 경량의 정책 네트워크를 강화 학습을 통해 배울 수 있어. 중요한 것은, FID 같은 간단한 보상 설계나 미리 학습된 보상 모델이 원하는 품질이나 다양성을 보장하지 못할 수 있다는 거야. 그래서 우리는 정책 네트워크의 훈련을 효과적으로 안내하기 위해 적대적 보상 설계를 제안해.

ImageNet-256 & 512, MS-COCO, CC3M 등 네 개의 벤치마크 데이터셋에서의 포괄적인 실험 결과, AdaNAT의 효과가 입증됐어. 코드와 미리 학습된 모델은 이 HTTPS URL에서 공개될 거야.

================================================================================

URL:
https://arxiv.org/pdf/2409.00991.pdf

Title: 3D Priors-Guided Diffusion for Blind Face Restoration

Original Abstract:
Blind face restoration endeavors to restore a clear face image from a degraded counterpart. Recent approaches employing Generative Adversarial Networks (GANs) as priors have demonstrated remarkable success in this field. However, these methods encounter challenges in achieving a balance between realism and fidelity, particularly in complex degradation scenarios. To inherit the exceptional realism generative ability of the diffusion model and also constrained by the identity-aware fidelity, we propose a novel diffusion-based framework by embedding the 3D facial priors as structure and identity constraints into a denoising diffusion process. Specifically, in order to obtain more accurate 3D prior representations, the 3D facial image is reconstructed by a 3D Morphable Model (3DMM) using an initial restored face image that has been processed by a pretrained restoration network. A customized multi-level feature extraction method is employed to exploit both structural and identity information of 3D facial images, which are then mapped into the noise estimation process. In order to enhance the fusion of identity information into the noise estimation, we propose a Time-Aware Fusion Block (TAFB). This module offers a more efficient and adaptive fusion of weights for denoising, considering the dynamic nature of the denoising process in the diffusion model, which involves initial structure refinement followed by texture detail enhancement. Extensive experiments demonstrate that our network performs favorably against state-of-the-art algorithms on synthetic and real-world datasets for blind face restoration. The Code is released on our project page at this https URL.

Translated Abstract:
블라인드 얼굴 복원은 손상된 얼굴 이미지를 선명한 이미지로 복원하려는 시도야. 최근에는 생성적 적대 신경망(GAN)을 활용한 방법들이 이 분야에서 좋은 성과를 내고 있어. 하지만 이런 방법들은 복잡한 손상 상황에서 현실감과 충실도를 균형 있게 맞추는 데 어려움을 겪고 있어.

우리는 확산 모델의 뛰어난 현실감 생성 능력을 물려받으면서도, 정체성을 고려한 충실도를 유지하는 새로운 확산 기반 프레임워크를 제안해. 여기서는 3D 얼굴 정보를 구조와 정체성 제약으로 삼아 노이즈 제거 확산 과정에 통합해. 특히, 더 정확한 3D 정보 표현을 얻기 위해, 초기 복원된 얼굴 이미지를 사용해 3D 변형 가능한 모델(3DMM)로 3D 얼굴 이미지를 재구성해.

또한, 3D 얼굴 이미지의 구조적 정보와 정체성 정보를 활용하기 위해 맞춤형 다중 수준 특징 추출 방법을 사용하고, 이를 노이즈 추정 과정에 매핑해. 정체성 정보의 통합을 강화하기 위해, 시간 인식 융합 블록(TAFB)을 제안해. 이 모듈은 확산 모델의 노이즈 제거 과정이 초기 구조 개선과 텍스처 세부 사항 향상으로 진행되는 동적인 특성을 고려해, 노이즈 제거를 위한 가중치의 효율적이고 적응적인 융합을 제공해.

폭넓은 실험 결과, 우리의 네트워크가 합성 데이터와 실제 데이터셋에서 블라인드 얼굴 복원에 대한 최신 알고리즘보다 성능이 좋음을 보여줘. 코드는 우리 프로젝트 페이지에서 제공되고 있어.

================================================================================

URL:
https://arxiv.org/pdf/2409.02310.pdf

Title: Geometry-aware Feature Matching for Large-Scale Structure from Motion

Original Abstract:
Establishing consistent and dense correspondences across multiple images is crucial for Structure from Motion (SfM) systems. Significant view changes, such as air-to-ground with very sparse view overlap, pose an even greater challenge to the correspondence solvers. We present a novel optimization-based approach that significantly enhances existing feature matching methods by introducing geometry cues in addition to color cues. This helps fill gaps when there is less overlap in large-scale scenarios. Our method formulates geometric verification as an optimization problem, guiding feature matching within detector-free methods and using sparse correspondences from detector-based methods as anchor points. By enforcing geometric constraints via the Sampson Distance, our approach ensures that the denser correspondences from detector-free methods are geometrically consistent and more accurate. This hybrid strategy significantly improves correspondence density and accuracy, mitigates multi-view inconsistencies, and leads to notable advancements in camera pose accuracy and point cloud density. It outperforms state-of-the-art feature matching methods on benchmark datasets and enables feature matching in challenging extreme large-scale settings.

Translated Abstract:
여러 이미지 사이에서 일관되고 밀집된 대응 관계를 만드는 것은 구조에서 운동(SfM) 시스템에 매우 중요해. 특히 공중에서 지면으로의 시점 변화 같은 경우, 겹치는 부분이 거의 없어서 대응 관계를 찾는 게 더 어렵지. 

우리는 기존의 특징 매칭 방법을 크게 개선하는 새로운 최적화 기반 접근 방식을 소개해. 이 방법은 색상 정보뿐만 아니라 기하학적인 단서를 추가해서, 큰 규모의 상황에서 겹치는 부분이 적을 때도 빈틈을 메울 수 있도록 도와줘. 

우리 방법은 기하학적 검증을 최적화 문제로 설정해서, 탐지기 없이 특징 매칭을 유도하고 탐지기 기반 방법에서 얻은 희박한 대응 관계를 앵커 포인트로 사용해. 샘슨 거리(Sampson Distance)를 통해 기하학적 제약을 적용함으로써, 탐지기 없이 얻은 밀집된 대응 관계가 기하학적으로 일관되고 더 정확하도록 보장해. 

이 혼합 전략은 대응 관계의 밀도와 정확성을 크게 개선하고, 다중 시점의 불일치를 줄여줘. 결과적으로 카메라 자세의 정확성과 포인트 클라우드 밀도가 눈에 띄게 향상돼. 이 방법은 최신 특징 매칭 방법보다 벤치마크 데이터셋에서 성능이 뛰어나고, 극단적으로 큰 규모의 어려운 상황에서도 특징 매칭을 가능하게 해.

================================================================================

URL:
https://arxiv.org/pdf/2409.04086.pdf

Title: Introducing a Class-Aware Metric for Monocular Depth Estimation: An Automotive Perspective

Original Abstract:
The increasing accuracy reports of metric monocular depth estimation models lead to a growing interest from the automotive domain. Current model evaluations do not provide deeper insights into the models' performance, also in relation to safety-critical or unseen classes. Within this paper, we present a novel approach for the evaluation of depth estimation models. Our proposed metric leverages three components, a class-wise component, an edge and corner image feature component, and a global consistency retaining component. Classes are further weighted on their distance in the scene and on criticality for automotive applications. In the evaluation, we present the benefits of our metric through comparison to classical metrics, class-wise analytics, and the retrieval of critical situations. The results show that our metric provides deeper insights into model results while fulfilling safety-critical requirements. We release the code and weights on the following repository: this https URL

Translated Abstract:
단일 카메라 깊이 추정 모델의 정확도가 높아지면서 자동차 분야에서 관심이 커지고 있어. 하지만 현재 모델 평가 방법은 모델의 성능에 대한 깊은 통찰을 제공하지 못하고, 특히 안전과 관련된 중요 클래스나 보지 못한 클래스에 대해서도 부족해. 

이 논문에서는 깊이 추정 모델을 평가하는 새로운 접근 방식을 제안해. 우리가 제안하는 평가는 세 가지 요소를 활용해. 첫 번째는 클래스별 요소, 두 번째는 엣지와 코너 이미지 특징 요소, 그리고 세 번째는 전역 일관성을 유지하는 요소야. 각 클래스는 장면 내 거리와 자동차 응용에 대한 중요도에 따라 가중치를 두고 평가해.

우리는 평가에서 고전적인 지표와 비교하고, 클래스별 분석과 중요한 상황을 불러오는 방법을 통해 우리의 지표의 장점을 보여줘. 결과적으로 우리 지표가 모델 결과에 대한 깊은 통찰을 제공하면서 안전 요구 사항을 충족한다는 걸 알 수 있었어. 코드와 가중치는 다음 저장소에서 받을 수 있어: 이 URL

================================================================================

URL:
https://arxiv.org/pdf/2409.04732.pdf

Title: VidLPRO: A $\underline{Vid}$eo-$\underline{L}$anguage $\underline{P}$re-training Framework for $\underline{Ro}$botic and Laparoscopic Surgery

Original Abstract:
We introduce VidLPRO, a novel video-language (VL) pre-training framework designed specifically for robotic and laparoscopic surgery. While existing surgical VL models primarily rely on contrastive learning, we propose a more comprehensive approach to capture the intricate temporal dynamics and align video with language. VidLPRO integrates video-text contrastive learning, video-text matching, and masked language modeling objectives to learn rich VL representations. To support this framework, we present GenSurg+, a carefully curated dataset derived from GenSurgery, comprising 17k surgical video clips paired with captions generated by GPT-4 using transcripts extracted by the Whisper model. This dataset addresses the need for large-scale, high-quality VL data in the surgical domain. Extensive experiments on benchmark datasets, including Cholec80 and AutoLaparo, demonstrate the efficacy of our approach. VidLPRO achieves state-of-the-art performance in zero-shot surgical phase recognition, significantly outperforming existing surgical VL models such as SurgVLP and HecVL. Our model demonstrates improvements of up to 21.5\% in accuracy and 15.7% in F1 score, setting a new benchmark in the field. Notably, VidLPRO exhibits robust performance even with single-frame inference, while effectively scaling with increased temporal context. Ablation studies reveal the impact of frame sampling strategies on model performance and computational efficiency. These results underscore VidLPRO's potential as a foundation model for surgical video understanding.

Translated Abstract:
우리는 로봇 수술과 복강경 수술을 위해 특별히 설계된 새로운 비디오-언어(VL) 사전 학습 프레임워크인 VidLPRO를 소개해. 기존의 수술 VL 모델들은 주로 대조 학습에 의존하는데, 우리는 비디오와 언어의 복잡한 시간적 동역학을 포착하고 잘 맞추기 위해 더 포괄적인 접근 방식을 제안해.

VidLPRO는 비디오-텍스트 대조 학습, 비디오-텍스트 매칭, 그리고 마스킹된 언어 모델링 목표를 통합해서 풍부한 VL 표현을 학습해. 이 프레임워크를 지원하기 위해, 우리는 GenSurg+라는 데이터셋을 소개하는데, 이 데이터셋은 GenSurgery에서 파생된 17,000개의 수술 비디오 클립과 Whisper 모델로 추출한 대본을 이용해 GPT-4가 생성한 캡션을 쌍으로 이루고 있어. 이 데이터셋은 수술 분야에서 대규모의 고품질 VL 데이터가 필요하다는 점을 해결해.

Cholec80와 AutoLaparo 같은 벤치마크 데이터셋에 대한 광범위한 실험을 통해 우리의 접근 방식의 효과를 입증했어. VidLPRO는 제로샷 수술 단계 인식에서 최첨단 성능을 달성하며, SurgVLP와 HecVL 같은 기존의 수술 VL 모델들을 크게 능가했어. 우리 모델은 정확도에서 최대 21.5% 향상, F1 점수에서 15.7% 향상을 보여주며, 이 분야에서 새로운 기준을 세웠어. 특히, VidLPRO는 단일 프레임 추론을 하더라도 강력한 성능을 발휘하고, 시간적 맥락이 늘어날수록 효과적으로 확장할 수 있어.

프레임 샘플링 전략이 모델 성능과 계산 효율성에 미치는 영향을 보여주는 소규모 연구도 진행했어. 이 결과들은 VidLPRO가 수술 비디오 이해를 위한 기초 모델로서의 잠재력을 강조해.

================================================================================

URL:
https://arxiv.org/pdf/2409.04747.pdf

Title: Explicit Mutual Information Maximization for Self-Supervised Learning

Original Abstract:
Recently, self-supervised learning (SSL) has been extensively studied. Theoretically, mutual information maximization (MIM) is an optimal criterion for SSL, with a strong theoretical foundation in information theory. However, it is difficult to directly apply MIM in SSL since the data distribution is not analytically available in applications. In practice, many existing methods can be viewed as approximate implementations of the MIM criterion. This work shows that, based on the invariance property of MI, explicit MI maximization can be applied to SSL under a generic distribution assumption, i.e., a relaxed condition of the data distribution. We further illustrate this by analyzing the generalized Gaussian distribution. Based on this result, we derive a loss function based on the MIM criterion using only second-order statistics. We implement the new loss for SSL and demonstrate its effectiveness via extensive experiments.

Translated Abstract:
최근에 자기 지도 학습(SSL)이 많이 연구되고 있어. 이론적으로, 상호 정보 최대화(MIM)는 SSL에 가장 적합한 기준으로, 정보 이론에서 강한 이론적 기반을 갖고 있어. 하지만, 실제로는 데이터 분포를 직접 사용할 수 없어서 MIM을 SSL에 적용하는 게 어려워.

실제로는 많은 기존 방법들이 MIM 기준의 근사 구현으로 볼 수 있어. 이 연구는 MI의 불변성 속성을 바탕으로, 일반적인 분포 가정 하에서 SSL에 명시적인 MI 최대화를 적용할 수 있음을 보여줘. 즉, 데이터 분포의 완화된 조건을 의미해. 우리는 이걸 일반화된 가우시안 분포를 분석함으로써 더 잘 설명해.

이 결과를 바탕으로, 우리는 2차 통계만을 사용해서 MIM 기준에 기반한 손실 함수를 도출했어. 그리고 이 새로운 손실 함수를 SSL에 적용하고, 광범위한 실험을 통해 그 효과를 입증했어.

================================================================================

URL:
https://arxiv.org/pdf/2409.04918.pdf

Title: Training-free ZS-CIR via Weighted Modality Fusion and Similarity

Original Abstract:
Composed image retrieval (CIR), which formulates the query as a combination of a reference image and modified text, has emerged as a new form of image search due to its enhanced ability to capture users' intentions. However, training a CIR model in a supervised manner typically requires labor-intensive collection of (reference image, text modifier, target image) triplets. While existing zero-shot CIR (ZS-CIR) methods eliminate the need for training on specific downstream datasets, they still require additional pretraining with large-scale image-text pairs. In this paper, we introduce a training-free approach for ZS-CIR. Our approach, \textbf{Wei}ghted \textbf{Mo}dality fusion and similarity for \textbf{CIR} (WeiMoCIR), operates under the assumption that image and text modalities can be effectively combined using a simple weighted average. This allows the query representation to be constructed directly from the reference image and text modifier. To further enhance retrieval performance, we employ multimodal large language models (MLLMs) to generate image captions for the database images and incorporate these textual captions into the similarity computation by combining them with image information using a weighted average. Our approach is simple, easy to implement, and its effectiveness is validated through experiments on the FashionIQ and CIRR datasets.

Translated Abstract:
복합 이미지 검색(CIR)은 질의를 참조 이미지와 수정된 텍스트의 조합으로 구성하는 새로운 이미지 검색 방식으로, 사용자 의도를 더 잘 포착하는 장점이 있어요. 하지만 CIR 모델을 감독 방식으로 학습하려면 (참조 이미지, 텍스트 수정자, 목표 이미지) 삼중항을 수집하는 데 많은 노력이 필요해요. 

기존의 제로샷 CIR(ZS-CIR) 방법은 특정 하위 데이터셋에서 학습할 필요를 없애지만, 여전히 대규모 이미지-텍스트 쌍으로 사전 학습이 필요해요. 이 논문에서는 ZS-CIR을 위한 학습 없는 접근 방식을 제안해요. 우리의 방법인 \textbf{Wei}ghted \textbf{Mo}dality fusion and similarity for \textbf{CIR} (WeiMoCIR)은 이미지와 텍스트 모달리티를 간단한 가중 평균을 사용해 효과적으로 결합할 수 있다고 가정해요. 이 방식으로 질의 표현을 참조 이미지와 텍스트 수정자로부터 직접 구성할 수 있어요. 

검색 성능을 더욱 높이기 위해, 우리는 다중 모달 대형 언어 모델(MLLM)을 활용해 데이터베이스의 이미지 캡션을 생성하고, 이 텍스트 캡션을 이미지 정보와 결합해 가중 평균을 사용해 유사도 계산에 포함시켜요. 우리의 접근 방식은 간단하고 구현하기 쉬우며, FashionIQ와 CIRR 데이터셋에서 실험을 통해 효과가 검증되었어요.

================================================================================

URL:
https://arxiv.org/pdf/2409.05099.pdf

Title: DreamMapping: High-Fidelity Text-to-3D Generation via Variational Distribution Mapping

Original Abstract:
Score Distillation Sampling (SDS) has emerged as a prevalent technique for text-to-3D generation, enabling 3D content creation by distilling view-dependent information from text-to-2D guidance. However, they frequently exhibit shortcomings such as over-saturated color and excess smoothness. In this paper, we conduct a thorough analysis of SDS and refine its formulation, finding that the core design is to model the distribution of rendered images. Following this insight, we introduce a novel strategy called Variational Distribution Mapping (VDM), which expedites the distribution modeling process by regarding the rendered images as instances of degradation from diffusion-based generation. This special design enables the efficient training of variational distribution by skipping the calculations of the Jacobians in the diffusion U-Net. We also introduce timestep-dependent Distribution Coefficient Annealing (DCA) to further improve distilling precision. Leveraging VDM and DCA, we use Gaussian Splatting as the 3D representation and build a text-to-3D generation framework. Extensive experiments and evaluations demonstrate the capability of VDM and DCA to generate high-fidelity and realistic assets with optimization efficiency.

Translated Abstract:
스코어 증류 샘플링(SDS)은 텍스트를 3D로 변환하는 데 많이 쓰이는 기술로, 텍스트에서 2D 가이드를 통해 3D 콘텐츠를 만드는 방식이야. 하지만 이 방법은 색상이 너무 과하게 발색되거나, 너무 매끄럽게 나오는 문제점이 있어.

이번 논문에서는 SDS에 대한 자세한 분석을 하고, 그 공식화를 개선했어. 우리가 발견한 핵심은 렌더링된 이미지의 분포를 모델링하는 거야. 이 인사이트를 바탕으로 새로운 전략인 변이 분포 매핑(VDM)을 도입했는데, 이 방법은 렌더링된 이미지를 확산 기반 생성에서의 저하 사례로 간주하여 분포 모델링 과정을 더 빠르게 만들어.

이 특별한 설계 덕분에 확산 U-Net에서 자코비안 계산을 건너뛰면서 변이 분포를 효율적으로 훈련할 수 있어. 또, 디스틸링 정밀도를 높이기 위해 타임스텝 의존적인 분포 계수 어닐링(DCA)도 도입했어.

VDM과 DCA를 활용해서 우리는 가우시안 스플래팅을 3D 표현으로 사용하고, 텍스트에서 3D로 생성하는 프레임워크를 구축했어. 다양한 실험과 평가를 통해 VDM과 DCA가 고충실도와 사실감 있는 자산을 생성하면서 최적화 효율성도 높일 수 있음을 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.05463.pdf

Title: DriveScape: Towards High-Resolution Controllable Multi-View Driving Video Generation

Original Abstract:
Recent advancements in generative models have provided promising solutions for synthesizing realistic driving videos, which are crucial for training autonomous driving perception models. However, existing approaches often struggle with multi-view video generation due to the challenges of integrating 3D information while maintaining spatial-temporal consistency and effectively learning from a unified model. We propose DriveScape, an end-to-end framework for multi-view, 3D condition-guided video generation, capable of producing 1024 x 576 high-resolution videos at 10Hz. Unlike other methods limited to 2Hz due to the 3D box annotation frame rate, DriveScape overcomes this with its ability to operate under sparse conditions. Our Bi-Directional Modulated Transformer (BiMot) ensures precise alignment of 3D structural information, maintaining spatial-temporal consistency. DriveScape excels in video generation performance, achieving state-of-the-art results on the nuScenes dataset with an FID score of 8.34 and an FVD score of 76.39. Our project homepage: this https URL

Translated Abstract:
최근 생성 모델의 발전 덕분에 현실적인 주행 비디오를 합성하는 데 좋은 해결책이 나왔어. 이건 자율주행 인식 모델을 훈련하는 데 매우 중요해. 하지만 기존 방법들은 3D 정보를 통합하면서 공간-시간 일관성을 유지하고 통합된 모델로 효과적으로 학습하는 데 어려움을 겪고 있어.

그래서 우리는 DriveScape라는 새로운 프레임워크를 제안해. 이건 다중 시점, 3D 조건에 기반한 비디오 생성을 위한 엔드 투 엔드 시스템이야. 1024 x 576 해상도의 고해상도 비디오를 10Hz로 생성할 수 있어. 다른 방법들은 3D 박스 주석의 프레임 속도 때문에 2Hz로 제한되는데, DriveScape는 희소한 조건에서도 잘 작동해.

우리의 Bi-Directional Modulated Transformer(BiMot)는 3D 구조 정보를 정확하게 정렬해 주고, 공간-시간 일관성을 유지해줘. DriveScape는 비디오 생성 성능에서 뛰어나서, nuScenes 데이터셋에서 FID 점수 8.34와 FVD 점수 76.39로 최첨단 결과를 달성했어. 

우리 프로젝트 홈페이지: 이 링크

================================================================================

URL:
https://arxiv.org/pdf/2409.05587.pdf

Title: DSDFormer: An Innovative Transformer-Mamba Framework for Robust High-Precision Driver Distraction Identification

Original Abstract:
Driver distraction remains a leading cause of traffic accidents, posing a critical threat to road safety globally. As intelligent transportation systems evolve, accurate and real-time identification of driver distraction has become essential. However, existing methods struggle to capture both global contextual and fine-grained local features while contending with noisy labels in training datasets. To address these challenges, we propose DSDFormer, a novel framework that integrates the strengths of Transformer and Mamba architectures through a Dual State Domain Attention (DSDA) mechanism, enabling a balance between long-range dependencies and detailed feature extraction for robust driver behavior recognition. Additionally, we introduce Temporal Reasoning Confident Learning (TRCL), an unsupervised approach that refines noisy labels by leveraging spatiotemporal correlations in video sequences. Our model achieves state-of-the-art performance on the AUC-V1, AUC-V2, and 100-Driver datasets and demonstrates real-time processing efficiency on the NVIDIA Jetson AGX Orin platform. Extensive experimental results confirm that DSDFormer and TRCL significantly improve both the accuracy and robustness of driver distraction detection, offering a scalable solution to enhance road safety.

Translated Abstract:
운전자의 주의 산만은 교통사고의 주요 원인으로, 전 세계적으로 도로 안전에 큰 위협이 되고 있어. 지능형 교통 시스템이 발전하면서 운전자의 주의 산만을 정확하고 실시간으로 파악하는 게 점점 더 중요해졌어. 그런데 기존 방법들은 전체적인 맥락과 세부적인 지역적 특징을 잘 잡아내지 못하고, 훈련 데이터셋의 노이즈가 있는 레이블 때문에 어려움을 겪고 있어.

이런 문제를 해결하기 위해 우리는 DSDFormer라는 새로운 프레임워크를 제안해. 이건 Transformer와 Mamba 아키텍처의 장점을 통합한 것으로, 이중 상태 도메인 주의(DSDA) 메커니즘을 통해 긴 거리 의존성과 세부 피처 추출 간의 균형을 맞춰서 강력한 운전 행동 인식을 가능하게 해.

또한, 우리는 영상 시퀀스의 시공간 상관관계를 활용해 노이즈가 있는 레이블을 정제하는 비지도 학습 방법인 Temporal Reasoning Confident Learning (TRCL)을 소개해. 우리의 모델은 AUC-V1, AUC-V2, 100-Driver 데이터셋에서 최첨단 성능을 보여주고, NVIDIA Jetson AGX Orin 플랫폼에서 실시간 처리 효율성도 입증했어. 

광범위한 실험 결과는 DSDFormer와 TRCL이 운전자의 주의 산만 탐지의 정확성과 강건성을 크게 향상시킨다는 걸 확인해주고, 도로 안전을 높이기 위한 확장 가능한 솔루션을 제공해.

================================================================================

URL:
https://arxiv.org/pdf/2409.06002.pdf

Title: Enhanced Generative Data Augmentation for Semantic Segmentation via Stronger Guidance

Original Abstract:
Data augmentation is a widely used technique for creating training data for tasks that require labeled data, such as semantic segmentation. This method benefits pixel-wise annotation tasks requiring much effort and intensive labor. Traditional data augmentation methods involve simple transformations like rotations and flips to create new images from existing ones. However, these new images may lack diversity along the main semantic axes in the data and not change high-level semantic properties. To address this issue, generative models have emerged as an effective solution for augmenting data by generating synthetic images. Controllable generative models offer a way to augment data for semantic segmentation tasks using a prompt and visual reference from the original image. However, using these models directly presents challenges, such as creating an effective prompt and visual reference to generate a synthetic image that accurately reflects the content and structure of the original. In this work, we introduce an effective data augmentation method for semantic segmentation using the Controllable Diffusion Model. Our proposed method includes efficient prompt generation using Class-Prompt Appending and Visual Prior Combination to enhance attention to labeled classes in real images. These techniques allow us to generate images that accurately depict segmented classes in the real image. In addition, we employ the class balancing algorithm to ensure efficiency when merging the synthetic and original images to generate balanced data for the training dataset. We evaluated our method on the PASCAL VOC datasets and found it highly effective for synthesizing images in semantic segmentation.

Translated Abstract:
데이터 증강은 레이블이 필요한 작업, 특히 의미 분할 같은 작업을 위해 훈련 데이터를 만드는 데 많이 사용되는 기법이야. 이 방법은 픽셀 단위로 주석을 달아야 하는 작업에서 많은 노력과 노동을 요구하는데 도움이 돼. 

전통적인 데이터 증강 방법은 회전이나 뒤집기 같은 간단한 변환을 통해 기존 이미지에서 새로운 이미지를 만드는 방식이야. 하지만 이렇게 만든 새로운 이미지들은 데이터의 주요 의미 축에서 다양성이 부족할 수 있고, 고수준의 의미 속성은 변화하지 않아. 이 문제를 해결하기 위해 생성 모델이 등장했어. 생성 모델은 합성 이미지를 생성함으로써 데이터를 증강하는 효과적인 해결책이야.

조절 가능한 생성 모델은 원본 이미지의 프롬프트와 시각적 참조를 사용해 의미 분할 작업을 위한 데이터 증강 방법을 제공해. 하지만 이러한 모델을 직접 사용하는 건 어려움이 있어. 예를 들어, 원본의 내용과 구조를 정확하게 반영하는 합성 이미지를 만들기 위한 효과적인 프롬프트와 시각적 참조를 만드는 게 문제야.

이번 연구에서는 조절 가능한 확산 모델을 사용해 의미 분할을 위한 효과적인 데이터 증강 방법을 소개할 거야. 우리가 제안한 방법은 클래스 프롬프트 추가(Class-Prompt Appending)와 시각적 우선 조합(Visual Prior Combination)을 이용해 실제 이미지에서 레이블이 붙은 클래스에 대한 주의를 높이는 효율적인 프롬프트 생성을 포함해. 이런 기술 덕분에 실제 이미지에서 세분화된 클래스를 정확하게 보여주는 이미지를 생성할 수 있어.

또한, 합성 이미지와 원본 이미지를 결합할 때 균형 잡힌 데이터를 생성하기 위해 클래스 균형 알고리즘을 사용해. 우리는 PASCAL VOC 데이터셋에서 이 방법을 평가해봤고, 의미 분할에 있어 이미지를 합성하는 데 매우 효과적이라는 결과를 얻었어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06259.pdf

Title: ALSS-YOLO: An Adaptive Lightweight Channel Split and Shuffling Network for TIR Wildlife Detection in UAV Imagery

Original Abstract:
Unmanned aerial vehicles (UAVs) equipped with thermal infrared (TIR) cameras play a crucial role in combating nocturnal wildlife poaching. However, TIR images often face challenges such as jitter, and wildlife overlap, necessitating UAVs to possess the capability to identify blurred and overlapping small targets. Current traditional lightweight networks deployed on UAVs struggle to extract features from blurry small targets. To address this issue, we developed ALSS-YOLO, an efficient and lightweight detector optimized for TIR aerial images. Firstly, we propose a novel Adaptive Lightweight Channel Split and Shuffling (ALSS) module. This module employs an adaptive channel split strategy to optimize feature extraction and integrates a channel shuffling mechanism to enhance information exchange between channels. This improves the extraction of blurry features, crucial for handling jitter-induced blur and overlapping targets. Secondly, we developed a Lightweight Coordinate Attention (LCA) module that employs adaptive pooling and grouped convolution to integrate feature information across dimensions. This module ensures lightweight operation while maintaining high detection precision and robustness against jitter and target overlap. Additionally, we developed a single-channel focus module to aggregate the width and height information of each channel into four-dimensional channel fusion, which improves the feature representation efficiency of infrared images. Finally, we modify the localization loss function to emphasize the loss value associated with small objects to improve localization accuracy. Extensive experiments on the BIRDSAI and ISOD TIR UAV wildlife datasets show that ALSS-YOLO achieves state-of-the-art performance, Our code is openly available at this https URL.

Translated Abstract:
무인 항공기(UAV)에 장착된 열 적외선(TIR) 카메라는 야행성 야생 동물 밀렵 방지에 중요한 역할을 해. 하지만 TIR 이미지에서는 흔들림이나 야생 동물의 겹침 같은 문제가 자주 발생해서, UAV가 흐릿하고 겹쳐진 작은 목표를 식별할 수 있는 능력이 필요해. 현재 UAV에 배치된 기존의 가벼운 네트워크는 흐릿한 작은 목표에서 특징을 추출하는 데 어려움을 겪고 있어.

이 문제를 해결하기 위해 우리는 ALSS-YOLO라는 효율적이고 가벼운 탐지기를 개발했어. 먼저, 새로운 적응형 경량 채널 분할 및 셔플링(ALSS) 모듈을 제안해. 이 모듈은 적응형 채널 분할 전략을 사용해서 특징 추출을 최적화하고, 채널 간 정보 교환을 강화하기 위해 채널 셔플링 메커니즘을 통합해. 이렇게 하면 흔들림으로 인한 흐림과 겹치는 목표를 처리하는 데 중요한 흐릿한 특징 추출이 개선돼.

두 번째로, 우리는 경량 좌표 주의(LCA) 모듈을 개발했어. 이 모듈은 적응형 풀링과 그룹화된 합성을 사용해서 차원 간 특징 정보를 통합해. 이 모듈은 가벼운 작동을 보장하면서도 높은 탐지 정확도와 흔들림 및 목표 겹침에 대한 강건성을 유지해.

또한, 우리는 각 채널의 너비와 높이 정보를 네 차원 채널 융합으로 집계하는 단일 채널 집중 모듈도 개발했어. 이게 열화상 이미지의 특징 표현 효율성을 높여줘. 마지막으로, 우리는 작은 객체와 관련된 손실 값을 강조하기 위해 위치 손실 함수도 수정해, 위치 정확도를 개선했어.

BIRDSAI와 ISOD TIR UAV 야생 동물 데이터셋에서의 광범위한 실험 결과, ALSS-YOLO가 최첨단 성능을 달성했어. 우리의 코드는 이 링크에서 공개되어 있어.

================================================================================

URL:
https://arxiv.org/pdf/2401.13751.pdf

Title: A Training Rate and Survival Heuristic for Inference and Robustness Evaluation (TRASHFIRE)

Original Abstract:
Machine learning models -- deep neural networks in particular -- have performed remarkably well on benchmark datasets across a wide variety of domains. However, the ease of finding adversarial counter-examples remains a persistent problem when training times are measured in hours or days and the time needed to find a successful adversarial counter-example is measured in seconds. Much work has gone into generating and defending against these adversarial counter-examples, however the relative costs of attacks and defences are rarely discussed. Additionally, machine learning research is almost entirely guided by test/train metrics, but these would require billions of samples to meet industry standards. The present work addresses the problem of understanding and predicting how particular model hyper-parameters influence the performance of a model in the presence of an adversary. The proposed approach uses survival models, worst-case examples, and a cost-aware analysis to precisely and accurately reject a particular model change during routine model training procedures rather than relying on real-world deployment, expensive formal verification methods, or accurate simulations of very complicated systems (\textit{e.g.}, digitally recreating every part of a car or a plane). Through an evaluation of many pre-processing techniques, adversarial counter-examples, and neural network configurations, the conclusion is that deeper models do offer marginal gains in survival times compared to more shallow counterparts. However, we show that those gains are driven more by the model inference time than inherent robustness properties. Using the proposed methodology, we show that ResNet is hopelessly insecure against even the simplest of white box attacks.

Translated Abstract:
머신러닝 모델, 특히 딥 뉴럴 네트워크는 다양한 분야의 벤치마크 데이터셋에서 매우 뛰어난 성능을 보여줬어. 하지만 훈련 시간이 몇 시간 또는 며칠 걸리는 반면, 성공적인 적대적 예제를 찾는 데 걸리는 시간은 몇 초에 불과하기 때문에, 적대적 예제를 찾는 게 여전히 큰 문제야. 많은 연구가 이 적대적 예제를 생성하고 방어하는 데 쏟아졌지만, 공격과 방어의 상대적인 비용에 대한 논의는 거의 없어. 게다가 머신러닝 연구는 대부분 테스트/훈련 지표에 의해 진행되는데, 이 지표를 충족하려면 수십억 개의 샘플이 필요해.

이번 연구는 특정 모델의 하이퍼파라미터가 적대자가 존재할 때 모델 성능에 어떤 영향을 미치는지를 이해하고 예측하는 문제를 다뤄. 제안된 접근법은 생존 모델, 최악의 사례, 비용을 고려한 분석을 사용해서 실제 배포나 비싼 공식 검증 방법, 복잡한 시스템의 정확한 시뮬레이션에 의존하지 않고 일상적인 모델 훈련 절차 중에 특정 모델 변경을 정확하고 정밀하게 거부할 수 있어. 

여러 전처리 기법, 적대적 예제, 신경망 구성에 대한 평가를 통해, 더 깊은 모델이 더 얕은 모델에 비해 생존 시간에서 근소한 이점을 제공한다는 결론을 내렸어. 하지만 이 이점은 본질적인 강인성보다 모델 추론 시간에 더 많이 영향을 받는다는 걸 보여줘. 제안된 방법론을 통해, ResNet이 가장 단순한 화이트 박스 공격조차도 방어하기에 절망적으로 불안정하다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2403.11694.pdf

Title: Object Segmentation-Assisted Inter Prediction for Versatile Video Coding

Original Abstract:
In modern video coding standards, block-based inter prediction is widely adopted, which brings high compression efficiency. However, in natural videos, there are usually multiple moving objects of arbitrary shapes, resulting in complex motion fields that are difficult to represent compactly. This problem has been tackled by more flexible block partitioning methods in the Versatile Video Coding (VVC) standard, but the more flexible partitions require more overhead bits to signal and still cannot be made arbitrarily shaped. To address this limitation, we propose an object segmentation-assisted inter prediction method (SAIP), where objects in the reference frames are segmented by some advanced technologies. With a proper indication, the object segmentation mask is translated from the reference frame to the current frame as the arbitrary-shaped partition of different regions without any extra signal. Using the segmentation mask, motion compensation is separately performed for different regions, achieving higher prediction accuracy. The segmentation mask is further used to code the motion vectors of different regions more efficiently. Moreover, the segmentation mask is considered in the joint rate-distortion optimization for motion estimation and partition estimation to derive the motion vector of different regions and partition more accurately. The proposed method is implemented into the VVC reference software, VTM version 12.0. Experimental results show that the proposed method achieves up to 1.98%, 1.14%, 0.79%, and on average 0.82%, 0.49%, 0.37% BD-rate reduction for common test sequences, under the Low-delay P, Low-delay B, and Random Access configurations, respectively.

Translated Abstract:
현대 비디오 코딩 표준에서는 블록 기반의 인터 예측 방식이 널리 사용되는데, 이 덕분에 높은 압축 효율을 얻을 수 있어. 하지만 자연 비디오에서는 여러 개의 움직이는 물체가 다양한 형태로 존재해서 복잡한 움직임 필드가 생기고, 이걸 간단하게 표현하기가 어려워. 이 문제는 VVC(다목적 비디오 코딩) 표준에서 더 유연한 블록 분할 방법으로 해결하려고 했지만, 더 유연한 분할은 더 많은 비트를 신호로 전달해야 하고, 원하는 형태로 만들기엔 한계가 있어.

이런 한계를 극복하기 위해, 우리는 객체 분할을 도와주는 인터 예측 방법(SAIP)을 제안해. 이 방법은 참조 프레임에 있는 객체를 고급 기술로 분할하는 거야. 적절한 표시가 있으면, 객체 분할 마스크가 참조 프레임에서 현재 프레임으로 변환되어 다양한 영역의 임의 형태로 분할돼. 이 분할 마스크를 사용해서 각기 다른 영역에 대해 별도로 움직임 보상을 수행할 수 있어, 그래서 예측 정확도가 더 높아져. 분할 마스크는 또 다른 영역의 움직임 벡터를 더 효율적으로 코딩하는 데도 사용돼.

게다가, 이 분할 마스크는 움직임 추정과 분할 추정을 위한 공동 비율-왜곡 최적화에도 고려돼서, 다양한 영역의 움직임 벡터와 분할을 더 정확하게 도출할 수 있어. 이 방법은 VVC 참조 소프트웨어인 VTM 12.0에 구현됐고, 실험 결과는 제안된 방법이 일반 테스트 시퀀스에서 저지연 P, 저지연 B, 랜덤 접근 설정 하에 각각 최대 1.98%, 1.14%, 0.79%, 평균적으로 0.82%, 0.49%, 0.37% BD-rate 감소를 달성했다는 걸 보여줘.

================================================================================

URL:
https://arxiv.org/pdf/2405.09586.pdf

Title: Factual Serialization Enhancement: A Key Innovation for Chest X-ray Report Generation

Original Abstract:
A radiology report comprises presentation-style vocabulary, which ensures clarity and organization, and factual vocabulary, which provides accurate and objective descriptions based on observable findings. While manually writing these reports is time-consuming and labor-intensive, automatic report generation offers a promising alternative. A critical step in this process is to align radiographs with their corresponding reports. However, existing methods often rely on complete reports for alignment, overlooking the impact of presentation-style vocabulary. To address this issue, we propose FSE, a two-stage Factual Serialization Enhancement method. In Stage 1, we introduce factuality-guided contrastive learning for visual representation by maximizing the semantic correspondence between radiographs and corresponding factual descriptions. In Stage 2, we present evidence-driven report generation that enhances diagnostic accuracy by integrating insights from similar historical cases structured as factual serialization. Experiments on MIMIC-CXR and IU X-ray datasets across specific and general scenarios demonstrate that FSE outperforms state-of-the-art approaches in both natural language generation and clinical efficacy metrics. Ablation studies further emphasize the positive effects of factual serialization in Stage 1 and Stage 2. The code is available at this https URL.

Translated Abstract:
방사선 보고서는 명확성과 조직을 보장하는 발표 스타일의 어휘와 관찰 가능한 발견에 기반한 정확하고 객관적인 설명을 제공하는 사실적 어휘로 구성되어 있어. 수동으로 이러한 보고서를 작성하는 건 시간과 노력이 많이 드는 작업이야. 그래서 자동 보고서 생성을 대안으로 제안해. 

이 과정에서 중요한 단계는 방사선 사진과 그에 해당하는 보고서를 일치시키는 거야. 그런데 기존 방법은 일치시키기 위해 완전한 보고서에 의존하는 경우가 많고, 발표 스타일의 어휘가 미치는 영향을 무시해. 이 문제를 해결하기 위해 우리는 FSE라는 두 단계의 사실 직렬화 강화 방법을 제안해. 

1단계에서는 사실성을 기반으로 한 대조 학습을 통해 방사선 사진과 그에 대한 사실적 설명 간의 의미적 일치를 극대화해. 2단계에서는 유사한 과거 사례에서 통찰력을 통합하여 진단 정확도를 높이는 증거 기반 보고서 생성을 제시해. 

MIMIC-CXR와 IU X-ray 데이터셋을 활용한 실험 결과, FSE가 자연어 생성과 임상 효능 지표 모두에서 최신 기술보다 우수한 성능을 보여줬어. 또한, 단계 1과 단계 2에서 사실 직렬화의 긍정적인 효과를 강조하는 연구도 진행했어. 코드는 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2406.11548.pdf

Title: AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation

Original Abstract:
The ability to reflect on and correct failures is crucial for robotic systems to interact stably with real-life objects.Observing the generalization and reasoning capabilities of Multimodal Large Language Models (MLLMs), previous approaches have aimed to utilize these models to enhance robotic systems accordingly.However, these methods typically focus on high-level planning corrections using an additional MLLM, with limited utilization of failed samples to correct low-level contact poses. To address this gap, we propose an Autonomous Interactive Correction (AIC) MLLM, which makes use of previous low-level interaction experiences to correct SE(3) pose predictions. Specifically, AIC MLLM is initially fine-tuned to acquire both pose prediction and feedback prompt comprehension abilities.We carefully design two types of prompt instructions through interactions with objects: 1) visual masks to highlight unmovable parts for position correction, and 2)textual descriptions to indicate potential directions for rotation correction.During inference, a Feedback Information Extraction module is introduced to recognize the failure cause, allowing AIC MLLM to adaptively correct the pose prediction using the corresponding prompts. To further enhance manipulation stability, we devise a Test Time Adaptation strategy that enables AIC MLLM to better adapt to the current scene configuration.Finally, extensive experiments are conducted in both simulated and real-world environments to evaluate the proposed method. The results demonstrate that our AIC MLLM can efficiently correct failure samples by leveraging interaction experience prompts.Real-world demonstration can be found at this https URL

Translated Abstract:
로봇 시스템이 실제 물체와 안정적으로 상호작용하려면 실패를 반성하고 수정하는 능력이 중요해. 멀티모달 대형 언어 모델(MLLM)의 일반화 및 추론 능력을 관찰한 이전 연구들은 이 모델들을 활용해서 로봇 시스템을 개선하려고 했어. 

하지만 이 방법들은 보통 추가적인 MLLM을 사용해 고수준 계획 수정에 초점을 맞추고, 실패한 샘플을 활용해 저수준 접촉 자세를 수정하는 데는 한계가 있었어. 이 문제를 해결하기 위해 우리는 자율 상호작용 수정(AIC) MLLM을 제안해. 이 모델은 이전의 저수준 상호작용 경험을 활용해서 SE(3) 자세 예측을 수정해.

특히 AIC MLLM은 처음에 자세 예측과 피드백 프롬프트 이해 능력을 갖추기 위해 미세 조정돼. 우리는 물체와의 상호작용을 통해 두 가지 유형의 프롬프트 지침을 신중하게 설계했어: 1) 위치 수정을 위한 고정된 부분을 강조하는 시각적 마스크, 2) 회전 수정을 위한 잠재적 방향을 나타내는 텍스트 설명이야. 

추론 중에는 피드백 정보 추출 모듈이 도입돼서 실패 원인을 인식하고, AIC MLLM이 해당 프롬프트를 사용해 자세 예측을 적절하게 수정할 수 있도록 해. 조작의 안정성을 더욱 높이기 위해, 우리는 AIC MLLM이 현재 장면 구성에 더 잘 적응할 수 있도록 하는 테스트 타임 적응 전략을 고안했어.

마지막으로, 제안된 방법을 평가하기 위해 시뮬레이션 환경과 실제 환경 모두에서 광범위한 실험을 진행했어. 결과적으로 우리의 AIC MLLM은 상호작용 경험 프롬프트를 활용해서 실패 샘플을 효율적으로 수정할 수 있다는 걸 보여줬어. 실제 환경에서의 시연은 이 URL에서 확인할 수 있어.

================================================================================

URL:
https://arxiv.org/pdf/2406.18977.pdf

Title: RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulation

Original Abstract:
Utilizing Vision-Language Models (VLMs) for robotic manipulation represents a novel paradigm, aiming to enhance the model's ability to generalize to new objects and instructions. However, due to variations in camera specifications and mounting positions, existing methods exhibit significant performance disparities across different robotic platforms. To address this challenge, we propose RoboUniView in this paper, an innovative approach that decouples visual feature extraction from action learning. We first learn a unified view representation from multi-perspective views by pre-training on readily accessible data, and then derive actions from this unified view representation to control robotic manipulation. This unified view representation more accurately mirrors the physical world and is not constrained by the robotic platform's camera parameters. Thanks to this methodology, we achieve state-of-the-art performance on the demanding CALVIN benchmark, enhancing the success rate in the $D \to D$ setting from 93.0% to 96.2%, and in the $ABC \to D$ setting from 92.2% to 94.2%. Moreover, our model exhibits outstanding adaptability and flexibility: it maintains high performance under unseen camera parameters, can utilize multiple datasets with varying camera parameters, and is capable of joint cross-task learning across datasets. Code is provided for re-implementation. this https URL

Translated Abstract:
로봇 조작에 비전-언어 모델(VLMs)을 사용하는 것은 새로운 접근 방식으로, 모델이 새로운 물체와 지시에 잘 적응하도록 만드는 목표를 가지고 있어. 하지만 카메라 사양이나 장착 위치가 다르면, 기존 방법들은 다양한 로봇 플랫폼에서 성능 차이가 커지는 문제가 있어. 

이 문제를 해결하기 위해, 우리는 이 논문에서 RoboUniView라는 혁신적인 접근 방식을 제안해. 이 방법은 시각적 특징 추출을 행동 학습과 분리해. 먼저, 여러 관점에서의 시각적 정보를 통합하는 방법을 배우고, 쉽게 구할 수 있는 데이터로 사전 학습을 해. 그 다음, 이 통합된 시각적 표현을 사용해서 로봇 조작을 위한 행동을 유도해. 이 통합된 시각적 표현은 실제 세계를 더 정확하게 반영하고, 로봇 플랫폼의 카메라 파라미터에 제약받지 않아.

이 방법 덕분에 우리는 CALVIN 벤치마크에서 최첨단 성능을 달성했어. $D \to D$ 설정에서는 성공률이 93.0%에서 96.2%로, $ABC \to D$ 설정에서는 92.2%에서 94.2%로 향상되었어. 게다가, 우리 모델은 적응력과 유연성이 뛰어나: 보지 못한 카메라 파라미터에서도 높은 성능을 유지하고, 서로 다른 카메라 파라미터를 가진 여러 데이터셋을 활용할 수 있으며, 데이터셋 간의 공동 학습도 가능해. 코드도 재구현을 위해 제공해. 이 링크를 확인해.

================================================================================

URL:
https://arxiv.org/pdf/2407.05941.pdf

Title: Pruning One More Token is Enough: Leveraging Latency-Workload Non-Linearities for Vision Transformers on the Edge

Original Abstract:
This paper investigates how to efficiently deploy vision transformers on edge devices for small workloads. Recent methods reduce the latency of transformer neural networks by removing or merging tokens, with small accuracy degradation. However, these methods are not designed with edge device deployment in mind: they do not leverage information about the latency-workload trends to improve efficiency. We address this shortcoming in our work. First, we identify factors that affect ViT latency-workload relationships. Second, we determine token pruning schedule by leveraging non-linear latency-workload relationships. Third, we demonstrate a training-free, token pruning method utilizing this schedule. We show other methods may increase latency by 2-30%, while we reduce latency by 9-26%. For similar latency (within 5.2% or 7ms) across devices we achieve 78.6%-84.5% ImageNet1K accuracy, while the state-of-the-art, Token Merging, achieves 45.8%-85.4%.

Translated Abstract:
이 논문은 엣지 디바이스에서 작은 작업량을 위해 비전 트랜스포머를 효율적으로 배포하는 방법을 조사해. 최근 방법들은 트랜스포머 신경망의 지연 시간을 줄이기 위해 토큰을 제거하거나 병합하는 방식을 사용하지만, 정확도가 조금 떨어지는 문제도 있어. 하지만 이런 방법들은 엣지 디바이스에 맞춰 설계되지 않았어. 그래서 지연 시간과 작업량의 관계를 활용해서 효율성을 높이지 못해.

우리는 이 문제를 해결하기 위해 세 가지를 해. 첫째, ViT의 지연 시간과 작업량 관계에 영향을 미치는 요소들을 찾아. 둘째, 비선형 지연 시간-작업량 관계를 활용해 토큰 프루닝 일정을 정해. 셋째, 이 일정을 이용해 훈련 없이 토큰을 프루닝하는 방법을 보여줘. 

우리가 제안한 방법은 다른 방법들이 지연 시간을 2-30% 증가시키는 반면, 우리는 지연 시간을 9-26% 줄였어. 비슷한 지연 시간(5.2% 또는 7ms 이내)에서, 우리는 78.6%-84.5%의 ImageNet1K 정확도를 달성했어. 반면, 최신 기술인 토큰 병합(Token Merging)은 45.8%-85.4%의 정확도밖에 안 돼.

================================================================================

URL:
https://arxiv.org/pdf/2407.11698.pdf

Title: NITRO-D: Native Integer-only Training of Deep Convolutional Neural Networks

Original Abstract:
Quantization has become increasingly pivotal in addressing the steadily increasing computational and memory requirements of Deep Neural Networks (DNNs). By reducing the number of bits used to represent weights and activations (typically from 32-bit floating-point to 16-bit or 8-bit integers), quantization reduces the memory footprint, energy consumption, and execution time of DNN models. However, traditional quantization methods typically focus on the inference of DNNs, while the training process still relies on floating-point operations. To date, only one work in the literature has addressed integer-only training for Multi-Layer Perceptron (MLP) architectures. This work introduces NITRO-D, a new framework for training arbitrarily deep integer-only Convolutional Neural Networks (CNNs) that operate entirely in the integer-only domain for both training and inference. NITRO-D is the first framework in the literature enabling the training of integer-only CNNs without the need to introduce a quantization scheme. Specifically, NITRO-D introduces a novel architecture integrating multiple integer local-loss blocks, which include the proposed NITRO Scaling Layer and the NITRO-ReLU activation function. Additionally, it introduces a novel integer-only learning algorithm derived from Local Error Signals (LES), utilizing IntegerSGD, an optimizer specifically designed to operate in an integer-only context. NITRO-D is implemented in an open-source Python library. Extensive experimental evaluations demonstrate its effectiveness across several state-of-the-art image recognition datasets. Results show significant performance improvements from 2.47% to 5.96% for integer-only MLP architectures over the state-of-the-art solution, and the capability of training integer-only CNN architectures with minimal accuracy degradation from -0.15% to -4.22% compared to floating-point LES.

Translated Abstract:
양자화는 깊은 신경망(DNN)의 계산 및 메모리 요구 사항이 점점 더 증가함에 따라 점점 더 중요해지고 있어. 양자화는 가중치와 활성화를 표현하는 데 사용하는 비트 수를 줄여서(보통 32비트 부동소수점에서 16비트 또는 8비트 정수로) DNN 모델의 메모리 사용량, 에너지 소비 및 실행 시간을 줄여줘.

하지만 전통적인 양자화 방법은 보통 DNN의 추론에 초점을 맞추고 있고, 학습 과정은 여전히 부동소수점 연산에 의존하고 있어. 지금까지 문헌에서는 다층 퍼셉트론(MLP) 아키텍처를 위한 정수 전용 학습을 다룬 연구는 하나뿐이야. 이 연구는 NITRO-D라는 새로운 프레임워크를 소개하는데, 이는 학습과 추론 모두에서 정수 전용으로 작동하는 임의의 깊이의 합성곱 신경망(CNN)을 훈련할 수 있게 해.

NITRO-D는 양자화 방식을 도입하지 않고도 정수 전용 CNN을 훈련할 수 있는 문헌에서 첫 번째 프레임워크야. 구체적으로, NITRO-D는 제안된 NITRO 스케일링 레이어와 NITRO-ReLU 활성화 함수를 포함한 여러 개의 정수 국소 손실 블록을 통합한 새로운 아키텍처를 도입해. 또한, 정수 전용 컨텍스트에서 작동하도록 특별히 설계된 최적화기인 IntegerSGD를 활용한 국소 오류 신호(LES)에서 파생된 새로운 정수 전용 학습 알고리즘도 소개해.

NITRO-D는 오픈소스 파이썬 라이브러리로 구현되어 있어. 다양한 최첨단 이미지 인식 데이터셋에서의 효과성을 입증하기 위해 광범위한 실험 평가가 이루어졌어. 결과적으로, 정수 전용 MLP 아키텍처는 최첨단 솔루션에 비해 2.47%에서 5.96%까지 성능이 향상되었고, 부동소수점 LES와 비교할 때 정수 전용 CNN 아키텍처를 훈련할 수 있는 능력은 -0.15%에서 -4.22%까지의 최소한의 정확도 저하로 보여졌어.

================================================================================

URL:
https://arxiv.org/pdf/2408.12093.pdf

Title: LLM-enhanced Scene Graph Learning for Household Rearrangement

Original Abstract:
The household rearrangement task involves spotting misplaced objects in a scene and accommodate them with proper places. It depends both on common-sense knowledge on the objective side and human user preference on the subjective side. In achieving such task, we propose to mine object functionality with user preference alignment directly from the scene itself, without relying on human intervention. To do so, we work with scene graph representation and propose LLM-enhanced scene graph learning which transforms the input scene graph into an affordance-enhanced graph (AEG) with information-enhanced nodes and newly discovered edges (relations). In AEG, the nodes corresponding to the receptacle objects are augmented with context-induced affordance which encodes what kind of carriable objects can be placed on it. New edges are discovered with newly discovered non-local relations. With AEG, we perform task planning for scene rearrangement by detecting misplaced carriables and determining a proper placement for each of them. We test our method by implementing a tiding robot in simulator and perform evaluation on a new benchmark we build. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on misplacement detection and the following rearrangement planning.

Translated Abstract:
가정용 물건 재배치 작업은 장면에서 잘못 놓인 물건을 찾아서 적절한 위치에 배치하는 걸 말해. 이 작업은 물체의 일반적인 기능에 대한 지식과 인간 사용자의 선호도에 따라 달라져. 

우리는 사람의 개입 없이 장면 자체에서 객체 기능과 사용자 선호를 분석하는 방법을 제안해. 이를 위해 장면 그래프 표현을 사용하고, LLM으로 강화된 장면 그래프 학습을 제안해. 이 방법은 입력된 장면 그래프를 정보를 강화한 노드와 새롭게 발견된 엣지(관계)를 가진 어포던스 강화 그래프(AEG)로 변환해.

AEG에서는 용기 객체에 해당하는 노드가 맥락에 의해 유도된 어포던스로 보강돼서 어떤 종류의 운반할 수 있는 물체를 그 위에 놓을 수 있는지를 인코딩해. 그리고 새롭게 발견된 비국소 관계로 새로운 엣지를 찾아내.

AEG를 사용해서 잘못 놓인 운반 가능한 물체를 감지하고 각각의 적절한 위치를 찾으면서 장면 재배치를 위한 작업 계획을 수행해. 우리는 이 방법을 시뮬레이터에서 정리 로봇을 구현해서 테스트하고, 우리가 만든 새로운 벤치마크에서 평가를 진행했어. 

폭넓은 평가 결과, 우리의 방법이 잘못된 물체 감지와 이후의 재배치 계획에서 최첨단 성능을 달성한다는 걸 보여줬어.

================================================================================

URL:
https://arxiv.org/pdf/2409.02241.pdf

Title: What Makes a Face Look like a Hat: Decoupling Low-level and High-level Visual Properties with Image Triplets

Original Abstract:
In visual decision making, high-level features, such as object categories, have a strong influence on choice. However, the impact of low-level features on behavior is less understood partly due to the high correlation between high- and low-level features in the stimuli presented (e.g., objects of the same category are more likely to share low-level features). To disentangle these effects, we propose a method that de-correlates low- and high-level visual properties in a novel set of stimuli. Our method uses two Convolutional Neural Networks (CNNs) as candidate models of the ventral visual stream: the CORnet-S that has high neural predictivity in high-level, IT-like responses and the VGG-16 that has high neural predictivity in low-level responses. Triplets (root, image1, image2) of stimuli are parametrized by the level of low- and high-level similarity of images extracted from the different layers. These stimuli are then used in a decision-making task where participants are tasked to choose the most similar-to-the-root image. We found that different networks show differing abilities to predict the effects of low-versus-high-level similarity: while CORnet-S outperforms VGG-16 in explaining human choices based on high-level similarity, VGG-16 outperforms CORnet-S in explaining human choices based on low-level similarity. Using Brain-Score, we observed that the behavioral prediction abilities of different layers of these networks qualitatively corresponded to their ability to explain neural activity at different levels of the visual hierarchy. In summary, our algorithm for stimulus set generation enables the study of how different representations in the visual stream affect high-level cognitive behaviors.

Translated Abstract:
시각적 의사결정에서 객체 카테고리 같은 고급 특징이 선택에 큰 영향을 미쳐. 하지만 저급 특징이 행동에 미치는 영향은 잘 이해되지 않고 있어. 그 이유 중 하나는 제시된 자극에서 고급 특징과 저급 특징 간의 상관관계가 높기 때문이야. (예: 같은 카테고리의 객체들은 저급 특징을 공유할 가능성이 더 높아.)

이런 영향을 분리하기 위해, 우리는 저급과 고급 시각 속성을 분리할 수 있는 새로운 자극 세트를 제안해. 우리 방법은 두 개의 합성곱 신경망(CNN)을 사용해: CORnet-S는 고급 IT와 같은 반응에서 높은 신경 예측력을 가지고 있고, VGG-16은 저급 반응에서 높은 신경 예측력을 가져.

세 개의 자극(루트, 이미지1, 이미지2) 쌍은 각기 다른 레이어에서 추출된 이미지의 저급 및 고급 유사성 수준에 따라 매개변수화돼. 이 자극들은 참가자들이 루트 이미지와 가장 유사한 이미지를 선택하는 의사결정 과제에서 사용돼. 우리는 서로 다른 네트워크들이 저급 대 고급 유사성의 효과를 예측하는 능력이 다르다는 걸 발견했어. CORnet-S는 고급 유사성에 기반한 인간 선택을 설명하는 데 더 뛰어나고, 반면 VGG-16은 저급 유사성에 기반한 선택을 설명하는 데 더 잘해.

Brain-Score를 사용해서, 이 네트워크의 서로 다른 레이어의 행동 예측 능력이 시각 계층의 서로 다른 수준에서 신경 활동을 설명하는 능력과 질적으로 일치하는 걸 관찰했어. 요약하자면, 우리의 자극 세트 생성 알고리즘은 시각 스트림의 다양한 표현이 고급 인지 행동에 어떤 영향을 미치는지 연구할 수 있게 해.

================================================================================

URL:
https://arxiv.org/pdf/2409.04631.pdf

Title: Zero-Shot Whole Slide Image Retrieval in Histopathology Using Embeddings of Foundation Models

Original Abstract:
We have tested recently published foundation models for histopathology for image retrieval. We report macro average of F1 score for top-1 retrieval, majority of top-3 retrievals, and majority of top-5 retrievals. We perform zero-shot retrievals, i.e., we do not alter embeddings and we do not train any classifier. As test data, we used diagnostic slides of TCGA, The Cancer Genome Atlas, consisting of 23 organs and 117 cancer subtypes. As a search platform we used Yottixel that enabled us to perform WSI search using patches. Achieved F1 scores show low performance, e.g., for top-5 retrievals, 27% +/- 13% (Yottixel-DenseNet), 42% +/- 14% (Yottixel-UNI), 40%+/-13% (Yottixel-Virchow), 41%+/-13% (Yottixel-GigaPath), and 41%+/-14% (GigaPath WSI).

Translated Abstract:
우리는 최근에 발표된 기초 모델을 사용해서 조직병리학 이미지를 검색해봤어. 여기서 우리가 측정한 것들은 top-1 검색의 F1 점수, top-3 검색의 대다수, 그리고 top-5 검색의 대다수야.

우리는 제로샷 검색을 수행했어. 즉, 임베딩을 변경하지 않고, 어떤 분류기도 훈련하지 않았다는 거지. 테스트 데이터로는 TCGA, 즉 암 유전체 아틀라스의 진단 슬라이드를 사용했어. 이 데이터는 23개의 장기와 117개의 암 하위 유형으로 구성되어 있어.

검색 플랫폼으로는 패치를 사용해 WSI 검색을 할 수 있는 Yottixel을 썼어. 우리가 얻은 F1 점수는 성능이 별로 좋지 않았어. 예를 들어, top-5 검색의 경우 각각 27% +/- 13% (Yottixel-DenseNet), 42% +/- 14% (Yottixel-UNI), 40% +/- 13% (Yottixel-Virchow), 41% +/- 13% (Yottixel-GigaPath), 그리고 41% +/- 14% (GigaPath WSI)였어.

================================================================================

URL:
https://arxiv.org/pdf/2409.06716.pdf

Title: Detailed delineation of the fetal brain in diffusion MRI via multi-task learning

Original Abstract:
Diffusion-weighted MRI is increasingly used to study the normal and abnormal development of fetal brain in-utero. Recent studies have shown that dMRI can offer invaluable insights into the neurodevelopmental processes in the fetal stage. However, because of the low data quality and rapid brain development, reliable analysis of fetal dMRI data requires dedicated computational methods that are currently unavailable. The lack of automated methods for fast, accurate, and reproducible data analysis has seriously limited our ability to tap the potential of fetal brain dMRI for medical and scientific applications. In this work, we developed and validated a unified computational framework to (1) segment the brain tissue into white matter, cortical/subcortical gray matter, and cerebrospinal fluid, (2) segment 31 distinct white matter tracts, and (3) parcellate the brain's cortex and delineate the deep gray nuclei and white matter structures into 96 anatomically meaningful regions. We utilized a set of manual, semi-automatic, and automatic approaches to annotate 97 fetal brains. Using these labels, we developed and validated a multi-task deep learning method to perform the three computations. Our evaluations show that the new method can accurately carry out all three tasks, achieving a mean Dice similarity coefficient of 0.865 on tissue segmentation, 0.825 on white matter tract segmentation, and 0.819 on parcellation. The proposed method can greatly advance the field of fetal neuroimaging as it can lead to substantial improvements in fetal brain tractography, tract-specific analysis, and structural connectivity assessment.

Translated Abstract:
확산 강조 MRI는 태아 뇌의 정상 및 비정상 발달을 연구하는 데 점점 더 많이 사용되고 있어. 최근 연구에 따르면, dMRI는 태아 단계의 신경 발달 과정에 대한 귀중한 통찰을 제공할 수 있대. 하지만 데이터 품질이 낮고 뇌 발달이 빠르기 때문에, 태아 dMRI 데이터를 신뢰성 있게 분석하려면 현재로선 없는 전용 컴퓨터 방법이 필요해.

자동화된 방법이 없어서 빠르고 정확하며 재현 가능한 데이터 분석이 어렵고, 이로 인해 태아 뇌 dMRI의 의료 및 과학적 응용 가능성을 제대로 활용하지 못하고 있어. 이 연구에서는 (1) 뇌 조직을 백질, 피질/피질 하 회색질, 그리고 뇌척수액으로 나누고, (2) 31개의 독특한 백질 경로를 나누고, (3) 뇌의 피질을 구획화하고 깊은 회색핵과 백질 구조를 96개의 해부학적으로 의미 있는 영역으로 나누는 통합 컴퓨터 프레임워크를 개발하고 검증했어.

97개의 태아 뇌를 주석 달기 위해 수동, 반자동, 자동 접근 방식을 사용했어. 이 레이블을 바탕으로 세 가지 계산을 수행할 수 있는 다중 작업 딥러닝 방법을 개발하고 검증했지. 평가 결과, 새로운 방법이 세 가지 작업 모두를 정확하게 수행할 수 있다는 걸 보여줬어. 조직 분할에서 평균 다이스 유사도 계수가 0.865, 백질 경로 분할에서 0.825, 구획화에서 0.819를 기록했어.

제안된 방법은 태아 신경영상 분야를 크게 발전시킬 수 있을 거야. 이 방법이 태아 뇌 경로 추적, 경로별 분석, 그리고 구조적 연결성 평가에서 상당한 개선을 이끌어낼 수 있을 것 같아.

================================================================================

URL:
https://arxiv.org/pdf/2409.07236.pdf

Title: 3DGCQA: A Quality Assessment Database for 3D AI-Generated Contents

Original Abstract:
Although 3D generated content (3DGC) offers advantages in reducing production costs and accelerating design timelines, its quality often falls short when compared to 3D professionally generated content. Common quality issues frequently affect 3DGC, highlighting the importance of timely and effective quality assessment. Such evaluations not only ensure a higher standard of 3DGCs for end-users but also provide critical insights for advancing generative technologies. To address existing gaps in this domain, this paper introduces a novel 3DGC quality assessment dataset, 3DGCQA, built using 7 representative Text-to-3D generation methods. During the dataset's construction, 50 fixed prompts are utilized to generate contents across all methods, resulting in the creation of 313 textured meshes that constitute the 3DGCQA dataset. The visualization intuitively reveals the presence of 6 common distortion categories in the generated 3DGCs. To further explore the quality of the 3DGCs, subjective quality assessment is conducted by evaluators, whose ratings reveal significant variation in quality across different generation methods. Additionally, several objective quality assessment algorithms are tested on the 3DGCQA dataset. The results expose limitations in the performance of existing algorithms and underscore the need for developing more specialized quality assessment methods. To provide a valuable resource for future research and development in 3D content generation and quality assessment, the dataset has been open-sourced in this https URL.

Translated Abstract:
3D 생성 콘텐츠(3DGC)는 제작 비용을 줄이고 디자인 일정을 빠르게 할 수 있는 장점이 있지만, 전문적으로 제작된 3D 콘텐츠와 비교했을 때 품질이 떨어지는 경우가 많아. 이런 3DGC는 흔히 품질 문제가 발생하니까, 적시에 효과적인 품질 평가가 중요해. 이런 평가를 통해 최종 사용자에게 더 높은 품질의 3DGC를 제공하고, 생성 기술 발전에 필요한 중요한 통찰력을 얻을 수 있어.

이 논문에서는 이 분야의 기존 문제를 해결하기 위해 새로운 3DGC 품질 평가 데이터셋인 3DGCQA를 소개해. 이 데이터셋은 7가지 대표적인 텍스트-투-3D 생성 방법을 사용해서 만들어졌어. 데이터셋을 만드는 과정에서 50개의 고정된 프롬프트를 활용해 모든 방법으로 콘텐츠를 생성했고, 이로 인해 313개의 텍스처가 있는 메시가 생성되어 3DGCQA 데이터셋을 구성하게 됐어.

시각화를 통해 생성된 3DGC에서 6가지 일반적인 왜곡 카테고리가 존재하는 걸 쉽게 볼 수 있어. 3DGC의 품질을 더 알아보기 위해 평가자들이 주관적인 품질 평가를 진행했는데, 그 결과 생성 방법에 따라 품질이 크게 다르다는 걸 보여줬어. 게다가, 여러 가지 객관적인 품질 평가 알고리즘도 3DGCQA 데이터셋에서 테스트했어. 결과적으로 기존 알고리즘의 성능에 한계가 있다는 걸 알게 됐고, 더 전문화된 품질 평가 방법을 개발해야 한다는 필요성을 강조했어.

앞으로 3D 콘텐츠 생성과 품질 평가에 유용한 자원이 될 수 있도록 데이터셋은 오픈 소스로 제공돼.

================================================================================

URL:
https://arxiv.org/pdf/2409.07253.pdf

Title: Alignment of Diffusion Models: Fundamentals, Challenges, and Future

Original Abstract:
Diffusion models have emerged as the leading paradigm in generative modeling, excelling in various applications. Despite their success, these models often misalign with human intentions, generating outputs that may not match text prompts or possess desired properties. Inspired by the success of alignment in tuning large language models, recent studies have investigated aligning diffusion models with human expectations and preferences. This work mainly reviews alignment of diffusion models, covering advancements in fundamentals of alignment, alignment techniques of diffusion models, preference benchmarks, and evaluation for diffusion models. Moreover, we discuss key perspectives on current challenges and promising future directions on solving the remaining challenges in alignment of diffusion models. To the best of our knowledge, our work is the first comprehensive review paper for researchers and engineers to comprehend, practice, and research alignment of diffusion models.

Translated Abstract:
확산 모델은 생성 모델링에서 가장 중요한 패러다임으로 떠올랐고, 다양한 응용 분야에서 뛰어난 성과를 보여주고 있어. 하지만 이런 모델들은 인간의 의도와 잘 맞지 않는 경우가 많아서, 텍스트 프롬프트와 일치하지 않거나 원하는 특성을 가진 결과물을 생성하지 못하는 경우도 있어. 

그래서 최근 연구에서는 대형 언어 모델의 조정에서 성공한 경험을 바탕으로, 확산 모델을 인간의 기대와 선호에 맞추는 방법을 조사했어. 이 연구는 확산 모델의 정렬(alignment) 관련 내용을 주로 다루고, 정렬의 기본 개념, 확산 모델의 정렬 기법, 선호 기준, 그리고 확산 모델의 평가에 대한 발전 상황을 다뤄. 

또한, 현재의 문제점과 확산 모델 정렬의 남은 도전 과제를 해결하기 위한 유망한 미래 방향에 대한 주요 관점도 이야기할 거야. 우리가 아는 바로는, 이 연구가 확산 모델의 정렬을 이해하고 실천하며 연구하는 데 있어 처음으로 포괄적인 리뷰 논문이야.

================================================================================

